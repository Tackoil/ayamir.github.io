<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Ayamir&#39;s Blog</title>
        <link>https://ayamir.github.io/</link>
        <description>Welcome to Ayamir&#39;s blog.</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miracle_l@bupt.edu.cn (Ayamir)</managingEditor>
            <webMaster>miracle_l@bupt.edu.cn (Ayamir)</webMaster><lastBuildDate>Sat, 11 Dec 2021 16:14:15 &#43;0800</lastBuildDate>
            <atom:link href="https://ayamir.github.io/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>Note for RainbowDQN and Multitype Tiles</title>
    <link>https://ayamir.github.io/2021/12/note-for-rainbowdqn-tiles/</link>
    <pubDate>Sat, 11 Dec 2021 16:14:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-rainbowdqn-tiles/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Level：IEEE Transaction on multimedia 21</p>
<p>Keyword：Rainbow-DQN, Multi-type tiles, Full streaming system</p>
<h2 id="问题形式化">问题形式化</h2>
<h3 id="模型">模型</h3>
<ol>
<li>
<p>原始视频用网格划分成$N$块tile，每个tile都被转码成$M$个不同的质量等级$q_i$。</p>
</li>
<li>
<p>基于传输控制模块得出的结果，播放器请求$t_i$个tile的$q_i$质量的版本并将其存储在缓冲区中，对应的缓冲区大小为$l_i$。</p>
</li>
<li>
<p>用户Viewport的信息用$V$表示，可以确定FOV的中心。</p>
</li>
<li>
<p>根据$V$可以将tile划分成3种类型：FOV、OOS、Base。</p>
</li>
<li>
<p>FOV中的tile被分配更高的码率；</p>
<p>OOS按照与$V$的距离逐步降低质量等级$q_i$；</p>
<p>Base总是使用低质量等级$q_{Base}$但使用完整的分辨率。</p>
</li>
<li>
<p>传输的tile在同步完成之后交给渲染器渲染。</p>
</li>
<li>
<p>播放器根据各项指标计算可以评估播放性能：</p>
<p>$&lt;V, B, Q, F, E&gt;$：viewport信息$V$，网络带宽$B$，FOV质量$Q$，重缓冲频率$F$，传输效率$E$。</p>
</li>
<li>
<p>传输控制模块用于确定每个tile的质量等级$q_i$和缓冲区大小$l_i$。</p>
</li>
<li>
<p>传输控制模块优化的最终目标是获取最大的性能：
$$
performance = E_{max},\ QoE \in accept\ range
$$</p>
</li>
</ol>
<h3 id="带宽评估">带宽评估</h3>
<ol>
<li>
<p>收集每个tile的下载日志来评估带宽。</p>
</li>
<li>
<p>使用<a href="https://zhuanlan.zhihu.com/p/32335746" target="_blank" rel="noopener noreffer">指数加权移动平均算法EWMA</a>使评估结果光滑，来应对网络波动。</p>
</li>
<li>
<p>第$t$次评估结果使用$B_t$表示，用下式计算：
$$
B_t = \beta B_{t-1} + (1-\beta)b_t
$$
$b_t$是B的第$t$次测量值；$\beta$是EWMA的加权系数。</p>
</li>
<li>
<p>$t=0$时，$B_0$被初始化为0；所以在初始的$t$比较小的时候，$B_t$与理想值相比就很小。</p>
<p>这种影响会随着$t$增大而减少。</p>
</li>
<li>
<p>为了优化启动过程，对公式做出修改：
$$
B_t = \frac{\beta B_{t-1} + (1-\beta)b_t}{1 - \beta^t}
$$
$t$较小的时候，分母会放大$B_t$；$t$较大时，分母趋近于1，影响随之消失。</p>
</li>
</ol>
<h3 id="fov表示和预测">FOV表示和预测</h3>
<ol>
<li></li>
</ol>
<h2 id="系统架构">系统架构</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>将原始视频转码为有不同比特率的多个版本。</li>
<li>转码后的视频被划分成多个tile。</li>
<li>传输协议使用MPEG-DASH。</li>
</ol>
<h3 id="客户端">客户端</h3>
<h4 id="评估器">评估器</h4>
<ul>
<li>任务：获取 QoE、FOV预测、传输效率、网络带宽</li>
<li>组成：
<ul>
<li>QoE评估器：评估当前FOV质量=&gt;Q和重缓冲频率=&gt;F（近似为Q+F=QoE）</li>
<li>FOV预测器：基于历史FOV信息预测短期未来的FOV=&gt;P</li>
<li>根据下载和播放日志：计算传输效率=&gt;E并估计带宽=&gt;B</li>
</ul>
</li>
</ul>
<h4 id="控制器">控制器</h4>
<ul>
<li>任务：控制传输过程中的推流</li>
<li>目标：保证QoE在可接受的范围之内、最大化传输效率</li>
<li>详细：基于FOV预测将tile划分成3种类型：FOV、OOS、Base</li>
<li>输入：Q、F、E、B（QoE+传输效率和带宽）</li>
<li>过程：Rainbow-DQN</li>
<li>输出：决定每个tile流的码率和缓冲区大小（作为下载器的输入）</li>
</ul>
<h4 id="下载器">下载器</h4>
<ul>
<li>输入：tile码率和缓冲区大小</li>
<li>过程：基于HTTP/2进行并行下载</li>
<li>输出：下载好的tile</li>
</ul>
<h4 id="视频缓冲区">视频缓冲区</h4>
<ul>
<li>任务：解码、同步、存储下载好的tile等待渲染器消耗，大小供控制器调节</li>
<li>随着FOV的切换缓冲区内容可能被循环利用</li>
</ul>
<h4 id="全景渲染器">全景渲染器</h4>
<ul>
<li>任务：将同步好的tile拼接，tile质量：FOV&gt;OOS&gt;Base</li>
<li>投影方式：ERP</li>
</ul>
<h2 id="控制器-1">控制器</h2>
<h3 id="控制过程">控制过程</h3>
<ol>
<li>
<p>设定QoE的可接受范围。</p>
</li>
<li>
<p>将网络带宽和用户FOV设定为外部因素而非环境</p>
<p>为什么：因为这两个因素变化太快，在面对不同传输条件时，直接作为环境会导致决策过程的不稳定性并且难以收敛。</p>
</li>
<li>
<p>最优化的对象只是最大化累积的传输效率。</p>
<p>为什么：简单</p>
</li>
</ol>
<h3 id="tile聚合和决策">tile聚合和决策</h3>
<ol>
<li>
<p>tile分类原则：</p>
<ul>
<li>
<p>控制器无需为每个tile独立决定码率Q和缓冲区大小L</p>
</li>
<li>
<p>FOV内的tile应该被分配相近的码率，FOV内的tile应该聚集成一组，OSS和Base同理</p>
<p>为什么：避免相邻tile的锐利边界，只考虑3组而非所有tile降低了计算复杂性和决策延迟</p>
<p>（能否实现独立的tile码率计算或更细粒度的划分值得调研？与内容感知的方案结合？）</p>
</li>
</ul>
</li>
<li>
<p>基于距离的tile分类实现方式：</p>
<ul>
<li>
<p>使用评估器预测出的FOV坐标来分类FOV和OOS的tile</p>
</li>
<li>
<p>tile出现在未来FOV的可能性由距离计算</p>
<p>tile中心点坐标$(\omega_i, \mu_i)$、FOV坐标$(\hat{U}, \hat{V})$</p>
<p>距离的变化区间内存在一个临界点，临界点之内的划分为FOV，之外的划分为OOS</p>
<ul>
<li>
<p>度量距离的方式：
$$
\Delta Dis_U = min{|\omega_i - \hat{U}|, |1+\omega_i - \hat{U}|}
$$</p>
<p>（这里为何不直接使用$|\omega_i - \hat{U}|$？）
$$
Dis_i =
\begin{cases}
{\sqrt{({\Delta Dis_{U}})^2 + {(\mu_i - \hat{V})}^2},\  \frac{R}{H} \le \hat{V} \le 1 - \frac{R}{H}}
\
{\Delta Dis_U + |\mu_i - \hat{V}|,\ Others}
\end{cases}
$$</p>
</li>
<li>
<p>因为ERP的投影方式会在两级需要更多的tile，因此使用一个矩形来代表两极的FOV</p>
<p>（可以深入调研ERP在两极处的处理方式）</p>
</li>
<li>
<p>$Dis_i$使用曼哈顿距离来测量。临界点初始化为$2\cdot R$，并随着FOV中心和两极的垂直距离增长。</p>
</li>
<li>
<p>FOV看作是半径为R的圆，使用欧式距离测量。临界点初始化为$R$</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>聚合tile的决策</p>
<ul>
<li>使用2个变量：$K$作为FOV和非FOV的tile的带宽分配比率；$Len$作为tile缓冲区的大小。
<ul>
<li>
<p>$K$确定之后，分配给FOV内tile的带宽被均匀分配（可否非均匀分配）</p>
<p>$K$不直接与网络状况相关因此可以保持控制的稳定性</p>
</li>
<li>
<p>$Len$：所有传输的tile的缓冲区长度$l_i$都被设为$Len$  （文中并没有这样做的原因解释）</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="基于drl的传输控制算法">基于DRL的传输控制算法</h3>
<p>相关术语解释：<a href="https://www.jianshu.com/p/1dfd84cd2e69" target="_blank" rel="noopener noreffer">Rainbow DQN</a>、<a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e" target="_blank" rel="noopener noreffer">RL Dictionary</a>、<a href="https://zhuanlan.zhihu.com/p/38358183" target="_blank" rel="noopener noreffer">PER</a>、<a href="https://zhuanlan.zhihu.com/p/34747205" target="_blank" rel="noopener noreffer">TD-Error</a></p>
<ol>
<li>
<p>控制过程</p>
<ol>
<li>
<p>首先调整buffer长度Len，并划分FOV与非FOV的带宽分配。</p>
</li>
<li>
<p>等viewport预测完成之后，tile被分类为属于FOV和OOS的tile。</p>
</li>
<li>
<p>FOV的带宽被平均分给其中每一个tile并决定FOV内tile的质量等级$q_i$。</p>
<p>非FOV的带宽按照与FOV的距离分配，每超过一个距离单位$Dis_i$就降低一级质量$q_i$。</p>
</li>
<li>
<p>最终的输出是请求序列，每个请求序列中包括质量等级$q_i$和预期的缓冲区大小$l_i$。</p>
</li>
<li>
<p>根据输出做出调整之后，接收奖励反馈并不断完成自身更新。</p>
</li>
</ol>
</li>
<li>
<p>状态设计</p>
<p>状态设计为5元组：$&lt;K, Len, Q, F, E&gt;$（传输控制参数$K$，$Len$、QoE指标：FOV质量Q和重缓冲频率$F$、传输效率$E$）</p>
<p>没有直接使用带宽$B$和viewport轨迹$V$，因为：</p>
<ol>
<li>随机性强与变化幅度较大带来的不稳定性（如何定义随机性强弱和变化幅度大小？）</li>
<li>希望设计的模型有一定的通用性，可以与不同的网络情况和用户轨迹相兼容</li>
</ol>
</li>
<li>
<p>动作设计</p>
<p>两种动作：调整$K$和$Len$（两者的连续变化区间被离散化，调整的每一步分别用$\Delta k$和$\Delta l$表示）</p>
<p>调整的方式被形式化为二元组：$&lt;n_1, n_2&gt;$，$n_1$和$n_2$分别用于表示$K$和$Len$的调整</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">-n</th>
<th style="text-align:center">0</th>
<th style="text-align:center">n</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">K</td>
<td style="text-align:center">减少n$\Delta k$</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">增加n$\Delta k$</td>
</tr>
<tr>
<td style="text-align:center">Len</td>
<td style="text-align:center">减少n$\Delta l$</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">增加n$\Delta l$</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>奖励函数</p>
<p>因为QoE的各项指标权重难以确定，没有使用传统的基于加权的方式。</p>
<p>设定了<strong>能接受的QoE范围</strong>和<strong>在此基础上最大化的传输效率</strong>作为最后的<strong>性能</strong>指标，形式化之后如下：
$$
Reward =
\begin{cases}
-INF,\ F \ge F^{max}
\
-INF,\ E \le E^{min}
\
E,\ Others
\end{cases}
$$</p>
<p>$-INF$意味着终止当前episode；动作越能使系统满足高QoE的同时高效运行，得分越高；</p>
<p>为了最大化传输效率，使用$E$作为奖励回报。</p>
<p>FOV质量$Q$并没有参与到奖励函数中，因为：<strong>高Q意味着高性能，但是低Q不一定意味着低性能</strong>，详细解释如下：</p>
<ul>
<li>在带宽不足的情况下，低Q可能已经是这种条件下的满足性能的最好选择。</li>
<li>高传输效率意味着传输了更多的FOV数据，也能满足高FOV质量的目标。</li>
</ul>
</li>
<li>
<p>模型设计
基于Rainbow-DQN模型：</p>
<ul>
<li>
<p>输入是5元组$&lt;K, Len, Q, F, E&gt;$。</p>
</li>
<li>
<p>神经网络使用64维的3隐层模型。</p>
</li>
<li>
<p>为了提高鲁棒性，神经网络的第3层使用Dueling DQN的方式，将Q值$Q(s, a)$分解为状态价值$V(s)$和优势函数$A(s,a)$：
$$
Q(s, a) = V(s) + A(s, a)
$$</p>
<p>$V(s)$表示系统处于状态$s$时的性能；$A(s,a)$表示系统处于状态$s$时动作$a$带来的性能；</p>
</li>
<li>
<p>为了避免价值过高估计，使用Double DQN的方式，设计了两个独立的神经网络：评估网络和目标网络。</p>
<p>评估网络用于动作选择；目标网络是评估网络从最后一个episode的拷贝用于动作评估。</p>
</li>
<li>
<p>为了缓解神经网络的不稳定性（更快收敛），使用大小为$v$的回放池来按照时间序列保存客户端的经验。</p>
<p>因为网络带宽和FOV轨迹在短期内存在特定的规律性，回放池中有相似状态和相似采样时间的样本更加重要，出现了优先级</p>
<p>所以使用优先经验回放PER，而优先级使用时间查分误差TD-error定义
$$
\delta_i = r_{i+1} + \gamma Q(s_{i+1}, arg\underset{a}{max}Q(s_{i+1}, a; \theta_i); \theta_i^{'}) - Q(s_i, a_i; \theta_i)
$$</p>
<p>$r_i$是奖励；$\gamma$是折扣因子</p>
</li>
<li>
<p>损失函数使用均方误差定义
$$
J = \frac{1}{v} \sum_{i=1}^{v} \omega_i(\delta_i)^2
$$</p>
<p>$\omega_i$是回放缓冲中第i个样本的重要性采样权重</p>
</li>
</ul>
</li>
</ol>
<h2 id="实验验证">实验验证</h2>
<ol>
<li>
<p>环境设定</p>
<ul>
<li>
<p>传输控制模块：基于<a href="https://tensorforce.readthedocs.io/en/latest/" target="_blank" rel="noopener noreffer">TensorForce</a>（配置教程：<a href="https://zhuanlan.zhihu.com/p/60241809" target="_blank" rel="noopener noreffer">用TensorForce快速搭建深度强化学习模型</a>）；</p>
<p>开发工具集：<a href="https://gym.openai.com/" target="_blank" rel="noopener noreffer">OpenAI Gym</a></p>
</li>
<li>
<p>数据来源：使用全景视频播放设备收集，加入高斯噪声来产生更多数据。</p>
</li>
</ul>
</li>
<li>
<p>结果分析</p>
<ul>
<li>
<p>与其他DQN算法的对比——DQN、Double DQN、Dueling DQN</p>
<ul>
<li>
<p>对比训练过程中每个episode中的最大累计奖励：$MAX_{reward}$</p>
</li>
<li>
<p>对比模型收敛所需要的最少episode：$MIN_{episode}$</p>
<p>相同的带宽和FOV轨迹</p>
</li>
</ul>
</li>
<li>
<p>与其他策略对比性能——高QoE和高传输效率</p>
<ul>
<li>
<p>随机控制策略：随机确定K和Len</p>
</li>
<li>
<p>固定分配策略：固定K和Len的值</p>
</li>
<li>
<p>只预测Viewport策略：使用LSTM做预测，不存在OSS与Base，所有带宽都用于FOV</p>
<p>带宽和FOV轨迹的均值和方差相等</p>
</li>
</ul>
</li>
<li>
<p>与其他全景视频推流系统的对比</p>
<ul>
<li>
<p>DashEntire360：使用Dash直接传送完整的360度视频，使用线性回归来估计带宽并动态调整视频比特率</p>
</li>
<li>
<p>360ProbDash：在DashEntire360的基础上划分tile基于Dash传输，使用可能性模型为tile分配比特率</p>
</li>
<li>
<p>DRL360：使用DRL来优化多项QoE指标</p>
<p>实现三种系统、使用随机网络带宽和FOV轨迹。</p>
<p>使用DRL360中提出的方式测量QoE：
$$
V_{QoE} = \eta_1 Q - \eta_2 F - \eta_3 A
$$</p>
<p>$A$是viewport随时间的平均变化，反映FOV质量Q的变化；</p>
<p>$\eta_1, \eta_2, \eta_3$分别是3种QoE指标的非负加权，使用4种加权方式来训练模型并对比：</p>
<p>$&lt;1, 1, 1&gt;$，$&lt;1, 0.25, 0.25&gt;$，$&lt;1, 4, 1&gt;$，$&lt;1,1,4&gt;$</p>
</li>
</ul>
</li>
<li>
<p>在不同环境下的性能评估——带宽是否充足、FOV轨迹是否活跃（4种环境）</p>
</li>
</ul>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for 360ProbDASH</title>
    <link>https://ayamir.github.io/2021/12/note-for-360probdash/</link>
    <pubDate>Thu, 09 Dec 2021 10:20:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-360probdash/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3123266.3123291" target="_blank" rel="noopener noreffer">360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming</a></p>
<p>Level: ACM MM 17</p>
<p>Keyword:</p>
<p>Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation</p>
<h2 id="工作范围与目标">工作范围与目标</h2>
<p>应用层-&gt;基于tile-&gt;viewport预测的可能性模型+预期质量的最大化</p>
<ul>
<li>
<p>针对小buffer提出了<code>target-buffer-based rate control</code>算法来避免重缓冲事件（避免卡顿）</p>
</li>
<li>
<p>提出viewport预测的可能性模型计算tile被看到的可能性（避免边缘效应）</p>
</li>
<li>
<p>形式化QoE-driven优化问题：</p>
<p>在传输率受限的情况下最小化viewport内的质量失真和空间质量变化（获取受限状态下最好的视频质量）</p>
</li>
</ul>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>形式化参数</p>
<p>$M*N$个tile，M指tile序列的序号，N指不同的码率等级</p>
<p>$r_{i, j}$指比特率，$d_{i, j}$指失真，$p_{i}$指被看到的可能性（$\sum_{i=1}^{N}p_{i} = 1$）</p>
<p>$\Phi(X)$指质量失真，$\Psi(X)$指质量变化</p>
</li>
<li>
<p>目标</p>
<p>找到推流段的集合：$X = {x_{i, j}}$，其中${x_{i, j}} = 1$指被第$&lt;i, j&gt;$个tile被选中；$x_{i, j} = 0$则是未选中。
$$
\underset{X}{min}\ \Phi(X) + \eta \cdot \Psi(X) \
s.t. \sum_{i=1}^{N}\sum_{j=1}^{M}x_{i, j}\cdot r_{i, j} \le R, \
\sum_{j=1}^{M}x_{i, j} \le 1, x_{i, j} \in {0, 1}, \forall i.
$$
整个公式即为前所述的问题的形式化表达的公式化结果。</p>
</li>
<li>
<p>模型细节</p>
<ol>
<li>
<p>$\Phi(X)$和$\Psi(X)$的计算=&gt;通过考虑球面到平面的映射</p>
<p>通过计算球面上点的Mean Squared Error来得到S-PSNR进而评估质量：$d_{i, j}$来表示第${&lt;i, j&gt;}$个段的MSE</p>
<p>
$$
\phi_i = \frac{\pi}{2} - h_i \cdot \frac{\pi}{H}, \Delta\phi = \Delta h \cdot \frac{\pi}{H}, \
\theta_i = w_i \cdot \frac{2\pi}{W}, \ \Delta\theta = \Delta w \cdot \frac{2\pi}{W},
$$
$H$和$W$分别指按照ERP格式投影之后的视频高度和宽度</p>
<p>第$i$个tile的空间面积用$s_i$表示：
$$
s_i\ =\ \iint_{\Omega_i}Rd\phi Rcos\phi d\theta \
=\Delta\theta R^2[sin(\phi_i + \Delta\phi) - sin\phi_i],
$$
$R$指球的半径（$R = W/2\pi$），所以整体的球面质量失真$D_{i, j}$可以计算出来：
$$
D_{i, j} = d_{i, j} \cdot s_i,
$$
结合每个tile被看到的概率$p_i$可以得出$\Phi(X)$和$\Psi(X)$
$$
\Phi(X)=\frac{\sum_{i=1}^N\sum_{j=1}^MD_{i, j}\cdot x_{i,j}\cdot p_i}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i},\
\Psi(X) = \frac{\sum_{i=1}^N\sum_{j=1}^Mx_{i, j}\cdot p_i \cdot\ (D_{i,j}-s_{i} \cdot \Phi(X))^2}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i}.
$$</p>
</li>
<li>
<p>Viewport的可能性模型</p>
<ol>
<li>
<p>方向预测=&gt;<strong>线性回归模型</strong></p>
<p>将用户的欧拉角看作是$yaw(\alpha)$，$pitch(\beta)$和$rool(\gamma)$，应用线性回归做预测
$$
\begin{cases}
\hat{\alpha}(t_0 + \delta) = m_{\alpha}\delta+\alpha(t_0),\
\hat{\beta}(t_0 + \delta) = m_{\beta}\delta+\beta(t_0),\
\hat{\gamma}(t_0 + \delta) = m_{\gamma}\delta+\gamma(t_0).
\end{cases}
$$</p>
</li>
<li>
<p>预测错误的分布=&gt;<strong>高斯分布</strong>，根据公式均值和标准差都能从统计信息中计算出来</p>
<p>收集5名志愿者的头部移动轨迹并投影到3个方向上绘制成图，实验结果为预测错误呈现高斯分布（样本数可能不够？）</p>
<p>
$$
\begin{cases}
P_{yaw}(\alpha) = \frac{1}{\sigma_{\alpha}\sqrt{2\pi}}exp{-\frac{[\alpha-(\hat{\alpha}+\mu_{\alpha})]^2}{2\sigma_{\alpha}^2}},\
P_{pitch}(\beta) = \frac{1}{\sigma_{\beta}\sqrt{2\pi}}exp{-\frac{[\beta-(\hat{\beta}+\mu_{\beta})]^2}{2\sigma_{\beta}^2}},\
P_{roll}(\gamma) = \frac{1}{\sigma_{\gamma}\sqrt{2\pi}}exp{-\frac{[\gamma-(\hat{\gamma}+\mu_{\gamma})]^2}{2\sigma_{\gamma}^2}}.
\end{cases}
$$
3个方向各自<strong>独立</strong>，因此最终的预测错误$P_E(\alpha,\beta,\gamma)$可以表示为：
$$
P_E(\alpha, \beta, \gamma) = P_{yaw}(\alpha)P_{pitch}(\beta)P_{roll}(\gamma).
$$</p>
</li>
<li>
<p>球面上点被看到的可能性</p>
<p>球面坐标为$(\phi, \theta)$点的可能性表示为$P_s(\phi, \theta)$</p>
<p>因为一个点可能在多个不同的viewport里面，所以定义按照用户方向从点$(\phi, \theta)$出发能看到的点集$L(\phi, theta)$</p>
<p>因此空间点$s$被看到的可能性可以表示为：
$$
P_s(\phi, \theta) = \frac{1}{|L(\phi, \theta)|}\sum_{(\alpha, \beta, \gamma) \in L(\phi, \theta)}P_E(\alpha, \beta, \gamma),
$$</p>
</li>
<li>
<p>球面上tile被看到的可能性</p>
<p>tile内各个点被看到的可能性的<strong>均值</strong>即为tile被看到的可能性（可否使用其他方式？）
$$
p_i = \frac{1}{|U_i|} \sum_{(\phi, \theta) \in U_i} P_s(\phi, \theta).
$$
$U_i$表示tile内的空间点集</p>
</li>
</ol>
</li>
<li>
<p><code>Target-Buffer-based</code> Rate Control</p>
<p>因为长期的头部移动预测会产生较高的预测错误，所以不能采用大缓冲区（没有cite来证明这一点）</p>
<p></p>
<p>将处于相同时刻的段集合成一个块存储在缓冲区中。</p>
<p>在自适应的第k步，定义$d_k$作为此时的buffer占用情况（等到第k个块被下载完毕）
$$
b_k = b_{k-1} - \frac{R_k \cdot T}{C_k} + T
$$
$C_k$表示平均带宽，$R_k$表示总计的码率</p>
<p>为了避免重新缓冲设定目标buffer占用$B_{target}$，并使buffer占用保持在$B_{target}$（$b_k = B_{target}$）</p>
<p>因此总计的码率需要满足：
$$
R_k = \frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T),
$$
这里的$C_k$表示可以从历史的段下载信息中估计出来的带宽</p>
<p>设定$R$的下界$R_{min}$之后（没有说明为何需要设定下界），公式12可以修正为如下：
$$
R_k = max{\frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T), R_{min}}.
$$</p>
</li>
</ol>
</li>
</ol>
<h2 id="实现">实现</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>
<p>视频裁剪器</p>
<p>将视频帧切割成tile</p>
</li>
<li>
<p>编码器</p>
<p>对tile进行划分并将其编码成多种码率的段</p>
</li>
<li>
<p>MPD产生器</p>
<p>添加<strong>SRD特性</strong>来表示段之间的空间关系</p>
<p>添加经度和<strong>纬度</strong>属性来表示</p>
<p>添加<strong>质量失真</strong>和<strong>尺寸</strong>属性</p>
</li>
<li>
<p>Apache HTTP服务器</p>
<p>存储视频段和mpd文件，向客户端推流</p>
</li>
</ol>
<h3 id="客户端">客户端</h3>
<ol>
<li>
<p>基础：dash.js</p>
</li>
<li>
<p>额外的模块</p>
<ul>
<li>
<p><code>QoE-driver Optimizer</code>
$$
Output = HTTP\ GET请求中的最优段
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Target\ buffer\ based\ Rate\ Controller\
Viewport\ Probabilistic\ Model\
QR\ Map
\end{cases}
$$</p>
</li>
<li>
<p><code>Target-buffer-based Rate Controller</code>
$$
Output = 总计的传输码率，按照公式13计算而来
$$</p>
<p>$$
Input = Output\ of\ {Bandwidth\ Estimation\ module
$$</p>
</li>
<li>
<p><code>Viewport Probabilistic Model</code>
$$
Output = 每个tile被看到的可能性，按照公式10计算而来
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Orientation\ Prediction\ module\
SRD\ information
\end{cases}
$$</p>
</li>
<li>
<p><code>QR Map</code>QR=&gt;Quality-Rate
$$
Output = 所有段的QR映射
$$</p>
<p>$$
Input = MPD中的属性
$$</p>
</li>
<li>
<p><code>Bandwidth Estimation</code>（没有展开研究，因为不是关键？）
$$
Output = 前3秒带宽估计的平均值
$$</p>
<p>$$
Input = 下载段过程中的吞吐量变化
$$</p>
<p>可以通过<code>onProgess()</code>的回调函数<code>XMLHttpRequest API</code>获取</p>
</li>
<li>
<p><code>Orientation Prediction</code>
$$
Output = 用户方向信息的预测结果（yaw, pitch, roll）
$$</p>
<p>$$
Input = Web\ API中获取的DeviceOrientation信息，使用线性回归做预测
$$</p>
</li>
</ul>
</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>整体设定</p>
<ol>
<li>将用户头部移动轨迹编码进播放器来模拟用户头部移动</li>
<li>积极操控网络状况来观察不同方案对网络波动的反应</li>
</ol>
</li>
<li>
<p>详细设定</p>
<ul>
<li>
<p>服务端</p>
<ol>
<li>
<p>视频选择</p>
<p>2880x1440分辨率、时长3分钟、投影格式ERP</p>
</li>
<li>
<p>切分设置</p>
<p>每个块长1s（$T=1$）、每个块被分成6x12个tile（$N=72$）</p>
<p>每个段的码率设置为${20, 50, 100, 200, 300}$，单位kpbs</p>
</li>
<li>
<p>视频编码</p>
<p><a href="http://www.videolan.org/developers/x264.html" target="_blank" rel="noopener noreffer">开源编码器x264</a></p>
</li>
<li>
<p>视频分包</p>
<p><a href="https://gpac.wp.mines-telecom.fr/mp4box/" target="_blank" rel="noopener noreffer">MP4Box</a></p>
</li>
<li>
<p>注意事项</p>
<p>每个段的确切尺寸可能与其码率不同，尤其对于长度较短的块。</p>
<p>为了避免这影响到码率自适应，将段的确切尺寸也写入MPD文件中</p>
</li>
</ol>
</li>
<li>
<p>客户端</p>
<ol>
<li>
<p>缓冲区设定（经过实验得出的参数）</p>
<p>$B_{max}=3s$，$B_{target}=2.5s$，$R_{min}=200kbps$，$权重\eta=0.0015$</p>
</li>
</ol>
</li>
<li>
<p>高斯分布设定</p>
<table>
<thead>
<tr>
<th style="text-align:center">Yaw</th>
<th style="text-align:center">Pitch</th>
<th style="text-align:center">Roll</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\mu_{\alpha}=-0.54,\ \sigma_{\alpha}=7.03$</td>
<td style="text-align:center">$\mu_{\beta}=0.18,\ \sigma_{\beta}=2.55$</td>
<td style="text-align:center">$\mu_{\gamma}=2.16,\ \sigma_{\gamma}=0.15$</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li>
<p>比较对象</p>
<ul>
<li>ERP：原始视频格式</li>
<li>Tile：只请求用户当前viewport的tile，不使用viewport预测，作为baseline</li>
<li>Tile-LR：使用线性回归做预测，每个tile的码率被平均分配</li>
</ul>
</li>
<li>
<p>性能指标</p>
<ul>
<li>卡顿率：卡顿时间占播放总时长的比例</li>
<li>Viewport PSNR：直接反应Viewport内的视频质量</li>
<li>空间质量差异：Viewport内质量的协方差</li>
<li>Viewport偏差：空白区域在Viewport中的比例</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for Dante</title>
    <link>https://ayamir.github.io/2021/12/note-for-dante/</link>
    <pubDate>Wed, 08 Dec 2021 22:14:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-dante/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3232565.3234686" target="_blank" rel="noopener noreffer">https://dl.acm.org/doi/10.1145/3232565.3234686</a></p>
<p>Level: SIGCOMM 18</p>
<p>Keyword: UDP+FOV-aware+FEC</p>
<h2 id="工作范围">工作范围</h2>
<p></p>
<h2 id="目标">目标</h2>
<p>在给定序列的帧中，为<strong>每个tile</strong>设定FEC冗余，根据其被看到的可能性的加权最小化平均质量降低。</p>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>输入
估计的丢包率$p$、发送速率$f$、有$n$个tile的$m$个帧($&lt;i, j&gt;$来表示第$i$个帧的第$j$个tile</p>
<p>第$&lt;i, j&gt;$个tile的大小$v_{i, j}$、第$&lt;i, j&gt;$个tile被看到的可能性$\gamma_{i, j}$、</p>
<p>如果第$&lt;i, j&gt;$ 个tile没有被恢复的质量降低率、最大延迟$T$</p>
</li>
<li>
<p>输出</p>
<p>第$&lt;i, j&gt;$个tile的FEC冗余率$r_{i, j} = \frac{冗余包数量}{原始包数量}$</p>
</li>
<li>
<p>最优化问题的形式化
$$
minimize\  \sum_{0&lt;i\le m}\sum_{0&lt;j\le n} \gamma_{i, j}d_{i, j}(p, r_{i, j})
$$</p>
<p>$$
subject\ \ to\ \  \frac{1}{f}\sum_{0&lt;i\le m}\sum_{0&lt;j\le n}v_{i, j}(1+r_{i, j}) \le T
$$</p>
<p>$$
r_{i, j} \le 0
$$</p>
<p>（1）：最小化最终被看到的tile的质量衰减的加权和，权重按照被看到的可能性分配。</p>
<p>（2）：经过重新编码的包和原始的包需要在T时刻之前发出。</p>
<p>​      Dante将1个GOP(Group of Pictures)中的所有帧当作一批处理，$T$作为GOP的持续时间</p>
<p>​      $f$：使用TCP Friendly Rate Control algorithm，基于估计的丢包率和网络延迟来计算得出</p>
<p>（3）：确保冗余率总是非负的。</p>
</li>
<li>
<p>关键变量是$d_{i, j}(p, r)$：丢包率是p情况下，采用r作为冗余率的第$&lt;i, j&gt;$个tile的质量衰减
$$
d_{i, j}(p, r) = \delta_{i, j},\ if\ r &lt; \frac{1}{1-p}; 0, otherwise.
$$</p>
<p>假设帧中有k个原始包，质量衰减发生在丢失的包不能被恢复的情况下。</p>
<p>FEC可以容忍 $r \cdot k$ 个丢包=&gt;即当 $p(r<em>k+k)$ 大于  $r</em>k$  时会发生质量衰减。</p>
</li>
<li>
<p>过多的丢包会导致依赖链上所有帧的质量衰减，因此考虑帧之间的依赖关系之后，可以重新计算质量衰减：</p>
<p>$$
d^{*}<em>{i, j}(p, r) = \sum</em>{0&lt;c\le i}w_{c, i}d_{c, j}(p, r)
$$</p>
<p>$w_{c, i}$ 编码帧i对帧c的依赖作为单独的第c个帧的质量衰减的权重；</p>
<p>最终第i个帧的第j个tile的最终质量衰减就是所有依赖的质量衰减的和。</p>
</li>
</ol>
<h2 id="fec冗余的自适应逻辑">FEC冗余的自适应逻辑</h2>
<ol>
<li>
<p>关于$d_{i, j}(p, r)$ ：因为是分段函数，所以其值会因为r和p的大小关系而急剧改变。</p>
<p>利用背包问题的思想可以将其规约成NP完全问题：</p>
<p>将每个tile看作是一个物品，共有m*n个。</p>
<p><strong>如果$r_{i, j} &lt; \frac{1}{1-p}$ ，则表示不把第&lt;i,j&gt;和物品放入背包；否则就是将其放入背包。</strong></p>
<p>公式1可以转化为：最大化所有物品二元变量的线性组合；</p>
<p>公式2可以转化为：二元变量的另一个线性组合必须低于阈值约束。</p>
<p>因此整个问题就能被完全转化为<strong>0-1背包</strong>问题</p>
</li>
<li>
<p>算法</p>
<p></p>
<p>整体上是背包问题的标准解法，能以线性复杂度（因为变量只是B)解决问题。</p>
</li>
</ol>
<h2 id="原型设计">原型设计</h2>
<p></p>
<ul>
<li>使用基于TCP和UDP的两条连接来分别传输控制信息（双向：到客户端的播放会话的起至点和到服务端的网络信息反馈）和视频数据包</li>
<li>服务端根据反馈的网络信息，在每个GOP的边界时刻运行算法1来确定下一个GOP的帧和tile的FEC冗余。
确定之后服务端使用RS码来插入冗余包，和原始视频数据包一起重新编码，并使用基于TFRC的发送率发送数据。</li>
<li>Dante的实现是对应用程序级比特率适配策略的补充，并且可以通过对视频播放器进行最小更改来替换现有的底层传输协议来部署。</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>环境：使用Gilbert模型来模拟实现丢包事件（而非使用统一随机丢包）</p>
<p>创造了两种网络条件good（丢包率0.5%）和bad（丢包率2%）</p>
</li>
</ul>
<h2 id="局限性">局限性</h2>
<ul>
<li>效果主要依赖于Viewport预测的结果是否准确</li>
</ul>
]]></description>
</item>
<item>
    <title>沉浸式流媒体传输的实际度量</title>
    <link>https://ayamir.github.io/2021/11/note11/</link>
    <pubDate>Mon, 22 Nov 2021 15:21:59 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note11/</guid>
    <description><![CDATA[<h2 id="度量指标">度量指标</h2>
<ol>
<li>viewport预测精度。
<ul>
<li>使用预测的viewport坐标和实际用户的viewport坐标的大圈距离来量化。</li>
</ul>
</li>
<li>视频质量。
<ul>
<li>viewport内部的tile质量（1～5）。</li>
<li>tile在最高质量层之上花费的时间。</li>
<li>根据用户视线的分布而提出的加权质量度量。</li>
</ul>
</li>
</ol>
<h2 id="度量参数">度量参数</h2>
<ol>
<li>分块策略</li>
<li>带宽</li>
<li>延迟</li>
<li>viewport预测</li>
<li>HTTP版本</li>
<li>持久化的连接数量</li>
</ol>
]]></description>
</item>
<item>
    <title>沉浸式推流中应用层的优化</title>
    <link>https://ayamir.github.io/2021/11/note10/</link>
    <pubDate>Mon, 15 Nov 2021 10:13:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note10/</guid>
    <description><![CDATA[<h2 id="背景">背景</h2>
<p>大多数的HAS方案使用HTTP/1.1协议进行请求-回应的事务来取得需要的资源、缓冲取到的视频段并以线性的顺序播放。传统的HAS中，只需要1个GET请求来取得下一个视频的暂时的部分。只要视频段的持续时间比网络内的时延高，这种方法就可行。</p>
<p>在基于VR的HAS方案中，播放1条视频片段就需要取得多种资源：1次GET请求需要同时请求基础的tile层和每个空间视频tile。使用4x4的tile方案时，客户端需要发起不少于17次GET请求。使用 1 s 数量级的分段持续时间，即使是 20 ms 的微小网络延迟也会显着阻碍客户端和服务器之间的整体吞吐量，因此会导致较低的视频质量。</p>
<h2 id="解决方案">解决方案</h2>
<h3 id="使用多条持久的tcp连接">使用多条持久的TCP连接</h3>
<p>大多数的现代浏览器都支持同时建立并维持多达6条TCP连接来减少页面加载时间，并行地获取请求的资源。这允许增加整体吞吐量，并部分消除网络延迟引入的空闲 RTT 周期。</p>
<p>类似地，基于 VR 的 HAS 客户端可以使用多个 TCP 连接并行下载不同的tile。</p>
<h3 id="使用http2协议的服务端push特性">使用HTTP/2协议的服务端push特性</h3>
<p>HTTP/2协议引入了请求和相应的多路复用、头部压缩和请求优先级的特性，这可以减少页面加载时间。</p>
<p>服务端直接push短视频片段可以减少视频的启动时间和端到端延迟。</p>
<p>并且，服务端push特性可以应用在基于tile的VR视频推流中，客户端可以向服务器同时请求一条视频片段的所有tile。</p>
<p>服务端可以使用特制的请求处理器，允许客户端为每个tile定义一系列质量等级。</p>
<p>因此可以将应用的启发式自适应的速率的决定传达给服务器，这允许客户端以期望的质量级别取得所有图块。</p>
]]></description>
</item>
<item>
    <title>沉浸式流媒体面临的挑战和启示</title>
    <link>https://ayamir.github.io/2021/11/note9/</link>
    <pubDate>Sun, 14 Nov 2021 19:06:10 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note9/</guid>
    <description><![CDATA[<h2 id="最终的目标">最终的目标</h2>
<p>主要的挑战是用户的临场感，这可以通过避免虚拟的线索来创造出接近真实的世界。</p>
<h2 id="具体的任务">具体的任务</h2>
<ol>
<li>
<p>从360度视频的采集到显示的过程中，引入了好几种失真。</p>
<p>应该重点增加新的拼接、投影和分包方式以减少噪音。</p>
</li>
<li>
<p>除了捕获和使用360度视频来表示真实世界和实际交互内容之外，环境中还包括3D对象。</p>
<p>3D对象的合并对于真实的视图而言是一个挑战。</p>
</li>
<li>
<p>因为在推流会话中，用户的头部移动高度可变，所以固定的tiling方案可能会导致非最优的viewport质量。</p>
<p>推流框架中的tile数量应该被动态选择，进而提高推流质量。</p>
</li>
<li>
<p>自适应的机制应该足够智能来根据环境因素精确地做出适应。</p>
<p>应该制定基于深度强化学习的策略，来给360度视频帧中不同区域的tile分配合适的比特率。</p>
</li>
<li>
<p>用户在360度视频中的自由导航很容易让其感觉忧虑自己错过了什么重要的东西。</p>
<p>在360度视频中导航的时候，需要支持自然的可见角度方向。</p>
<p>丰富的环境应配备新颖的定向机制，以支持360度视频，同时降低认知负荷，以克服此问题。</p>
</li>
<li>
<p>真实的导航依赖viewport预测机制。</p>
<p>现代的预测方式应该使用时空图像特性以及用户的位置信息，采用合适的编解码器卷积LSTM结构来减少长期预测误差。</p>
</li>
<li>
<p>沉浸式的场景随着用户的交互应该发生变化。</p>
<p>由于用户与场景的交互而产生的新挑战是通过编码和传输透视图创建的。</p>
<p>因此预测用户的行为来实现对交互内容的高效编码和推流非常关键。</p>
</li>
<li>
<p>对360度视频的质量获取方法和度量手段需要进一步研究。</p>
</li>
<li>
<p>360度视频中特殊的音效需要引起注意。</p>
</li>
</ol>
]]></description>
</item>
<item>
    <title>360度视频的音频处理</title>
    <link>https://ayamir.github.io/2021/11/note8/</link>
    <pubDate>Sun, 14 Nov 2021 16:52:20 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note8/</guid>
    <description><![CDATA[<h2 id="背景">背景</h2>
<p>空间音频是一种全球状空间环绕的声音方式，采用多个声音通道来模拟现实世界中听到的声音。</p>
<p>360度视频由于空间音频而变得更加可靠，因为声音的通道特性使其能够穿越时间和空间。</p>
<p>360度视频显示系统在制作空间音频音轨方面的重要性无论怎样强调都不为过</p>
<h2 id="空间音频的再现技术">空间音频的再现技术</h2>
<h3 id="物理重建">物理重建</h3>
<p>物理重建技术用于合成尽可能接近所需信号的整个声场。</p>
<p>立体声配置在最流行的声音再现方法中使用两个扬声器，以促进更多的空间信息（包括距离、方向感、环境和舞台合奏）。而多信道再现方法在声学环境中使用，并在消费类设备中流行。</p>
<h4 id="多信道再现技术">多信道再现技术</h4>
<p>同样的声压场也通过其他物理重建技术产生，如环境中存在的环境声学和波场合成（WFS）。</p>
<p>需要麦克风阵列来捕获更多的空间声场。</p>
<p>因为不能直接用于声场特性分析，麦克风记录的内容需要后期处理。</p>
<p>麦克风阵列用于语音增强、声源分离、回声消除和声音再现。</p>
<h3 id="感知重建">感知重建</h3>
<p>心理声学技术用于感知重建，以产生对空间声音特征的感知。</p>
<p>感知重建技术复制空间音频的自然听觉感受来表示物理音频。</p>
<h4 id="双耳录制技术">双耳录制技术</h4>
<p>双耳录制技术是立体声录制的一种扩展形式，提供3D的听觉体验。</p>
<p>双耳录制技术通过使用两个360度麦克风尽可能的复制人耳，这与使用定向麦克风捕捉声音的常规立体声录音相同。</p>
<p>假人头部的360度麦克风用作人耳的代理，因为它提供了耳朵的精确几何坐标。</p>
<p>假人头部还产生与人头轮廓相互作用的声波。借助360度麦克风，与任何其他记录方法相比，空间立体图像的捕获更精确。</p>
<h5 id="头部相关传递函数hrtf">头部相关传递函数（HRTF）</h5>
<p>用于双耳音频的实时技术中，以再现复杂的线索，帮助我们通过过滤音频信号来定位声音。</p>
<p>多个因素（如耳朵、头部和听力环境）会影响线索，因为在现实中，我们会重新定位自己以定位声音。</p>
<p>选择合适的录音/重放技术对于使听到的声音与真实场景中的体验相同至关重要。</p>
<h2 id="环境声学">环境声学</h2>
<h3 id="概述">概述</h3>
<p>环境声学也被称为3D音频，被用于记录、混成和播放一个中心点周围的360度音频。</p>
<h3 id="区别">区别</h3>
<p>环境音频和传统的环绕声技术不同。</p>
<ol>
<li>
<p>双声道和传统环绕声技术背后的原理是相同的，都是通过将声音信号送到特定的扬声器来创建音频。</p>
<p>环境音频不受任何特定扬声器的预先限制，因为它在即使音域旋转的情况下，也能创造出平滑的音频。</p>
</li>
<li>
<p>传统环绕声的格式只有在声音场景保持静态的情况下才能提供出色的成像效果。</p>
<p>环境音频提供一个完整的球体，将声音均匀地传播到整个球体。</p>
</li>
</ol>
<h3 id="格式">格式</h3>
<p>环境音频有6种格式，分别为：A、B、C、D、E、G。</p>
<h3 id="用途">用途</h3>
<h4 id="一阶环境音频的用途">一阶环境音频的用途</h4>
<p>第一阶的环境音频或B格式的环境音频，其麦克风用于使用四面体阵列表示线性VR。</p>
<p>此外，这些在四个通道中进行处理，例如提供非定向压力水平的“W”。同时，“X、Y和Z”分别促进了从前到后、从侧到侧以及从上到下的方向信息。</p>
<p>一阶环境音频仅适用于相对较小的场景，因为其有限的空间保真度会影响声音定位。</p>
<h4 id="高阶环境音频的用途">高阶环境音频的用途</h4>
<p>高阶环境音频通过增加更多的麦克风来增强一阶环境音频的性能效率。</p>
<h2 id="总结">总结</h2>
<p></p>
]]></description>
</item>
<item>
    <title>自适应策略之viewport依赖型</title>
    <link>https://ayamir.github.io/2021/11/note7/</link>
    <pubDate>Sun, 14 Nov 2021 13:24:59 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note7/</guid>
    <description><![CDATA[<h2 id="概述">概述</h2>
<p>在360度视频的推流过程中，根据用户头部的运动自适应地动态选择推流的区域，调整其比特率，以达到节省带宽的目的。</p>
<h2 id="通常的实现方式">通常的实现方式</h2>
<p>在服务端提供几个自适应集，来在遇到用户头部的突然运动的情况时，能保证viewport的平滑转换。</p>
<p>提出QER(Quality-focused Regios)的概念使viewport内部的视频分辨率高于viewport之外的视频分辨率。</p>
<p>非对称的方式以不同的空间分辨率推流来节省带宽。</p>
<ul>
<li>在播放过程中，客户端根据用户的方向来请求不同分辨率版本的视频。</li>
<li>优点是即使客户端对用户的方面做了错误预测，低质量的内容仍然可以在viewport中生成。</li>
<li>缺点是在大多数场景下，这种方案需要巨大的存储开销和处理负载。</li>
</ul>
<h2 id="自适应推流参数">自适应推流参数</h2>
<ol>
<li>可用带宽和网络吞吐量</li>
<li>Viewport预测的位置</li>
<li>客户端播放器的可用缓冲</li>
</ol>
<h2 id="参数计算公式">参数计算公式</h2>
<ul>
<li>
<p>第n个估计的Viewport：$V^e(n)$</p>
<p>$V^e(n) = V_{fb}$</p>
<p>$V_{fb}$是最新报告的viewport位置</p>
</li>
<li>
<p>第n个估计的吞吐量：$T^e(n)$</p>
<p>$T^e(n) = T_{fb}$</p>
<p>$T_{fb}$是最新报告的吞吐量</p>
</li>
<li>
<p>比特率：$R_{bits}$</p>
<p>$R_{bits} = (1-\beta)T^e(n)$</p>
<p>$\beta$是安全边缘</p>
</li>
<li>
<p>第n个帧的客观度量质量：$VQ(k)$和最终客观度量质量$VQ$</p>
<p>$VQ=\frac{1}{L}\sum^L_{k=1}VQ(k)$</p>
<p>$VQ(k) = \sum_{t=1}^{T^n}w_k(k) * D^n_t(V_t, k)$</p>
<p>$w_k = \frac{A(t,k)}{A_{vp}}$</p>
<p>$L=总帧数$</p>
<p>$w_k$表示在第k个帧中与viewport所重叠的tile程度</p>
<p>$A(t,k)$表示第k个帧中tile $t$ 重叠的区域</p>
<p>$A_{vp}$表示viewport中总共的区域</p>
</li>
</ul>
]]></description>
</item>
<item>
    <title>沉浸式流媒体现有标准</title>
    <link>https://ayamir.github.io/2021/11/note6/</link>
    <pubDate>Thu, 11 Nov 2021 20:08:03 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note6/</guid>
    <description><![CDATA[<h2 id="omafomnidirectional-media-format">OMAF(Omnidirectional Media Format)</h2>
<p><code>OMAF</code>是第1个国际化的沉浸式媒体格式，描述了对360度视频进行编码、演示、消费的方法。</p>
<p><code>OMAF</code>与与现有格式兼容，包括编码（例如<code>HEVC</code>），文件格式（例如<code>ISOBMFF</code>），交付信号（例如<code>DASH</code>，<code>MMT</code>）。</p>
<p><code>OMAF</code>中还包括编码、投影、分包和viewport方向的元数据。</p>
<h2 id="omafdash-mpd">OMAF+DASH-&gt;MPD</h2>
<p>OMAF与DASH相结合，再加上一些额外的描述构成了MPD文件格式，用于向客户端通知360度媒体的属性。</p>
<p>OMAF规定了9中媒体配置文件，包括3种视频配置文件：基于HEVC的viewport独立型、基于HEVC的viewport依赖型、基于AVC的viewport依赖型。</p>
<p>OMAF为视角独立型的推流提供了无视viewport位置的连续的视频帧质量。</p>
<p>常规的HEVC编码方式和DASH推流格式可以用于viewport独立型的推流工作。</p>
<p>但是使用HEVC/AVC编码方式的基于viewport的自适应操作是OMAF的一项技术开发，允许无限制地使用矩形RWP来增强viewport区域的质量。</p>
<h2 id="cmafcommon-media-application-format">CMAF(Common Media Application Format)</h2>
<p>致力于提供跨多个应用和设备之间的统一的编码格式和媒体配置文件。</p>
<p>CMAF使请求低延迟的segment成为可能。</p>
<h2 id="isobmffiso-base-media-file-format">ISOBMFF(ISO Base Media File Format)</h2>
<p>ISOBMFF是用于定时数据交换、管理和显示的最流行的文件格式。</p>
<ul>
<li>
<p>文件由一系列兼容并且可扩展的文件级别的box组成。</p>
</li>
<li>
<p>每个box表示1个由4个指针字符代码组成的数据结构。</p>
</li>
<li>
<p>ISOBMFF的媒体数据流和元数据流被分别分发。</p>
<ul>
<li>媒体数据流中包括编码过的音频和视频数据。</li>
<li>元数据流中包括媒体类型、编码属性、时间戳、大小等元数据，也包括全向内容的额外信息如投影格式、旋转、帧分包、编码和分发等元数据。</li>
</ul>
</li>
<li>
<p>ISOBMFF为了访问方便，保证有价值信息能灵活聚合。</p>
</li>
</ul>
<h2 id="3dof3-degree-of-freedom">3DoF(3 Degree of Freedom)</h2>
<p>在3DoF场景中，用户可以自由的移动头部以三个方向：摆动、俯仰、旋转。</p>
<h2 id="3dof">3DoF+</h2>
<p>用户的头部可以以任意方向移动：上下、左右、前后</p>
<h2 id="6dof">6DoF</h2>
<p>不只用户的头部，用户的身体也是自由的。同时支持方向与位置的自由。</p>
]]></description>
</item>
<item>
    <title>自适应360度视频推流挑战</title>
    <link>https://ayamir.github.io/2021/11/note5/</link>
    <pubDate>Thu, 04 Nov 2021 11:01:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note5/</guid>
    <description><![CDATA[<h1 id="背景">背景</h1>
<p>用户使用头戴设备比使用传统显示器观看360度视频内容时的满意度对于扰乱更加敏感。</p>
<p>沉浸式的体验受到不完美的视角预测和高度动态化的网络状况的消极影响。</p>
<p>目前主要面临的挑战有以下4个：</p>
<p></p>
<h2 id="viewport预测">Viewport预测</h2>
<h3 id="背景-1">背景</h3>
<p>HMD的本质特征是快速响应用户头部的移动。当用户改变viewport时HMD处理交互并检测相关的viewport来精确播放器的信息，这样视野就能以正常的可视角度被提供给用户。Viewport预测在优化的360度视频推流中非常必要。配备有位置传感器的可穿戴HMD允许客户端更新其视角方向相应的视角场景。</p>
<h3 id="分类">分类</h3>
<ul>
<li><em>内容不可知</em>的方式基于历史信息对viewport进行预测。</li>
<li><em>内容感知</em>的方式需要视频内容信息来预测未来的viewport。</li>
</ul>
<h3 id="内容不可知方式">内容不可知方式</h3>
<h4 id="分类-1">分类</h4>
<ul>
<li>平均线性回归LR</li>
<li>航位推算DR</li>
<li>聚类</li>
<li>机器学习ML</li>
<li>编解码器体系结构</li>
</ul>
<h4 id="现有成果">现有成果</h4>
<h5 id="qians-worklr">Qian&rsquo;s work——LR</h5>
<p>使用平均线性回归和加权线性回归模型来做viewport预测，之后对与预测区域重叠的tile进行整体推流。</p>
<ul>
<li>当预测后0.5s、1s、2s加权线性回归表现更好</li>
</ul>
<h5 id="petrangelis-worklr">Petrangeli&rsquo;s work——LR</h5>
<p>将被划分成tile的等矩形的帧分成3个区域：viewport区、相邻区、其他区。</p>
<p>结合观察者头部的移动，将可变比特率分配给可见和不可见区域。</p>
<p>作者利用最近（100毫秒）用户观看历史的线性外推来预测未来的注视点。</p>
<h5 id="mavlankar-and-girods-work运动向量">Mavlankar and Girod&rsquo;s work——运动向量</h5>
<p>使用运动向量比如观察者的平移、倾斜、缩放等方向上的速度和加速度，来执行视角区域预测。</p>
<h5 id="la-fuentes-work运动向量">La Fuente&rsquo;s work——运动向量</h5>
<p>考虑了两种预测变体：角速度和角加速度，从用户以前的方向数据来估计未来的头部方向。按照预测结果分配不同的量化参数到每个tile上。</p>
<p>当进行进一步的预测时（超过2s），这种方式限制了预测的精度。</p>
<p>如果视频tile被基于错误的预测而被请求，用户的实际viewport可能会被没有请求因而没有内容的黑色tile所覆盖。</p>
<h5 id="bans-workknnlr">Ban&rsquo;s work——KNN+LR</h5>
<p>使用KNN算法利用跨用户观看历史，使用LR模型利用户个体化的行为。</p>
<p>就视角预测的准确率而言，分别取得了20%和48%的绝对和相对改进。</p>
<h5 id="lius-workcluster">Liu&rsquo;s work——cluster</h5>
<p>提出了使用数据融合方法，通过考虑几个特征来估计未来视角位置。特征例如：用户的参与度、用户观看同一视频的行为、单个用户观看多个视频的行为、最终用户设备、移动性水平。</p>
<h5 id="petrangelis-workcluster">Petrangeli&rsquo;s work——cluster</h5>
<p>基于车辆轨迹预测的概念，考虑了类似的轨迹形成一个簇来预测未来的viewport。</p>
<p>结果表明这种方法为更长的视野提高了精确度。</p>
<p>检查了来自三个欧拉角的不同轨迹，这样做可能导致性能不足。</p>
<h5 id="rossis-workcluster">Rossi&rsquo;s work——cluster</h5>
<p>提出了一种聚类的方法，基于球形空间中有意义的viewport重叠来确认用户的簇。</p>
<p>基于Bron-Kerbosch（BK）算法的聚类算法能够识别大量用户，这些用户观看的是相同的60%的3s长球形视频块。</p>
<p>与基准相比，该方法为簇提供了可兼容且重要的几何viewport重叠。</p>
<h5 id="jiangs-work">Jiang&rsquo;s work</h5>
<p>背景：</p>
<p>LR方法对于长期的预测视野会导致较差的预测精度。长短时记忆（LSTM）是一种递归神经网络（RNN）架构，适用于序列建模和模式开发。</p>
<p>方法：</p>
<p>为了在FoV预测中获取比LR方法更高的精确度，开发了一种使用带有128个神经元的LSTM模型的viewport预测方法。</p>
<ul>
<li>分析了360度数据集，观察到用户在水平方向头部有快速转向，但是在垂直方向几乎是稳定的。</li>
<li>实验表明，这种方法同时考虑水平和垂直方向的头部移动时，比LR等方法产生了更少的预测错误。</li>
</ul>
<h5 id="baos-work">Bao&rsquo;s work</h5>
<p>背景：</p>
<p>对150个用户进行了16个视频剪辑的主观实验，并对其行为进行了分析。</p>
<p>使用3个方向的欧拉角$\theta$, $\phi$, $\psi$来表示用户在3D空间中头部的移动，结果表明不同方向的动作有强自相关性和消极的互相关性。因此多个角度的预测可以分开进行。</p>
<p>方法：</p>
<p>开发两个独立的LSTM模型来分别预测$\theta$和$\phi$，之后将预测结果应用于目标区域流来有效利用可用网络资源。</p>
<h5 id="hous-work">Hou&rsquo;s work</h5>
<ul>
<li>提出一种基于深度学习的视角产生方法来只对提前预测的360度视频和3自由度的VR应用的viewport tile进行抽取和推流。（使用了大规模的数据集来训练模型）</li>
<li>使用包含多层感知器和LSTM模型来预测6自由度的VR环境中头部乃至身体的移动，预测的视野被预渲染来做到低延迟的VR体验。</li>
</ul>
<h5 id="heyses-work">Heyse&rsquo;s work</h5>
<p>背景：</p>
<p>在某些例子中，用户的移动在视频的不同部分中非常不稳定。这增加了机器学习方式的训练压力。</p>
<p>方法：</p>
<p>提出了一个基于RL模型的上下文代理，这个模型首先检测用户的显著移动，然后预测移动的方向。这种分层自学习执行器优于球形轨迹外推法（这种方法将用户运动建模为轨迹的一部分，而不是单位球体上的完整轨迹）</p>
<h5 id="qians-work">Qian&rsquo;s work</h5>
<p>提出了一种叫做Flare的算法来最小化实际viewport和预测viewport之间的不匹配。</p>
<ul>
<li>应用了一种ML方法来执行频繁的viewport预测，包括从130名用户收集的1300条头部运动轨迹的4个间隔。</li>
<li>使用viewport轨迹预测，Flare可以将错误预测替换成最新预测。</li>
</ul>
<h5 id="yu-and-lius-work">Yu and Liu&rsquo;s work</h5>
<p>背景：</p>
<p>LSTM网络本身具有耗时的线性训练特性。编解码器的LSTM模型把训练过程并行化，相比于LR和LSTM本身而言，改善了预测精度。</p>
<p>方法：</p>
<p>使用基于注意力的LSTM编解码器网络体系结构来避免昂贵的递归并能更好地捕获viewport变化。</p>
<ul>
<li>提出的体系结构相比于传统的RNN，获得了更高的预测精度，更低的训练复杂度和更快的收敛。</li>
</ul>
<h5 id="jamalis-work">Jamali&rsquo;s work</h5>
<p>提出使用LSTM编解码器网络来做长期的viewport预测（例如3.5s）。</p>
<p>收集了低延迟异质网络上跨用户的方向反馈来调整高延迟网络上目标用户的预测性能。</p>
<h3 id="内容感知方式">内容感知方式</h3>
<h4 id="背景-2">背景</h4>
<p>内容感知方式可以提高预测效率。</p>
<h4 id="具体方法">具体方法</h4>
<h5 id="aladaglis-work">Aladagli&rsquo;s work</h5>
<p>提出了一个显著性驱动的模型来提高预测精度。</p>
<ul>
<li>没有考虑用户在360度视频中的视角行为。</li>
<li>viewport预测错误可以通过理解用户对360度视频独特的可见注意力最小化。</li>
</ul>
<h5 id="nguyens-work">Nguyen&rsquo;s work</h5>
<p>背景：</p>
<p>大多数现存的方法把显著性图看作是360度显示中的位置信息来获得更好的预测结果。</p>
<p>通用的显著性和位置信息体系结构基于固定预测模型。</p>
<p>方法：</p>
<p>提出了<code>PanoSalNet</code>来捕获用户在360度帧中独特的可见注意力来改善显著性检测的性能。</p>
<ul>
<li>同时使用HMD特性和显著性图的固定预测模型获得了可测量的结果。</li>
</ul>
<h5 id="xus-work">Xu&rsquo;s work</h5>
<p>提出了两个DRL(Deep Reinforcement Learning)模型用于同时考虑运动轨迹和可见注意力特性的viewport预测网络。</p>
<ul>
<li>离线模型基于内容流行度检测每个帧里的显著性。</li>
<li>在线模型基于从离线模型获得的显著性图和之前的viewport预测信息预测viewport方向和大小。</li>
<li>这个网络只能预测30ms的下一个viewport位置。</li>
</ul>
<h5 id="xus-work-1">Xu&rsquo;s work</h5>
<p>收集了大规模的被使用带有眼部轨迹跟踪的HMD的45个观测者观察的动态360度视频数据集，提出了基于历史扫描路径和图像特征预测注视位移的方法。</p>
<ul>
<li>在与当前注视点、viewport和整个图像相关的三个空间尺度上执行了显著性计算。</li>
<li>可能的图像特性被通过向CNN喂图像和相应的显著性图，同时LSTM模型捕获历史信息来抽取出来。</li>
<li>之后将LSTM和CNN特性耦合起来，用于下一次的用户注视信息预测。</li>
</ul>
<h5 id="fans-work">Fan&rsquo;s work</h5>
<p>用户更容易被运动的物体吸引，因此除了显著性图之外，Fan等人也考虑了使用预训练  的CNN来估计用户未来注视点的内容运动图。</p>
<ul>
<li>由于可能存在多个运动，这让预测变得不可靠，因此运动贴图的开发还需要进一步的研究。</li>
</ul>
<h5 id="yangs-work">Yang&rsquo;s work</h5>
<ul>
<li>使用CNN模型基于历史观测角度信息预测了单viewport。</li>
<li>接着考虑了一种使用内容不可知和内容感知方法如RNN和CFVT模型的融合层的viewport轨迹预测策略。</li>
<li>融合模型使其同时支持更好地预测并且提高了大概40%的精度。</li>
</ul>
<h5 id="ozcinars-work">Ozcinar&rsquo;s work</h5>
<p>将viewport轨迹转换为基于viewport的视觉注意图，然后对不同大小的tile进行推流以保证更高的编码效率。</p>
<h5 id="lis-work">Li&rsquo;s work</h5>
<p>现有的预测模型对未来的预测能力有限，Li等人提出了两种模型，分别用于viewport相关和基于tile的推流系统。</p>
<ul>
<li>第一个模型应用了基于用户轨迹的LSTM编解码网络体系结构。</li>
<li>第二个模型应用了卷积LSTM编解码体系结构，使用序列的热图来预测用户的未来方向。</li>
</ul>
<h3 id="总结">总结</h3>
<p>精确的方向预测使360度视频的客户端可以以高分辨率下载最相关的tile。</p>
<p>当前采用显著性和位置信息的神经网络模型的性能比直接利用当前观察位置进行未来viewport位置估计的简单无运动的基线方法表现差。估计的显著性中的噪音等级限制了这些模型的预测精度。并且这些模型也引入了额外的计算复杂度。</p>
<p>对于360度视频注意点的可靠预测和用户观看可能性与显著性图之间关系的理解，显著性模型必须被改善并通过训练大规模的数据集来适应，尤其是被配备了不同摄像机旋转的镜头所捕获的数据。</p>
<p>另一方面，卷积LSTM编解码器和基于轨迹的预测方法适合长期预测，并能带来相当大的QoE改进，特别是在协作流媒体环境中。</p>
<h2 id="qoe评估">QoE评估</h2>
<h3 id="背景-3">背景</h3>
<p>由于全方位视频非常普遍，因此，通过这种类型的视频分发来确定用户的特定质量方面是至关重要的。QoE在视频推流应用中扮演着重要角色。在传统视频推流中，QoE很大程度上被网络负载和分发性能所影响。现有的次优目标度量方法并不适用于全向视频，因为全向视频受网络状况和用户视角行为的影响很大。</p>
<h3 id="主观质量评估">主观质量评估</h3>
<p>主观质量评估是估计360度视频推流质量的现实并且可靠的方法。</p>
<h4 id="upeniks-work">Upenik&rsquo;s work</h4>
<p>用一台MergeVR HMD执行了主观测试来体验360度图像。</p>
<ul>
<li>实验数据包括主观分数、视角轨迹、在每个图像上花费的时间由软件上获得。</li>
<li>视角方向信息被用于计算显著性图。</li>
<li>但是这项研究没有考虑对360度视频的评估。</li>
</ul>
<h4 id="zhangs-work">Zhang&rsquo;s work</h4>
<p>为了弥补360度视频和常规视频度量方式之间的性能差距，为全景视频提出了一种主观质量评估方法，称为<em>SAMPVIQ</em>。</p>
<ul>
<li>23位参与者被允许观看4个受损视频，整体视频质量体验的评分在0～5分之间。</li>
<li>参与者之间存在较大的评分差异。</li>
</ul>
<h4 id="xus-work-2">Xu&rsquo;s work</h4>
<p>提出两种主观测量方式：总体区分平均意见分数(O-DMOS)和矢量区分平均意见分数(V-DMOS)来获得360度视频的质量损失。</p>
<ul>
<li>类似于传统食品的DMOS度量方式，O-DMOS度量方式计算主观测试序列的总计区分分数。</li>
</ul>
<h4 id="schatzs-work">Schatz&rsquo;s work</h4>
<p>研究了使用HMD观看360度内容时停顿事件的影响。</p>
<ul>
<li>沉浸式内容的主观质量评估并非不重要，可能导致比实际推荐更多的开放性问题。</li>
<li>通常来讲人们的期望于传统的HAS相似，即如果可能的话，根本没有停顿。</li>
</ul>
<h4 id="可用的开源工具">可用的开源工具</h4>
<p>AVTrack360，OpenTrack和360player能捕获用户观看360度视频的头部轨迹。</p>
<p>VRate是一个在VR环境中提供主观问卷调查的基于Unity的工具。</p>
<p>安卓应用*<a href="https://github.com/zerepolbap/miro360" target="_blank" rel="noopener noreffer">MIRO360</a>*，支持未来VR主观测试的指南开发。</p>
<h4 id="cybersickness"><code>Cybersickness</code></h4>
<p><code>Cybersickness</code>是一种获得高QoE的潜在障碍，它能引起疲劳、恶心、不适和呕吐。</p>
<h5 id="singlas-work">Singla&rsquo;s work</h5>
<p>使用受限的带宽和分辨率，在不同的延迟情况下进行了两个主观实验。</p>
<ul>
<li>开发了主观测试平台、测试方法和指标来评估viewport自适应360度视频推流中的视频感知等级和<code>Cybersickness</code>。</li>
<li>基于tile的推流在带宽受限的情况下表现很好。</li>
<li>47ms的延迟实际上不影响感知质量。</li>
</ul>
<h5 id="trans-work">Tran&rsquo;s work</h5>
<p>考虑了几个影响因子例如内容的空间复杂性，数量参数，分辨率特性和渲染模型来评估cybersickness，质量，可用性和用户的存在。</p>
<ul>
<li>VR环境中快速移动的内容很容易引发cybersickness。</li>
<li>由于高可用性和存在性，用户的cybersickness也可能加剧。</li>
</ul>
<h5 id="singlas-work-1">Singla&rsquo;s work</h5>
<p>评估了28名受试者在Oculus Rift和HTC Vive头戴式电脑上观看6个全高清和超高清分辨率YouTube视频时的观看不适感。</p>
<ul>
<li>HMD的类型轻微地影响感知质量。</li>
<li>分辨率和内容类型强烈影响个人体验。</li>
<li>女性用户感到<code>cybersickness</code>的人数更多。</li>
</ul>
<h4 id="空间存在感">空间存在感</h4>
<p>空间存在感能增强沉浸感。</p>
<h5 id="zous-work">Zou&rsquo;s work</h5>
<p>方法：</p>
<p>提出了一个主观框架来测量25名受试者的空间存在感。</p>
<ul>
<li>提出的框架包括三层，从上到下分别为：空间存在层、感知层、科技影响层。</li>
<li>心理上的空间存在感形成了空间存在层。</li>
<li>感知层以视频真实感、音频真实感和交互元素为特征。</li>
<li>科技影响层由几个模块组成，这些模块与感知层相连，以反映传感器的真实性。</li>
</ul>
<h5 id="huponts-work">Hupont&rsquo;s work</h5>
<p>应用通用感知的原则来研究在Oculus HMD和传统2D显示器上玩游戏的用户的空间存在感。</p>
<ul>
<li>与2D显示器相比，3D虚拟现实主义显示出更高的惊奇、沉浸感、存在感、可用性和兴奋感。</li>
</ul>
<h4 id="生理特征度量">生理特征度量</h4>
<h5 id="salgados-work">Salgado&rsquo;s work</h5>
<p>方法：</p>
<p>捕获多种多样的生理度量，例如心率HR，皮肤电活性EDA、皮肤温度、心电图信号ECG、呼吸速率、血压BVP、脑电图信号EEG来评价沉浸式模拟器的质量。</p>
<h5 id="egans-work">Egan&rsquo;s work</h5>
<p>基于HR和EDA信号评估VR和非VR渲染模式质量分数。</p>
<ul>
<li>相比于HR，EDA对质量分数有强烈的影响。</li>
</ul>
<h4 id="技术因素感知">技术因素感知</h4>
<p>不同的技术和感知特征，如失真、清晰度、色彩、对比度、闪烁等，用于评估感知视频质量。</p>
<h5 id="fremereys-work">Fremerey&rsquo;s work</h5>
<p>确定了可视质量强烈地依赖于应用的运动插值（MI）算法和视频特征，例如相机旋转和物体的运动。</p>
<p>在一项主观实验中，12位视频专家回顾了使用FFmpeg混合、FFmpeg MCI（运动补偿插值）和butterflow插值到90 fps的四个视频序列。作者发现，与其他算法相比，MCI在QoE方面提供了极好的改进。</p>
<h4 id="总结-1">总结</h4>
<p>主观测试与人眼直接相关，并揭示了360度视频质量评估的不同方面的影响。</p>
<p>在这些方面中，空间存在感和由佩戴VR头戴设备观看360度视频导致的<em>cybersickness</em>极为重要，因为这些效果并不在传统的2D视频观看中出现。</p>
<p>主观评估需要综合的手工努力并因此昂贵耗时并易于出错，相对而言，客观评估更易于管理和可行。</p>
<h3 id="客观质量评估">客观质量评估</h3>
<p>由于类似的编码结构和2D平面投影格式，对360度内容应用客观质量评估很自然。</p>
<h4 id="计算psnr">计算PSNR</h4>
<p>现有投影方式中的采样密度在每个像素位置并不均匀。</p>
<h5 id="yus-work">Yu&rsquo;s work</h5>
<p>为基于球形的PSNR计算引入S-PSNR和L-PSNR。</p>
<ul>
<li>S-PSNR通过对球面上所有位置的像素点做同等加权来计算PSNR。</li>
<li>利用插值算法，S-PSNR可以完成对支持多种投影模式的360度视频的客观质量评估。</li>
<li>L-PSNR通过基于纬度和访问频率的像素点加权测量PSNR。</li>
<li>L-PSNR可以测量viewport的平均PSNR而无需特定的头部运动轨迹。</li>
</ul>
<h5 id="zakharchenkos-work">Zakharchenko&rsquo;s work</h5>
<p>提出了一种Craster Parabolic Projection-PSNR (CPP-PSNR) 度量方式来比较多种投影方案，通过不改变空间分辨率和不计算实际像素位置的PSNR，将像素重新映射成CPP投影。</p>
<ul>
<li>CPP投影方式可能使视频分辨率大幅下降。</li>
</ul>
<h5 id="suns-work">Sun&rsquo;s work</h5>
<p>提出了一种叫做weighted-to-spherically-uniform PSNR (WS-PSNR)的质量度量方式，以此来测量原始和受损内容之间的质量变化。</p>
<ul>
<li>根据像素在球面上的位置考虑权重。</li>
</ul>
<h4 id="计算ssim">计算SSIM</h4>
<p>SSIM是另一种质量评估指标，它通过三个因素反映图像失真，包括亮度、对比度和结构。</p>
<h5 id="chens-work">Chen&rsquo;s work</h5>
<p>为2D和360度视频分析了SSIM结果，引入了球型结构的相似性度量（S-SSIM）来计算原始和受损的360度视频之间的相似性。</p>
<ul>
<li>在S-SSIM中，使用重投影来计算两个提取的viewport之间的相似性。</li>
</ul>
<h5 id="zhous-work">Zhou&rsquo;s work</h5>
<p>考虑相似性的权重提出了WS-SSIM来测量投影区域中窗口的相似性。</p>
<ul>
<li>性能评估表明，与其他质量评估指标相比，WS-SSIM更接近人类感知。</li>
</ul>
<h5 id="van-der-hoofts-work">Van der Hooft&rsquo;s work</h5>
<p>提出了<em>ProbGaze</em>度量方式，基于tile的空间尺寸和viewport中的注视点。</p>
<ul>
<li>考虑外围tile的权重来提供合适的质量测量。</li>
<li>相比于基于中心和基于平均的PSNR和SSIM度量方式，<em>ProbGaze</em>能估计当用户突然改变viewport位置时的视频质量变化。</li>
</ul>
<h5 id="xus-work-3">Xu&rsquo;s work</h5>
<p>引入了两种客观质量评估度量手段：基于内容感知的PSNR和非内容感知的PSNR，用于编码360度视频。</p>
<ul>
<li>第一种方式基于空间全景内容对像素失真进行加权。</li>
<li>第二种方式考虑人类偏好的统计数据来估计质量损失。</li>
</ul>
<h4 id="基于psnr和ssim方式的改进">基于PSNR和SSIM方式的改进</h4>
<p>尽管各种基于PSNR和SSIM的方式被广阔地应用到了360度视频的质量评估中，但这些方式都没有真正地捕获到感知质量，特别是当HMD被用于观看视频时。因此需要为360度内容特别设计一种优化的质量度量方式。</p>
<h5 id="upeniks-work-1">Upenik&rsquo;s work</h5>
<p>考虑了一场使用4张高质量360度全景图像来让45名受试者在不同的编码设定下评估和比较客观质量度量方式性能的主观实验。</p>
<ul>
<li>现有的客观度量方式和主观感知到的质量相关性较低。</li>
</ul>
<h5 id="trans-work-1">Tran&rsquo;s work</h5>
<p>论证主观度量和客观度量之间相关性较高，但是使用的数据集较小。</p>
<h4 id="基于ml的方式">基于ML的方式</h4>
<p>基于ML的方式可以弥补客观评估和主观评估之间的差距。</p>
<h5 id="da-costa-filhos-work">Da Costa Filho&rsquo;s work</h5>
<p>提出了一个有两个阶段的模型。</p>
<ul>
<li>首先自适应VR视频的播放性能由机器学习算法所确定。</li>
<li>之后模型利用估计的度量手段如视频质量、质量变化、卡顿时间和启动延迟来确定用户的QoE。</li>
</ul>
<h5 id="lis-work-1">Li&rsquo;s work</h5>
<p>引入了基于DRL的质量获取模型，在一次推流会话中同时考虑头部和眼部的移动。</p>
<ul>
<li>360度视频被分割成几个补丁。</li>
<li>低观看概率的补丁被消除。</li>
<li>参考和受损视频序列都被输入到深度学习可执行文件中，以计算补丁的质量分数。</li>
<li>之后分数被加权并加到一起得到最终的分数。</li>
</ul>
<h5 id="yangs-work-1">Yang&rsquo;s work</h5>
<p>考虑了多质量等级的特性和融合模型。</p>
<ul>
<li>质量特性用<code>region of interest(ROI)</code>图来计算，其中包括像素点等级、区域等级、对象等级和赤道偏差。</li>
<li>混合模型由后向传播的神经网络构造而成，这个神经网络组合了多种质量特性来获取整体的质量评分。</li>
</ul>
<h3 id="总结-2">总结</h3>
<p>精确的QoE获取是优化360度视频推流服务中重要的因素，也是自适应分发方案中基础的一环。</p>
<p>单独考虑VR中的可视质量对完整的QoE框架而言并不足够。</p>
<p>为能获得学界的认可，找到其他因素的影响也很必要，例如<code>cybersickness</code>，生理症状，用户的不适感，HMD的重量和可用性，VR音频，viewport降级率，网络特性（延迟，抖动，带宽等），内容特性（相机动作，帧率，编码，投影等），推流特性（viewport偏差，播放缓冲区，时空质量变化等）。</p>
<h2 id="低延迟推流">低延迟推流</h2>
<h3 id="背景-4">背景</h3>
<p>360度全景视频推流过程中的延迟由几部分组成：传感器延迟、云/边处理延迟、网络延迟、请求开销、缓冲延迟、渲染延迟和反馈延迟。</p>
<p>低延迟的要求对于云VR游戏、沉浸式临场感和视频会议等更为严格。</p>
<p>要求极低的终端处理延迟、快速的云/边计算和极低的网络延迟来确保对用户头部移动做出反馈。</p>
<p>现代HMD可以做到使传感器延迟降低到用户无法感知的程度。</p>
<p>传输延迟已经由5G移动和无线通信技术大幅减少。</p>
<p>但是，对于减少处理、缓冲和渲染延迟的工作也是必要的。</p>
<p>许多沉浸式应用的目标是MTP的延迟少于20ms，理想情况是小于15ms。</p>
<h3 id="减少启动时间">减少启动时间</h3>
<h4 id="减少初始化请求的数据量">减少初始化请求的数据量</h4>
<p>通常来讲，较小的视频segment能减少启动和下载时间。</p>
<h5 id="van-der-hoofts-work-1">Van der Hooft&rsquo;s work</h5>
<p>考虑了新闻相关内容的推流，使用的技术有：</p>
<ol>
<li>服务端编码</li>
<li>服务端的用户分析</li>
<li>服务器推送策略</li>
<li>客户端积极存储视频数据</li>
</ol>
<p>取得的效果：</p>
<ul>
<li>降低了启动时间</li>
<li>允许不同网络设定下的快速内容切换</li>
<li>较长的响应时间降低了性能</li>
</ul>
<h5 id="nguyens-work-1">Nguyen&rsquo;s work</h5>
<p>基于viewport依赖的自适应策略分析了自适应间隔延迟和缓冲延迟的影响。</p>
<ul>
<li>使用服务端比特率计算策略来最小化响应延迟的影响。</li>
<li>根据客户端的响应估计可用的网络吞吐量和未来的viewport位置。</li>
<li>服务端的决策引擎推流合适的tile来满足延迟限制。</li>
</ul>
<p>取得的效果：</p>
<ul>
<li>对于viewport依赖型推流方案而言，较少的自适应和缓冲延迟不可避免。</li>
</ul>
<h3 id="降低由tile分块带来的网络负载">降低由tile分块带来的网络负载</h3>
<p>在HTTP/1.1中，在空间上将视频帧分成矩形tile会增加网络负载，因为每个tile会产生独立的网络请求。</p>
<p>请求爆炸的问题导致了较长的响应延迟，但是可以通过使用HTTP/2的服务器推送特性解决。这个特型使服务器能使用一条HTTP请求复用多条消息。</p>
<h5 id="weis-work">Wei&rsquo;s work</h5>
<p>利用HTTP/2协议来促进低延迟的HTTP自适应推流。</p>
<ul>
<li>提出的服务端推送的策略使用一条请求同时发送几个segment避免多个GET请求。</li>
</ul>
<h5 id="petrangelis-work">Petrangeli&rsquo;s work</h5>
<p>结合特定请求参数与HTTP/2的服务端推送特性来促进360度视频推流。</p>
<ul>
<li>客户端为一个segment发送一条call，服务器使用FCFS策略传送k个tile。</li>
<li>利用HTTP/2的优先级特性可以使高优先级的tile以紧急的优先级被获取，进而改善网络环境中的高往返时间的性能。</li>
</ul>
<h5 id="xus-work-4">Xu&rsquo;s work</h5>
<p>为360度内容采用了<code>k-push</code>策略：将k个tile推送到客户端，组成一个单独的时间段。</p>
<ul>
<li>提出的方法与QoE感知的比特率自适应算法一起，在不同的RTT设定下，提高了20%的视频质量，减少了30%的网络传输延迟。</li>
</ul>
<h5 id="yahias-work">Yahia&rsquo;s work</h5>
<p>使用HTTP/2的优先级和多路复用功能，在两个连续的viewport预测之间，即在交付相同片段之前和期间，组织紧急视频块的受控自适应传输。</p>
<h5 id="yens-work">Yen&rsquo;s work</h5>
<p>开发了一种支持QUIC的体系结构来利用流优先级和多路复用的特性来实现360度视频的安全和低优先级的传输。</p>
<ul>
<li>当viewport变化发生时，QUIC能让常规的tile以低优先级推流，viewport内的tile以高优先级推流，都通过一条QUIC连接来降低viewport tile的缺失率。</li>
<li>作者说测试表明基于QUIC的自适应360度推流比HTTP/1.1和HTTP/2的方案表现更好。</li>
</ul>
<h3 id="使用移动边缘计算降低延迟">使用移动边缘计算降低延迟</h3>
<h5 id="mangiantes-work">Mangiante&rsquo;s work</h5>
<p>提出了利用基于边缘处理的viewport渲染方案来减少延迟，同时利用终端设备上的电源和计算负载。</p>
<ul>
<li>但是作者没有给出有效的算法或是建立一个实践执行平台。</li>
</ul>
<h5 id="lius-work">Liu&rsquo;s work</h5>
<p>采用远端渲染技术，通过为不受约束的VR系统获取高刷新率来隐藏网络延迟。</p>
<ul>
<li>采用60GHz的无线链路支持的高端GPU，来加快计算速度和4K渲染，减少显示延迟。</li>
<li>尽管提供了高质量和低延迟的推流，但是使用了昂贵的带宽连接，这通常并不能获得。</li>
</ul>
<h5 id="viitanens-work">Viitanen&rsquo;s work</h5>
<p>引入了端到端的VR游戏系统。通过执行边缘渲染来降低延迟，能源和计算开销。</p>
<ul>
<li>为1080p 30fps的视频格式实现了端到端的低延迟（30ms）的系统。</li>
<li>前提是有充足的带宽资源、终端设备需要性能强劲的游戏本。</li>
</ul>
<h5 id="shis-work">Shi&rsquo;s work</h5>
<p>考虑了不重视viewport预测的高质量360度视频渲染。</p>
<ul>
<li>提出的MEC-VR系统采用了一个远端服务器通过使用一个自适应裁剪过滤器来动态适应viewport覆盖率，这个过滤器按照观测到的系统延迟增加viewport之外的区域。</li>
<li>基于viewport覆盖率的延迟调整允许客户端容纳和补偿突然的头部移动。</li>
</ul>
<h3 id="共享vr环境中的延迟处理">共享VR环境中的延迟处理</h3>
<p>共享VR环境中用户的延迟取决于用户的位置和边缘资源的分发。</p>
<h5 id="parks-work">Park&rsquo;s work</h5>
<p>通过考虑多个用户和边缘服务器之间的双向通信，提出了一种使用线性蜂窝拓扑中的带宽分配策略，以最小化端到端系统延迟。确定了推流延迟强烈地依赖于：</p>
<ul>
<li>边缘服务器的处理性能</li>
<li>多个交互用户之间的物理和虚拟空间</li>
</ul>
<h5 id="perfectos-work">Perfecto&rsquo;s work</h5>
<p>集成了深度神经网络和毫米波多播传输技术来降低协同VR环境中的延迟。</p>
<ul>
<li>神经网络模型估计了用户即将来临的viewport。</li>
<li>用户被基于预测的相关性和位置分组，以此来优化正确的viewport许可。</li>
<li>执行积极的多播资源调度来最小化延迟和拥塞。</li>
</ul>
<h3 id="总结-3">总结</h3>
<p>在单用户和多用户的环境中，边缘辅助的解决方式对于控制延迟而言占主要地位。</p>
<p>此外还有服务端的viewport计算、服务端push机制和远程渲染机制都能用于低延迟的控制。</p>
<p>现有的4G网络足以支持早期的自适应沉浸式多媒体，正在成长的5G网络更能满足沉浸式内容的需求。</p>
<h2 id="360度直播推流">360度直播推流</h2>
<h3 id="背景-5">背景</h3>
<p>传统的广播电视频道是直播推流的流行来源。现在私人的360度直播视频在各个社交媒体上也有大幅增长。</p>
<p>因为视频生产者和消费者之间在云端的转码操作，360度视频推流是更为延迟敏感的应用。</p>
<p>现有的处理设备在诸如转码、渲染等实时处理任务上受到了限制。</p>
<h4 id="内容分发">内容分发</h4>
<h5 id="hus-work">Hu&rsquo;s work</h5>
<p>提出了一套基于云端的直播推流系统，叫做<code>MELiveOV</code>，它使高分辨率的全向内容的处理任务以毛细管分布的方式分发到多个支持5G的云端服务器。</p>
<ul>
<li>端到端的直播推流系统包括内容创作模块、传输模块和viewport预测模块。</li>
<li>移动边缘辅助的推流设计减少了50%的带宽需求。</li>
</ul>
<h5 id="griwodzs-work">Griwodz&rsquo;s work</h5>
<p>为360度直播推流开发了优化FoV的原型，结合了RTP和基于DASH的<code>pull-patching</code>来传送两种质量等级的360度视频给华为IPTV机顶盒和Gear VR头戴设备。</p>
<ul>
<li>作者通过在单个H.265硬件解码器上多路复用多个解码器来实现集体解码器的想法，以此减少切换时间。</li>
</ul>
<h4 id="视频转码">视频转码</h4>
<h5 id="lius-work-1">Liu&rsquo;s work</h5>
<p>研究表明只转码viewport区域有潜力大幅减少高性能转码的计算需求。</p>
<h5 id="baigs-work">Baig&rsquo;s work</h5>
<p>开发了快速编码方案来分发直播的4K视频到消费端设备。</p>
<ul>
<li>采用了分层视频编码的方式来在高度动态且不可预测的WiGig和WiFi链路上分发质量可变的块。</li>
</ul>
<h5 id="les-work">Le&rsquo;s work</h5>
<p>使用RTSP网络控制协议为CCTV的360度直播推流提出了实时转码和加密系统。</p>
<ul>
<li>转码方式基于ARIA加密库，Intel媒体SDK和FFmpeg库。</li>
<li>系统可以管理并行的转码操作，实现高速的转码性能。</li>
</ul>
<h4 id="内容拼接缝合">内容拼接缝合</h4>
<p>相比于其他因素如捕获、转码、解码、渲染，内容拼接在决定整体上的推流质量时扮演至关重要的角色。</p>
<h5 id="chens-work-1">Chen&rsquo;s work</h5>
<p>提出了一种内容驱动的拼接方式，这种方式将360度帧的语义信息的不同类型看作事件，以此来优化拼接时间预算。</p>
<ul>
<li>基于VR帧中的语义信息，tile执行器模块选择合适的tile设计。</li>
<li>拼接器模块然后执行基于tile的拼接，这样，基于可用资源，事件tile有更高的拼接质量。</li>
<li>评估表明系统通过实现89.4%的时间预算，很好地适应了不同的事件和时间限制。</li>
</ul>
<h3 id="总结-4">总结</h3>
<p>相比于点播式流媒体，360度直播推流面临多个挑战，例如在事先不知情的情况下处理用户导航、视频的首次流式传输以及实时视频的转码。在多用户场景中，这些挑战更为棘手。</p>
<p>关于处理多个用户的观看模式，可伸缩的多播可以用于在低带宽和高带宽网络上以接近于按需推流的质量等级。</p>
<p>基于ROI的tile拼接和转码可以显著地减少延迟敏感的交互型应用的延迟需求。</p>
]]></description>
</item>
</channel>
</rss>

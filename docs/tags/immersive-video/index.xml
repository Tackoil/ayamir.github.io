<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Immersive Video - 标签 - Ayamir&#39;s Blog</title>
        <link>https://ayamir.github.io/tags/immersive-video/</link>
        <description>Immersive Video - 标签 - Ayamir&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miracle_l@bupt.edu.cn (Ayamir)</managingEditor>
            <webMaster>miracle_l@bupt.edu.cn (Ayamir)</webMaster><lastBuildDate>Sat, 15 Jan 2022 18:46:02 &#43;0800</lastBuildDate><atom:link href="https://ayamir.github.io/tags/immersive-video/" rel="self" type="application/rss+xml" /><item>
    <title>Note for srlABR Cross User</title>
    <link>https://ayamir.github.io/posts/papers/note-for-srlABR-cross-user/</link>
    <pubDate>Sat, 15 Jan 2022 18:46:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-srlABR-cross-user/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9234071" target="_blank" rel="noopener noreffer">Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network</a></p>
<p>Level：IEEE Transactions on Broadcasting 2021</p>
<p>Keywords：Cross-user vp, Sequetial RL ABR</p>
<h2 id="主要工作">主要工作</h2>
<ul>
<li>使用跨用户注意力网络<code>CUAN</code>来做 VP；</li>
<li>使用<code>360SRL</code>来做 ABR</li>
<li>将上面两者集成到了推流框架中；</li>
</ul>
<h2 id="vp">VP</h2>
<h3 id="motivation">Motivation</h3>
<p>形式化 VP 问题如下：</p>
<p>给出 $p^{th}$ 用户的 $1-t$ 时间内的历史视点坐标 $L^{p}_{1:t} = \lbrace l^p_1, l^p_2, &hellip;, l^p_t \rbrace$ ，其中 $l^p_t = (x_t, y_t), x_t \in [-180, 180]; y_t \in [-90, 90]$ ；</p>
<p>同一视频的不同用户视点表示为 $L^{1:M}_{1:t+T}$ ， $M$ 表示其他用户的数量；</p>
<p>目标是预测未来的 $T$ 个时刻的视点位置 $L^p_i, i = t+1, &hellip;, t+T$ ；</p>
<p>最终可以用数学公式表达为：
$$
\underset{F}{min} \sum^{t+T}_{k = t+1} {\parallel l^p_k - \hat{l}^p_k \parallel}_1
$$</p>
<p>现有的用<code>KNN</code>做的跨用户预测基于 LR 的模型，而 LR 的模型很容易产生偏差，所以为了增强<code>KNN</code>的性能，同时考虑单用户的历史视点轨迹和跨用户的视点轨迹。</p>
<ul>
<li>提出一种注意力机制来自动提取来自其他用户视口的有用信息；</li>
<li>对于与当前用户有相似偏好的用户轨迹信息给与更多的注意；</li>
<li>相似性通过基于过去时间段内其他用户的轨迹计算出来；</li>
</ul>
<h3 id="design">Design</h3>
<p></p>
<ol>
<li>
<p>轨迹编码器模块从用户的历史视点位置提取时间特征；</p>
<p>使用<code>LSTM</code>来编码用户的观看路径；</p>
<p>为了预测 ${(t+1)}^{th}$ 帧的视点位置，首先向<code>LSTM</code>输入 $p^{th}$ 用户的历史视点坐标：
$$
f^{p}_{t+1} = h(l^p_1, l^p_2, &hellip;, l^p_t)
$$
$h(\cdot)$ 是<code>LSTM</code>的输入输出函数；</p>
<p>接着使用相同的<code>LTSM</code>编码其他用户的观看轨迹：
$$
f^{i}_{t+1} = h(l^i_1, l^i_2, &hellip;, l^i_{t+1}), i \in \lbrace 1, &hellip;, M \rbrace
$$</p>
</li>
<li>
<p>注意力模块从其他用户的视点轨迹中提取与 $p^{th}$ 用户相关的信息</p>
<p>首先推导出 $p^{th}$ 用户和其他用户之间的相关系数：
$$
s^{pi}_{t+1} = z(f^{i}_{t+1}, l^{p}_{t+1}), i \in \lbrace 1, &hellip;, M \rbrace \cup \lbrace p \rbrace;
$$
$s^{th}_{t+1}$ 表示 $p^{th}$ 用户和 $i^{th}$ 用户之间的相似性；$z()$ 由内积运算建模（还可用其他方式建模比如多个 FC 层）；</p>
<p>接着将相关系数规范化：
$$
{\alpha}^{pi}_{t+1} = \frac{e^{s^{pi}_{t+1}}}{\sum_{i \in \lbrace 1,&hellip; M \rbrace \cup {\lbrace p \rbrace}^{e^{s^{pi}_{t+1}}}}}
$$
最后得到融合特征：
$$
g^{p}_{t+1} = \sum_{i \in {\lbrace 1,&hellip;M \rbrace \cup \lbrace p \rbrace}} {\alpha}^{pi}_{t+1} \cdot f^{i}_{t+1}
$$
融合特征被最后用于 VP。</p>
</li>
<li>
<p>VP 模块预测 ${(t+1)}^{th}$ 帧的视点位置</p>
<p>$$
\hat{l}^{p}_{t+1} = r(g^{p}_{t+1})
$$
函数 $r(\cdot)$ 由一层 FC 建模。值得注意的是，对应于未来 T 帧的视点是以滚动方式预测的。</p>
</li>
</ol>
<h3 id="loss">Loss</h3>
<p>损失函数定义为预测的视点位置和实际视点位置之间的所有绝对差异的总和：
$$
L = \sum^{t+T}_{i=t} {|\hat{l}^p_i - l^p_i|}_1
$$</p>
<h3 id="details">Details</h3>
<ul>
<li>使用<code>PyTorch</code>实现；</li>
<li>函数 $h(\cdot)$ 由两个堆叠的<code>LSTM</code>层组成，两者都有 32 个神经元；</li>
<li>函数 $r(\cdot)$ 包含一个带有 32 个神经元的 FC 层，接着是<code>Tanh</code>函数；</li>
<li>历史视点和未来视点的长度设定为 1 秒和 5 秒；</li>
<li>每次迭代从数据集中随机产生 2048 个样本；</li>
<li>所有训练变量的优化函数采用<code>Adam</code>；</li>
<li>$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$；</li>
<li>$learning\ rate = 10^{-3}, training\ epoch = 50$；</li>
</ul>
<h2 id="abr">ABR</h2>
<h3 id="formulation">Formulation</h3>
<p>全景视频被切分成 $m$ 个长度为 $T$ 秒的视频片段，每个视频片段空间上划分成 $N$ 个分块，分别以 $M$ 个不同的码率等级编码。因此对于每段有 $N \times M$ 个可选的编码块。</p>
<p>ABR 的目标是为每个片段找到最优的码率集 $X = \lbrace x_{i, j} \rbrace \in Z^{N \times M}$ （ $x_{i, j} = 1$ 意味着为 $i^{th}$ 块选择 $j^{th}$ 的码率等级）：
$$
\underset{X}{max} \sum^{m}_{t=1} Q_t
$$
$Q_t$ 表示 $t^{th}$ 段的 QoE 分数，与以下几个方面有关：</p>
<ul>
<li>
<p>VIewport Quality：
$$
Q^1_t = \sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot p_i \cdot r_{i,j}
$$
$p_i$ 表示 $i^{th}$ 分块的规范化观看概率； $r_{i,j}$ 记录块 $(i, j)$ 的码率；</p>
</li>
<li>
<p>Viewport Temporal Variation：
$$
Q^2_t = |Q^1_t - Q^{1}_{t-1}|
$$</p>
</li>
<li>
<p>Viewport Spatial Variation：
$$
Q^3_t = \frac{1}{2} \sum^{N}_{i=1} \sum_{u \in U_i} p_i \cdot p_u \sum^{M}_{j=1} |x_{i,j} \cdot r_{i,j} - x_{u,j} \cdot r_{u,j}|
$$
$U_i$ 表示 $i^{th}$ 个分块的 1 跳邻居中的 tile 索引<a href="https://ieeexplore.ieee.org/document/8486606" target="_blank" rel="noopener noreffer">[1]</a>；</p>
</li>
<li>
<p>Rebuffering：
$$
Q^4_t = max(\frac{\sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot r_{i,j} \cdot T}{\xi_t} - b_{t-1}, 0)
$$
$\xi_t$ 表示网络吞吐量； $b_{t-1}$ 表示播放器的缓冲区占用率；</p>
<p>最终的 QoE 可以由上面的指标定义：
$$
Q_t = Q^1_t - \eta_1 \cdot Q^2_t - \eta_2 \cdot Q^3_t - \eta_3 \cdot Q^4_t
$$
$\eta_*$ 是可调节的参数，与不同的用户偏好对应。</p>
</li>
</ul>
<h3 id="sequential-rl-based-abr">Sequential RL-Based ABR</h3>
<p>假设基于 tile 的全景推流 ABR 过程也是 MDP。</p>
<p></p>
<p>细节在<a href="https://ayamir.github.io/posts/note-for-360srl/" target="_blank" rel="noopener noreffer">360SRL</a>中已经说明清楚。</p>
]]></description>
</item>
<item>
    <title>Note for 360SRL</title>
    <link>https://ayamir.github.io/posts/papers/note-for-360srl/</link>
    <pubDate>Thu, 13 Jan 2022 12:08:36 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-360srl/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/8784927" target="_blank" rel="noopener noreffer">360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming</a></p>
<p>Level：ICME 2019</p>
<p>Keywords：ABR、RL、Sequential decision</p>
<h2 id="创新点">创新点</h2>
<ul>
<li>在 MDP 中，将 N 维决策空间内的一次决策转换为 1 维空间内的 N 次级联顺序决策处理来降低复杂度。</li>
</ul>
<h2 id="问题定义">问题定义</h2>
<p>原始的全景视频被划分成每段固定长度为 $T$ 的片段，</p>
<p>每个片段包含 $N$ 个分块，并以 $M$ 的码率等级独立编码，</p>
<p>因此对每个片段，有 $N \times M$ 种可选的编码块。</p>
<p>为了保证播放时的流畅性，需要确定最优的预取集合：</p>
<p>${a_0, &hellip;, a_i, &hellip;, a_{N-1}}, i \in \lbrace 0, &hellip;, N-1 \rbrace, a_i \in \lbrace 0, &hellip;, M-1 \rbrace $</p>
<p>分别用 $q_{i, a_i}$ 和 $w_{i, a_i}$ 表示码率选择为 $a^{th}_i$ 的 $i^{th}$ 分块的质量和相应的分块片段大小。</p>
<p>用 $p_i \in [0, 1]$ 表示 $i^{th}$ 块的被看到的可能性。</p>
<h2 id="顺序-abr-决策">顺序 ABR 决策</h2>
<p></p>
<h2 id="代理设计">代理设计</h2>
<h3 id="状态">状态</h3>
<p>对于 $i^{th}$ 维，输入状态包括原始的环境状态 $s_t$ ；</p>
<p>与之前维度的动作集合相关的信号： $u^{i}_{s_t} = \lbrace Th, C_i, p_{0:i-1}, q_{0:i-1}, b_t, p_i, S_i, Q_{t-1} \rbrace$</p>
<p>$Th$ ：表示过去 m 次下载一个段的平均吞吐量；</p>
<p>$C_i \in R^M$ ：表示 $i^{th}$ 个分块的可用块大小向量；</p>
<p>$p_{0:i-1}$ 和 $q_{0:i-1, a^{0:i-1}_{t}}$ 分别表示选中的码率集合和看到之前 $i-1$ 个分块的概率集；</p>
<p>$b_t$ 是缓冲区大小；</p>
<p>$p_i$ 是 $i^{th}$ 个分块被看到的可能性；</p>
<p>$S_i$ 是之前选择的 $i-1$ 个分块的块大小之和： $S_i = \sum^{i-1}_{h=0} C_{h, a^h_t}$ ；</p>
<p>$Q_{t-1}$ 记录了最后一个段中 $N$ 个分块的平均视频质量；</p>
<h3 id="动作">动作</h3>
<p>动作空间离散，代理输出定义为价值函数：$f(u^i_{s_t}, a^i_t)$</p>
<p>表示所选状态的价值 $a^i_t \in \lbrace 0, &hellip;, M-1 \rbrace$ 处于状态 $u_{s_t}^i$ .</p>
<h3 id="回报">回报</h3>
<p>回报定义为下列因素的加权和：</p>
<p>平均视频质量 $q^{avg}_t$，空间视频质量方差 $q^{s_v}_t$，时间视频质量方差 $q^{t_v}_t$ ，重缓冲时间 $T^r_t$</p>
<p>$$
q^{avg}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot q_{i, a_i}
$$</p>
<p>$$
q^{s_v}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot |q_{i, a_i} - q^{avg}_t|
$$</p>
<p>$$
q^{t_v}_t = |q^{avg}_{t-1} - q^{avg}_t|
$$</p>
<p>$$
T^r_t = max \lbrace T_t - b_{t-1}, 0 \rbrace
$$</p>
<p>$$
R_t = w_1 \cdot q^{avg}_t - w_2 \cdot q^{s_v}_t - w_3 \cdot q^{t_v}_t - w_4 \cdot T^r_t
$$</p>
<h2 id="训练方法">训练方法</h2>
<p>使用<code>DQN</code>作为基本的算法来学习动作-价值函数 $Q(s_t, a_t; \theta)$ ，其中 $\theta$ 作为参数，对应的贪心策略为 $\pi(s_t; \theta) = \underset{\theta}{argmax} Q(s_t, a_t; \theta)$ 。</p>
<p><code>DQN</code>网络的关键想法是更新最小化损失函数的方向上的参数：
$$
L(\theta) = E[y_t - Q(s_t, a_t; \theta)]
$$</p>
<p>$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \pi(s_{t+1}; {\theta}&rsquo;); {\theta}&rsquo;)
$$
${\theta}&rsquo;$ 表示固定且分离的目标网络的参数；</p>
<p>$r(\cdot)$ 是即时奖励函数，即上面公式 5 中的 $R_t$ ；</p>
<p>$\gamma \in [0, 1]$ 是折扣因子；</p>
<p>为了缓解过拟合，引入 <code>double-DQN</code> 的结构，所以公式 7 被重写为：
$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, {\pi}(s_{t+1}; \theta); {\theta}&rsquo;)
$$
利用公式 6 和公式 8 可以得出 $i^{th}$ 维的暂时损失函数：
$$
l^i_t = Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$
其中 $Q_{target}$ 满足：</p>
<p>$$
Q_{target} = r_t + {\gamma}_u \cdot Q(u^0_{s_{t+1}}, \pi(u^0_{s_{t+1}}; 0); {\theta}&rsquo;)
$$</p>
<p>${\gamma}_u$ 和 ${\gamma}_b$ 分别代表”Top MDP“和”Bottom MDP“的折扣因子，训练中设定 ${\gamma}_b = 1$ 。</p>
<p>观察公式 9 和公式 10 可以看出每维都有相同的目标函数，意味着无法区别每个独立维度的动作 $a^i_t$ 对 $r_t$ 的贡献。</p>
<p>为了克服限制，根据某个分块的动作 $a^i_t$ 与其观看概率成正比的先验知识，向 $l^i_t$ 添加一个额外的 $r^i_{extra}$ ：
$$
l^i_t = r^i_{extra} + Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$</p>
<p>$$
r^i_{extra} =
\begin{cases}
0, p_i &gt; P ;
\
-a^i_t, p_i \le P
\end{cases}
$$</p>
<p>通过设定一个观看概率的阈值 $P$ ，对观看概率低于 $P$ 但选择了高码率的分块施加 $-a^i_t$ 的奖励。</p>
<p>因此最终的平均损失可以形式化为：
$$
l^{avg}_t = \frac{1}{N} \sum^{N-1}_{i=0} l^i_t
$$
接着使用梯度下降法来更新模型，学习率设定为 $\alpha$：
$$
\theta \larr \theta + \alpha \triangledown l^{avg}_t
$$
同时，在训练阶段利用经验回放法来提高<code>360SRL</code>的泛化性。</p>
<p></p>
<p></p>
<h2 id="实现细节">实现细节</h2>
<p></p>
<p>特征从输入状态中通过特征提取网络提取出来。</p>
<p>初始的 4 个输入通过带有 128 个过滤器的 1 维卷积层被传递，4 个输入核心大小分别为 $1 \times m$ 、 $1 \times M$ 、 $1 \times N$ 、 $1 \times M$ ，后续这 4 个输入被喂给有 128 个神经元的全连接层；</p>
<p>随后特征映射被连接成一个张量，接着是具有 1024 个神经元和 256 个神经元的前向网络；</p>
<p>整个动作-价值网络的输出是 M 维的向量。</p>
<p>特征提取层和前向网络层都使用 <code>Leaky-ReLU</code>作为激活函数，最后是层归一化层。</p>
]]></description>
</item>
<item>
    <title>Note for GPAC</title>
    <link>https://ayamir.github.io/posts/papers/note-for-gpac/</link>
    <pubDate>Thu, 30 Dec 2021 10:23:26 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-gpac/</guid>
    <description><![CDATA[<h2 id="dash-客户端自适应逻辑">Dash 客户端自适应逻辑</h2>
<ol>
<li><em>tile priority setup</em>：根据定义的规则对 tile 进行优先级排名。</li>
<li><em>rate allocation</em>：收集网络吞吐量信息和 tile 码率信息，使用确定的 tile 优先级排名为其分配码率，努力最大化视频质量。</li>
<li><em>rate adaption</em>：在播放过程中，执行码率自适应算法，基于播放速度、质量切换的次数、缓冲区占用情况等。</li>
</ol>
<h3 id="tile-priority-setup">tile priority setup</h3>
<ol>
<li>
<p>Dash 客户端加载带有 SRD 信息的 MPD 文件时，首先确定使用 SRD 描述的 tile 集合。</p>
</li>
<li>
<p>确定 tile 之间的编码依赖（尤其是使用 HEVC 编码的 tile 时）</p>
</li>
<li>
<p>为每个独立的 tile 向媒体渲染器请求一个视频对象，并向其通知 tile 的 SRD 信息。</p>
</li>
<li>
<p>渲染器根据需要的显示大小调整 SRD 信息之后，执行视频对象的最终布局。</p>
</li>
<li>
<p>一旦 tile 集合被确定，客户端向每个 tile 分配优先级。（每次码率自适应执行的时候都需要分配 tile 优先级）</p>
<p></p>
</li>
</ol>
<h3 id="rate-allocation">Rate allocation</h3>
<ol>
<li>首先需要估计可用带宽（tile 场景和非 tile 场景的估计不同）</li>
<li>在一个视频段播放过程中，客户端需要去下载多个段（并行-HTTP/2）</li>
<li>带宽可以在下载单个段或多个段的平均指标中估计出来。</li>
<li>一旦带宽估计完成，码率分配将 tile 根据其优先级进行分类。</li>
<li>一开始所有的 tile 都分配成最低的优先级对应的码率，然后从高到低依次增长优先级高的 tile 的码率。</li>
<li>一旦每个 tile 的码率分配完成，将为目标带宽等于所选比特率的每个 tile 调用常规速率自适应算法</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for TBRA</title>
    <link>https://ayamir.github.io/posts/papers/note-for-tbra/</link>
    <pubDate>Tue, 21 Dec 2021 10:11:23 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-tbra/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3474085.3475590" target="_blank" rel="noopener noreffer">TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming</a></p>
<p>Level：ACM MM 21</p>
<p>Keywords：Adaptive tiling and bitrate，Mobile streaming</p>
<h2 id="创新点">创新点</h2>
<h3 id="背景">背景</h3>
<p>现有的固定的 tile 划分方式严重依赖 viewport 预测的精度，然而 viewport 预测的准确率往往变化极大，这导致基于 tile 的策略实际效果并不一定能实现其设计初衷：保证 QoE 的同时减少带宽浪费。</p>
<p>考虑同样的 viewport 预测结果与不同的 tile 划分方式组合的结果：</p>
<p></p>
<p>从上图可以看到：</p>
<ul>
<li>如果采用$6 \times 6$的分块方式，就会浪费 26，32 两个 tile 的带宽，同时 15，16，17 作为本应在实际 viewport 中的 tile 并没有分配最高的优先级去请求。</li>
<li>如果采用$5 \times 5$的分块方式，即使预测的结果与实际的 viewport 有所出入，但是得益于 tile 分块较大，所有应该被请求的 tile 都得到了最高的优先级，用户的 QoE 得到了保证。</li>
</ul>
<p>另一方面，基于 tile 的方式带来了额外的编解码开销（可以看这一篇论文：<a href="https://ayamir.github.io/2021/12/note-for-optile/" target="_blank" rel="noopener noreffer">note-for-optile</a>），而这样的性能需求对于移动设备而言是不可忽略的。</p>
<h3 id="创新">创新</h3>
<p>除了考虑常见的因素如带宽波动和缓冲区占用之外，提出同时自适应分块策略和码率分配以应对变化的 viewport 预测性能和受限的移动设备的解码能力。</p>
<h3 id="论文组织">论文组织</h3>
<ol>
<li>首先使用现实世界的轨迹分析了典型的 viewport 预测算法并确定了其性能的不确定性。</li>
<li>接着讨论了不同的分块策略在 tile 选择和解码效率上的影响。</li>
<li>自适应的分块策略可以适应 viewport 预测的错误，并能保证 tile 选择的质量。</li>
<li>为解码时间建构了分析模型，可以在给定受限的计算资源时用于选择恰当的分块策略和码率。</li>
<li>形式化了优化模型，讨论了自适应算法的细节。</li>
<li>评估证明了方案的优越性。</li>
</ol>
<h2 id="motivation">Motivation</h2>
<h3 id="分块策略对-tile-选择的影响">分块策略对 tile 选择的影响</h3>
<p>实现 4 种轻量的 viewport 预测算法：线性回归 LR、岭回归 RR、支持向量回归、长短期记忆 LSTM。</p>
<p>设置历史窗口大小为 2s，预测窗口大小为 1s；viewport 的宽度和高度分别为 100°和 90°。</p>
<p>默认的分块策略为$6 \times 6$；头部移动数据集来自<a href="https://dl.acm.org/doi/10.1145/3204949.3208139" target="_blank" rel="noopener noreffer">公开数据集</a>。</p>
<h4 id="viewport-预测的不准确性">viewport 预测的不准确性</h4>
<p>研究表明，用户的头部运动主要发生在水平方向而较少发生在垂直方向，所以只分析水平方向的预测。</p>
<p>实际的商业移动终端只有有限的传感和处理能力，并不能支持高频的 viewport 预测采样。</p>
<p>视频内容的不同类型会显著影响预测的精度，基于录像环境（室内或户外）和相机的运动状态分类。</p>
<ul>
<li>
<p>改变采样频率会直接影响 viewport 预测的精度，频率越低，精度越低。</p>
</li>
<li>
<p>相机运动的 viewport 预测错误率比相机静止的明显更高。</p>
</li>
</ul>
<h4 id="通过分块容忍预测错误">通过分块容忍预测错误</h4>
<p>因为不管 tile 的哪个部分被包含在预测的 viewport 中，只要包含一部分就会请求整个 tile，所以增大每个 tile 的尺寸能吸收预测错误。</p>
<p>实验验证：</p>
<p>设定从$4 \times 4$到$10 \times 10$的分块方式，使用不同的预测误差来检查分块设定可以容纳的最大预测误差，同时保持 tile 选择结果的相同质量。</p>
<p>用$F_1$分数来表示 tile 选择的质量：$F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall}$。</p>
<p>实验结果表明更大的 tile 尺寸更能容忍预测错误。</p>
<h3 id="分块策略对解码复杂性的影响">分块策略对解码复杂性的影响</h3>
<p>虽然当前的移动设备硬件性能发展迅速，但是实时的高码率高分辨率全景视频的解码任务还是充满挑战。</p>
<p>分块对于编码的影响：</p>
<ul>
<li>tile 越小，帧内和帧间内容的相关区域就越小，编码效率越低。</li>
</ul>
<p>直接影响解码复杂性的因素：</p>
<ul>
<li>tile 的数量。</li>
<li>视频的分辨率。</li>
<li>用于解码的资源。</li>
</ul>
<p>固定其中 1 个因素改变另外 2 个因素来检查其对解码的影响：</p>
<p></p>
<p>根据对图的观察可以得出这 3 个因素在经验上是相互独立的，因为这三幅图之中的图像几乎相同。</p>
<p>分别用$F_n(x), F_r(x), F_c(x)$表示 tile 数量、分辨率、线程数量为$x$时，解码时间与基线时间的比值。</p>
<p>将这 3 个比值作为 3 个乘子建立分析模型：
$$
D = D_0 \cdot F_n(x_1) \cdot F_r(x_2) \cdot F_c(x_3)
$$
上式表示计算整体的解码时间，其中 tile 数量为$x_1$、分辨率为$x_2$、线程数量为$x_3$；$D_0$时解码的基线时间。</p>
<p>这个模型将用于帮助做出分块和码率适应的决策。</p>
<p>注意在实际情况中，可供使用的计算资源（线程数）是受限的，需要根据设备当前可用的计算资源来分配。</p>
<h2 id="tbra-的设计">TBRA 的设计</h2>
<ul>
<li>$S = \lbrace s_1, s_2, &hellip; \rbrace$ 表示 360°视频分块方式的集合；</li>
<li>对于分块方式$s_i$，$|s_i|$ 表示这种方案中 tile 的数量；</li>
<li>当 $i &lt; j$ 时，假设 $|s_i| &lt; |s_j|$；</li>
<li>对于分块方式$s$， $b_{i, j}$ 表示第 $i$ 块的 tile $j$，$i \le 块的数量, j \le |s|$；</li>
<li>目标是确定分块方式$s$，并为每个 tile 确定其码率$b_{i, j}$；</li>
</ul>
<h3 id="分块自适应">分块自适应</h3>
<h4 id="自适应的概念">自适应的概念</h4>
<p>分块尺寸大小会导致 viewport 容错率和传输效率的变化。</p>
<ul>
<li>分块尺寸小，极端情况下每个像素点作为一个 tile，viewport 容错率最小，但是传输效率达到 100%；</li>
<li>分块尺寸大，极端情况下整个视频帧作为一个 tile，viewport 容错率最大，但是传输效率最小；</li>
</ul>
<p>优化的目标就是在这两种极端条件中找到折中的最优解。</p>
<h4 id="分块选择">分块选择</h4>
<p>以$\overline{r_d}, d \in \lbrace left, right, up, down \rbrace$为半径扩大预测区域；$e_d$表示过去 n 秒中方向 $d$ 的预测错误平均值；
$$
\overline{r_d} = (1-\alpha) \cdot \overline{r_d} + \alpha \cdot e_d
$$
预测区域的扩展被进一步用于 tile 选择，受过去预测精度的动态影响。</p>
<p>下一步检查不同分块方式，进而找到 QoE 和传输效率之间的折中。</p>
<p>对于每个分块方式，比较基于扩展的预测区域的 tile 选择的质量。使用 2 个比值作为 QoE 和传输效率的度量：
$$
Miss\ Ratio = \frac{of\ missed\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p>$$
Waste\ ratio = \frac{of\ unnecessary\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p></p>
<p>这 2 个比值的 tradeoff 可以在上图中清晰地看出。</p>
<p>使用分块方式对应的惩罚$Tiling\ i_{penalty}$来评估其性能：
$$
Tiling\ i_{penalty} = \beta \cdot Miss\ Ratio + |1/cos(\phi_i)| \cdot Waste\ Ratio
$$
$\phi_i$ 是 viewport $i$ 的中心纬度坐标，它表明随着 viewport 的垂直移动，浪费率的权重会发生变化。（因为投影方式是 ERP）</p>
<p>检查完所有的方式之后，最终选择惩罚最小的分块方式。</p>
<h3 id="码率自适应">码率自适应</h3>
<h4 id="视频质量">视频质量</h4>
<p>$w_{i, j}$表示在第 $i$ 个视频块播放时，tile $j$ 的权重；在当前方案中 $w_{i, j} = 0\ or\ 1$ 取决于 tile 是否在预测的 viewport 中。</p>
<p>$q(b_{i, j})$ 是 tile 比特率选择 $b_{i, j}$ 与用户实际感知到的质量之间的非递减映射函数。</p>
<p>第 $i$ 个视频块的质量等级可以定义为：</p>
<p>$$
Q^{(1)}_i = \sum^n_{j=1} w_{i, j} q(b_{i, j})
$$</p>
<p>使用最新研究的<a href="https://ieeexplore.ieee.org/document/8979422/citations?tabFilter=papers" target="_blank" rel="noopener noreffer">主观视频质量模型</a>：
$$
subjective\ PSNR:\ q_i = PSNR_i \cdot [M(v_i)]^{\gamma} [R(v_i)]^{\delta}
$$
$M(v_i)$ 是检测阈值；$R(v_i)$ 是视网膜滑移率；$v_i$ 是第播放 $i$ 个视频块时 viewport 的移动速度；$\gamma = 0.172, \delta = -0.267$</p>
<h4 id="质量变化">质量变化</h4>
<p>连续视频块之间的强烈质量变化会损害 QoE，定义质量变化作为响铃两个视频块之间质量的变化：
$$
Q^{(2)}_i = |Q^{(1)}_1 - Q^{(1)}_{i-1}|,\ i \in [2, m]
$$</p>
<h4 id="重缓冲时间">重缓冲时间</h4>
<p>参数设置：</p>
<ul>
<li>$C_i$ 表示下载视频块 $i$ 的预计吞吐量；</li>
<li>$B_i$ 表示客户端开始下载视频块 $i$ 时缓冲区的占用率；</li>
<li>$B_{default}$ 表示在启动阶段默认的缓冲区填充等级，记 $B_{default} = B_1$；</li>
<li>下载第 $i$ 个视频块需要时间 $\sum^n_{j=1} b_{i, j} / C_i$ ；</li>
<li>每个视频块的长度为 $L$ ；</li>
</ul>
<p>缓冲区的状态应该在每次视频块被下载的时候都得到更新，则下一个视频块 $i+1$ 的缓冲区占用情况可以计算为：
$$
B_{i+1} = max\lbrace B_1 - \sum^n_{j=1} b_{i, j} / C_i,\ 0\rbrace + L
$$
下载第 $i$ 个视频块时的重缓冲时间可以计算为：</p>
<p>$$
Q^{(3)}_i = max \lbrace \sum^n_{j=1} b_{i, j} / C_i - B_i,\ 0 \rbrace + t_{miss}
$$</p>
<p>第一部分是下载时间过长且缓冲区耗尽，视频无法播放情况下的重新缓冲时间；</p>
<p>第二部分 $t_{miss}$ 表示下载缺失的 tile 所花费的时间（在视频块播放过程中被看到但是之前没有分配码率的 tile）。</p>
<h4 id="优化目标">优化目标</h4>
<p>第 $i$ 个视频块的整体优化目标可以定义为前述 3 个指标的加权和：
$$
Q_i = pQ^{(1)}_i - qQ^{(2)}_i - rQ^{(3)}_i
$$
各个系数的符号分配表示：最大化视频质量、最小化块间质量变化、最小化重缓冲时间。</p>
<p>传统意义上使用所有视频块的平均 QoE 作为优化对象，但实际上很难获得从块 $1$ 到块 $m$ 的整个视界的完美的未来信息。</p>
<p>为了处理预测长期吞吐量和用户行为的难度，采用<a href="https://dl.acm.org/doi/10.1145/2785956.2787486" target="_blank" rel="noopener noreffer">基于 MPC 的框架</a>，在有限的范围内优化多个视频块的 QoE，最终的目标函数可以形式化为：
$$
\underset{b_{i, j}, i \in [t, t+k-1], j \in [1, n]}{max} \sum^{t+k-1}_{i=t} Q_i
$$
因为短期内的 viewport 预测性能和网络状况可以很容易得到，QoE 优化可以通过使用窗口 $[t, t+k-1]$ 内的预测信息；</p>
<p>接着将视界向前移动到 $[t+1, t+k]$ ，更新新的优化窗口的信息，为下一个视频块执行 QoE 优化，直到最后一个窗口。</p>
<p>使用基于 MPC 的公式的优点：由于受限的问题规模，每个优化问题的实例都是实际可解的。</p>
<h4 id="高效求解">高效求解</h4>
<p>提出的公式天然适合在线求解，得益于短窗口的实例问题规模很小，QoE 优化可以通过详尽搜索定期解决。</p>
<p>但是因为优化过程需要高频调用，所以对于大的搜索空间还是充满挑战。</p>
<p>为了支持实时优化，需要对搜索空间进行高效剪枝，确定几点约束：</p>
<ul>
<li>
<p>解码时间需要被约束；</p>
<p>解码时间应该短于回放长度。</p>
<p>给定移动设备上可用的计算资源，可以得到支持的最大解码线程数。</p>
<p>基于解码时间的分析模型，由于解码复杂度和分辨率的单调性，可以找到设备能够限定时间内解码的最大质量水平，这会将码率选择限制在有界搜索空间内。</p>
</li>
<li>
<p>码率选择应该考虑吞吐量的限制：$\sum^n_{j=1} b_{i, j} \le LC_i$ ；</p>
<p>不会主动耗尽缓冲区，无需让其处理吞吐量的波动。</p>
</li>
<li>
<p>码率选择应该考虑 tile 的分类；</p>
<p>tile 的码率不应该低于同一个视频块中更低权重 tile 的码率： $b_{i, j} \ge b_{i, j&rsquo;}, \forall w_{i, j} &gt; w_{i, j&rsquo;}$ 。</p>
</li>
<li>
<p>属于相同类别的 tile 比特率选择应该是同一个等级；</p>
<p>这使码率自适应在 tile 类的级别上执行而非单个 tile 的级别，大大减小了搜索空间的规模。</p>
</li>
<li>
<p>当优化窗口中的吞吐量和用户行为保持稳定时，同一个窗口中的 tile 应该有相同的结果。</p>
</li>
</ul>
<h3 id="tbra-workflow">TBRA workflow</h3>
<p></p>
<p>这样的方式需要在服务端存储大量的按照不同分块方式划分的不同码率版本的视频块，这一点可以进一步研究。</p>
<p>但是对于移动终端设备而言，这样的解决方案只引入了可以忽略不计的开销。</p>
<p>观察到 tile 自适应问题具有全局最优通常就是局部最优的特点，因此可以大大减少计算量。</p>
<p>基于 MPC 的优化 workflow 还可以有效地解决码率自适应问题。</p>
]]></description>
</item>
<item>
    <title>Note for Content Motion Viewport Prediction</title>
    <link>https://ayamir.github.io/posts/papers/note-for-content-motion-viewport-prediction/</link>
    <pubDate>Mon, 20 Dec 2021 10:47:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-content-motion-viewport-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/abs/10.1145/3328914" target="_blank" rel="noopener noreffer">Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking</a></p>
<p>Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019</p>
<p>Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model</p>
<h2 id="workflow">Workflow</h2>
<ul>
<li>Tracking：VR motion 追踪算法：应用了高斯混合模型来检测物体的运动。</li>
<li>Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户 viewport 来自动更正潜在的预测错误。</li>
<li>Update：viewport 动态更新算法：动态调整预测的 viewport 大小去覆盖感兴趣的潜在 viewport，同时尽可能保证最低的带宽消耗。</li>
<li>Evaluation：经验用户/视频评估：构建 VR viewport 预测方法原型，使用经验 360°视频和代表性的头部移动数据集评估。</li>
</ul>
<h2 id="全景直播推流的预备知识">全景直播推流的预备知识</h2>
<h3 id="vr-推流直播">VR 推流直播</h3>
<p></p>
<p>相比于传统的 2D 视频推流的特别之处：</p>
<ul>
<li>VR 系统是交互式的，viewport 的选择权在客户端；</li>
<li>呈现给用户的最终视图是整个视频的一部分；</li>
</ul>
<h3 id="用户头部移动的模式">用户头部移动的模式</h3>
<p>在大量的 360°视频观看过程中，用户主要的头部移动模式有 4 种，使用$i-j\ move$来表示；</p>
<p>其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。</p>
<ul>
<li>$1-1\ move$：单个物体以单一方向移动；</li>
<li>$1-n\ move$：单个物体以多个方向移动；</li>
<li>$m-n\ move$：多个物体以多个方向移动；</li>
<li>$Arbitrary\ move$：用户不跟随任何感兴趣的物体而移动，viewport 切换随机；</li>
</ul>
<p></p>
<p>现有的直播 VR 推流中的 viewport 预测方法是基于速度的方式，这种方式只对$1-1\ move$这一种模式有效。</p>
<p>本方案的目标是提出对 4 种模式都有效的预测策略。</p>
<h2 id="系统架构">系统架构</h2>
<p></p>
<h3 id="理论创新">理论创新</h3>
<ul>
<li>
<p>核心功能模块：</p>
<ol>
<li>
<p>motion detection：区分运动物体与静止的背景。</p>
</li>
<li>
<p>feature selection：选择代表性的特征并对运动物体做追踪。</p>
<p>这两个模块使系统能识别用户可能感兴趣的 viewport。</p>
</li>
</ol>
</li>
<li>
<p>使用贝叶斯方法分析用户观看行为并形式化用户的兴趣模型。</p>
<ol>
<li>
<p>使用错误恢复机制来使当预测错误被检测到时的预测 viewport 去适应实际的 viewport，尽管不能消除预测错误但是能避免在此基础上进一步的预测错误。</p>
</li>
<li>
<p>使用动态 viewport 更新算法来产生大小可变的 viewport，通过同时考虑跟踪到的 viewport 轨迹和用户当前的速度（矢量）。</p>
<p>这样，即使用户的运动模式很复杂也能有更高的概率去覆盖潜在的视图。</p>
</li>
</ol>
</li>
</ul>
<h3 id="具体实施">具体实施</h3>
<ul>
<li>
<p>虽然提出的运动追踪和错误处理机制是计算密集型的任务，但是这些组件都部署在 video packager 中，运行在服务端。</p>
</li>
<li>
<p>将生成 VR 视图的工作负载移动到服务端，进一步减少了客户端的计算开销以及网络开销。</p>
</li>
</ul>
<h2 id="形式化">形式化</h2>
<h3 id="基于运动轨迹的-viewport-预测">基于运动轨迹的 viewport 预测</h3>
<p>使用<a href="https://ieeexplore.ieee.org/document/1333992" target="_blank" rel="noopener noreffer">GMM</a>完成运动检测，使用<a href="https://ieeexplore.ieee.org/document/323794" target="_blank" rel="noopener noreffer">Shi-Tomasi algorithm</a>解决运动轨迹跟踪问题。</p>
<p></p>
<ol>
<li>
<p>运动检测</p>
<p>GMM 前景提取</p>
</li>
<li>
<p>特征选取与过滤</p>
<p>采用 Shi-Tomasi algorithm 从视频中检测代表性的特征，直接检测得到的代表性特征数量较多而难以追踪。</p>
<p>采用两种过滤的方法来减少要追踪的特征数量。</p>
<ul>
<li>
<p>比较当前帧和前一帧的特征，只保留其共有的部分。</p>
</li>
<li>
<p>采用第 1 步中运动检测的方式，只保留运动的部分。</p>
</li>
</ul>
</li>
<li>
<p>viewport 生成</p>
<p>经过选择和过滤之后的特征通常分布在不能被单一用户视图所覆盖的广阔区域中。</p>
<p>在整个 360°视频中可能存在多个运动的物体，即$m-n\ move$。</p>
<p>提出一种系统的方式来产生用户最可能跟随观看的 viewport。</p>
<p>直觉是用户更可能将大部分注意力放在两种类型的物体上：</p>
<ul>
<li>离用户更近的物体。</li>
<li>就物理形状而言更“重要”的物体。</li>
</ul>
<p>这两种类型的物体大多包含最密集和最大量的特征，因此通过所有特征的重心来计算预测用户视图的中心。</p>
<p>对于剩余的特征列表：$\vec{F} = [f_1, f_2, f_3, &hellip;, f_k]$，其中$f_i(i = 1 &hellip; k)$表示特征$f_i = &lt;f^{(x)}_i, f^{(y)}_i&gt;$的像素点坐标，则预测出的 viewport 中心坐标可以计算出来：
$$
l_x = \frac{1}{k} \sum^k_{i=1} f^{(x)}_i;\ l_y = \frac{1}{k} \sum^k_{i=1} f^{(y)}_i.
$$
考虑到即使预测的 viewport 中包含用户观看的物体，预测得到的 viewport 也可能会与实际的 viewport 存在差异。</p>
<p>所以预测的 viewport 可能比实际的 viewport 要大，所以使用缩放因子$S_c$来产生预测的 viewport。</p>
<p>给出用户 viewport 的大小$S_{user}$，预测的 viewport 可以通过$S_{pre} = S_c \cdot S_{user}$计算出来。</p>
</li>
</ol>
<h3 id="基于用户反馈的错误恢复">基于用户反馈的错误恢复</h3>
<p>video packager 可以通过 HMD 和 web 服务器通过反向路径从用户处检索用户实际视图的反馈信息。</p>
<p>基于反馈的错误恢复机制在以下两种场景中表现良好：</p>
<ol>
<li>
<p>没有运动的物体</p>
<p>如果没有检测到运动的物体，则用户很可能是在观看静止的物体，这会导致基于运动目标的 viewport 预测失败。</p>
<p>在这种场景中，可以认为视频内容已经不再是决定用户 viewport 的因素，而只取决于用户自身的行为。</p>
<p>因此采用基于速度的方式来预测 viewport。（这样的决策可以在运动检测模块没有检测到运动物体时就做出）</p>
<p>一旦从反馈路径上得到用户信息，可以产生用户 viewport 位置向量：$\vec{L} = [l_1, l_2, l_3, &hellip;, l_M]$，其中$l_i$表示第$i$个帧中用户 viewport 的位置，$M$表示视频播放缓冲区中的帧数。那么可以计算 viewport 速度：
$$
\vec{V} = \frac{\vec{(l_2 - l_1)} + \vec{(l_3 - l_2)} &hellip;.(l_M - l_{M-1})}{M-1} = \frac{(\vec{l_M - l_1})}{M-1}
$$
下一帧的预测位置$L_{M=1}$也可以计算出来：
$$
l_{M+1} = l_M + \vec{V}
$$</p>
</li>
<li>
<p>预测视图与实际视图的不匹配</p>
<p>一旦运动追踪策略检测到用户实际的视图和预测的视图不同，就会触发恢复机制去追踪用户实际在看着的物体。</p>
<p>可以使用运动追踪方式确定用户实际观察的物体的速度。</p>
<p>给出前一帧匹配的特征$\vec{FA} = [fA_1, fA_2, fA_3, &hellip;, fA_p]$和当前帧的特征$\vec{FB} = [fB_1, fB_2, fB_3, &hellip;, fB_p]$，可以计算出速度：
$$
V_x = \frac{1}{p} (\sum^p_{i=1} fB^{(x)}_i - \sum^p_{i=1}fA^{(x)}_i),\
V_y = \frac{1}{p} (\sum^p_{i=1} fB^{(y)}_i - \sum^p_{i=1}fA^{(y)}_i),
$$
假设预测的 viewpoint 是$(l_x, l_y)$，修改之后的 viewpoint 是$(l_x + V_x,\ l_y + V_y)$。</p>
</li>
</ol>
<h3 id="动态-viewport-更新">动态 viewport 更新</h3>
<p>前述的错误恢复机制发生在 viewport 预测错误出现之后，任务是避免未来更多的错误。</p>
<p>动态的 viewport 更新则努力避免 viewport 预测错误。</p>
<p>关键思想是扩大预测的 viewport 大小，以高概率去覆盖$m-n\ move$和$arbitrary\ move$下所有潜在的运动目标；更重要的是动态调整视图的大小去获得更高效的带宽利用率。</p>
<ul>
<li>
<p>对于一个 360°全景视频，将 360°的帧均分为$N = n \times n$个网格，每个网格看作是一个 tile，预测的 viewport 即为$N$个 tile 的子集。</p>
</li>
<li>
<p>使用贝叶斯方法分析用户的观看行为，每个 tile 分配一个独立的贝叶斯模型，所以每个 tile 可以独立更新。</p>
</li>
<li>
<p>设$X$表示用户 viewport，$Y$表示静态内容，$Z$表示运动物体。</p>
</li>
<li>
<p>未来的用户 viewport 可以以条件概率计算为$P(X|Y,\ Z)$，$Y$与$Z$相互独立。</p>
</li>
<li>
<p>用户的 viewport 可以通过反馈信息得出$P(X)$；用户观看静态特征可以表示为$P(X|Y)$；用户观看动态特征可以表示为$P(X|Z)$。</p>
</li>
<li>
<p>$P(X|Y, Z)$可以计算为：
$$
P(X|Y, Z) = \frac{P(Y|X) \cdot P(Z|X) \cdot P(X)}{P(Y, Z)}
$$</p>
</li>
<li>
<p>只要用户开始观看，对于 tile $T_i$，就能得到其先验概率$P(Y_i|X_i)$和$P(Z_i|X_i)$，进而根据贝叶斯模型计算出$P(X|Y, Z)$。</p>
</li>
</ul>
<p>为每个 tile 定义两种属性：</p>
<ol>
<li>当前状态：表示此 tile 是否属于预测的 viewport（属于标记为$PREDICTED$，不属于标记为$NONPREDICTED$）。</li>
<li>生存期：表示此 tile 会在 view port 中存在多长时间（例如定义 3 种等级：$ZERO$，$MEDIUM$，$HIGH$，实际的定义划分可以根据具体的用户和视频设定）。</li>
</ol>
<h2 id="预测步骤">预测步骤</h2>
<p>按照形式化中提出的 3 步，分为系统初始化、帧级别的更新、缓冲区级别的更新。</p>
<ol>
<li>
<p>系统初始化</p>
<p>初始化阶段中，view 更新算法将所有的$N$个 tile 标注为$PREDICTED$，并将生存期设置为$MEDIUM$，即系统向用户发送完整的一帧作为自举。</p>
<p>这样设定的原因在于：当用户第一次启动视频会话时，允许“环视”类型的移动，这可能会覆盖 360°帧的任意 viewport。</p>
</li>
<li>
<p>帧级别的更新</p>
<p>给定一帧，应用修改后的 motion 追踪算法在运动区域中选择特征，而不使用特征的密度做进一步的过滤。</p>
<p>使用有多个 tile 的多个视图来覆盖一个放大的区域，该区域包含作为预测 viewport 的移动对象上的所有特征，这样就能适应$m-n\ move$中的用户行为。</p>
<p>设计帧级别的算法标记选择的 tile 作为$PREDICTED$并设置其生存期为$HIGH$（直觉上讲运动中的物体或用户所感兴趣的静态特征会更以长时间保留在 viewport 之中）。</p>
</li>
<li>
<p>缓冲区级别的更新</p>
<p>以缓冲区长度为间隔检索用户的实际视图，基于此可以对 tile 的两种属性做出调整。</p>
<ol>
<li>对于与用户实际视图重叠的 tile，设置为$PREDICTED$和$HIGH$。</li>
<li>对于用户实际视图没有出现但出现在预测的视图中的 tile，生存期减 1，如果生存期减为$ZERO$，就重设其状态为$NONPREDICTED$，将其从预测的 viewport 中移除。</li>
</ol>
<p></p>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for RnnQoE</title>
    <link>https://ayamir.github.io/posts/papers/note-for-rnnQoE/</link>
    <pubDate>Thu, 16 Dec 2021 19:53:10 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-rnnQoE/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9580281" target="_blank" rel="noopener noreffer">QoE-driven Mobile 360 Video Streaming: Predictive
View Generation and Dynamic Tile Selection</a></p>
<p>Level：ICCC 2021</p>
<p>Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles</p>
<h2 id="系统建模与形式化">系统建模与形式化</h2>
<h3 id="视频划分">视频划分</h3>
<p>先将视频划分成片段：$\Iota = {1, 2, &hellip;, I}$表示片段数为$I$的片段集合。</p>
<p>接着将片段在空间上均匀划分成$M \times N$个 tile，FOV 由被用户看到的 tile 所确定。</p>
<p>使用 ERP 投影，$(\phi_i, \theta_i),\ \phi_i \in (-180\degree, 180\degree], \theta_i \in (-90\degree, 90\degree]$来表示用户在第$i$个片段中的视点坐标。</p>
<p>播放过程中记录用户头部运动的轨迹，积累的数据可以用于 FOV 预测。</p>
<p>跨用户之间的 FOV 轨迹可以用于提高预测精度。</p>
<h3 id="qoe-模型">QoE 模型</h3>
<ul>
<li>
<p>前提</p>
<p>视频编解码器预先确定，无法调整每个 tile 的码率。</p>
</li>
<li>
<p>实现</p>
<ol>
<li>每个 tile 都以不同的码率编码成不同的版本。</li>
<li>每个 tile 都有两种分辨率的版本。</li>
</ol>
</li>
<li>
<p>QoE 内容</p>
<p>客户端收到的视频质量和观看时的卡顿时间。</p>
</li>
<li>
<p>质量形式化</p>
<p>对于每个片段$i \in \Iota$，$S_i = {\tau_{i, j}}_{j=1}^{M \times N}$是用来表示用户实际看到的 tile 的集合的向量。</p>
<p>$\tau_{i, j} = 1$表示第$i$个段中的第$j$个 tile 被看到；$\tau_{i, j} = 0$表示未被看到。</p>
<p>同样的， $\tilde{S}_i = {\tilde{\tau}_{i, j}}_{j = 1}^{M \times N}$ 表示经过 FOV 预测和 tile 选择之后成功被传送到用户头戴设备上的 tile 集合的向量。</p>
<p>$\tilde{\tau}_{i, j} = 1$表示第$i$个段中的第$j$个 tile 被用户接收；$\tilde{\tau}_{i, j} = 0$表示未被接收。</p>
<p>第$i$个段的可感知到的质量可以表示为：</p>
<p>$$
Q_i = \sum_{j = 1}^{M \times N} p_{i, j}b_{i, j}\tau_{i, j}\tilde{\tau}_{i, j}
$$</p>
<p>$b_{i, j}$表示第$i$个片段的第$j$个 tile 的码率；$p_{i, j}$表示对不同位置 tile 所分配的权重；</p>
</li>
<li>
<p>关于权重$p_{i, j}$</p>
<p>研究表明用户在全景视频 FOV 中的注意力分配并不是均等的，越靠近 FOV 中心的 tile 对用户的 QoE 贡献越大。</p>
<p>下面讨论单个片段的情况：用$(\phi_j, \theta_j)$表示 tile 中心点的坐标，并映射到笛卡尔坐标系上$(x_j, y_j, z_j)$：</p>
<p>$$
x_j = cos\theta_jcos\phi_j,\ y_j = sin\theta_j,\ z_j = -cos\theta_jsin\phi_j
$$</p>
<p>则两个 tile 之间的半径距离$d_{j, j&rsquo;}$可以表示为：</p>
<p>$$
d_{j, j&rsquo;} = arccos(x_j x_{j&rsquo;} + y_j y_{j&rsquo;} + z_j z_{j&rsquo;})
$$</p>
<p>对于第$i$个片段，假设用户 FOV 中心的 tile 为$j^*$，那么第$j$个 tile 的权重可以计算出来：</p>
<p>$$
p_{i, j} = (1 - d_{j, j^*} / \pi) \tau_{i, j}
$$</p>
</li>
<li>
<p>卡顿时间形式化</p>
<p>当$\tilde{\tau}_{i, j}$与$\tau_{i, j}$出现分歧时，用户就不能成功收到请求的 tile，头戴设备中显示的内容就会被冻结，由此导致卡顿。</p>
<p>对于任意的片段$i \in \Iota$，相应的卡顿时间$D_i$可以计算出来：</p>
<p>$$
D_i = \frac{\sum_{j = 1}^{M \times N} b_{i, j} \cdot [\tau_{i, j} - \tilde{\tau}_{i, j}]^+}{\xi}
$$</p>
<p>$[x]^+ = max \lbrace x, 0 \rbrace $；$\xi$表示可用的网络资源（已知，并且在推流过程中保持为常数）</p>
<p>卡顿发生于在播放时，用户 FOV 内的 tile 还没有被传输到用户头戴设备中的时刻，终止于所有 FOV 内 tile 被成功传送的时刻。</p>
</li>
<li>
<p>质量与卡顿时间的结合</p>
<p>$$
max\ QoE = \sum_{i = 1}^I (Q_i - wD_i)
$$</p>
<p>$w$表示卡顿事件的惩罚权重。例如，w＝1000 意味着 1 秒视频暂停接收的 QoE 惩罚与将片段的比特率降低 1000 bps 相同。</p>
</li>
</ul>
<h2 id="联合-viewport-预测与-tile-选择">联合 viewport 预测与 tile 选择</h2>
<p>联合框架包括 viewport 预测和动态 tile 选择两个阶段。</p>
<p>viewport 预测阶段集成带有注意力机制的 RNN，接收用户的历史头部移动信息作为输入，输出每个 tile 出现在 FOV 中的可能性分布。</p>
<p>选择 tile 阶段为预测的输出建立的上下文空间，基于上下文赌博机学习算法来选择 tile 并确定所选 tile 的质量版本。</p>
<p></p>
<p></p>
<h3 id="viewport-预测">Viewport 预测</h3>
<p>FOV 预测问题可以看作是序列预测问题。</p>
<p>不同用户观看相同视频时的头部移动轨迹有强相关性，所以跨用户的行为分析可以用于提高新用户的 viewport 预测精度。</p>
<p>被广泛使用的 LSTM 的变体——Bi-LSTM（Bi-directional LSTM）用于 FOV 预测。</p>
<ol>
<li>
<p>输入参数构造</p>
<p>为了构造 Bi-LSTM 学习网络，需要对不同用户的 viewpoint 特性作表征。</p>
<ul>
<li>
<p>在用户观看事先划分好的$I$个片段时，记录每个片段对应的 viewpoint 坐标：</p>
<p>$\Phi_{1:I} = {\phi_i}^I_{i = 1},\ \Theta_{1:I} = {\theta_i}^I_{i=1}$</p>
</li>
<li>
<p>预测时使用的历史信息的窗口大小记为$k$；</p>
</li>
<li>
<p>对于第$i, (i &gt; k)$个片段，相应的 viewpoint 特性由$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$所给出；</p>
</li>
<li>
<p>列索引$m_i$和行索引$n_i$作为 viewpoint tile $(\phi_i, \theta_i)$的标签，由<a href="https://zh.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener noreffer">独热编码</a>表示；</p>
</li>
<li>
<p>通过滑动预测的窗口，所看到的视频片段的特性和标签可以被获取。</p>
</li>
</ul>
</li>
<li>
<p>LSTM 网络构造</p>
<p>整个网络包含 3 层：</p>
<ul>
<li>遗忘门层决定丢弃哪些信息；</li>
<li>更新门层决定哪类信息需要存储；</li>
<li>输出门层过滤输出信息。</li>
</ul>
<p>为了预测用户在第$i$个段的 viewpoint，LSTM 网络接受$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$作为输入；输出行列索引；</p>
<p>$$
m_i = LSTM(\theta_{i-k}, &hellip;, \phi_{i-1}; \alpha)
$$</p>
<p>$$
n_i = LSTM(\theta_{i-k}, &hellip;, \theta_{i-1}; \beta)
$$</p>
<p>$\alpha, \beta$是学习网络的参数；分类交叉熵被用作网络训练的损失函数。</p>
</li>
<li>
<p>Bi-LSTM 的特殊构造</p>
<ul>
<li>
<p>将公共单向的 LSTM 划分成 2 个方向。</p>
<p>当前片段的输出利用前向和反向信息，这为网络提供了额外的上下文，加速了学习过程。</p>
</li>
<li>
<p>双向的 LSTM 不直接连接，不共享参数。</p>
</li>
<li>
<p>每个时间槽的输入会被分别传输到前向和反向的 LSTM 中，并分别根据其状态产生输出。</p>
</li>
<li>
<p>两个输出直接连接到 Bi-LSTM 的输出节点。</p>
</li>
<li>
<p>引入注意力机制为每步时间自动分配权重，从大量信息中选择性地筛选出重要信息。</p>
</li>
<li>
<p>将 Softmax 层堆叠在网络顶部，以获取不同 tile 的 viewpoint 概率。</p>
</li>
</ul>
</li>
</ol>
<p></p>
<h3 id="动态-tile-选择">动态 tile 选择</h3>
<p>使用上下文赌博机学习算法来补偿 viewport 预测错误对 QoE 造成的影响。</p>
<ul>
<li>
<p>上下文赌博机学习算法概况</p>
<p>上下文赌博机学习算法是一个基于特征的 exploration-exploitation 技术。</p>
<p>通过在多条手臂上重复执行选择过程，可以获得在不同上下文中的每条手臂的回报。</p>
<p>通过 exploration-exploitation，目标是最大化累积的回报。</p>
</li>
<li>
<p>组成部分形式化</p>
<ol>
<li>
<p>上下文</p>
<p>直觉上讲，当预测的 viewpoint 不够精确时，需要扩大 FOV 并选择更多的 tile 进行传输。</p>
<p>为了做出第$i$个片段上的预测 viewpoint 填充决策，定义串联的上下文向量：</p>
<p>$c_i = [f^s_i, f^c_i]$，$f^s_i$表示自预测的上下文，$f^c_i$表示跨用户之间的预测上下文。</p>
<p>预测输出的用户$u$的 viewpoint tile 索引用$[\tilde{m}^u_{i-1}, \tilde{n}^u_{i-1}]$表示；</p>
<p>实际的用户$u$的 viewpoint tile 索引用$[m_{i-1}^u, n_{i-1}^u]$表示；</p>
<p>那么对第$i$个片段而言，自预测的上下文可以计算出来：</p>
<p>$$
f_i^s = [|m_{i-1}^u - \tilde{m}^u_{i-1}|, |n_{i-1}^u - \tilde{n}^u_{i-1}|]
$$
跨用户的上下文信息获取：使用 KNN 准则选择一组用户，其在前$k$个片段中的轨迹最接近用户$u$的轨迹。</p>
<p>使用$\zeta$表示获得的用户集合，使用</p>
<p>$$E_{\zeta_u}(m_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u} |m_i^u - \tilde{m}_i^u|$$</p>
<p>$$E_{\zeta_u}(n_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u}|n_i^u - \tilde{n}_i^u|$$</p>
<p>表示预测误差，则：</p>
<p>$$
f_i^u = [E_{\zeta_u}(m_i), E_{\zeta_u}(n_i)]
$$</p>
</li>
<li>
<p>手臂</p>
<p>根据从第一个阶段得到的每个 tile 的可能性分布，所有的 tile 可以用倒序的方式排列。</p>
<p>最高可能性的 tile 被看作 FOV 的中心，高码率以此 tile 为中心分配。</p>
<p>剩余的带宽用于扩展 FOV，低可能性的 tile 被顺序选择来扩展 FOV 直至带宽耗尽。</p>
<p>手臂的状态$a \in {0, 1}$表示 tile 选择的策略：</p>
<ul>
<li>
<p>$a = 0$表示 viewpoint 预测准确，填充 tile 分配了高质量；</p>
</li>
<li>
<p>$a = 1$表示 viewpoint 预测不准确，填充 tile 分配的质量较低，为了传送尽可能多的 tile 而减少卡顿；</p>
</li>
</ul>
</li>
<li>
<p>回报</p>
<p>给定上下文$c_i$，选择手臂$a$，预期的回报$r_{i, a}$建模为$c_i$和$a$组合的线性函数：</p>
<p>$$
\Epsilon[r_{i, a}|c_{i, a}] = c_{i, a}^T \theta_a^*
$$</p>
<p>未知参数$\theta_a$表示每个手臂的特性，目标是为第$i$个片段选择最优的手臂：</p>
<p>$$
a_i^* = \underset{a}{argmax}\ c_{i, a}^T \theta_a^*
$$</p>
<p>使用<a href="https://zhuanlan.zhihu.com/p/38875273" target="_blank" rel="noopener noreffer">LinUCB</a>算法做出特征向量的精确估计并获取$\theta_a^*$。</p>
<p></p>
</li>
</ol>
</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>评估准备</p>
<ul>
<li>使用现有的<a href="https://github.com/xuyanyu-shh/VR-EyeTracking" target="_blank" rel="noopener noreffer">viewpoint 轨迹数据集</a>，所有视频被编码为至少每秒 25 帧，长度为 20 到 60 秒；</li>
<li>视频每个片段被划分为$6 \times 12$的 tile，每个的角度是$30\degree \times 30\degree$；</li>
<li>初始 FOV 设定为$90\degree \times 90\degree$，在 viewpoint 周围是$3 \times 3$的 tile；</li>
<li>每个片段的长度为 500ms；</li>
<li>默认的预测滑动窗口大小$k = 5$；</li>
<li>HD 和 LD 版本分别以按照 HEVC 的$QP={32, 22}$的参数编码而得到；</li>
<li>训练集和测试集的比例为$7:3$；</li>
<li>Bi-LSTM 层配置有 128 个隐单元；</li>
<li>batch 大小为 64；</li>
<li>epoch 次数为 60；</li>
</ul>
</li>
<li>
<p>性能参数</p>
<ul>
<li>
<p>预测精度</p>
</li>
<li>
<p>视频质量</p>
<p>由传送给用户的有效码率决定：例如实际 FOV 中的 tile 码率总和</p>
</li>
<li>
<p>卡顿时间</p>
</li>
<li>
<p>规范化的 QoE</p>
<p>实际取得的 QoE 与在 viewpoint 轨迹已知情况下的 QoE 的比值</p>
</li>
</ul>
</li>
<li>
<p>对比目标</p>
<ul>
<li>预测阶段——预测精度
<ol>
<li>LSTM</li>
<li>LR</li>
<li>KNN</li>
</ol>
</li>
<li>取 tile 的阶段——规范化的 QoE
<ol>
<li>两个阶段都使用纯 LR</li>
<li>只预测而不做动态选择</li>
</ol>
</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for OpTile</title>
    <link>https://ayamir.github.io/posts/papers/note-for-optile/</link>
    <pubDate>Mon, 13 Dec 2021 16:19:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-optile/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3123266.3123339" target="_blank" rel="noopener noreffer">OpTile: Toward Optimal Tiling in 360-degree Video Streaming</a></p>
<p>Level：ACM MM 17</p>
<p>Keyword：Dynamic tile division, Optimize encoding efficiency, Optimize tile size</p>
<h2 id="背景知识">背景知识</h2>
<h3 id="编码过程概述">编码过程概述</h3>
<ol>
<li>
<p>对一帧图像中的每一个 block，编码算法在当前帧的已解码部分或由解码器缓冲的临近的帧中搜索类似的 block。</p>
<p>当编码器在邻近的帧中找到一个 block 与当前 block 紧密匹配时，它会将这个类似的 block 编码进一个动作向量中。</p>
</li>
<li>
<p>编码器计算当前 block 和引用 block 之间像素点的差异，通过应用变换（如离散余弦变换），量化变换系数以及对剩余稀疏矩阵系数集应用无损熵编码（如 Huffman 编码）对计算出的差异进行编码。</p>
</li>
</ol>
<h3 id="对编码过程的影响">对编码过程的影响</h3>
<ol>
<li>基于 tile 的方式会减少可用于拷贝的 block 数量，增大了可供匹配的 tile 之间的距离。</li>
<li>不同的投影方式会影响编码变换输出的系数稀疏性，而这会降低视频编码效率。</li>
</ol>
<h3 id="投影过程">投影过程</h3>
<p>因为直接对 360 度图像和视频的编码技术还没有成熟，所以 360 度推流系统目前还需要先将 3D 球面投影到 2D 平面上。</p>
<p>目前应用最广的投影技术主要是 ERP 和 CMP，分别被 YouTube 和 Meta 采用。</p>
<h4 id="erp-投影">ERP 投影</h4>
<p>基于球面上点的左右偏航角$\theta$与上下俯仰角$\phi$将其映射到宽高分别为$W$和$H$的矩形上。</p>
<p>对于平面坐标为$(x, y)$的点，其球面坐标分别为：</p>
<p>$$
\theta = (\frac{x}{W} - 0.5) * 360
$$</p>
<p>$$
\phi = (0.5 - \frac{y}{H}) * 180
$$</p>
<h4 id="cmp-投影">CMP 投影</h4>
<p>将球面置于一个立方体中，光线从球心向外发射，并分别与球面和立方体相交于两点，这两点之间便建立了映射关系。</p>
<p>之后将立方体 6 个平面拼接成矩形，就可以使用标准的视频编码方式进行压缩。</p>
<p>关于投影方式还可以参考这里的讲解：<a href="https://zhuanlan.zhihu.com/p/106922217" target="_blank" rel="noopener noreffer">谈谈全景视频投影方式</a></p>
<h3 id="tile-方式的缺点">tile 方式的缺点</h3>
<ul>
<li>
<p>降低编码效率</p>
<p>tile 划分越细，编码越低效</p>
</li>
<li>
<p>增加更大的整体存储需求</p>
</li>
<li>
<p>可能要求更多的带宽</p>
</li>
</ul>
<h2 id="optile-的设计">OpTile 的设计</h2>
<p>直觉上需要增大一些 tile 的大小来使与这些 tile 相关联的片段能捕获高效编码所需的类似块。</p>
<p>同时也需要 tile 来分割视频帧来减少传输过程中造成的带宽浪费。</p>
<ul>
<li>
<p>为了明白哪些片段的空间部分可以被高效独立编码，需要关于 tile 的存储大小的不同维度的信息。</p>
</li>
<li>
<p>为了找到切分视频的最好位置，需要在片段播放过程中用户 viewport 运动轨迹的偏好。</p>
</li>
</ul>
<p>将编码效率和浪费数据的竞争考虑到同一个问题之中，这个问题关注的是<strong>一个片段中所有可能的视图的分布</strong>。</p>
<p>片段的每个可能的视图可以被 tile 的不同组合所覆盖。</p>
<p>目标是为一个片段选择一个 tile 覆盖层，以<strong>最小化固定时间段内视图分布的总传输带宽</strong>。</p>
<ul>
<li>目标分离的部分考虑整个固定时间段的表示（representation）的存储开销。</li>
<li>目标的存储部分与下载的带宽部分相竞争。例如，如果一个不受欢迎的视频一年只观看一次，那么我们更喜欢一个紧凑的表示，我们可以期望向用户发送更多未观看的像素。</li>
</ul>
<h2 id="问题形式化">问题形式化</h2>
<table>
<thead>
<tr>
<th style="text-align:center">segment/片段</th>
<th style="text-align:center">推流过程中可以被下载的连续播放的视频单元</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">basic sub-rectangle/基本子矩形</td>
<td style="text-align:center">推流过程中可以被下载的片段中最小的空间划分块</td>
</tr>
<tr>
<td style="text-align:center">solution sub-rectangle/解子矩形</td>
<td style="text-align:center">片段中由若干基本子矩形组成的任何矩形部分</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">$x$</th>
<th style="text-align:center">用于表示子矩形在解中的存在的二元向量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$c^{(stor)}$</td>
<td style="text-align:center">每个子矩形存储开销相关的向量</td>
</tr>
<tr>
<td style="text-align:center">$c^{(view)}$</td>
<td style="text-align:center">给定一个 segment 中用户 viewport 的分布，$c^{(view)}$指相关子矩形的预期下载字节</td>
</tr>
<tr>
<td style="text-align:center">$\alpha$</td>
<td style="text-align:center">分配到$c^{(view)}$的权重，以此来控制相对于传输一个片段的存储开销</td>
</tr>
</tbody>
</table>
<p>考虑将 1 个矩形片段划分成 4 个基本子矩形，其对应的坐标如下：</p>
<p></p>
<p>4 个基本子矩形可以有 9 种分配方式，成为解子矩形，如下（因为需要保持对应的空间关系，所以只有 9 种）：</p>
<p></p>
<ul>
<li>
<p>$x$的形式化</p>
<p>可以用向量$x$来分别表示上图中子矩形在解中的存在：</p>
<p>$$
[1 \times 1\ at\ (0, 0),\ 1 \times 1\ at\ (0, 1),\ 1 \times 1\ at\ (1, 0),
\
1 \times 1\ at\ (1, 1),\ 1 \times 2\ at\ (0, 0),\ 1 \times 2\ at\ (1, 0),
\
2 \times 1\ at\ (0, 0),\ 2 \times 1\ at\ (0,1),\ 2 \times 2\ at\ (0,0).]
$$</p>
<p>（$x$中每个二元变量的的组成：$1 \times 1$表示子矩形的形状，$(0,0)$表示所处的位置）</p>
<p>要使$x$有效，<strong>每个基本子矩形必须被$x$中编码的子矩形精确覆盖一次</strong>。例如：</p>
<ul>
<li>
<p>$[0, 0, 0, 0, 1, 1, 0, 0, 0]$=&gt;有效（第 5 和第 6 次序的位置分别对应$e$和$f$子矩形，恰好覆盖了所有基本子矩形 1 次）</p>
</li>
<li>
<p>$[0,0,0,1,1,0,0,0,0]$=&gt;无效（第 4 和第 5 次序的位置分别对应$d$和$e$子矩形，没有覆盖到$(1,0)$基本子矩形）</p>
</li>
<li>
<p>$[0,0,0,1,1,1,0,0,0]$=&gt;无效（第 4、第 5 和第 6 次序的位置分别对应$d$、$e$和$f$子矩形，$(1,1)$基本子矩形被覆盖了两次）</p>
</li>
</ul>
</li>
<li>
<p>$c^{(stor)}$的形式化</p>
<p>与每个$x$相对应的向量$c^{(stor)}$长度与$x$相等，其中每个元素是$x$中对应位置的子矩形的存储开销的估计值。</p>
</li>
<li>
<p>$c^{(view)}$的形式化</p>
<p>考虑分发子矩形的网络带宽开销时，需要考虑所有可能被分发的 360 度表面的视图。</p>
<p>为了简化问题，将片段所有可能的视图离散化到一个大小为$V$的集合中。</p>
<p>集合中每个元素表示一个<strong>事件</strong>，即向观看 360 度视频片段的用户显示基本子矩形的唯一子集。</p>
<p>注意到片段中被看到的视频区域可以包含来自多个视角的区域。</p>
<p>将之前离散化好的大小为$V$的集合中每个元素与可能性相关联：$[p_1, p_2, &hellip;, p_V]$。</p>
<p>考虑为给定的解下载视图$V$的开销，作为需要为该视图下载的数据量：</p>
<p>$$
quantity = x^{\top}diag(d_v)c^{(stor)}
$$</p>
<p>$d_v$是一个二元向量，其内容是按照$x$所描述的表示方案，对所有覆盖视图的子矩形的选择。</p>
<p>例如对于 ERP 投影中位置坐标为$yaw = 0, pitch = 90$即处于等矩形顶部的图像，对应的$d_{view-(0, 90)} = [1, 1, 0, 0, 1, 0, 1, 1, 1]$</p>
<p>（即上面图中$a, b, e, g, h, i$位置的子矩形包含此视图所需的基本子矩形）。</p>
<p>给出一个片段中的用户 viewport 分布，$c^{(view)}$的元素是相关联的子矩形预期的下载字节。</p>
<p>$$
c^{(view)} = \sum_v p_v diag(d_v) c^{(stor)}
$$</p>
<p>最后，将优化问题的基本子矩形覆盖约束编码为矩阵$A$。</p>
<p>$A$是一个列中包含给定子矩形解所覆盖的基本子矩形信息的二元矩阵。</p>
<p>对于$2 \times 2$的矩形片段，其$A$有 4 行 9 列，例子如下：</p>
<p></p>
<p>因此最终的问题可以形式化为一个整数线性程序：</p>
<p></p>
<ul>
<li>
<p>$c^{(stor)}$</p>
<p>可以理解为存储一段$\Delta t$时间长的片段的子矩形的存储开销；</p>
</li>
<li>
<p>$c^{(view)}$</p>
<p>可以理解为传输一个视图所需要的所有的子矩形的传输开销。</p>
</li>
<li>
<p>$\alpha$</p>
<p>控制相比于传输一个片段的相对存储开销，同时应该考虑片段的流行度。</p>
<p>即$\alpha$应该与所期望的片段在$\Delta t$的时间间隔内的下载次数成比例，$\alpha$应该可以通过经验测量以合理的精度进行估计。</p>
<p>可以通过将$x$的二元离散限制放松到$0 \le x_i \le 1\ \forall i$构成一个线性程序，其解为整数。</p>
<p>（对于有 33516 个变量的$x$，其解可以在单核 CPU 上用 7~10 秒求出）</p>
</li>
</ul>
</li>
</ul>
<h2 id="开销向量建构">开销向量建构</h2>
<p>首先需要建构出存储开销向量$c^{(stor)}$，但是对于有$n$个基本子矩形的子矩形，其建构复杂度为$O(n^2)$。</p>
<p>因此对每个子矩形进行编码来获得存储开销并不可行，所以利用视频压缩与运动估计之间的强相关性来预测$c^{(stor)}$的值。</p>
<ol>
<li>
<p>给定一个视频，首先暂时将其分成长度为 1 秒的片段，每个片段被限定为只拥有 1 个 GOP，片段的大小表示为$S_{orig}$。</p>
</li>
<li>
<p>接着抽取出每个片段中的动作序列用于之后的分析。</p>
</li>
<li>
<p>将片段从空间上划分成基本子矩形，每个基本子矩形包含$4 \times 4 = 16$个宏块（例如：$64 \times 64$个像素点）。</p>
</li>
<li>
<p>独立编码每个基本子矩形，其大小表示为$S_i$。</p>
</li>
<li>
<p>通过分析动作向量信息，可以推断出如果对基本子矩形$i$进行独立编码，指向基本子矩形$i$的原始运动向量应该重新定位多少。</p>
<p>将其表示为$r_i$。</p>
</li>
<li>
<p>每个运动向量的存储开销可以计算为：</p>
<p>$$
o = \frac{\sum_i S_i - S_{orig}}{\sum_i r_i}
$$</p>
<p>即：存储开销的整体增长除以被基本子矩形边界所分割的运动向量数。</p>
</li>
<li>
<p>如果基本子矩形被融合进更大的子矩形$t$，使用$m_t$来表示由于融合操作而无须再进行重定位的运动向量的数量：</p>
<p>$$
m_t = \sum_{i \in t} r_i - r_t
$$</p>
<p>$i \in t$表示基本子矩形位于子矩形$t$中。</p>
</li>
<li>
<p>为了估计任意子矩形$t$的大小，使用下面 5 个参数：
$$
\sum_{i \in t} S_i,\ \sum_{i \in t} r_i,\ m_t,\ o,\ n
$$
$n$表示$t$中基本子矩形的数量。</p>
</li>
</ol>
<p>实际操作：</p>
<ol>
<li>
<p>创建了来自 4 个单视角 360 度视频的 6082 个 tile 数据集。4 个视频都以两种分辨率进行编码：$1920 \times 960$和$3980 \times 1920$。</p>
</li>
<li>
<p>为了产生 tile，从视频中随机选取片段，随机选取 tile 的位置，宽度和高度。</p>
<p>设置 tile 的 size 最大为$12 \times 12$个基本子矩形。</p>
<p>对于每个选择的 tile，为其建构一个数据集元素：</p>
<ol>
<li>计算上面提到的 5 参数的特性向量。</li>
<li>使用 FFmpeg 编码 tile 的视频段来得到存储该段需要的空间。</li>
</ol>
</li>
<li>
<p>使用多层感知机 MLP 来估计 tile 的大小。</p>
<p>MLP 中包含 50 个节点的单隐层，激活函数为 ReLU 函数，300 次迭代的训练过程使用<a href="https://zhuanlan.zhihu.com/p/29672873" target="_blank" rel="noopener noreffer">L-BFGS 算法</a>。</p>
<p>为了评估 MLP 的预测效果，使用 4 折的交叉验证法。</p>
<p>每次折叠时先从 3 个视频训练 MLP，接着使用训练好的模型去预测第 4 个视频的 tile 大小。</p>
</li>
</ol>
<h2 id="实现">实现</h2>
<p></p>
<p>将视频划分成 1 秒长的片段，之后为每个片段解决整数线性问题来确定最优的 tile 划分策略。</p>
<ol>
<li>使用 MLP 模型估计每个 tile 的存储开销。</li>
<li>根据视图的集合$d$及其对应的可能性分布$p$，来估计视图的下载开销$c^{(view)}$。</li>
<li>构造矩阵$A$时，限制最大的 tile 大小为$12 \times 12$的基本子矩形（如果设置每个基本子矩形包含$64 \times 64$的像素，tile 的最大尺寸即为$768 \times 768$的像素）。</li>
<li>使用<a href="https://www.gnu.org/software/glpk/" target="_blank" rel="noopener noreffer">GNU Linear Programming Kit</a>来解决问题。</li>
<li>将所有可能的解子矩形编码进一个二元向量$x$中来表示解。</li>
<li>GLPK 的解表明一个可能的解子矩形是否应该被放入解中。</li>
<li>基于最终得到的解，划分片段并使用 ffmepg 以同样参数的 x264 格式进行编码。</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>度量指标</p>
<ol>
<li>服务端存储需求。</li>
<li>客户端需要下载的字节数。</li>
</ol>
</li>
<li>
<p>数据来源</p>
<p>数据集：<a href="http://dash.ipv6.enstb.fr/headMovements/" target="_blank" rel="noopener noreffer">dash.ipv6.enstb.fr</a></p>
</li>
<li>
<p>评估准备</p>
<p>下载 5 个使用 ERP 投影的视频，抽取出测试中用户看到的对应部分。</p>
<p>每个视频都有$1920 \times 960$和$3840 \times 1920$的两种分辨率的版本。</p>
<p>$1920 \times 960$视频的基本子矩形尺寸为$64 \times 64$的像素。</p>
<p>$3840 \times 1920$视频的基本子矩形尺寸为$128 \times 128$的像素。</p>
<p>将视频划分成 1 秒长的片段，对每个片段都产生出 MLP 所需的 5 元组特性。</p>
<p>之后使用训练好的 MLP 模型来预测所有可能的 tile 的大小。</p>
</li>
<li>
<p>数据选择</p>
<ol>
<li>
<p>从数据集中随机选择出 40 个用户的集合。</p>
</li>
<li>
<p>假设 100°的水平和垂直 FOV，并使用 40 个用户的头部方向来为每个片段产生$p_v$和$d_v$。</p>
<p>即：分块的决策基于每个片段的内容特征信息与用户的经验视图模式。</p>
</li>
</ol>
</li>
<li>
<p>参数设定：$\alpha = 0,1,1000$.</p>
</li>
<li>
<p>对比实验：</p>
<p>一组使用由 ILP 得出的结构进行分块；</p>
<p>另外一组：</p>
<ul>
<li>
<p>$1920 \times 960$的视频片段分别使用$64 \times 64$，$128 \times 128$，$256 \times 256$，$512 \times 512$的方案固定大小分块。</p>
</li>
<li>
<p>$3840 \times 1920$的视频片段分别使用$128 \times 128$，$256 \times 256$，$512 \times 512$，$1024 \times 1024$的方案固定大小分块。</p>
</li>
</ul>
</li>
<li>
<p>划分结果对比</p>
<p></p>
</li>
</ul>
<h3 id="服务端的存储大小">服务端的存储大小</h3>
<p></p>
<p>按照$\alpha = 0$方案分块之后的视频大小几乎与未分块之前的视频大小持平，有时甚至略微小于未分块前的视频大小。</p>
<p>因为所有分块方案都使用相同的编码参数，所以重新编码带来的有损压缩并不会影响竞争的公平性。</p>
<p>如果将$\alpha$的值调大，存储的大小会略微增大；固定分块大小的方案所得到的存储大小也会随 tile 变小而变大。</p>
<h3 id="客户端的下载大小">客户端的下载大小</h3>
<ul>
<li>
<p>预测完美的情况——下载的 tile 没有任何浪费</p>
<p></p>
<p>$\alpha= 1000$的情况下，OpTile 的表现总是最好的。</p>
</li>
<li>
<p>正常预测的情况</p>
<p>预测的方法：假设用户的头部方向不会改变，预测的位置即为按照当前方向几秒之后的位置。</p>
<p></p>
<p>相比于完美假设的预测，所有分块方案的下载大小都增大了。</p>
<p>$\alpha = 1000$的方案在两个视频的情况下都取得了最小的下载大小。在剩下的 3 个视频中，OpTile 方案的下载大小比起最优的固定分块大小方案不超过 25%。</p>
<p>尽管固定分块大小的方案可能表现更好，但是这种表现随视频的改变而变化显著。</p>
<p><strong>因为固定分块的方案没有考虑视频内容的特性与用户的观看行为。</strong></p>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for RainbowDQN and Multitype Tiles</title>
    <link>https://ayamir.github.io/posts/papers/note-for-rainbowDQN&#43;tiles/</link>
    <pubDate>Sat, 11 Dec 2021 16:14:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-rainbowDQN&#43;tiles/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Level：IEEE Transaction on multimedia 21</p>
<p>Keyword：Rainbow-DQN, Multi-type tiles, Full streaming system</p>
<h2 id="问题形式化">问题形式化</h2>
<h3 id="模型">模型</h3>
<ol>
<li>
<p>原始视频用网格划分成$N$块 tile，每个 tile 都被转码成$M$个不同的质量等级$q_i$。</p>
</li>
<li>
<p>基于传输控制模块得出的结果，播放器请求$t_i$个 tile 的$q_i$质量的版本并将其存储在缓冲区中，对应的缓冲区大小为$l_i$。</p>
</li>
<li>
<p>用户 Viewport 的信息用$V$表示，可以确定 FOV 的中心。</p>
</li>
<li>
<p>根据$V$可以将 tile 划分成 3 种类型：FOV、OOS、Base。</p>
</li>
<li>
<p>FOV 中的 tile 被分配更高的码率；</p>
<p>OOS 按照与$V$的距离逐步降低质量等级$q_i$；</p>
<p>Base 总是使用低质量等级$q_{Base}$但使用完整的分辨率。</p>
</li>
<li>
<p>传输的 tile 在同步完成之后交给渲染器渲染。</p>
</li>
<li>
<p>播放器根据各项指标计算可以评估播放性能：</p>
<p>$&lt;V, B, Q, F, E&gt;$：viewport 信息$V$，网络带宽$B$，FOV 质量$Q$，重缓冲频率$F$，传输效率$E$。</p>
</li>
<li>
<p>传输控制模块用于确定每个 tile 的质量等级$q_i$和缓冲区大小$l_i$。</p>
</li>
<li>
<p>传输控制模块优化的最终目标是获取最大的性能：
$$
performance = E_{max},\ QoE \in accept\ range
$$</p>
</li>
</ol>
<h3 id="带宽评估">带宽评估</h3>
<ol>
<li>
<p>收集每个 tile 的下载日志来评估带宽。</p>
</li>
<li>
<p>使用<a href="https://zhuanlan.zhihu.com/p/32335746" target="_blank" rel="noopener noreffer">指数加权移动平均算法 EWMA</a>使评估结果光滑，来应对网络波动。</p>
</li>
<li>
<p>第$t$次评估结果使用$B_t$表示，用下式计算：
$$
B_t = \beta B_{t-1} + (1-\beta)b_t
$$
$b_t$是 B 的第$t$次测量值；$\beta$是 EWMA 的加权系数。</p>
</li>
<li>
<p>$t=0$时，$B_0$被初始化为 0；所以在初始的$t$比较小的时候，$B_t$与理想值相比就很小。</p>
<p>这种影响会随着$t$增大而减少。</p>
</li>
<li>
<p>为了优化启动过程，对公式做出修改：
$$
B_t = \frac{\beta B_{t-1} + (1-\beta)b_t}{1 - \beta^t}
$$
$t$较小的时候，分母会放大$B_t$；$t$较大时，分母趋近于 1，影响随之消失。</p>
</li>
</ol>
<h3 id="fov-表示和预测">FOV 表示和预测</h3>
<ol>
<li>
<p>3D 虚拟相机用于渲染视频，处于全景视频球面上的某条轨道，其坐标可以表示为$(\theta, \phi)$，可以直接从系统中获取。</p>
<p>相机始终朝向球的中心，所以用户的 FOV 中心坐标$(\theta^{&rsquo;}, \phi^{&rsquo;})$可以用$(\theta, \phi)$表示：
$$
\begin{cases}
\theta^{&rsquo;} = (\theta + \pi)\ mod\ 2\pi,\ 0 \le \theta \le 2\pi
\
\phi^{&rsquo;} = \pi - \phi,\ 0 \le \phi \le \pi
\end{cases}
$$</p>
</li>
<li>
<p>2D 网格中 tile 坐标$(u, v)$可以通过球面坐标使用 ERP 投影获得
$$
\begin{cases}
u = \frac{\theta^{&rsquo;}}{2\pi} \cdot W, 0 \le u \le W.
\
v = \frac{\phi^{&rsquo;}}{\pi} \cdot H, 0 \le v \le H.
\end{cases}
$$
$W$和$H$分别表示使用 ERP 投影得到的矩形宽度和高度</p>
</li>
<li>
<p>短期的 FOV 预测基于目前和历史的 FOV 信息。</p>
<p>使用$(U_t, V_t)$表示$t$时刻的 FOV 中心位置；$U_{t1:t2}$和$V_{t1:t2}$分别表示从$t1$到$t2$过程中$U$和$V$的序列；
$$
\begin{cases}
\hat{U}<em>{t+T_f} = f_U (U</em>{t-T_p:t}).
\
\hat{V}<em>{t+T_f} = f_V (V</em>{t-T_p:t}).
\end{cases}
$$
$T_p$是过去记录的滑动窗口；$T_f$是短期的预测窗口；$f_U$和$f_V$分别对应$U$和$V$方向上的映射函数；</p>
<p>因为是时间序列回归模型，所以映射函数使用 LSTM。</p>
</li>
</ol>
<h3 id="qoe-评估">QoE 评估</h3>
<p>QoE 由 3 个部分组成：平均 FOV 质量$Q$、重缓冲频率$F$与 FOV 内 tile 的质量变化（因为平均分配所以不考虑）。</p>
<ol>
<li>
<p>FOV 质量$Q$</p>
<p>第$t$次的 FOV 质量评估表示为$Q_t$：
$$
Q_t = \frac{\beta Q_{t-1} + (1-\beta) \frac{1}{k} \cdot \sum_{j=1}^{k} max{q_j, q_b}}{1 - \beta^t}
$$
$q_j$表示第$j$条 FOV tile 流的质量；$k$表示 FOV 内 tile 的数量；</p>
<p>为了避免评估结果的大幅波动，使用了 EWMA 来光滑结果。</p>
<p>当第$j$条 tile 流因为缓冲区不足不能成功播放时，$q_j = q_{Base}$（这表明了 Base tile 在提高 QoE 中的作用）。</p>
</li>
<li>
<p>重缓冲频率$F$</p>
<p>在基于 tile 的传输中，每条流都属于一个缓冲区。所以当 FOV 中 tile 的缓冲区处于饥饿状态时，重缓冲就会发生。</p>
<p>重缓冲频率描述了 FOV 内的 tile 流在一段时间内的重新缓冲频率。</p>
<p>第$t$次重缓冲频率的评估表示为$F_t$：
$$
F_t = \frac{\beta F_{t-\tau} + (1-\beta) \frac{f_t}{\tau}}{1 - \beta^{\tau}}
$$
$f_t$表示播放失败的次数；$\tau$表示一段时间；</p>
</li>
</ol>
<h3 id="传输效率评估">传输效率评估</h3>
<p>第$t$次传输效率评估表示为$E_t$，$E_t$通过传输的 FOV 内 tile 占总 tile 的比率来计算：
$$
E_t = \frac{\beta E_{t-1} + (1-\beta) \frac{total^{FOV}}{total^{ALL}}}{1 - \beta^t}
$$
$total^{FOV}$表示 FOV 内 tile 的数据量；$total^{ALL}$表示 tile 的总共数据量；</p>
<p>效率计算并不在传输过程中完成，因为需要获取哪些 tile 在 FOV 中的信息，效率评估滞后于播放过程。</p>
<h3 id="问题形式化-1">问题形式化</h3>
<p>传输控制的任务：确定所有 tile 流的质量等级$\chi$和缓冲区大小$\psi$。
$$
\chi = &lt;q_1, q_2, &hellip;, q_N&gt;
\
\psi = &lt;l_1, l_2, &hellip;, l_N&gt;
\
&lt;Q, F, E&gt; = \xi (B, V, \chi, \psi)
$$
$\chi$和$\psi$与带宽$B$和 Viewport 轨迹$V$一起作用于系统$\xi$，最终影响 FOV 质量$Q$，重缓冲频率$F$和传输效率$E$。</p>
<p>进一步，将目标形式化为获得每条 tile 流的$q_i$和$l_i$通过限制 QoE 满足可接受的范围、在此基础上最大化传输效率：
$$
\underset{\chi, \psi}{argmax} \sum_{t=0}^{+\infty} E_t,
$$</p>
<p>$$
s.t.:\ 0 \le q_i \le M,
$$</p>
<p>$$
0 \le l_i \le L,
$$</p>
<p>$$
Q^{min} \le Q_t \le M,
$$</p>
<p>$$
0 \le F_t \le F^{max}.
$$</p>
<p>$q_i$和$l_i$分别受限于质量版本数$M$和最大缓冲区大小$L$；</p>
<p>$Q_t$受限于最低 QoE 标准$Q^{min}$；</p>
<p>$F_t$受限于最大能忍受的重缓冲频率$F^{max}$。</p>
<h2 id="系统架构">系统架构</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>将原始视频转码为有不同比特率的多个版本。</li>
<li>转码后的视频被划分成多个 tile。</li>
<li>传输协议使用 MPEG-DASH。</li>
</ol>
<h3 id="客户端">客户端</h3>
<h4 id="评估器">评估器</h4>
<ul>
<li>任务：获取 QoE、FOV 预测、传输效率、网络带宽</li>
<li>组成：
<ul>
<li>QoE 评估器：评估当前 FOV 质量=&gt;Q 和重缓冲频率=&gt;F（近似为 Q+F=QoE）</li>
<li>FOV 预测器：基于历史 FOV 信息预测短期未来的 FOV=&gt;P</li>
<li>根据下载和播放日志：计算传输效率=&gt;E 并估计带宽=&gt;B</li>
</ul>
</li>
</ul>
<h4 id="控制器">控制器</h4>
<ul>
<li>任务：控制传输过程中的推流</li>
<li>目标：保证 QoE 在可接受的范围之内、最大化传输效率</li>
<li>详细：基于 FOV 预测将 tile 划分成 3 种类型：FOV、OOS、Base</li>
<li>输入：Q、F、E、B（QoE+传输效率和带宽）</li>
<li>过程：Rainbow-DQN</li>
<li>输出：决定每个 tile 流的码率和缓冲区大小（作为下载器的输入）</li>
</ul>
<h4 id="下载器">下载器</h4>
<ul>
<li>输入：tile 码率和缓冲区大小</li>
<li>过程：基于 HTTP/2 进行并行下载</li>
<li>输出：下载好的 tile</li>
</ul>
<h4 id="视频缓冲区">视频缓冲区</h4>
<ul>
<li>任务：解码、同步、存储下载好的 tile 等待渲染器消耗，大小供控制器调节</li>
<li>随着 FOV 的切换缓冲区内容可能被循环利用</li>
</ul>
<h4 id="全景渲染器">全景渲染器</h4>
<ul>
<li>任务：将同步好的 tile 拼接，tile 质量：FOV&gt;OOS&gt;Base</li>
<li>投影方式：ERP</li>
</ul>
<h2 id="控制器-1">控制器</h2>
<h3 id="控制过程">控制过程</h3>
<ol>
<li>
<p>设定 QoE 的可接受范围。</p>
</li>
<li>
<p>将网络带宽和用户 FOV 设定为外部因素而非环境</p>
<p>为什么：因为这两个因素变化太快，在面对不同传输条件时，直接作为环境会导致决策过程的不稳定性并且难以收敛。</p>
</li>
<li>
<p>最优化的对象只是最大化累积的传输效率。</p>
<p>为什么：简单</p>
</li>
</ol>
<h3 id="tile-聚合和决策">tile 聚合和决策</h3>
<ol>
<li>
<p>tile 分类原则：</p>
<ul>
<li>
<p>控制器无需为每个 tile 独立决定码率 Q 和缓冲区大小 L</p>
</li>
<li>
<p>FOV 内的 tile 应该被分配相近的码率，FOV 内的 tile 应该聚集成一组，OSS 和 Base 同理</p>
<p>为什么：避免相邻 tile 的锐利边界，只考虑 3 组而非所有 tile 降低了计算复杂性和决策延迟</p>
<p>（能否实现独立的 tile 码率计算或更细粒度的划分值得调研？与内容感知的方案结合？）</p>
</li>
</ul>
</li>
<li>
<p>基于距离的 tile 分类实现方式：</p>
<ul>
<li>
<p>使用评估器预测出的 FOV 坐标来分类 FOV 和 OOS 的 tile</p>
</li>
<li>
<p>tile 出现在未来 FOV 的可能性由距离计算</p>
<p>tile 中心点坐标$(\omega_i, \mu_i)$、FOV 坐标$(\hat{U}, \hat{V})$</p>
<p>距离的变化区间内存在一个临界点，临界点之内的划分为 FOV，之外的划分为 OOS</p>
<ul>
<li>
<p>度量距离的方式：
$$
\Delta Dis_U = min{|\omega_i - \hat{U}|, |1+\omega_i - \hat{U}|}
$$</p>
<p>（这里为何不直接使用$|\omega_i - \hat{U}|$？）
$$
Dis_i =
\begin{cases}
{\sqrt{({\Delta Dis_{U}})^2 + {(\mu_i - \hat{V})}^2},\  \frac{R}{H} \le \hat{V} \le 1 - \frac{R}{H}}
\
{\Delta Dis_U + |\mu_i - \hat{V}|,\ Others}
\end{cases}
$$</p>
</li>
<li>
<p>因为 ERP 的投影方式会在两级需要更多的 tile，因此使用一个矩形来代表两极的 FOV</p>
<p>（可以深入调研 ERP 在两极处的处理方式）</p>
</li>
<li>
<p>$Dis_i$使用曼哈顿距离来测量。临界点初始化为$2\cdot R$，并随着 FOV 中心和两极的垂直距离增长。</p>
</li>
<li>
<p>FOV 看作是半径为 R 的圆，使用欧式距离测量。临界点初始化为$R$</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>聚合 tile 的决策</p>
<ul>
<li>使用 2 个变量：$K$作为 FOV 和非 FOV 的 tile 的带宽分配比率；$Len$作为 tile 缓冲区的大小。
<ul>
<li>
<p>$K$确定之后，分配给 FOV 内 tile 的带宽被均匀分配（可否非均匀分配）</p>
<p>$K$不直接与网络状况相关因此可以保持控制的稳定性</p>
</li>
<li>
<p>$Len$：所有传输的 tile 的缓冲区长度$l_i$都被设为$Len$  （文中并没有这样做的原因解释）</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="基于-drl-的传输控制算法">基于 DRL 的传输控制算法</h3>
<p>相关术语解释：<a href="https://www.jianshu.com/p/1dfd84cd2e69" target="_blank" rel="noopener noreffer">Rainbow DQN</a>、<a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e" target="_blank" rel="noopener noreffer">RL Dictionary</a>、<a href="https://zhuanlan.zhihu.com/p/38358183" target="_blank" rel="noopener noreffer">PER</a>、<a href="https://zhuanlan.zhihu.com/p/34747205" target="_blank" rel="noopener noreffer">TD-Error</a></p>
<ol>
<li>
<p>控制过程</p>
<ol>
<li>
<p>首先调整 buffer 长度 Len，并划分 FOV 与非 FOV 的带宽分配。</p>
</li>
<li>
<p>等 viewport 预测完成之后，tile 被分类为属于 FOV 和 OOS 的 tile。</p>
</li>
<li>
<p>FOV 的带宽被平均分给其中每一个 tile 并决定 FOV 内 tile 的质量等级$q_i$。</p>
<p>非 FOV 的带宽按照与 FOV 的距离分配，每超过一个距离单位$Dis_i$就降低一级质量$q_i$。</p>
</li>
<li>
<p>最终的输出是请求序列，每个请求序列中包括质量等级$q_i$和预期的缓冲区大小$l_i$。</p>
</li>
<li>
<p>根据输出做出调整之后，接收奖励反馈并不断完成自身更新。</p>
</li>
</ol>
</li>
<li>
<p>状态设计</p>
<p>状态设计为 5 元组：$&lt;K, Len, Q, F, E&gt;$（传输控制参数$K$，$Len$、QoE 指标：FOV 质量 Q 和重缓冲频率$F$、传输效率$E$）</p>
<p>没有直接使用带宽$B$和 viewport 轨迹$V$，因为：</p>
<ol>
<li>随机性强与变化幅度较大带来的不稳定性（如何定义随机性强弱和变化幅度大小？）</li>
<li>希望设计的模型有一定的通用性，可以与不同的网络情况和用户轨迹相兼容</li>
</ol>
</li>
<li>
<p>动作设计</p>
<p>两种动作：调整$K$和$Len$（两者的连续变化区间被离散化，调整的每一步分别用$\Delta k$和$\Delta l$表示）</p>
<p>调整的方式被形式化为二元组：$&lt;n_1, n_2&gt;$，$n_1$和$n_2$分别用于表示$K$和$Len$的调整</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">-n</th>
<th style="text-align:center">0</th>
<th style="text-align:center">n</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">K</td>
<td style="text-align:center">减少 n$\Delta k$</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">增加 n$\Delta k$</td>
</tr>
<tr>
<td style="text-align:center">Len</td>
<td style="text-align:center">减少 n$\Delta l$</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">增加 n$\Delta l$</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>奖励函数</p>
<p>因为 QoE 的各项指标权重难以确定，没有使用传统的基于加权的方式。</p>
<p>设定了<strong>能接受的 QoE 范围</strong>和<strong>在此基础上最大化的传输效率</strong>作为最后的<strong>性能</strong>指标，形式化之后如下：
$$
Reward =
\begin{cases}
-INF,\ F \ge F^{max}
\
-INF,\ E \le E^{min}
\
E,\ Others
\end{cases}
$$</p>
<p>$-INF$意味着终止当前 episode；动作越能使系统满足高 QoE 的同时高效运行，得分越高；</p>
<p>为了最大化传输效率，使用$E$作为奖励回报。</p>
<p>FOV 质量$Q$并没有参与到奖励函数中，因为：<strong>高 Q 意味着高性能，但是低 Q 不一定意味着低性能</strong>，详细解释如下：</p>
<ul>
<li>在带宽不足的情况下，低 Q 可能已经是这种条件下的满足性能的最好选择。</li>
<li>高传输效率意味着传输了更多的 FOV 数据，也能满足高 FOV 质量的目标。</li>
</ul>
</li>
<li>
<p>模型设计
基于 Rainbow-DQN 模型：</p>
<ul>
<li>
<p>输入是 5 元组$&lt;K, Len, Q, F, E&gt;$。</p>
</li>
<li>
<p>神经网络使用 64 维的 3 隐层模型。</p>
</li>
<li>
<p>为了提高鲁棒性，神经网络的第 3 层使用 Dueling DQN 的方式，将 Q 值$Q(s, a)$分解为状态价值$V(s)$和优势函数$A(s,a)$：
$$
Q(s, a) = V(s) + A(s, a)
$$</p>
<p>$V(s)$表示系统处于状态$s$时的性能；$A(s,a)$表示系统处于状态$s$时动作$a$带来的性能；</p>
</li>
<li>
<p>为了避免价值过高估计，使用 Double DQN 的方式，设计了两个独立的神经网络：评估网络和目标网络。</p>
<p>评估网络用于动作选择；目标网络是评估网络从最后一个 episode 的拷贝用于动作评估。</p>
</li>
<li>
<p>为了缓解神经网络的不稳定性（更快收敛），使用大小为$v$的回放池来按照时间序列保存客户端的经验。</p>
<p>因为网络带宽和 FOV 轨迹在短期内存在特定的规律性，回放池中有相似状态和相似采样时间的样本更加重要，出现了优先级</p>
<p>所以使用优先经验回放 PER，而优先级使用时间查分误差 TD-error 定义
$$
\delta_i = r_{i+1} + \gamma Q(s_{i+1}, arg\underset{a}{max}Q(s_{i+1}, a; \theta_i); \theta_i^{&rsquo;}) - Q(s_i, a_i; \theta_i)
$$</p>
<p>$r_i$是奖励；$\gamma$是折扣因子</p>
</li>
<li>
<p>损失函数使用均方误差定义
$$
J = \frac{1}{v} \sum_{i=1}^{v} \omega_i(\delta_i)^2
$$</p>
<p>$\omega_i$是回放缓冲中第 i 个样本的重要性采样权重</p>
</li>
</ul>
</li>
</ol>
<h2 id="实验验证">实验验证</h2>
<ol>
<li>
<p>环境设定</p>
<ul>
<li>
<p>传输控制模块：基于<a href="https://tensorforce.readthedocs.io/en/latest/" target="_blank" rel="noopener noreffer">TensorForce</a>（配置教程：<a href="https://zhuanlan.zhihu.com/p/60241809" target="_blank" rel="noopener noreffer">用 TensorForce 快速搭建深度强化学习模型</a>）；</p>
<p>开发工具集：<a href="https://gym.openai.com/" target="_blank" rel="noopener noreffer">OpenAI Gym</a></p>
</li>
<li>
<p>数据来源：使用全景视频播放设备收集，加入高斯噪声来产生更多数据。</p>
</li>
</ul>
</li>
<li>
<p>结果分析</p>
<ul>
<li>
<p>与其他 DQN 算法的对比——DQN、Double DQN、Dueling DQN</p>
<ul>
<li>
<p>对比训练过程中每个 episode 中的最大累计奖励：$MAX_{reward}$</p>
</li>
<li>
<p>对比模型收敛所需要的最少 episode：$MIN_{episode}$</p>
<p>相同的带宽和 FOV 轨迹</p>
</li>
</ul>
</li>
<li>
<p>与其他策略对比性能——高 QoE 和高传输效率</p>
<ul>
<li>
<p>随机控制策略：随机确定 K 和 Len</p>
</li>
<li>
<p>固定分配策略：固定 K 和 Len 的值</p>
</li>
<li>
<p>只预测 Viewport 策略：使用 LSTM 做预测，不存在 OSS 与 Base，所有带宽都用于 FOV</p>
<p>带宽和 FOV 轨迹的均值和方差相等</p>
</li>
</ul>
</li>
<li>
<p>与其他全景视频推流系统的对比</p>
<ul>
<li>
<p>DashEntire360：使用 Dash 直接传送完整的 360 度视频，使用线性回归来估计带宽并动态调整视频比特率</p>
</li>
<li>
<p>360ProbDash：在 DashEntire360 的基础上划分 tile 基于 Dash 传输，使用可能性模型为 tile 分配比特率</p>
</li>
<li>
<p>DRL360：使用 DRL 来优化多项 QoE 指标</p>
<p>实现三种系统、使用随机网络带宽和 FOV 轨迹。</p>
<p>使用 DRL360 中提出的方式测量 QoE：
$$
V_{QoE} = \eta_1 Q - \eta_2 F - \eta_3 A
$$</p>
<p>$A$是 viewport 随时间的平均变化，反映 FOV 质量 Q 的变化；</p>
<p>$\eta_1, \eta_2, \eta_3$分别是 3 种 QoE 指标的非负加权，使用 4 种加权方式来训练模型并对比：</p>
<p>$&lt;1, 1, 1&gt;$，$&lt;1, 0.25, 0.25&gt;$，$&lt;1, 4, 1&gt;$，$&lt;1,1,4&gt;$</p>
</li>
</ul>
</li>
<li>
<p>在不同环境下的性能评估——带宽是否充足、FOV 轨迹是否活跃（4 种环境）</p>
</li>
</ul>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for 360ProbDASH</title>
    <link>https://ayamir.github.io/posts/papers/note-for-360ProbDASH/</link>
    <pubDate>Thu, 09 Dec 2021 10:20:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-360ProbDASH/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3123266.3123291" target="_blank" rel="noopener noreffer">360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming</a></p>
<p>Level: ACM MM 17</p>
<p>Keyword:</p>
<p>Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation</p>
<h2 id="工作范围与目标">工作范围与目标</h2>
<p>应用层-&gt;基于 tile-&gt;viewport 预测的可能性模型+预期质量的最大化</p>
<ul>
<li>
<p>针对小 buffer 提出了<code>target-buffer-based rate control</code>算法来避免重缓冲事件（避免卡顿）</p>
</li>
<li>
<p>提出 viewport 预测的可能性模型计算 tile 被看到的可能性（避免边缘效应）</p>
</li>
<li>
<p>形式化 QoE-driven 优化问题：</p>
<p>在传输率受限的情况下最小化 viewport 内的质量失真和空间质量变化（获取受限状态下最好的视频质量）</p>
</li>
</ul>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>形式化参数</p>
<p>$M*N$个 tile，M 指 tile 序列的序号，N 指不同的码率等级</p>
<p>$r_{i, j}$指比特率，$d_{i, j}$指失真，$p_{i}$指被看到的可能性（$\sum_{i=1}^{N}p_{i} = 1$）</p>
<p>$\Phi(X)$指质量失真，$\Psi(X)$指质量变化</p>
</li>
<li>
<p>目标</p>
<p>找到推流段的集合：$X = {x_{i, j}}$，其中${x_{i, j}} = 1$指被第$&lt;i, j&gt;$个 tile 被选中；$x_{i, j} = 0$则是未选中。
$$
\underset{X}{min}\ \Phi(X) + \eta \cdot \Psi(X) \
s.t. \sum_{i=1}^{N}\sum_{j=1}^{M}x_{i, j}\cdot r_{i, j} \le R, \
\sum_{j=1}^{M}x_{i, j} \le 1, x_{i, j} \in {0, 1}, \forall i.
$$
整个公式即为前所述的问题的形式化表达的公式化结果。</p>
</li>
<li>
<p>模型细节</p>
<ol>
<li>
<p>$\Phi(X)$和$\Psi(X)$的计算=&gt;通过考虑球面到平面的映射</p>
<p>通过计算球面上点的 Mean Squared Error 来得到 S-PSNR 进而评估质量：$d_{i, j}$来表示第${&lt;i, j&gt;}$个段的 MSE</p>
<p>
$$
\phi_i = \frac{\pi}{2} - h_i \cdot \frac{\pi}{H}, \Delta\phi = \Delta h \cdot \frac{\pi}{H}, \
\theta_i = w_i \cdot \frac{2\pi}{W}, \ \Delta\theta = \Delta w \cdot \frac{2\pi}{W},
$$
$H$和$W$分别指按照 ERP 格式投影之后的视频高度和宽度</p>
<p>第$i$个 tile 的空间面积用$s_i$表示：
$$
s_i\ =\ \iint_{\Omega_i}Rd\phi Rcos\phi d\theta \
=\Delta\theta R^2[sin(\phi_i + \Delta\phi) - sin\phi_i],
$$
$R$指球的半径（$R = W/2\pi$），所以整体的球面质量失真$D_{i, j}$可以计算出来：
$$
D_{i, j} = d_{i, j} \cdot s_i,
$$
结合每个 tile 被看到的概率$p_i$可以得出$\Phi(X)$和$\Psi(X)$
$$
\Phi(X)=\frac{\sum_{i=1}^N\sum_{j=1}^MD_{i, j}\cdot x_{i,j}\cdot p_i}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i},\
\Psi(X) = \frac{\sum_{i=1}^N\sum_{j=1}^Mx_{i, j}\cdot p_i \cdot\ (D_{i,j}-s_{i} \cdot \Phi(X))^2}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i}.
$$</p>
</li>
<li>
<p>Viewport 的可能性模型</p>
<ol>
<li>
<p>方向预测=&gt;<strong>线性回归模型</strong></p>
<p>将用户的欧拉角看作是$yaw(\alpha)$，$pitch(\beta)$和$rool(\gamma)$，应用线性回归做预测
$$
\begin{cases}
\hat{\alpha}(t_0 + \delta) = m_{\alpha}\delta+\alpha(t_0),\
\hat{\beta}(t_0 + \delta) = m_{\beta}\delta+\beta(t_0),\
\hat{\gamma}(t_0 + \delta) = m_{\gamma}\delta+\gamma(t_0).
\end{cases}
$$</p>
</li>
<li>
<p>预测错误的分布=&gt;<strong>高斯分布</strong>，根据公式均值和标准差都能从统计信息中计算出来</p>
<p>收集 5 名志愿者的头部移动轨迹并投影到 3 个方向上绘制成图，实验结果为预测错误呈现高斯分布（样本数可能不够？）</p>
<p>
$$
\begin{cases}
P_{yaw}(\alpha) = \frac{1}{\sigma_{\alpha}\sqrt{2\pi}}exp{-\frac{[\alpha-(\hat{\alpha}+\mu_{\alpha})]^2}{2\sigma_{\alpha}^2}},\
P_{pitch}(\beta) = \frac{1}{\sigma_{\beta}\sqrt{2\pi}}exp{-\frac{[\beta-(\hat{\beta}+\mu_{\beta})]^2}{2\sigma_{\beta}^2}},\
P_{roll}(\gamma) = \frac{1}{\sigma_{\gamma}\sqrt{2\pi}}exp{-\frac{[\gamma-(\hat{\gamma}+\mu_{\gamma})]^2}{2\sigma_{\gamma}^2}}.
\end{cases}
$$
3 个方向各自<strong>独立</strong>，因此最终的预测错误$P_E(\alpha,\beta,\gamma)$可以表示为：
$$
P_E(\alpha, \beta, \gamma) = P_{yaw}(\alpha)P_{pitch}(\beta)P_{roll}(\gamma).
$$</p>
</li>
<li>
<p>球面上点被看到的可能性</p>
<p>球面坐标为$(\phi, \theta)$点的可能性表示为$P_s(\phi, \theta)$</p>
<p>因为一个点可能在多个不同的 viewport 里面，所以定义按照用户方向从点$(\phi, \theta)$出发能看到的点集$L(\phi, theta)$</p>
<p>因此空间点$s$被看到的可能性可以表示为：
$$
P_s(\phi, \theta) = \frac{1}{|L(\phi, \theta)|}\sum_{(\alpha, \beta, \gamma) \in L(\phi, \theta)}P_E(\alpha, \beta, \gamma),
$$</p>
</li>
<li>
<p>球面上 tile 被看到的可能性</p>
<p>tile 内各个点被看到的可能性的<strong>均值</strong>即为 tile 被看到的可能性（可否使用其他方式？）
$$
p_i = \frac{1}{|U_i|} \sum_{(\phi, \theta) \in U_i} P_s(\phi, \theta).
$$
$U_i$表示 tile 内的空间点集</p>
</li>
</ol>
</li>
<li>
<p><code>Target-Buffer-based</code> Rate Control</p>
<p>因为长期的头部移动预测会产生较高的预测错误，所以不能采用大缓冲区（没有 cite 来证明这一点）</p>
<p></p>
<p>将处于相同时刻的段集合成一个块存储在缓冲区中。</p>
<p>在自适应的第 k 步，定义$d_k$作为此时的 buffer 占用情况（等到第 k 个块被下载完毕）
$$
b_k = b_{k-1} - \frac{R_k \cdot T}{C_k} + T
$$
$C_k$表示平均带宽，$R_k$表示总计的码率</p>
<p>为了避免重新缓冲设定目标 buffer 占用$B_{target}$，并使 buffer 占用保持在$B_{target}$（$b_k = B_{target}$）</p>
<p>因此总计的码率需要满足：
$$
R_k = \frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T),
$$
这里的$C_k$表示可以从历史的段下载信息中估计出来的带宽</p>
<p>设定$R$的下界$R_{min}$之后（没有说明为何需要设定下界），公式 12 可以修正为如下：
$$
R_k = max{\frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T), R_{min}}.
$$</p>
</li>
</ol>
</li>
</ol>
<h2 id="实现">实现</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>
<p>视频裁剪器</p>
<p>将视频帧切割成 tile</p>
</li>
<li>
<p>编码器</p>
<p>对 tile 进行划分并将其编码成多种码率的段</p>
</li>
<li>
<p>MPD 产生器</p>
<p>添加<strong>SRD 特性</strong>来表示段之间的空间关系</p>
<p>添加经度和<strong>纬度</strong>属性来表示</p>
<p>添加<strong>质量失真</strong>和<strong>尺寸</strong>属性</p>
</li>
<li>
<p>Apache HTTP 服务器</p>
<p>存储视频段和 mpd 文件，向客户端推流</p>
</li>
</ol>
<h3 id="客户端">客户端</h3>
<ol>
<li>
<p>基础：dash.js</p>
</li>
<li>
<p>额外的模块</p>
<ul>
<li>
<p><code>QoE-driver Optimizer</code>
$$
Output = HTTP\ GET请求中的最优段
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Target\ buffer\ based\ Rate\ Controller\
Viewport\ Probabilistic\ Model\
QR\ Map
\end{cases}
$$</p>
</li>
<li>
<p><code>Target-buffer-based Rate Controller</code>
$$
Output = 总计的传输码率，按照公式13计算而来
$$</p>
<p>$$
Input = Output\ of\ {Bandwidth\ Estimation\ module
$$</p>
</li>
<li>
<p><code>Viewport Probabilistic Model</code>
$$
Output = 每个tile被看到的可能性，按照公式10计算而来
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Orientation\ Prediction\ module\
SRD\ information
\end{cases}
$$</p>
</li>
<li>
<p><code>QR Map</code>QR=&gt;Quality-Rate
$$
Output = 所有段的QR映射
$$</p>
<p>$$
Input = MPD中的属性
$$</p>
</li>
<li>
<p><code>Bandwidth Estimation</code>（没有展开研究，因为不是关键？）
$$
Output = 前3秒带宽估计的平均值
$$</p>
<p>$$
Input = 下载段过程中的吞吐量变化
$$</p>
<p>可以通过<code>onProgess()</code>的回调函数<code>XMLHttpRequest API</code>获取</p>
</li>
<li>
<p><code>Orientation Prediction</code>
$$
Output = 用户方向信息的预测结果（yaw, pitch, roll）
$$</p>
<p>$$
Input = Web\ API中获取的DeviceOrientation信息，使用线性回归做预测
$$</p>
</li>
</ul>
</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>整体设定</p>
<ol>
<li>将用户头部移动轨迹编码进播放器来模拟用户头部移动</li>
<li>积极操控网络状况来观察不同方案对网络波动的反应</li>
</ol>
</li>
<li>
<p>详细设定</p>
<ul>
<li>
<p>服务端</p>
<ol>
<li>
<p>视频选择</p>
<p>2880x1440 分辨率、时长 3 分钟、投影格式 ERP</p>
</li>
<li>
<p>切分设置</p>
<p>每个块长 1s（$T=1$）、每个块被分成 6x12 个 tile（$N=72$）</p>
<p>每个段的码率设置为${20, 50, 100, 200, 300}$，单位 kpbs</p>
</li>
<li>
<p>视频编码</p>
<p><a href="http://www.videolan.org/developers/x264.html" target="_blank" rel="noopener noreffer">开源编码器 x264</a></p>
</li>
<li>
<p>视频分包</p>
<p><a href="https://gpac.wp.mines-telecom.fr/mp4box/" target="_blank" rel="noopener noreffer">MP4Box</a></p>
</li>
<li>
<p>注意事项</p>
<p>每个段的确切尺寸可能与其码率不同，尤其对于长度较短的块。</p>
<p>为了避免这影响到码率自适应，将段的确切尺寸也写入 MPD 文件中</p>
</li>
</ol>
</li>
<li>
<p>客户端</p>
<ol>
<li>
<p>缓冲区设定（经过实验得出的参数）</p>
<p>$B_{max}=3s$，$B_{target}=2.5s$，$R_{min}=200kbps$，$权重\eta=0.0015$</p>
</li>
</ol>
</li>
<li>
<p>高斯分布设定</p>
<table>
<thead>
<tr>
<th style="text-align:center">Yaw</th>
<th style="text-align:center">Pitch</th>
<th style="text-align:center">Roll</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\mu_{\alpha}=-0.54,\ \sigma_{\alpha}=7.03$</td>
<td style="text-align:center">$\mu_{\beta}=0.18,\ \sigma_{\beta}=2.55$</td>
<td style="text-align:center">$\mu_{\gamma}=2.16,\ \sigma_{\gamma}=0.15$</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li>
<p>比较对象</p>
<ul>
<li>ERP：原始视频格式</li>
<li>Tile：只请求用户当前 viewport 的 tile，不使用 viewport 预测，作为 baseline</li>
<li>Tile-LR：使用线性回归做预测，每个 tile 的码率被平均分配</li>
</ul>
</li>
<li>
<p>性能指标</p>
<ul>
<li>卡顿率：卡顿时间占播放总时长的比例</li>
<li>Viewport PSNR：直接反应 Viewport 内的视频质量</li>
<li>空间质量差异：Viewport 内质量的协方差</li>
<li>Viewport 偏差：空白区域在 Viewport 中的比例</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for Dante</title>
    <link>https://ayamir.github.io/posts/papers/note-for-dante/</link>
    <pubDate>Wed, 08 Dec 2021 22:14:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-dante/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3232565.3234686" target="_blank" rel="noopener noreffer">https://dl.acm.org/doi/10.1145/3232565.3234686</a></p>
<p>Level: SIGCOMM 18</p>
<p>Keyword: UDP+FOV-aware+FEC</p>
<h2 id="工作范围">工作范围</h2>
<p></p>
<h2 id="目标">目标</h2>
<p>在给定序列的帧中，为<strong>每个 tile</strong>设定 FEC 冗余，根据其被看到的可能性的加权最小化平均质量降低。</p>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>输入
估计的丢包率$p$、发送速率$f$、有$n$个 tile 的$m$个帧($&lt;i, j&gt;$来表示第$i$个帧的第$j$个 tile</p>
<p>第$&lt;i, j&gt;$个 tile 的大小$v_{i, j}$、第$&lt;i, j&gt;$个 tile 被看到的可能性$\gamma_{i, j}$、</p>
<p>如果第$&lt;i, j&gt;$ 个 tile 没有被恢复的质量降低率、最大延迟$T$</p>
</li>
<li>
<p>输出</p>
<p>第$&lt;i, j&gt;$个 tile 的 FEC 冗余率$r_{i, j} = \frac{冗余包数量}{原始包数量}$</p>
</li>
<li>
<p>最优化问题的形式化
$$
minimize\  \sum_{0&lt;i\le m}\sum_{0&lt;j\le n} \gamma_{i, j}d_{i, j}(p, r_{i, j})
$$</p>
<p>$$
subject\ \ to\ \  \frac{1}{f}\sum_{0&lt;i\le m}\sum_{0&lt;j\le n}v_{i, j}(1+r_{i, j}) \le T
$$</p>
<p>$$
r_{i, j} \le 0
$$</p>
<p>（1）：最小化最终被看到的 tile 的质量衰减的加权和，权重按照被看到的可能性分配。</p>
<p>（2）：经过重新编码的包和原始的包需要在 T 时刻之前发出。</p>
<p>​      Dante 将 1 个 GOP(Group of Pictures)中的所有帧当作一批处理，$T$作为 GOP 的持续时间</p>
<p>​      $f$：使用 TCP Friendly Rate Control algorithm，基于估计的丢包率和网络延迟来计算得出</p>
<p>（3）：确保冗余率总是非负的。</p>
</li>
<li>
<p>关键变量是$d_{i, j}(p, r)$：丢包率是 p 情况下，采用 r 作为冗余率的第$&lt;i, j&gt;$个 tile 的质量衰减
$$
d_{i, j}(p, r) = \delta_{i, j},\ if\ r &lt; \frac{1}{1-p}; 0, otherwise.
$$</p>
<p>假设帧中有 k 个原始包，质量衰减发生在丢失的包不能被恢复的情况下。</p>
<p>FEC 可以容忍 $r \cdot k$ 个丢包=&gt;即当 $p(r<em>k+k)$ 大于  $r</em>k$  时会发生质量衰减。</p>
</li>
<li>
<p>过多的丢包会导致依赖链上所有帧的质量衰减，因此考虑帧之间的依赖关系之后，可以重新计算质量衰减：</p>
<p>$$
d^{*}<em>{i, j}(p, r) = \sum</em>{0&lt;c\le i}w_{c, i}d_{c, j}(p, r)
$$</p>
<p>$w_{c, i}$ 编码帧 i 对帧 c 的依赖作为单独的第 c 个帧的质量衰减的权重；</p>
<p>最终第 i 个帧的第 j 个 tile 的最终质量衰减就是所有依赖的质量衰减的和。</p>
</li>
</ol>
<h2 id="fec-冗余的自适应逻辑">FEC 冗余的自适应逻辑</h2>
<ol>
<li>
<p>关于$d_{i, j}(p, r)$ ：因为是分段函数，所以其值会因为 r 和 p 的大小关系而急剧改变。</p>
<p>利用背包问题的思想可以将其规约成 NP 完全问题：</p>
<p>将每个 tile 看作是一个物品，共有 m*n 个。</p>
<p><strong>如果$r_{i, j} &lt; \frac{1}{1-p}$ ，则表示不把第&lt;i,j&gt;和物品放入背包；否则就是将其放入背包。</strong></p>
<p>公式 1 可以转化为：最大化所有物品二元变量的线性组合；</p>
<p>公式 2 可以转化为：二元变量的另一个线性组合必须低于阈值约束。</p>
<p>因此整个问题就能被完全转化为<strong>0-1 背包</strong>问题</p>
</li>
<li>
<p>算法</p>
<p></p>
<p>整体上是背包问题的标准解法，能以线性复杂度（因为变量只是 B)解决问题。</p>
</li>
</ol>
<h2 id="原型设计">原型设计</h2>
<p></p>
<ul>
<li>使用基于 TCP 和 UDP 的两条连接来分别传输控制信息（双向：到客户端的播放会话的起至点和到服务端的网络信息反馈）和视频数据包</li>
<li>服务端根据反馈的网络信息，在每个 GOP 的边界时刻运行算法 1 来确定下一个 GOP 的帧和 tile 的 FEC 冗余。
确定之后服务端使用 RS 码来插入冗余包，和原始视频数据包一起重新编码，并使用基于 TFRC 的发送率发送数据。</li>
<li>Dante 的实现是对应用程序级比特率适配策略的补充，并且可以通过对视频播放器进行最小更改来替换现有的底层传输协议来部署。</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>环境：使用 Gilbert 模型来模拟实现丢包事件（而非使用统一随机丢包）</p>
<p>创造了两种网络条件 good（丢包率 0.5%）和 bad（丢包率 2%）</p>
</li>
</ul>
<h2 id="局限性">局限性</h2>
<ul>
<li>效果主要依赖于 Viewport 预测的结果是否准确</li>
</ul>
]]></description>
</item>
<item>
    <title>沉浸式流媒体传输的实际度量</title>
    <link>https://ayamir.github.io/posts/papers/note11/</link>
    <pubDate>Mon, 22 Nov 2021 15:21:59 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note11/</guid>
    <description><![CDATA[<h2 id="度量指标">度量指标</h2>
<ol>
<li>viewport 预测精度。
<ul>
<li>使用预测的 viewport 坐标和实际用户的 viewport 坐标的大圈距离来量化。</li>
</ul>
</li>
<li>视频质量。
<ul>
<li>viewport 内部的 tile 质量（1～5）。</li>
<li>tile 在最高质量层之上花费的时间。</li>
<li>根据用户视线的分布而提出的加权质量度量。</li>
</ul>
</li>
</ol>
<h2 id="度量参数">度量参数</h2>
<ol>
<li>分块策略</li>
<li>带宽</li>
<li>延迟</li>
<li>viewport 预测</li>
<li>HTTP 版本</li>
<li>持久化的连接数量</li>
</ol>
]]></description>
</item>
<item>
    <title>沉浸式推流中应用层的优化</title>
    <link>https://ayamir.github.io/posts/papers/note10/</link>
    <pubDate>Mon, 15 Nov 2021 10:13:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note10/</guid>
    <description><![CDATA[<h2 id="背景">背景</h2>
<p>大多数的 HAS 方案使用 HTTP/1.1 协议进行请求-回应的事务来取得需要的资源、缓冲取到的视频段并以线性的顺序播放。传统的 HAS 中，只需要 1 个 GET 请求来取得下一个视频的暂时的部分。只要视频段的持续时间比网络内的时延高，这种方法就可行。</p>
<p>在基于 VR 的 HAS 方案中，播放 1 条视频片段就需要取得多种资源：1 次 GET 请求需要同时请求基础的 tile 层和每个空间视频 tile。使用 4x4 的 tile 方案时，客户端需要发起不少于 17 次 GET 请求。使用 1 s 数量级的分段持续时间，即使是 20 ms 的微小网络延迟也会显着阻碍客户端和服务器之间的整体吞吐量，因此会导致较低的视频质量。</p>
<h2 id="解决方案">解决方案</h2>
<h3 id="使用多条持久的-tcp-连接">使用多条持久的 TCP 连接</h3>
<p>大多数的现代浏览器都支持同时建立并维持多达 6 条 TCP 连接来减少页面加载时间，并行地获取请求的资源。这允许增加整体吞吐量，并部分消除网络延迟引入的空闲 RTT 周期。</p>
<p>类似地，基于 VR 的 HAS 客户端可以使用多个 TCP 连接并行下载不同的 tile。</p>
<h3 id="使用-http2-协议的服务端-push-特性">使用 HTTP/2 协议的服务端 push 特性</h3>
<p>HTTP/2 协议引入了请求和相应的多路复用、头部压缩和请求优先级的特性，这可以减少页面加载时间。</p>
<p>服务端直接 push 短视频片段可以减少视频的启动时间和端到端延迟。</p>
<p>并且，服务端 push 特性可以应用在基于 tile 的 VR 视频推流中，客户端可以向服务器同时请求一条视频片段的所有 tile。</p>
<p>服务端可以使用特制的请求处理器，允许客户端为每个 tile 定义一系列质量等级。</p>
<p>因此可以将应用的启发式自适应的速率的决定传达给服务器，这允许客户端以期望的质量级别取得所有图块。</p>
]]></description>
</item>
<item>
    <title>沉浸式流媒体面临的挑战和启示</title>
    <link>https://ayamir.github.io/posts/papers/note9/</link>
    <pubDate>Sun, 14 Nov 2021 19:06:10 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note9/</guid>
    <description><![CDATA[<h2 id="最终的目标">最终的目标</h2>
<p>主要的挑战是用户的临场感，这可以通过避免虚拟的线索来创造出接近真实的世界。</p>
<h2 id="具体的任务">具体的任务</h2>
<ol>
<li>
<p>从 360 度视频的采集到显示的过程中，引入了好几种失真。</p>
<p>应该重点增加新的拼接、投影和分包方式以减少噪音。</p>
</li>
<li>
<p>除了捕获和使用 360 度视频来表示真实世界和实际交互内容之外，环境中还包括 3D 对象。</p>
<p>3D 对象的合并对于真实的视图而言是一个挑战。</p>
</li>
<li>
<p>因为在推流会话中，用户的头部移动高度可变，所以固定的 tiling 方案可能会导致非最优的 viewport 质量。</p>
<p>推流框架中的 tile 数量应该被动态选择，进而提高推流质量。</p>
</li>
<li>
<p>自适应的机制应该足够智能来根据环境因素精确地做出适应。</p>
<p>应该制定基于深度强化学习的策略，来给 360 度视频帧中不同区域的 tile 分配合适的比特率。</p>
</li>
<li>
<p>用户在 360 度视频中的自由导航很容易让其感觉忧虑自己错过了什么重要的东西。</p>
<p>在 360 度视频中导航的时候，需要支持自然的可见角度方向。</p>
<p>丰富的环境应配备新颖的定向机制，以支持 360 度视频，同时降低认知负荷，以克服此问题。</p>
</li>
<li>
<p>真实的导航依赖 viewport 预测机制。</p>
<p>现代的预测方式应该使用时空图像特性以及用户的位置信息，采用合适的编解码器卷积 LSTM 结构来减少长期预测误差。</p>
</li>
<li>
<p>沉浸式的场景随着用户的交互应该发生变化。</p>
<p>由于用户与场景的交互而产生的新挑战是通过编码和传输透视图创建的。</p>
<p>因此预测用户的行为来实现对交互内容的高效编码和推流非常关键。</p>
</li>
<li>
<p>对 360 度视频的质量获取方法和度量手段需要进一步研究。</p>
</li>
<li>
<p>360 度视频中特殊的音效需要引起注意。</p>
</li>
</ol>
]]></description>
</item>
<item>
    <title>360度视频的音频处理</title>
    <link>https://ayamir.github.io/posts/papers/note8/</link>
    <pubDate>Sun, 14 Nov 2021 16:52:20 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note8/</guid>
    <description><![CDATA[<h2 id="背景">背景</h2>
<p>空间音频是一种全球状空间环绕的声音方式，采用多个声音通道来模拟现实世界中听到的声音。</p>
<p>360 度视频由于空间音频而变得更加可靠，因为声音的通道特性使其能够穿越时间和空间。</p>
<p>360 度视频显示系统在制作空间音频音轨方面的重要性无论怎样强调都不为过</p>
<h2 id="空间音频的再现技术">空间音频的再现技术</h2>
<h3 id="物理重建">物理重建</h3>
<p>物理重建技术用于合成尽可能接近所需信号的整个声场。</p>
<p>立体声配置在最流行的声音再现方法中使用两个扬声器，以促进更多的空间信息（包括距离、方向感、环境和舞台合奏）。而多信道再现方法在声学环境中使用，并在消费类设备中流行。</p>
<h4 id="多信道再现技术">多信道再现技术</h4>
<p>同样的声压场也通过其他物理重建技术产生，如环境中存在的环境声学和波场合成（WFS）。</p>
<p>需要麦克风阵列来捕获更多的空间声场。</p>
<p>因为不能直接用于声场特性分析，麦克风记录的内容需要后期处理。</p>
<p>麦克风阵列用于语音增强、声源分离、回声消除和声音再现。</p>
<h3 id="感知重建">感知重建</h3>
<p>心理声学技术用于感知重建，以产生对空间声音特征的感知。</p>
<p>感知重建技术复制空间音频的自然听觉感受来表示物理音频。</p>
<h4 id="双耳录制技术">双耳录制技术</h4>
<p>双耳录制技术是立体声录制的一种扩展形式，提供 3D 的听觉体验。</p>
<p>双耳录制技术通过使用两个 360 度麦克风尽可能的复制人耳，这与使用定向麦克风捕捉声音的常规立体声录音相同。</p>
<p>假人头部的 360 度麦克风用作人耳的代理，因为它提供了耳朵的精确几何坐标。</p>
<p>假人头部还产生与人头轮廓相互作用的声波。借助 360 度麦克风，与任何其他记录方法相比，空间立体图像的捕获更精确。</p>
<h5 id="头部相关传递函数hrtf">头部相关传递函数（HRTF）</h5>
<p>用于双耳音频的实时技术中，以再现复杂的线索，帮助我们通过过滤音频信号来定位声音。</p>
<p>多个因素（如耳朵、头部和听力环境）会影响线索，因为在现实中，我们会重新定位自己以定位声音。</p>
<p>选择合适的录音/重放技术对于使听到的声音与真实场景中的体验相同至关重要。</p>
<h2 id="环境声学">环境声学</h2>
<h3 id="概述">概述</h3>
<p>环境声学也被称为 3D 音频，被用于记录、混成和播放一个中心点周围的 360 度音频。</p>
<h3 id="区别">区别</h3>
<p>环境音频和传统的环绕声技术不同。</p>
<ol>
<li>
<p>双声道和传统环绕声技术背后的原理是相同的，都是通过将声音信号送到特定的扬声器来创建音频。</p>
<p>环境音频不受任何特定扬声器的预先限制，因为它在即使音域旋转的情况下，也能创造出平滑的音频。</p>
</li>
<li>
<p>传统环绕声的格式只有在声音场景保持静态的情况下才能提供出色的成像效果。</p>
<p>环境音频提供一个完整的球体，将声音均匀地传播到整个球体。</p>
</li>
</ol>
<h3 id="格式">格式</h3>
<p>环境音频有 6 种格式，分别为：A、B、C、D、E、G。</p>
<h3 id="用途">用途</h3>
<h4 id="一阶环境音频的用途">一阶环境音频的用途</h4>
<p>第一阶的环境音频或 B 格式的环境音频，其麦克风用于使用四面体阵列表示线性 VR。</p>
<p>此外，这些在四个通道中进行处理，例如提供非定向压力水平的“W”。同时，“X、Y 和 Z”分别促进了从前到后、从侧到侧以及从上到下的方向信息。</p>
<p>一阶环境音频仅适用于相对较小的场景，因为其有限的空间保真度会影响声音定位。</p>
<h4 id="高阶环境音频的用途">高阶环境音频的用途</h4>
<p>高阶环境音频通过增加更多的麦克风来增强一阶环境音频的性能效率。</p>
<h2 id="总结">总结</h2>
<p></p>
]]></description>
</item>
<item>
    <title>自适应策略之viewport依赖型</title>
    <link>https://ayamir.github.io/posts/papers/note7/</link>
    <pubDate>Sun, 14 Nov 2021 13:24:59 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note7/</guid>
    <description><![CDATA[<h2 id="概述">概述</h2>
<p>在 360 度视频的推流过程中，根据用户头部的运动自适应地动态选择推流的区域，调整其比特率，以达到节省带宽的目的。</p>
<h2 id="通常的实现方式">通常的实现方式</h2>
<p>在服务端提供几个自适应集，来在遇到用户头部的突然运动的情况时，能保证 viewport 的平滑转换。</p>
<p>提出 QER(Quality-focused Regios)的概念使 viewport 内部的视频分辨率高于 viewport 之外的视频分辨率。</p>
<p>非对称的方式以不同的空间分辨率推流来节省带宽。</p>
<ul>
<li>在播放过程中，客户端根据用户的方向来请求不同分辨率版本的视频。</li>
<li>优点是即使客户端对用户的方面做了错误预测，低质量的内容仍然可以在 viewport 中生成。</li>
<li>缺点是在大多数场景下，这种方案需要巨大的存储开销和处理负载。</li>
</ul>
<h2 id="自适应推流参数">自适应推流参数</h2>
<ol>
<li>可用带宽和网络吞吐量</li>
<li>Viewport 预测的位置</li>
<li>客户端播放器的可用缓冲</li>
</ol>
<h2 id="参数计算公式">参数计算公式</h2>
<ul>
<li>
<p>第 n 个估计的 Viewport：$V^e(n)$</p>
<p>$V^e(n) = V_{fb}$</p>
<p>$V_{fb}$是最新报告的 viewport 位置</p>
</li>
<li>
<p>第 n 个估计的吞吐量：$T^e(n)$</p>
<p>$T^e(n) = T_{fb}$</p>
<p>$T_{fb}$是最新报告的吞吐量</p>
</li>
<li>
<p>比特率：$R_{bits}$</p>
<p>$R_{bits} = (1-\beta)T^e(n)$</p>
<p>$\beta$是安全边缘</p>
</li>
<li>
<p>第 n 个帧的客观度量质量：$VQ(k)$和最终客观度量质量$VQ$</p>
<p>$VQ=\frac{1}{L}\sum^L_{k=1}VQ(k)$</p>
<p>$VQ(k) = \sum_{t=1}^{T^n}w_k(k) * D^n_t(V_t, k)$</p>
<p>$w_k = \frac{A(t,k)}{A_{vp}}$</p>
<p>$L=总帧数$</p>
<p>$w_k$表示在第 k 个帧中与 viewport 所重叠的 tile 程度</p>
<p>$A(t,k)$表示第 k 个帧中 tile $t$ 重叠的区域</p>
<p>$A_{vp}$表示 viewport 中总共的区域</p>
</li>
</ul>
]]></description>
</item>
<item>
    <title>沉浸式流媒体现有标准</title>
    <link>https://ayamir.github.io/posts/papers/note6/</link>
    <pubDate>Thu, 11 Nov 2021 20:08:03 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note6/</guid>
    <description><![CDATA[<h2 id="omafomnidirectional-media-format">OMAF(Omnidirectional Media Format)</h2>
<p><code>OMAF</code>是第 1 个国际化的沉浸式媒体格式，描述了对 360 度视频进行编码、演示、消费的方法。</p>
<p><code>OMAF</code>与与现有格式兼容，包括编码（例如<code>HEVC</code>），文件格式（例如<code>ISOBMFF</code>），交付信号（例如<code>DASH</code>，<code>MMT</code>）。</p>
<p><code>OMAF</code>中还包括编码、投影、分包和 viewport 方向的元数据。</p>
<h2 id="omafdash-mpd">OMAF+DASH-&gt;MPD</h2>
<p>OMAF 与 DASH 相结合，再加上一些额外的描述构成了 MPD 文件格式，用于向客户端通知 360 度媒体的属性。</p>
<p>OMAF 规定了 9 中媒体配置文件，包括 3 种视频配置文件：基于 HEVC 的 viewport 独立型、基于 HEVC 的 viewport 依赖型、基于 AVC 的 viewport 依赖型。</p>
<p>OMAF 为视角独立型的推流提供了无视 viewport 位置的连续的视频帧质量。</p>
<p>常规的 HEVC 编码方式和 DASH 推流格式可以用于 viewport 独立型的推流工作。</p>
<p>但是使用 HEVC/AVC 编码方式的基于 viewport 的自适应操作是 OMAF 的一项技术开发，允许无限制地使用矩形 RWP 来增强 viewport 区域的质量。</p>
<h2 id="cmafcommon-media-application-format">CMAF(Common Media Application Format)</h2>
<p>致力于提供跨多个应用和设备之间的统一的编码格式和媒体配置文件。</p>
<p>CMAF 使请求低延迟的 segment 成为可能。</p>
<h2 id="isobmffiso-base-media-file-format">ISOBMFF(ISO Base Media File Format)</h2>
<p>ISOBMFF 是用于定时数据交换、管理和显示的最流行的文件格式。</p>
<ul>
<li>
<p>文件由一系列兼容并且可扩展的文件级别的 box 组成。</p>
</li>
<li>
<p>每个 box 表示 1 个由 4 个指针字符代码组成的数据结构。</p>
</li>
<li>
<p>ISOBMFF 的媒体数据流和元数据流被分别分发。</p>
<ul>
<li>媒体数据流中包括编码过的音频和视频数据。</li>
<li>元数据流中包括媒体类型、编码属性、时间戳、大小等元数据，也包括全向内容的额外信息如投影格式、旋转、帧分包、编码和分发等元数据。</li>
</ul>
</li>
<li>
<p>ISOBMFF 为了访问方便，保证有价值信息能灵活聚合。</p>
</li>
</ul>
<h2 id="3dof3-degree-of-freedom">3DoF(3 Degree of Freedom)</h2>
<p>在 3DoF 场景中，用户可以自由的移动头部以三个方向：摆动、俯仰、旋转。</p>
<h2 id="3dof">3DoF+</h2>
<p>用户的头部可以以任意方向移动：上下、左右、前后</p>
<h2 id="6dof">6DoF</h2>
<p>不只用户的头部，用户的身体也是自由的。同时支持方向与位置的自由。</p>
]]></description>
</item>
<item>
    <title>自适应360度视频推流挑战</title>
    <link>https://ayamir.github.io/posts/papers/note5/</link>
    <pubDate>Thu, 04 Nov 2021 11:01:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note5/</guid>
    <description><![CDATA[<h1 id="背景">背景</h1>
<p>用户使用头戴设备比使用传统显示器观看 360 度视频内容时的满意度对于扰乱更加敏感。</p>
<p>沉浸式的体验受到不完美的视角预测和高度动态化的网络状况的消极影响。</p>
<p>目前主要面临的挑战有以下 4 个：</p>
<p></p>
<h2 id="viewport-预测">Viewport 预测</h2>
<h3 id="背景-1">背景</h3>
<p>HMD 的本质特征是快速响应用户头部的移动。当用户改变 viewport 时 HMD 处理交互并检测相关的 viewport 来精确播放器的信息，这样视野就能以正常的可视角度被提供给用户。Viewport 预测在优化的 360 度视频推流中非常必要。配备有位置传感器的可穿戴 HMD 允许客户端更新其视角方向相应的视角场景。</p>
<h3 id="分类">分类</h3>
<ul>
<li><em>内容不可知</em>的方式基于历史信息对 viewport 进行预测。</li>
<li><em>内容感知</em>的方式需要视频内容信息来预测未来的 viewport。</li>
</ul>
<h3 id="内容不可知方式">内容不可知方式</h3>
<h4 id="分类-1">分类</h4>
<ul>
<li>平均线性回归 LR</li>
<li>航位推算 DR</li>
<li>聚类</li>
<li>机器学习 ML</li>
<li>编解码器体系结构</li>
</ul>
<h4 id="现有成果">现有成果</h4>
<h5 id="qians-worklr">Qian&rsquo;s work——LR</h5>
<p>使用平均线性回归和加权线性回归模型来做 viewport 预测，之后对与预测区域重叠的 tile 进行整体推流。</p>
<ul>
<li>当预测后 0.5s、1s、2s 加权线性回归表现更好</li>
</ul>
<h5 id="petrangelis-worklr">Petrangeli&rsquo;s work——LR</h5>
<p>将被划分成 tile 的等矩形的帧分成 3 个区域：viewport 区、相邻区、其他区。</p>
<p>结合观察者头部的移动，将可变比特率分配给可见和不可见区域。</p>
<p>作者利用最近（100 毫秒）用户观看历史的线性外推来预测未来的注视点。</p>
<h5 id="mavlankar-and-girods-work运动向量">Mavlankar and Girod&rsquo;s work——运动向量</h5>
<p>使用运动向量比如观察者的平移、倾斜、缩放等方向上的速度和加速度，来执行视角区域预测。</p>
<h5 id="la-fuentes-work运动向量">La Fuente&rsquo;s work——运动向量</h5>
<p>考虑了两种预测变体：角速度和角加速度，从用户以前的方向数据来估计未来的头部方向。按照预测结果分配不同的量化参数到每个 tile 上。</p>
<p>当进行进一步的预测时（超过 2s），这种方式限制了预测的精度。</p>
<p>如果视频 tile 被基于错误的预测而被请求，用户的实际 viewport 可能会被没有请求因而没有内容的黑色 tile 所覆盖。</p>
<h5 id="bans-workknnlr">Ban&rsquo;s work——KNN+LR</h5>
<p>使用 KNN 算法利用跨用户观看历史，使用 LR 模型利用户个体化的行为。</p>
<p>就视角预测的准确率而言，分别取得了 20%和 48%的绝对和相对改进。</p>
<h5 id="lius-workcluster">Liu&rsquo;s work——cluster</h5>
<p>提出了使用数据融合方法，通过考虑几个特征来估计未来视角位置。特征例如：用户的参与度、用户观看同一视频的行为、单个用户观看多个视频的行为、最终用户设备、移动性水平。</p>
<h5 id="petrangelis-workcluster">Petrangeli&rsquo;s work——cluster</h5>
<p>基于车辆轨迹预测的概念，考虑了类似的轨迹形成一个簇来预测未来的 viewport。</p>
<p>结果表明这种方法为更长的视野提高了精确度。</p>
<p>检查了来自三个欧拉角的不同轨迹，这样做可能导致性能不足。</p>
<h5 id="rossis-workcluster">Rossi&rsquo;s work——cluster</h5>
<p>提出了一种聚类的方法，基于球形空间中有意义的 viewport 重叠来确认用户的簇。</p>
<p>基于 Bron-Kerbosch（BK）算法的聚类算法能够识别大量用户，这些用户观看的是相同的 60%的 3s 长球形视频块。</p>
<p>与基准相比，该方法为簇提供了可兼容且重要的几何 viewport 重叠。</p>
<h5 id="jiangs-work">Jiang&rsquo;s work</h5>
<p>背景：</p>
<p>LR 方法对于长期的预测视野会导致较差的预测精度。长短时记忆（LSTM）是一种递归神经网络（RNN）架构，适用于序列建模和模式开发。</p>
<p>方法：</p>
<p>为了在 FoV 预测中获取比 LR 方法更高的精确度，开发了一种使用带有 128 个神经元的 LSTM 模型的 viewport 预测方法。</p>
<ul>
<li>分析了 360 度数据集，观察到用户在水平方向头部有快速转向，但是在垂直方向几乎是稳定的。</li>
<li>实验表明，这种方法同时考虑水平和垂直方向的头部移动时，比 LR 等方法产生了更少的预测错误。</li>
</ul>
<h5 id="baos-work">Bao&rsquo;s work</h5>
<p>背景：</p>
<p>对 150 个用户进行了 16 个视频剪辑的主观实验，并对其行为进行了分析。</p>
<p>使用 3 个方向的欧拉角$\theta$, $\phi$, $\psi$来表示用户在 3D 空间中头部的移动，结果表明不同方向的动作有强自相关性和消极的互相关性。因此多个角度的预测可以分开进行。</p>
<p>方法：</p>
<p>开发两个独立的 LSTM 模型来分别预测$\theta$和$\phi$，之后将预测结果应用于目标区域流来有效利用可用网络资源。</p>
<h5 id="hous-work">Hou&rsquo;s work</h5>
<ul>
<li>提出一种基于深度学习的视角产生方法来只对提前预测的 360 度视频和 3 自由度的 VR 应用的 viewport tile 进行抽取和推流。（使用了大规模的数据集来训练模型）</li>
<li>使用包含多层感知器和 LSTM 模型来预测 6 自由度的 VR 环境中头部乃至身体的移动，预测的视野被预渲染来做到低延迟的 VR 体验。</li>
</ul>
<h5 id="heyses-work">Heyse&rsquo;s work</h5>
<p>背景：</p>
<p>在某些例子中，用户的移动在视频的不同部分中非常不稳定。这增加了机器学习方式的训练压力。</p>
<p>方法：</p>
<p>提出了一个基于 RL 模型的上下文代理，这个模型首先检测用户的显著移动，然后预测移动的方向。这种分层自学习执行器优于球形轨迹外推法（这种方法将用户运动建模为轨迹的一部分，而不是单位球体上的完整轨迹）</p>
<h5 id="qians-work">Qian&rsquo;s work</h5>
<p>提出了一种叫做 Flare 的算法来最小化实际 viewport 和预测 viewport 之间的不匹配。</p>
<ul>
<li>应用了一种 ML 方法来执行频繁的 viewport 预测，包括从 130 名用户收集的 1300 条头部运动轨迹的 4 个间隔。</li>
<li>使用 viewport 轨迹预测，Flare 可以将错误预测替换成最新预测。</li>
</ul>
<h5 id="yu-and-lius-work">Yu and Liu&rsquo;s work</h5>
<p>背景：</p>
<p>LSTM 网络本身具有耗时的线性训练特性。编解码器的 LSTM 模型把训练过程并行化，相比于 LR 和 LSTM 本身而言，改善了预测精度。</p>
<p>方法：</p>
<p>使用基于注意力的 LSTM 编解码器网络体系结构来避免昂贵的递归并能更好地捕获 viewport 变化。</p>
<ul>
<li>提出的体系结构相比于传统的 RNN，获得了更高的预测精度，更低的训练复杂度和更快的收敛。</li>
</ul>
<h5 id="jamalis-work">Jamali&rsquo;s work</h5>
<p>提出使用 LSTM 编解码器网络来做长期的 viewport 预测（例如 3.5s）。</p>
<p>收集了低延迟异质网络上跨用户的方向反馈来调整高延迟网络上目标用户的预测性能。</p>
<h3 id="内容感知方式">内容感知方式</h3>
<h4 id="背景-2">背景</h4>
<p>内容感知方式可以提高预测效率。</p>
<h4 id="具体方法">具体方法</h4>
<h5 id="aladaglis-work">Aladagli&rsquo;s work</h5>
<p>提出了一个显著性驱动的模型来提高预测精度。</p>
<ul>
<li>没有考虑用户在 360 度视频中的视角行为。</li>
<li>viewport 预测错误可以通过理解用户对 360 度视频独特的可见注意力最小化。</li>
</ul>
<h5 id="nguyens-work">Nguyen&rsquo;s work</h5>
<p>背景：</p>
<p>大多数现存的方法把显著性图看作是 360 度显示中的位置信息来获得更好的预测结果。</p>
<p>通用的显著性和位置信息体系结构基于固定预测模型。</p>
<p>方法：</p>
<p>提出了<code>PanoSalNet</code>来捕获用户在 360 度帧中独特的可见注意力来改善显著性检测的性能。</p>
<ul>
<li>同时使用 HMD 特性和显著性图的固定预测模型获得了可测量的结果。</li>
</ul>
<h5 id="xus-work">Xu&rsquo;s work</h5>
<p>提出了两个 DRL(Deep Reinforcement Learning)模型用于同时考虑运动轨迹和可见注意力特性的 viewport 预测网络。</p>
<ul>
<li>离线模型基于内容流行度检测每个帧里的显著性。</li>
<li>在线模型基于从离线模型获得的显著性图和之前的 viewport 预测信息预测 viewport 方向和大小。</li>
<li>这个网络只能预测 30ms 的下一个 viewport 位置。</li>
</ul>
<h5 id="xus-work-1">Xu&rsquo;s work</h5>
<p>收集了大规模的被使用带有眼部轨迹跟踪的 HMD 的 45 个观测者观察的动态 360 度视频数据集，提出了基于历史扫描路径和图像特征预测注视位移的方法。</p>
<ul>
<li>在与当前注视点、viewport 和整个图像相关的三个空间尺度上执行了显著性计算。</li>
<li>可能的图像特性被通过向 CNN 喂图像和相应的显著性图，同时 LSTM 模型捕获历史信息来抽取出来。</li>
<li>之后将 LSTM 和 CNN 特性耦合起来，用于下一次的用户注视信息预测。</li>
</ul>
<h5 id="fans-work">Fan&rsquo;s work</h5>
<p>用户更容易被运动的物体吸引，因此除了显著性图之外，Fan 等人也考虑了使用预训练  的 CNN 来估计用户未来注视点的内容运动图。</p>
<ul>
<li>由于可能存在多个运动，这让预测变得不可靠，因此运动贴图的开发还需要进一步的研究。</li>
</ul>
<h5 id="yangs-work">Yang&rsquo;s work</h5>
<ul>
<li>使用 CNN 模型基于历史观测角度信息预测了单 viewport。</li>
<li>接着考虑了一种使用内容不可知和内容感知方法如 RNN 和 CFVT 模型的融合层的 viewport 轨迹预测策略。</li>
<li>融合模型使其同时支持更好地预测并且提高了大概 40%的精度。</li>
</ul>
<h5 id="ozcinars-work">Ozcinar&rsquo;s work</h5>
<p>将 viewport 轨迹转换为基于 viewport 的视觉注意图，然后对不同大小的 tile 进行推流以保证更高的编码效率。</p>
<h5 id="lis-work">Li&rsquo;s work</h5>
<p>现有的预测模型对未来的预测能力有限，Li 等人提出了两种模型，分别用于 viewport 相关和基于 tile 的推流系统。</p>
<ul>
<li>第一个模型应用了基于用户轨迹的 LSTM 编解码网络体系结构。</li>
<li>第二个模型应用了卷积 LSTM 编解码体系结构，使用序列的热图来预测用户的未来方向。</li>
</ul>
<h3 id="总结">总结</h3>
<p>精确的方向预测使 360 度视频的客户端可以以高分辨率下载最相关的 tile。</p>
<p>当前采用显著性和位置信息的神经网络模型的性能比直接利用当前观察位置进行未来 viewport 位置估计的简单无运动的基线方法表现差。估计的显著性中的噪音等级限制了这些模型的预测精度。并且这些模型也引入了额外的计算复杂度。</p>
<p>对于 360 度视频注意点的可靠预测和用户观看可能性与显著性图之间关系的理解，显著性模型必须被改善并通过训练大规模的数据集来适应，尤其是被配备了不同摄像机旋转的镜头所捕获的数据。</p>
<p>另一方面，卷积 LSTM 编解码器和基于轨迹的预测方法适合长期预测，并能带来相当大的 QoE 改进，特别是在协作流媒体环境中。</p>
<h2 id="qoe-评估">QoE 评估</h2>
<h3 id="背景-3">背景</h3>
<p>由于全方位视频非常普遍，因此，通过这种类型的视频分发来确定用户的特定质量方面是至关重要的。QoE 在视频推流应用中扮演着重要角色。在传统视频推流中，QoE 很大程度上被网络负载和分发性能所影响。现有的次优目标度量方法并不适用于全向视频，因为全向视频受网络状况和用户视角行为的影响很大。</p>
<h3 id="主观质量评估">主观质量评估</h3>
<p>主观质量评估是估计 360 度视频推流质量的现实并且可靠的方法。</p>
<h4 id="upeniks-work">Upenik&rsquo;s work</h4>
<p>用一台 MergeVR HMD 执行了主观测试来体验 360 度图像。</p>
<ul>
<li>实验数据包括主观分数、视角轨迹、在每个图像上花费的时间由软件上获得。</li>
<li>视角方向信息被用于计算显著性图。</li>
<li>但是这项研究没有考虑对 360 度视频的评估。</li>
</ul>
<h4 id="zhangs-work">Zhang&rsquo;s work</h4>
<p>为了弥补 360 度视频和常规视频度量方式之间的性能差距，为全景视频提出了一种主观质量评估方法，称为<em>SAMPVIQ</em>。</p>
<ul>
<li>23 位参与者被允许观看 4 个受损视频，整体视频质量体验的评分在 0～5 分之间。</li>
<li>参与者之间存在较大的评分差异。</li>
</ul>
<h4 id="xus-work-2">Xu&rsquo;s work</h4>
<p>提出两种主观测量方式：总体区分平均意见分数(O-DMOS)和矢量区分平均意见分数(V-DMOS)来获得 360 度视频的质量损失。</p>
<ul>
<li>类似于传统食品的 DMOS 度量方式，O-DMOS 度量方式计算主观测试序列的总计区分分数。</li>
</ul>
<h4 id="schatzs-work">Schatz&rsquo;s work</h4>
<p>研究了使用 HMD 观看 360 度内容时停顿事件的影响。</p>
<ul>
<li>沉浸式内容的主观质量评估并非不重要，可能导致比实际推荐更多的开放性问题。</li>
<li>通常来讲人们的期望于传统的 HAS 相似，即如果可能的话，根本没有停顿。</li>
</ul>
<h4 id="可用的开源工具">可用的开源工具</h4>
<p>AVTrack360，OpenTrack 和 360player 能捕获用户观看 360 度视频的头部轨迹。</p>
<p>VRate 是一个在 VR 环境中提供主观问卷调查的基于 Unity 的工具。</p>
<p>安卓应用*<a href="https://github.com/zerepolbap/miro360" target="_blank" rel="noopener noreffer">MIRO360</a>*，支持未来 VR 主观测试的指南开发。</p>
<h4 id="cybersickness"><code>Cybersickness</code></h4>
<p><code>Cybersickness</code>是一种获得高 QoE 的潜在障碍，它能引起疲劳、恶心、不适和呕吐。</p>
<h5 id="singlas-work">Singla&rsquo;s work</h5>
<p>使用受限的带宽和分辨率，在不同的延迟情况下进行了两个主观实验。</p>
<ul>
<li>开发了主观测试平台、测试方法和指标来评估 viewport 自适应 360 度视频推流中的视频感知等级和<code>Cybersickness</code>。</li>
<li>基于 tile 的推流在带宽受限的情况下表现很好。</li>
<li>47ms 的延迟实际上不影响感知质量。</li>
</ul>
<h5 id="trans-work">Tran&rsquo;s work</h5>
<p>考虑了几个影响因子例如内容的空间复杂性，数量参数，分辨率特性和渲染模型来评估 cybersickness，质量，可用性和用户的存在。</p>
<ul>
<li>VR 环境中快速移动的内容很容易引发 cybersickness。</li>
<li>由于高可用性和存在性，用户的 cybersickness 也可能加剧。</li>
</ul>
<h5 id="singlas-work-1">Singla&rsquo;s work</h5>
<p>评估了 28 名受试者在 Oculus Rift 和 HTC Vive 头戴式电脑上观看 6 个全高清和超高清分辨率 YouTube 视频时的观看不适感。</p>
<ul>
<li>HMD 的类型轻微地影响感知质量。</li>
<li>分辨率和内容类型强烈影响个人体验。</li>
<li>女性用户感到<code>cybersickness</code>的人数更多。</li>
</ul>
<h4 id="空间存在感">空间存在感</h4>
<p>空间存在感能增强沉浸感。</p>
<h5 id="zous-work">Zou&rsquo;s work</h5>
<p>方法：</p>
<p>提出了一个主观框架来测量 25 名受试者的空间存在感。</p>
<ul>
<li>提出的框架包括三层，从上到下分别为：空间存在层、感知层、科技影响层。</li>
<li>心理上的空间存在感形成了空间存在层。</li>
<li>感知层以视频真实感、音频真实感和交互元素为特征。</li>
<li>科技影响层由几个模块组成，这些模块与感知层相连，以反映传感器的真实性。</li>
</ul>
<h5 id="huponts-work">Hupont&rsquo;s work</h5>
<p>应用通用感知的原则来研究在 Oculus HMD 和传统 2D 显示器上玩游戏的用户的空间存在感。</p>
<ul>
<li>与 2D 显示器相比，3D 虚拟现实主义显示出更高的惊奇、沉浸感、存在感、可用性和兴奋感。</li>
</ul>
<h4 id="生理特征度量">生理特征度量</h4>
<h5 id="salgados-work">Salgado&rsquo;s work</h5>
<p>方法：</p>
<p>捕获多种多样的生理度量，例如心率 HR，皮肤电活性 EDA、皮肤温度、心电图信号 ECG、呼吸速率、血压 BVP、脑电图信号 EEG 来评价沉浸式模拟器的质量。</p>
<h5 id="egans-work">Egan&rsquo;s work</h5>
<p>基于 HR 和 EDA 信号评估 VR 和非 VR 渲染模式质量分数。</p>
<ul>
<li>相比于 HR，EDA 对质量分数有强烈的影响。</li>
</ul>
<h4 id="技术因素感知">技术因素感知</h4>
<p>不同的技术和感知特征，如失真、清晰度、色彩、对比度、闪烁等，用于评估感知视频质量。</p>
<h5 id="fremereys-work">Fremerey&rsquo;s work</h5>
<p>确定了可视质量强烈地依赖于应用的运动插值（MI）算法和视频特征，例如相机旋转和物体的运动。</p>
<p>在一项主观实验中，12 位视频专家回顾了使用 FFmpeg 混合、FFmpeg MCI（运动补偿插值）和 butterflow 插值到 90 fps 的四个视频序列。作者发现，与其他算法相比，MCI 在 QoE 方面提供了极好的改进。</p>
<h4 id="总结-1">总结</h4>
<p>主观测试与人眼直接相关，并揭示了 360 度视频质量评估的不同方面的影响。</p>
<p>在这些方面中，空间存在感和由佩戴 VR 头戴设备观看 360 度视频导致的<em>cybersickness</em>极为重要，因为这些效果并不在传统的 2D 视频观看中出现。</p>
<p>主观评估需要综合的手工努力并因此昂贵耗时并易于出错，相对而言，客观评估更易于管理和可行。</p>
<h3 id="客观质量评估">客观质量评估</h3>
<p>由于类似的编码结构和 2D 平面投影格式，对 360 度内容应用客观质量评估很自然。</p>
<h4 id="计算-psnr">计算 PSNR</h4>
<p>现有投影方式中的采样密度在每个像素位置并不均匀。</p>
<h5 id="yus-work">Yu&rsquo;s work</h5>
<p>为基于球形的 PSNR 计算引入 S-PSNR 和 L-PSNR。</p>
<ul>
<li>S-PSNR 通过对球面上所有位置的像素点做同等加权来计算 PSNR。</li>
<li>利用插值算法，S-PSNR 可以完成对支持多种投影模式的 360 度视频的客观质量评估。</li>
<li>L-PSNR 通过基于纬度和访问频率的像素点加权测量 PSNR。</li>
<li>L-PSNR 可以测量 viewport 的平均 PSNR 而无需特定的头部运动轨迹。</li>
</ul>
<h5 id="zakharchenkos-work">Zakharchenko&rsquo;s work</h5>
<p>提出了一种 Craster Parabolic Projection-PSNR (CPP-PSNR) 度量方式来比较多种投影方案，通过不改变空间分辨率和不计算实际像素位置的 PSNR，将像素重新映射成 CPP 投影。</p>
<ul>
<li>CPP 投影方式可能使视频分辨率大幅下降。</li>
</ul>
<h5 id="suns-work">Sun&rsquo;s work</h5>
<p>提出了一种叫做 weighted-to-spherically-uniform PSNR (WS-PSNR)的质量度量方式，以此来测量原始和受损内容之间的质量变化。</p>
<ul>
<li>根据像素在球面上的位置考虑权重。</li>
</ul>
<h4 id="计算-ssim">计算 SSIM</h4>
<p>SSIM 是另一种质量评估指标，它通过三个因素反映图像失真，包括亮度、对比度和结构。</p>
<h5 id="chens-work">Chen&rsquo;s work</h5>
<p>为 2D 和 360 度视频分析了 SSIM 结果，引入了球型结构的相似性度量（S-SSIM）来计算原始和受损的 360 度视频之间的相似性。</p>
<ul>
<li>在 S-SSIM 中，使用重投影来计算两个提取的 viewport 之间的相似性。</li>
</ul>
<h5 id="zhous-work">Zhou&rsquo;s work</h5>
<p>考虑相似性的权重提出了 WS-SSIM 来测量投影区域中窗口的相似性。</p>
<ul>
<li>性能评估表明，与其他质量评估指标相比，WS-SSIM 更接近人类感知。</li>
</ul>
<h5 id="van-der-hoofts-work">Van der Hooft&rsquo;s work</h5>
<p>提出了<em>ProbGaze</em>度量方式，基于 tile 的空间尺寸和 viewport 中的注视点。</p>
<ul>
<li>考虑外围 tile 的权重来提供合适的质量测量。</li>
<li>相比于基于中心和基于平均的 PSNR 和 SSIM 度量方式，<em>ProbGaze</em>能估计当用户突然改变 viewport 位置时的视频质量变化。</li>
</ul>
<h5 id="xus-work-3">Xu&rsquo;s work</h5>
<p>引入了两种客观质量评估度量手段：基于内容感知的 PSNR 和非内容感知的 PSNR，用于编码 360 度视频。</p>
<ul>
<li>第一种方式基于空间全景内容对像素失真进行加权。</li>
<li>第二种方式考虑人类偏好的统计数据来估计质量损失。</li>
</ul>
<h4 id="基于-psnr-和-ssim-方式的改进">基于 PSNR 和 SSIM 方式的改进</h4>
<p>尽管各种基于 PSNR 和 SSIM 的方式被广阔地应用到了 360 度视频的质量评估中，但这些方式都没有真正地捕获到感知质量，特别是当 HMD 被用于观看视频时。因此需要为 360 度内容特别设计一种优化的质量度量方式。</p>
<h5 id="upeniks-work-1">Upenik&rsquo;s work</h5>
<p>考虑了一场使用 4 张高质量 360 度全景图像来让 45 名受试者在不同的编码设定下评估和比较客观质量度量方式性能的主观实验。</p>
<ul>
<li>现有的客观度量方式和主观感知到的质量相关性较低。</li>
</ul>
<h5 id="trans-work-1">Tran&rsquo;s work</h5>
<p>论证主观度量和客观度量之间相关性较高，但是使用的数据集较小。</p>
<h4 id="基于-ml-的方式">基于 ML 的方式</h4>
<p>基于 ML 的方式可以弥补客观评估和主观评估之间的差距。</p>
<h5 id="da-costa-filhos-work">Da Costa Filho&rsquo;s work</h5>
<p>提出了一个有两个阶段的模型。</p>
<ul>
<li>首先自适应 VR 视频的播放性能由机器学习算法所确定。</li>
<li>之后模型利用估计的度量手段如视频质量、质量变化、卡顿时间和启动延迟来确定用户的 QoE。</li>
</ul>
<h5 id="lis-work-1">Li&rsquo;s work</h5>
<p>引入了基于 DRL 的质量获取模型，在一次推流会话中同时考虑头部和眼部的移动。</p>
<ul>
<li>360 度视频被分割成几个补丁。</li>
<li>低观看概率的补丁被消除。</li>
<li>参考和受损视频序列都被输入到深度学习可执行文件中，以计算补丁的质量分数。</li>
<li>之后分数被加权并加到一起得到最终的分数。</li>
</ul>
<h5 id="yangs-work-1">Yang&rsquo;s work</h5>
<p>考虑了多质量等级的特性和融合模型。</p>
<ul>
<li>质量特性用<code>region of interest(ROI)</code>图来计算，其中包括像素点等级、区域等级、对象等级和赤道偏差。</li>
<li>混合模型由后向传播的神经网络构造而成，这个神经网络组合了多种质量特性来获取整体的质量评分。</li>
</ul>
<h3 id="总结-2">总结</h3>
<p>精确的 QoE 获取是优化 360 度视频推流服务中重要的因素，也是自适应分发方案中基础的一环。</p>
<p>单独考虑 VR 中的可视质量对完整的 QoE 框架而言并不足够。</p>
<p>为能获得学界的认可，找到其他因素的影响也很必要，例如<code>cybersickness</code>，生理症状，用户的不适感，HMD 的重量和可用性，VR 音频，viewport 降级率，网络特性（延迟，抖动，带宽等），内容特性（相机动作，帧率，编码，投影等），推流特性（viewport 偏差，播放缓冲区，时空质量变化等）。</p>
<h2 id="低延迟推流">低延迟推流</h2>
<h3 id="背景-4">背景</h3>
<p>360 度全景视频推流过程中的延迟由几部分组成：传感器延迟、云/边处理延迟、网络延迟、请求开销、缓冲延迟、渲染延迟和反馈延迟。</p>
<p>低延迟的要求对于云 VR 游戏、沉浸式临场感和视频会议等更为严格。</p>
<p>要求极低的终端处理延迟、快速的云/边计算和极低的网络延迟来确保对用户头部移动做出反馈。</p>
<p>现代 HMD 可以做到使传感器延迟降低到用户无法感知的程度。</p>
<p>传输延迟已经由 5G 移动和无线通信技术大幅减少。</p>
<p>但是，对于减少处理、缓冲和渲染延迟的工作也是必要的。</p>
<p>许多沉浸式应用的目标是 MTP 的延迟少于 20ms，理想情况是小于 15ms。</p>
<h3 id="减少启动时间">减少启动时间</h3>
<h4 id="减少初始化请求的数据量">减少初始化请求的数据量</h4>
<p>通常来讲，较小的视频 segment 能减少启动和下载时间。</p>
<h5 id="van-der-hoofts-work-1">Van der Hooft&rsquo;s work</h5>
<p>考虑了新闻相关内容的推流，使用的技术有：</p>
<ol>
<li>服务端编码</li>
<li>服务端的用户分析</li>
<li>服务器推送策略</li>
<li>客户端积极存储视频数据</li>
</ol>
<p>取得的效果：</p>
<ul>
<li>降低了启动时间</li>
<li>允许不同网络设定下的快速内容切换</li>
<li>较长的响应时间降低了性能</li>
</ul>
<h5 id="nguyens-work-1">Nguyen&rsquo;s work</h5>
<p>基于 viewport 依赖的自适应策略分析了自适应间隔延迟和缓冲延迟的影响。</p>
<ul>
<li>使用服务端比特率计算策略来最小化响应延迟的影响。</li>
<li>根据客户端的响应估计可用的网络吞吐量和未来的 viewport 位置。</li>
<li>服务端的决策引擎推流合适的 tile 来满足延迟限制。</li>
</ul>
<p>取得的效果：</p>
<ul>
<li>对于 viewport 依赖型推流方案而言，较少的自适应和缓冲延迟不可避免。</li>
</ul>
<h3 id="降低由-tile-分块带来的网络负载">降低由 tile 分块带来的网络负载</h3>
<p>在 HTTP/1.1 中，在空间上将视频帧分成矩形 tile 会增加网络负载，因为每个 tile 会产生独立的网络请求。</p>
<p>请求爆炸的问题导致了较长的响应延迟，但是可以通过使用 HTTP/2 的服务器推送特性解决。这个特型使服务器能使用一条 HTTP 请求复用多条消息。</p>
<h5 id="weis-work">Wei&rsquo;s work</h5>
<p>利用 HTTP/2 协议来促进低延迟的 HTTP 自适应推流。</p>
<ul>
<li>提出的服务端推送的策略使用一条请求同时发送几个 segment 避免多个 GET 请求。</li>
</ul>
<h5 id="petrangelis-work">Petrangeli&rsquo;s work</h5>
<p>结合特定请求参数与 HTTP/2 的服务端推送特性来促进 360 度视频推流。</p>
<ul>
<li>客户端为一个 segment 发送一条 call，服务器使用 FCFS 策略传送 k 个 tile。</li>
<li>利用 HTTP/2 的优先级特性可以使高优先级的 tile 以紧急的优先级被获取，进而改善网络环境中的高往返时间的性能。</li>
</ul>
<h5 id="xus-work-4">Xu&rsquo;s work</h5>
<p>为 360 度内容采用了<code>k-push</code>策略：将 k 个 tile 推送到客户端，组成一个单独的时间段。</p>
<ul>
<li>提出的方法与 QoE 感知的比特率自适应算法一起，在不同的 RTT 设定下，提高了 20%的视频质量，减少了 30%的网络传输延迟。</li>
</ul>
<h5 id="yahias-work">Yahia&rsquo;s work</h5>
<p>使用 HTTP/2 的优先级和多路复用功能，在两个连续的 viewport 预测之间，即在交付相同片段之前和期间，组织紧急视频块的受控自适应传输。</p>
<h5 id="yens-work">Yen&rsquo;s work</h5>
<p>开发了一种支持 QUIC 的体系结构来利用流优先级和多路复用的特性来实现 360 度视频的安全和低优先级的传输。</p>
<ul>
<li>当 viewport 变化发生时，QUIC 能让常规的 tile 以低优先级推流，viewport 内的 tile 以高优先级推流，都通过一条 QUIC 连接来降低 viewport tile 的缺失率。</li>
<li>作者说测试表明基于 QUIC 的自适应 360 度推流比 HTTP/1.1 和 HTTP/2 的方案表现更好。</li>
</ul>
<h3 id="使用移动边缘计算降低延迟">使用移动边缘计算降低延迟</h3>
<h5 id="mangiantes-work">Mangiante&rsquo;s work</h5>
<p>提出了利用基于边缘处理的 viewport 渲染方案来减少延迟，同时利用终端设备上的电源和计算负载。</p>
<ul>
<li>但是作者没有给出有效的算法或是建立一个实践执行平台。</li>
</ul>
<h5 id="lius-work">Liu&rsquo;s work</h5>
<p>采用远端渲染技术，通过为不受约束的 VR 系统获取高刷新率来隐藏网络延迟。</p>
<ul>
<li>采用 60GHz 的无线链路支持的高端 GPU，来加快计算速度和 4K 渲染，减少显示延迟。</li>
<li>尽管提供了高质量和低延迟的推流，但是使用了昂贵的带宽连接，这通常并不能获得。</li>
</ul>
<h5 id="viitanens-work">Viitanen&rsquo;s work</h5>
<p>引入了端到端的 VR 游戏系统。通过执行边缘渲染来降低延迟，能源和计算开销。</p>
<ul>
<li>为 1080p 30fps 的视频格式实现了端到端的低延迟（30ms）的系统。</li>
<li>前提是有充足的带宽资源、终端设备需要性能强劲的游戏本。</li>
</ul>
<h5 id="shis-work">Shi&rsquo;s work</h5>
<p>考虑了不重视 viewport 预测的高质量 360 度视频渲染。</p>
<ul>
<li>提出的 MEC-VR 系统采用了一个远端服务器通过使用一个自适应裁剪过滤器来动态适应 viewport 覆盖率，这个过滤器按照观测到的系统延迟增加 viewport 之外的区域。</li>
<li>基于 viewport 覆盖率的延迟调整允许客户端容纳和补偿突然的头部移动。</li>
</ul>
<h3 id="共享-vr-环境中的延迟处理">共享 VR 环境中的延迟处理</h3>
<p>共享 VR 环境中用户的延迟取决于用户的位置和边缘资源的分发。</p>
<h5 id="parks-work">Park&rsquo;s work</h5>
<p>通过考虑多个用户和边缘服务器之间的双向通信，提出了一种使用线性蜂窝拓扑中的带宽分配策略，以最小化端到端系统延迟。确定了推流延迟强烈地依赖于：</p>
<ul>
<li>边缘服务器的处理性能</li>
<li>多个交互用户之间的物理和虚拟空间</li>
</ul>
<h5 id="perfectos-work">Perfecto&rsquo;s work</h5>
<p>集成了深度神经网络和毫米波多播传输技术来降低协同 VR 环境中的延迟。</p>
<ul>
<li>神经网络模型估计了用户即将来临的 viewport。</li>
<li>用户被基于预测的相关性和位置分组，以此来优化正确的 viewport 许可。</li>
<li>执行积极的多播资源调度来最小化延迟和拥塞。</li>
</ul>
<h3 id="总结-3">总结</h3>
<p>在单用户和多用户的环境中，边缘辅助的解决方式对于控制延迟而言占主要地位。</p>
<p>此外还有服务端的 viewport 计算、服务端 push 机制和远程渲染机制都能用于低延迟的控制。</p>
<p>现有的 4G 网络足以支持早期的自适应沉浸式多媒体，正在成长的 5G 网络更能满足沉浸式内容的需求。</p>
<h2 id="360-度直播推流">360 度直播推流</h2>
<h3 id="背景-5">背景</h3>
<p>传统的广播电视频道是直播推流的流行来源。现在私人的 360 度直播视频在各个社交媒体上也有大幅增长。</p>
<p>因为视频生产者和消费者之间在云端的转码操作，360 度视频推流是更为延迟敏感的应用。</p>
<p>现有的处理设备在诸如转码、渲染等实时处理任务上受到了限制。</p>
<h4 id="内容分发">内容分发</h4>
<h5 id="hus-work">Hu&rsquo;s work</h5>
<p>提出了一套基于云端的直播推流系统，叫做<code>MELiveOV</code>，它使高分辨率的全向内容的处理任务以毛细管分布的方式分发到多个支持 5G 的云端服务器。</p>
<ul>
<li>端到端的直播推流系统包括内容创作模块、传输模块和 viewport 预测模块。</li>
<li>移动边缘辅助的推流设计减少了 50%的带宽需求。</li>
</ul>
<h5 id="griwodzs-work">Griwodz&rsquo;s work</h5>
<p>为 360 度直播推流开发了优化 FoV 的原型，结合了 RTP 和基于 DASH 的<code>pull-patching</code>来传送两种质量等级的 360 度视频给华为 IPTV 机顶盒和 Gear VR 头戴设备。</p>
<ul>
<li>作者通过在单个 H.265 硬件解码器上多路复用多个解码器来实现集体解码器的想法，以此减少切换时间。</li>
</ul>
<h4 id="视频转码">视频转码</h4>
<h5 id="lius-work-1">Liu&rsquo;s work</h5>
<p>研究表明只转码 viewport 区域有潜力大幅减少高性能转码的计算需求。</p>
<h5 id="baigs-work">Baig&rsquo;s work</h5>
<p>开发了快速编码方案来分发直播的 4K 视频到消费端设备。</p>
<ul>
<li>采用了分层视频编码的方式来在高度动态且不可预测的 WiGig 和 WiFi 链路上分发质量可变的块。</li>
</ul>
<h5 id="les-work">Le&rsquo;s work</h5>
<p>使用 RTSP 网络控制协议为 CCTV 的 360 度直播推流提出了实时转码和加密系统。</p>
<ul>
<li>转码方式基于 ARIA 加密库，Intel 媒体 SDK 和 FFmpeg 库。</li>
<li>系统可以管理并行的转码操作，实现高速的转码性能。</li>
</ul>
<h4 id="内容拼接缝合">内容拼接缝合</h4>
<p>相比于其他因素如捕获、转码、解码、渲染，内容拼接在决定整体上的推流质量时扮演至关重要的角色。</p>
<h5 id="chens-work-1">Chen&rsquo;s work</h5>
<p>提出了一种内容驱动的拼接方式，这种方式将 360 度帧的语义信息的不同类型看作事件，以此来优化拼接时间预算。</p>
<ul>
<li>基于 VR 帧中的语义信息，tile 执行器模块选择合适的 tile 设计。</li>
<li>拼接器模块然后执行基于 tile 的拼接，这样，基于可用资源，事件 tile 有更高的拼接质量。</li>
<li>评估表明系统通过实现 89.4%的时间预算，很好地适应了不同的事件和时间限制。</li>
</ul>
<h3 id="总结-4">总结</h3>
<p>相比于点播式流媒体，360 度直播推流面临多个挑战，例如在事先不知情的情况下处理用户导航、视频的首次流式传输以及实时视频的转码。在多用户场景中，这些挑战更为棘手。</p>
<p>关于处理多个用户的观看模式，可伸缩的多播可以用于在低带宽和高带宽网络上以接近于按需推流的质量等级。</p>
<p>基于 ROI 的 tile 拼接和转码可以显著地减少延迟敏感的交互型应用的延迟需求。</p>
]]></description>
</item>
<item>
    <title>沉浸式流媒体网络问题的相关解决方案</title>
    <link>https://ayamir.github.io/posts/papers/note4/</link>
    <pubDate>Sat, 30 Oct 2021 19:20:00 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note4/</guid>
    <description><![CDATA[<h1 id="概况">概况</h1>
<p>现有的沉浸式流媒体应用都对带宽、QoS 和计算需求有着高要求，这主要得益于 5G 网络。</p>
<p>传统的中心化云计算和云存储体系结构不适于实时的高码率内容分发。</p>
<p>边缘缓存和移动边缘计算成为了推动沉浸式流媒体发展的关键技术。</p>
<h1 id="解决方案">解决方案</h1>
<h2 id="360-度视频的边缘协助推流">360 度视频的边缘协助推流</h2>
<h3 id="背景">背景</h3>
<p>主要的视频内容可以被传送到边缘节点乃至下游客户端来满足高分辨率等级和严格的低延迟要求。</p>
<p>在边缘计算中，处理和存储的任务被从核心网转移到边缘节点例如基站、微型数据中心和机顶盒等。</p>
<h4 id="hous-work">Hou&rsquo;s work</h4>
<p>提出边缘/云服务器渲染可以使计算更加轻便，可以让无线 VR/AR 体验可行并且便携。</p>
<h4 id="zhangs-work">Zhang&rsquo;s work</h4>
<p>为 VR 多人游戏提出了一种混合边缘云基础架构，中心云负责更新全局游戏事件，边缘云负责管理视图更新和大规模的帧渲染任务，以此来支持大量的在线联机人数的低延迟游戏。</p>
<p>进一步陈述了一种服务器选择算法，它基于 QoS 和玩家移动的影响确保所有 VR 玩家之间的公平性。</p>
<h4 id="los-work">Lo&rsquo;s work</h4>
<p>考虑了为 360 度视频渲染提供边缘协助的设备的异质性。</p>
<ul>
<li>边缘服务器将 HEVC tile 流转码为 viewport 视频流并传输到多个客户端。</li>
<li>最优化算法根据视频质量、HMD 类型和带宽动态决定边缘节点服务哪个客户端。</li>
</ul>
<h4 id="边缘缓存策略">边缘缓存策略</h4>
<h5 id="背景-1">背景</h5>
<p>传统视频的缓冲方案并不能直接应用到 360 度视频上。</p>
<p>为了在启用边缘缓存的网络中促进 360 度视频的传输，两个传输节点之间的代理缓存被部署来使用户侧的内容可用。</p>
<p>边缘缓存能从实质上减少重复的传输并且可以使内容服务器更加可扩展。</p>
<h5 id="mahzais-work">Mahzai&rsquo;s work</h5>
<p>基于其他用户的观看行为为 360 度视频的流行内容提出了一种缓存策略。</p>
<ul>
<li>与最不常用 (LFU) 和最近最少使用 (LRU) 缓存策略相比，在缓存使用方面的性能分别提高了至少 40% 和 17%。</li>
</ul>
<h5 id="papaioannous-work">Papaioannou&rsquo;s work</h5>
<p>提出了基于 tile 分辨率和需求统计信息的缓存策略，用最少的错误，提高要求 tile 的和缓存 tile 这两种版本的 viewport 覆盖率。</p>
<ul>
<li>不同缓存和传输延迟的实验评估表明提高了缓存命中率，特别是对于分层编码的 tile。</li>
</ul>
<h5 id="lius-work">Liu&rsquo;s work</h5>
<p>背景：</p>
<p>边缘缓存可以被在 Evolved Packet Core 处执行，因为 packet 大小很小所以这样可能会产生次优的性能。</p>
<p>另一种替换的方式是在 Radio Access Network 处缓存数据。但这样由于数据隧道和分包会变得更加复杂。</p>
<p>研究内容：</p>
<p>为移动网络提出了一种同时使用 RAN 和 EPC 的基于 tile 的缓存方案，以此在视频流延迟的约束下节省传输带宽。</p>
<ul>
<li>为 EPC 和 RAN 的缓存节点分别被部署在 Packet Data Network Gateway 和 eNodeBs。</li>
<li>EPC 中的内容控制实体负责为 tile 内容改善缓存利用率。</li>
<li>这种联合的 tile 缓存设计能以优秀的可伸缩性为<a href="https://zh.wikipedia.org/wiki/%E8%A1%8C%E5%8B%95%E7%B6%B2%E8%B7%AF%E5%9B%9E%E5%82%B3" target="_blank" rel="noopener noreffer">回程网络</a>显著地减少带宽压力。</li>
</ul>
<h5 id="maniotiss-work">Maniotis&rsquo;s work</h5>
<p>为了利用协作传输的机会，提出了一种在包含宏蜂窝基站(MBS)和多个小基站(SBS)的蜂窝网络中的 tile 级别的视频流行度感知缓存和传输方案。</p>
<ul>
<li>应用了一种高级的编码方式来创建灵活的 tile 编码结构，使在每个 SBS 中能协同缓存。</li>
<li>这种协同允许在 SBS 只存储可能被观看的图块，而其他图块可以通过回程链路获取。</li>
</ul>
<h5 id="chens-work">Chen&rsquo;s work</h5>
<p>为被捕获内容从<code>Drone base station</code>到小基站的联合缓存和分发提出了一种<code>echo-liquid</code>状态的 DRL 模型，使用高频毫米波通信技术。</p>
<ul>
<li>为了满足即时延迟的目标，基站可以从数据中缓存流行内容。</li>
<li>但是，小基站的广泛部署实际上消耗了很多能源。</li>
</ul>
<h5 id="yangs-work">Yang&rsquo;s work</h5>
<p>在计算资源受限制的 MEC 架构中，利用缓存和计算资源来降低对通信资源的要求。</p>
<ul>
<li>但是这种结构需要资源敏感的任务调度来平衡通信开销和延迟。</li>
</ul>
<h5 id="chakareskis-work">Chakareski&rsquo;s work</h5>
<p>为<code>multi-cell</code>网络环境中的 AR/VR 应用探索了最前沿的缓存、计算和通信机制。</p>
<ul>
<li>提出的框架允许基站利用适当的计算和缓存资源来最大化总计的回报。</li>
<li>只关注了缓存和渲染，没有考虑用户视角的感受以及处理事件。</li>
</ul>
<h5 id="suns-work">Sun&rsquo;s work</h5>
<p>在内容到达终端之前，同时利用 FoV(Field of View)缓存和必要的计算操作来节省通信带宽而不牺牲响应时间。</p>
<ul>
<li>对于同质的 FoVs，联合缓存和计算框架执行关于缓存和后期处理的最优决策。</li>
<li>对于异质的 FoVs，应用凹凸表达式来得到有吸引力的结果。</li>
</ul>
<h5 id="rigazzis-work">Rigazzi&rsquo;s work</h5>
<p>基于一个开源项目 Fog05 提出了一个三层(3C)解决方案来分发密集的任务（例如编解码和帧重建），穿越中心云层，受约束的雾层和边缘节点层。</p>
<ul>
<li>利用了系统可伸缩性、互操作性和 360 度视频推流服务的生命周期循环。</li>
<li>实验性的评估表明在带宽、能源消耗、部署开销和终端复杂性方面取得了显著的减少。</li>
</ul>
<h5 id="elbambys-work">Elbamby&rsquo;s work</h5>
<p>通过在延迟和可靠性的约束下，应用积极的计算和毫米波传输，为交互式的 VR 游戏提出了一个联合框架。</p>
<ul>
<li>对视频帧做预计算和存储来减少 VR 流量。</li>
<li>评估表明这种联合机制可以减少多达 30%的端到端延迟。</li>
</ul>
<h4 id="边缘计算的优势">边缘计算的优势</h4>
<ol>
<li>
<p>减少延迟</p>
<p>传统的云端节点距离用户较远，边缘计算使用户可以共享多个服务器池的协同计算资源。</p>
</li>
<li>
<p>降低能耗</p>
<p>根据网络架构和资源供应将计算卸载到分布式计算集群，能显著提高移动设备的性能。</p>
</li>
<li>
<p>负载均衡</p>
<p>边缘节点例如基站、小蜂窝和终端设备可以在用户端存储内容，降低了核心网的负载。</p>
</li>
</ol>
<h4 id="现有利用边缘计算的解决方案">现有利用边缘计算的解决方案</h4>
<p></p>
<ul>
<li>大多数任务卸载的 MEC 方案只致力于优化带宽、能源或延迟。</li>
<li>发展中的方案同时致力于许多其他重要的目标：可靠性、可移动性、QoS、部署成本、安全性。</li>
<li>利用带缓存的边缘计算的能力可以增强可移动性、位置感知能力、高效的数据分发、网络上下文理解和提供服务的安全性。</li>
<li>层级化的边缘-云体系结构对于适应 360 度视频快速动态传输是必要的。</li>
<li>相比于单静态层，多个动态缓存模型可以帮助管理唐突的 viewport 和网络变化来改善多用户的 viewport 命中率。</li>
<li>无论环境怎样，主动缓存都可以通过采用预测机制来预取和缓存部分视频来提高感知质量。</li>
</ul>
<h2 id="360-度视频的协同传输">360 度视频的协同传输</h2>
<h3 id="背景-2">背景</h3>
<ul>
<li>360 度视频推流有较大的用户需求并且在逐渐增长。</li>
<li>目前推流 viewport 之外的冗余信息会浪费重要的网络带宽。</li>
<li>相同的 360 度视频内容，在带宽受限的网络之上被推流给多个用户时，码率的需求变得更难满足。</li>
</ul>
<p>几个方法应用了 360 度视频的协同传输，进而改善传输效率。</p>
<h3 id="方案">方案</h3>
<h4 id="ahmadis-work">Ahmadi&rsquo;s work</h4>
<p>引入了基于 DASH 的加权 tile 方法来优化子用户组请求的 tile 编码性能。</p>
<ul>
<li>提出了多播流方案基于被用户看到的可能性对 tile 分配适当的权重。</li>
<li>接着基于可用带宽和 tile 权重为每个子用户组选择 tile 的码率。</li>
<li>实际上因为相邻 tile 的不同质量导致了空间质量变化，最终造成糟糕的推流体验。</li>
<li>不必要的离散优化问题巨大，不能保证有积极的表现。</li>
</ul>
<h4 id="baos-work">Bao&rsquo;s work</h4>
<p>基于动作预测和并发观看用户的信道条件提出了一种多播框架，来只分发可能被看到的 360 度视频块。</p>
<ul>
<li>没有在无线多播传输中考虑优化资源分配。</li>
</ul>
<h4 id="guos-work">Guo&rsquo;s work</h4>
<p>为每个用户假设了一种随机动作模式和不稳定的信道条件，并且开发了多播机会来避免冗余数据传输。</p>
<p>作者考虑了两个非凸的问题：</p>
<ol>
<li>在给定视频质量的约束下，最小化平均传输时间和能源消耗。</li>
<li>在给定的传输时间和能源预算下，最大化每个用户的视频质量。</li>
</ol>
<h4 id="longs-work">Long&rsquo;s work</h4>
<p>考虑了传输时间、视频质量的平滑性和能源限制，在单服务器多用户无线网络环境中优化多个用户的聚合效用。</p>
<ul>
<li>为了减少传输复杂性，作者准备了多种质量的 tile，并为每组用户将 tile 划分到不相邻的子集中。</li>
</ul>
<h4 id="zhangs-work-1">Zhang&rsquo;s work</h4>
<p>引入了一种使用 SVC 质量自适应方法的协同推流策略，来改善移动自组网环境中，观看 360 度内容的多个用户间的带宽共享。</p>
<ul>
<li>当遇到可用网络资源限制时，提出的启发性方式基于被看到的可能性和聚合的组级别偏好设置选择最优的 tile 子集。</li>
</ul>
<h4 id="kans-work">Kan&rsquo;s work</h4>
<p>提出了一种服务端混合多播-单播协同推流方案来分发不同质量的 360 度视频到多个用户。</p>
<ul>
<li>基于用户的观看行为对其进行分簇，以此来轻松共享相同的视频内容。</li>
<li>为每个 tile 联合选择传输模式和适当的码率来提高整体的 QoE。</li>
</ul>
<h4 id="huang-and-zhangs-work">Huang and Zhang&rsquo;s work</h4>
<p>设计了一种 MIMO 网络中的 MAC 调度方式。</p>
<ul>
<li>资源分配策略基于三个主要的函数
<ol>
<li>基于延迟的<code>Motion-To-Photon</code>(MTP)VR 帧权重计算。</li>
<li>基于最大<code>Aggregate Delay-Capacity Utility</code>（ADCU）的用户选择。</li>
<li>用于平衡 VR 数据传输的极高需求的链路自适应方法。</li>
</ol>
</li>
</ul>
<h4 id="li-and-gaos-work">Li and Gao&rsquo;s work</h4>
<p>提出了多用户 VR 框架，其中边缘云自适应地存储和重用冗余 VR 帧，以减少计算和传输负载。</p>
<ul>
<li>两级 cache 的设计：用户端的小型本地 cache 和边缘的大型中央 cache。</li>
<li>通过为所有用户产生背景视图和无论何时都重用帧，使得减少了内存需求。</li>
<li>评估表明帧相关数据和计算负载分别减少了 95%和 90%。</li>
</ul>
<h4 id="总结">总结</h4>
<p>对推流到多个临近用户的流行内容共享例如 360 度视频是一种自然的选择。</p>
<p>然而非协作式的用户对带宽的竞争会快速使整个网络瘫痪。</p>
<p>为了为多个用户获得改善的 QoE，研究者从以下几个方面做了努力：</p>
<ol>
<li>确定多个用户可能的需求来公平地分配可用的网络资源。</li>
<li>分析跨用户的行为来精确传输要求的子帧到终端用户。</li>
<li>由于侧信道攻击，保护 VR 帧传输到多个终端用户。</li>
</ol>
]]></description>
</item>
<item>
    <title>自适应360度视频推流方案</title>
    <link>https://ayamir.github.io/posts/papers/note3/</link>
    <pubDate>Mon, 25 Oct 2021 09:34:10 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note3/</guid>
    <description><![CDATA[<h2 id="概述">概述</h2>
<p>360 度视频的推流手段逐渐从视角独立型方案变成基于 tile 的视角依赖型方案。</p>
<p>相比于常规视频，360 度视频被编码成全向的场景。</p>
<p>自适应 360 度视频推流利用 DASH 框架来实现比特率的自适应。</p>
<h2 id="分类">分类</h2>
<h3 id="viewport-independent-streaming">Viewport-Independent Streaming</h3>
<h4 id="服务端的任务">服务端的任务</h4>
<ul>
<li>使用如 ERP、CMP 等视角独立型的投影方式，360 度视频被投影到一个球体上。</li>
</ul>
<h4 id="客户端的任务">客户端的任务</h4>
<ul>
<li>投影之后的视频直接被传送到客户端，并不需要来自传感器的方向信息。</li>
<li>客户端需要支持对应的投影格式。</li>
<li>客户端像处理传统视频一样完成比特率自适应。
<ul>
<li>基于网络特征向将要到来的 segment 请求相同投影格式的表示</li>
</ul>
</li>
</ul>
<p>DASH 插件需要支持相同质量视频的推流。</p>
<h4 id="应用">应用</h4>
<p>视角独立型推流主要用于体育、教育和旅游视频内容。</p>
<h4 id="优点">优点</h4>
<ul>
<li>简单</li>
</ul>
<h4 id="缺点">缺点</h4>
<ul>
<li>相比于视角依赖型方案视频编码效率低了 30%。</li>
<li>为不可见的区域要求大量带宽和解码资源。</li>
</ul>
<h3 id="viewport-dependent-streaming">Viewport-Dependent Streaming</h3>
<h4 id="终端设备的任务">终端设备的任务</h4>
<ul>
<li>只接受特定的视频帧内容，包括等于或大于视角角度的可见信息。</li>
<li>监测相关的视角作为用户头部移动的回应，并且向服务端发送信号来精确播放器信息。</li>
<li>为服务端准备和用户方向相关的几个自适应集。</li>
</ul>
<h4 id="客户端的任务-1">客户端的任务</h4>
<ul>
<li>根据网络情况和估计的视角位置决定获取哪个自适应集。</li>
</ul>
<h4 id="难点">难点</h4>
<ul>
<li>可视区域的确定</li>
<li>与用户头部移动的同步</li>
<li>质量调整</li>
<li>提供平滑的播放体验</li>
</ul>
<h4 id="现有的工作">现有的工作</h4>
<h5 id="各种投影方式在实际推流中表现如何">各种投影方式在实际推流中表现如何？</h5>
<ul>
<li>相比于金字塔格式，为视角依赖型投影方案提出的多分辨率变体有最好的研究和开发(RD)性能。</li>
<li>偏移 CMP 获得了 5.6%到 16.4%的平均可见质量。
<ul>
<li>提出的框架可以基于已知的网络资源和未来的视角位置适应视角的尺寸和质量。</li>
<li>相比于理想的下载过程，这种二维自适应策略可以花费 20%的额外网络带宽下载超过 57%的额外视频块。</li>
</ul>
</li>
</ul>
<h5 id="如何在网络资源受限的情况下提供高质量的推流">如何在网络资源受限的情况下提供高质量的推流？</h5>
<ul>
<li>为视角依赖型推流产生不同质量的 segment。
<ul>
<li>当流中只有有限的 representation 时，利用 Quality Emphasized Regions 策略来缩放特定区域的分辨率。</li>
<li>在拥塞网络条件下，执行了基于网络回应的视角大小和比特率的联合适应，结果显示，相比于传送全部的 360 度场景，动态的视角覆盖率提供了更好的画面质量。</li>
<li>这种基于网络回应的自适应也确保基于整体拥塞变化做调整时能改善视频质量。</li>
</ul>
</li>
<li>为立体视频的背景和前景视图采用不对称质量。
<ul>
<li>可以分别为背景块和前景块分别节省 15%和 41%的比特率。</li>
</ul>
</li>
</ul>
<h5 id="dash-需要做什么">DASH 需要做什么？</h5>
<ul>
<li>manifest 中需要包含视角位置信息和投影元数据。</li>
<li>优化获取 random access point 的周期来优化视角分辨率自适应体验。</li>
<li>考虑低延迟和活跃的视角切换。</li>
</ul>
<h3 id="tile-based-streaming">Tile-based Streaming</h3>
<p>传统视频被分成多个块，360 度视频在块的基础上还被分成多个大小相等或者不等的 tile，以此更加精确地调整画面的细节质量。</p>
<h4 id="分块策略">分块策略</h4>
<ul>
<li>
<p>基本完全交付</p>
</li>
<li>
<p>高级完全交付</p>
</li>
<li>
<p>部分交付</p>
</li>
</ul>
<p></p>
<h4 id="分块模式">分块模式</h4>
<p>1x1，3x2，5x3，6x4，8x5</p>
<p>其中 6x4 的模式实现了较好的带宽消耗和编码效率的折中。</p>
<p>在不同的带宽条件下，基本完全交付策略获得了大约 65%的带宽节约。</p>
<h4 id="具体方案">具体方案</h4>
<h5 id="clustile">ClusTile</h5>
<p>基于分簇的方式，推送满足最小带宽需求的 tile 来克服编码效率和计算开销。</p>
<ul>
<li>相比于传统和高级的基于 tile 的推流方案，分别实现了 72%和 52%的带宽节约。</li>
<li>当实际看到的和下载的 tile 有差异时，基于分簇的 tile 选取可能会导致选择不当。</li>
</ul>
<h5 id="ghoshs-work">Ghosh&rsquo;s work</h5>
<p>提议以最低可获得的质量下载周围和远处的 tile。</p>
<ul>
<li>相比于其他算法，视角及其周边区域的可变质量提高了 20%的 QoE 水平。</li>
</ul>
<h5 id="ozcinars-work">Ozcinar&rsquo;s work</h5>
<p>介绍了一种自适应 360° 视频流框架。</p>
<ul>
<li>
<p>利用视觉注意力度量来计算每个帧的最佳平铺模式。</p>
</li>
<li>
<p>使用选中的模式，为不同区域的 tile 分配非统一的比特率。</p>
</li>
<li>
<p>比特率的选取取决于估计的视角和网络状况。</p>
</li>
<li>
<p>因为很大部分的带宽被用于传输非视角内的 tile，框架难以优化视角内的质量。</p>
</li>
</ul>
<h5 id="xies-work">Xie&rsquo;s work</h5>
<p>提出了一套优化框架，以此来最小化预取 tile 的错误，改善与不同比特率相关联的 tile 边界的平滑程度。</p>
<ul>
<li>
<p>定义了两个 QoE 函数，目标是最小化：</p>
<p>预期质量失真$\Phi(X)$</p>
<p>当考虑 tile 看到概率时视角的空间质量方差$\Psi(X)$：</p>
<p>$$
\Phi(X) = \frac{\sum_{i=1}^{N}\sum_{j=1}^{M}D_{i,j} * x_{i,j} * p_{i,j}}{\sum_{i=1}^{N}\sum_{j=1}^{M}x_{i,j} * s_{i}}
$$</p>
<p>$$
\Psi(X) = \frac{\sum_{i=1}^{N}\sum_{j=1}^{M}x_{i,j}*p_i * (D_{i,j} - s_i * \Phi(X))^{2}}{\sum_{i=1}^{N}\sum_{j=1}^{M}x_{i,j}*s_i}
$$</p>
</li>
<li>
<p>基于目标缓冲区的自适应方法用于在需要短期视口预测的小缓冲区下进行平滑播放</p>
<p>在自适应的第 k 步，当第 k 个 segment 集合下载完成时，缓冲区占用率$b_k$由下面的式子给出：</p>
<p>$$
b_k = b_{k-1} - \frac{R_k*T}{C_k} + T
$$</p>
<p>为了避免用尽所有块，缓冲区的占用率被通过设定一个目标缓冲区水平$B_{target}$所控制，即$b_k = B_{target}$。</p>
</li>
<li>
<p>平均空间质量方差是 0.97，比其他基于 tile 的策略小。</p>
</li>
<li>
<p>所提出的概率自适应框架在感知质量上实现了约 39% 的增益，平均降低了 46% 的空间质量方差。</p>
</li>
</ul>
<h5 id="vander-hoofts-work">Vander Hooft&rsquo;s work</h5>
<p>将 360 度帧划分成视角内区域和视角外区域。</p>
<ul>
<li>首先为所有区域都选择最低质量，然后提高视角内 tile 的质量。</li>
<li>如果带宽依然可用，接着提高剩下的 tile 的质量。</li>
<li>启发式的方式在带宽可用的基础上积极提高视角内 tile 的质量。</li>
<li>没有考虑视角比特率调整时视角预测的错误。</li>
</ul>
<h5 id="nguyens-work">Nguyen&rsquo;s work</h5>
<p>提出了一种新的自适应机制，它在每个 segment 中同时考虑头部移动和视角的预测错误，动态地决定视角内的比特率。</p>
<ul>
<li>联合适应扩展块的覆盖范围和比特率。</li>
<li>在不同记录的用户头部运动下的实验评估表明，在不获取非视角内区域过多带宽利用率的情况下，视角内容质量有所提高。</li>
</ul>
<h4 id="dash-srd-扩展">DASH SRD 扩展</h4>
<p>DASH 的 SRD 扩展提供了多种版本的 tile 的关联来节省更多的比特率。</p>
<h5 id="le-feuvre-and-concolatos-work">Le Feuvre and Concolato&rsquo;s work</h5>
<p>他们应用了这个 SRD 特性，引入了同时为独立的和运动受限的 HEVC tile 的不同优先级设定，以此来高效地实现基于 tile 的方案。</p>
<ul>
<li>使用开源的 GPAC 多媒体框架开发了一个 DASH 客户端，以此来执行带有可配置参数的基于 tile 的推流。</li>
</ul>
<h5 id="dacuntos-work">D&rsquo;Acunto&rsquo;s work</h5>
<p>提出了一种 <a href="https://github.com/tnomedialab/dash-srd.js" target="_blank" rel="noopener noreffer">MPEG-DASH SRD 方法</a>来促进可缩放和可平移视频的平滑推流。</p>
<ul>
<li>总是下载低分辨率的 tile 来避免用户移动视角时的重新缓冲。</li>
<li>当前视野区域被上采样并展示给用户，以此来支持高质量的缩放功能。</li>
<li>用<code>JavaScript</code>实现了 SRD 视频播放器。</li>
</ul>
<h5 id="hosseinis-work">Hosseini&rsquo;s work</h5>
<p>基于 SRD 实现了视角内容、相邻 tile 和剩余 tile 的优先级推流。</p>
<ul>
<li>用 6 个 3D 网格构建了一套 3D 座标系来在 3D 空间中平滑地表示 tile。</li>
<li>相比于基础的方式，这种区分质量的推流方案节省了 72%的带宽。</li>
</ul>
<h5 id="kim-and-yangs-work">Kim and Yang&rsquo;s work</h5>
<p>使用改进的 MPEG-DASH SRD 来在质量可变的 tile 层中作选择。</p>
<ul>
<li>基于他们之前的工作设计并实现了一个支持多层渲染的 360° VR 播放器，以支持高度不可预测的头部运动数据的高分辨率和低延迟流。</li>
</ul>
<h4 id="motion-constrained-tileset">Motion-Constrained TileSet</h4>
<p>在 HEVC 中，运动约束贴图集(MCTS)是将整个帧表示为子视频的相邻分割，并为自由选择的贴图集提供解码支持。</p>
<h5 id="zares-work">Zare&rsquo;s work</h5>
<p>将 MCTS 的概念应用到了全景视频推流中。</p>
<ul>
<li>将两个质量版本的视频分割成 tile，以原始的分辨率推流视角内的 tile，以低分辨率推流剩余的 tile。</li>
<li>它已经表明，选定图块的可变比特率会降低 30% 到 40% 的比特率。</li>
</ul>
<h5 id="skupins-work">Skupin&rsquo;s work</h5>
<p>陈述了一种使用 HEVC 编码器的基于 tile 的可变分辨率的推流系统。</p>
<ul>
<li>使用立方贴图投影的 360 度视频被分割成 24 个网格，每个代表了一个独立的比特流。</li>
<li>两种不同质量的版本被推流到客户端，例如 8 个 tile 以高质量推送，16 个 tile 以低质量推送。</li>
</ul>
<h5 id="sons-work">Son&rsquo;s work</h5>
<p>在基于视角的移动 VR 推流中，为独立的 tile 提取和传输实现了基于 MCTS 的 HEVC 和可缩放的 HEVC 编解码器。</p>
<ul>
<li>节省了超过 47%的带宽。</li>
<li>相比于原始的 HM 和 SHM 编码器表现不佳，因为 MCTS 限制了时间运动信息。</li>
</ul>
<h5 id="lees-work">Lee&rsquo;s work</h5>
<p>用 MCTS 编码 360 度视频 tile，并使用显著性检测网络将混合质量的视频 tile 推流给终端用户。</p>
<ul>
<li>通过显著性模型改进 MCTS 的使用，可以在不增加任何复杂性的情况下灵活地对感兴趣的 tile 区域进行解码支持。</li>
</ul>
<h4 id="scalable-video-code">Scalable Video Code</h4>
<p>可伸缩视频编码 SVC 是实现 viewport 自适应的一种替代策略。</p>
<p>基础层总被需要并且能从客户端预取来避免重新缓冲事件。</p>
<p>提高层改善 viewport 质量并且可以在带宽充足的时候被请求。</p>
<p>SVC 促进了一种高效的网络内缓存支持来减少多个客户端请求相同内容时的分发开销。</p>
<h5 id="nasrabadis-work">Nasrabadi&rsquo;s work</h5>
<p>使用了一种可伸缩编码方案来解决 360 度视频推流的重新缓冲的问题。</p>
<ul>
<li>存在质量波动的问题，因为没有使用任何机制来处理 viewport 的预测错误。</li>
</ul>
<h5 id="nguyens-work-1">Nguyen&rsquo;s work</h5>
<p>建议使用 SVC 协同 viewport 预测来克服网络信道和头部运动的随机性。</p>
<ul>
<li>实验表明，所提出的平铺层更新和后期平铺终止特征可使 viewport 质量提高 17%。</li>
</ul>
<h4 id="ai-方法的应用">AI 方法的应用</h4>
<p>背景：传统视频推流中使用强化学习来高效调整视频比特率和实现长期的 QoE 回报。</p>
<p>和传统视频内容不同，360 度视频包含几个新的方面比如 tile 大小、viewport 预测等。</p>
<p>直接将现有的强化学习自适应策略应用到 360 度视频上可能会降低推流性能。</p>
<h5 id="fus-work">Fu&rsquo;s work</h5>
<p>为 360 度视频提出了称为<em>360SRL</em>的一种序列化强化学习方法，它基于之前决策的 QoE 回报而非估计的带宽状况做出自适应决策。</p>
<ul>
<li>360SRL 使用基于 tile 的推流模拟器来增强训练阶段。</li>
<li>跟踪驱动的评估表明，360SRL 比基线适应方法取得了 12%的 QoE 改善。</li>
</ul>
<h5 id="jiangs-work">Jiang&rsquo;s work</h5>
<p>基于历史带宽、缓冲区空间、tile 大小和 viewport 预测错误等，利用强化学习来做 viewport 和非 viewport 内 tile 的比特率选择。</p>
<ul>
<li>所提出系统的架构由状态缓冲区、视口预测 (VPP) 和 tile 比特率选择 (TBS) 代理组成。</li>
<li>状态缓冲区向 VPP 和 TBS 代理提供用户查看模式和网络状态。</li>
<li>VPP 代理然后使用 LSTM 模型估计下一个 viewport 位置。</li>
<li>TBS 代理由 Asynchronous Advantage Actor-Critic (A3C)算法训练以执行合适的比特率决策。</li>
</ul>
<h5 id="quans-work">Quan&rsquo;s work</h5>
<p>通过卷积神经网络(CNN)提取像素运动来分析用户 QoE，并使用它对 tile 动态分组，从而在视频质量和编码效率之间提供重要的平衡。</p>
<ul>
<li>使用了基于强化学习的自适应代理，它可以智能地使每个图块的质量适应动态环境。</li>
<li>使用真实 LTE 带宽跟踪验证该方案，在感知质量方面表现出了卓越的性能，同时也节省了带宽资源。</li>
</ul>
<p>背景：深度学习使强化学习能够使用多方面的状态和动作空间进一步优化聚合回报。</p>
<h5 id="kan-and-xiaos-work">Kan and Xiao&rsquo;s work</h5>
<p>设计了一套深度强化学习的框架，基于对环境因素的探索和开发，自适应地调整推流策略。</p>
<ul>
<li>这两种方案都采用 DRL 的 A3C 算法来进行比特率决策，因为 A3C 算法能使代理变得越来越智能化。</li>
<li>性能评估表明，所提出的系统平衡了各种 QoE 指标，包括平均视觉质量、平均质量波动和重新缓冲事件等。</li>
</ul>
<h5 id="zhangs-work">Zhang&rsquo;s work</h5>
<p>提出了一个深度强化学习模型，它考虑 viewport 预测准确度和网络状况，使用基于 LSTM 的 ACTOR-CRITIC(AC)网络动态地学习适应比特率分配。</p>
<ul>
<li>方案能够很好地适应广泛的动态特性，并且与传统方法相比，提供了 20%到 30%的改进 QoE 回报。</li>
</ul>
<h4 id="总结">总结</h4>
<p>基于 tile 的推流只需要少量的服务端内容版本。</p>
<p>与依赖视图的推流相比，它包含更低的存储和处理开销。</p>
<p>提出的大多数方案为 viewport 及其临近的 tile 使用不同的分辨率，这会为高效推流减少带宽开销。</p>
<p>但是这种区分分辨率的 tile 为了防止 viewport 预测错误会显著地降低能察觉到的视频质量。</p>
<p>一个 50 个用户的主观实验表明，当混合 1920x1080 和 960x540 分辨率的块时，绝大多数用户能观察到明显的质量降低。</p>
<p>但是当混合 1920x1080 和 1600x900 分辨率的块时，用户只会注意到微小的差别。</p>
<p>对于高运动内容，这种混合效应甚至会导致严重的质量下降。</p>
<p>因此为了动态执行 tile 的选择和基于 DRL 的比特率适应，需要有一个推流分辨率的恰当选择，进而在流质量、空间质量方差、视口预测误差和带宽效率之间获得完美的平衡。</p>
]]></description>
</item>
<item>
    <title>自适应视频推流方案</title>
    <link>https://ayamir.github.io/posts/papers/note2/</link>
    <pubDate>Thu, 21 Oct 2021 10:50:54 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note2/</guid>
    <description><![CDATA[<h2 id="概述">概述</h2>
<p>自适应方案可以在处理不同目标对象时帮助改善推流体验。</p>
<p>目标主要包括视频质量、功耗、负载均衡等在移动无线网和有线网接入的情形。</p>
<p>适应性的视频比特率需要同时匹配网络条件和质量目标的需求。</p>
<h2 id="分类">分类</h2>
<h3 id="服务端适应">服务端适应</h3>
<p>大多数服务端适应的方案要求客户端发送系统或网络相关信息。</p>
<h4 id="质量导向的适应方案quality-oriented-adaptive-schemeqoas">质量导向的适应方案（Quality-Oriented Adaptive Scheme/QOAS）</h4>
<p>向终端用户提供了高知觉质量的媒体内容。</p>
<ol>
<li>
<p>QOAS 是 C-S 架构，决策在服务器端产生。</p>
</li>
<li>
<p>QOAS 基于客户知觉质量的反馈，提供对推流质量等级的调整。</p>
</li>
</ol>
<h4 id="智能优先级适应方案intelligent-prioritized-adaptive-schemeipas">智能优先级适应方案（intelligent Prioritized Adaptive Scheme/iPAS）</h4>
<p>专用于 802.11 网络。</p>
<ol>
<li>
<p>iPAS 服务器上的基于固有印象的带宽分配模块被用于组合 QoS 相关的参数和视频内容特征来进行内容的优先级分类和带宽份额分配。</p>
</li>
<li>
<p>通过区分多媒体流，iPAS 提供可用无线信道的优先级分配。</p>
</li>
</ol>
<h4 id="设备导向的适应方案device-oriented-adaptive-multimedia-schemedoas">设备导向的适应方案（Device-Oriented Adaptive multimedia Scheme/DOAS）</h4>
<p>专用于 LTE 网络，建立在 LTE 下行链路调度机制之上。</p>
<ol>
<li>DOAS 专门根据设备特性实现适配，尤其为多屏终端用户提供了卓越的 QoE。</li>
</ol>
<h3 id="客户端适应">客户端适应</h3>
<h4 id="基于吞吐量的自适应方案">基于吞吐量的自适应方案</h4>
<p>这类方案基于估计的网络吞吐量从服务端选择视频的比特率。</p>
<ol>
<li>HTTP 客户端通过之前的观察记录来估计网络的吞吐量。</li>
<li>通过测量端获取时间（segment fetch time/SFT）来代表发起和收到回复的瞬时 HTTP GET 请求之间的时间段，以此来确定一个推流会话中吞吐量的变化，进而独立地做出适应决策。</li>
<li>在分布式网络中，同时考虑并发和顺序的 SFT。通过比较实际的和理想的 SFT 来选择未来的 segment 的质量等级。</li>
</ol>
<h5 id="festive-算法">FESTIVE 算法</h5>
<p>适用于多个 HAS 客户端共享一个常见的拥塞带宽链路的情形。</p>
<p>以<strong>效率、稳定性、公平性</strong>为度量因素的适应性算法。</p>
<p>探索了一种为<strong>分段调度、吞吐量估计和比特率选择</strong>而生的健壮的机制。</p>
<p>包含一个随机调度器来调度下一个视频块的下载。</p>
<p>多个客户端共享容量为$W$的满带宽链路，每个客户端$x$在$t$时刻播放的视频比特率为$b_x,_t$ ，需要避免以下 3 种问题：</p>
<ul>
<li>
<p><em>Inefficiency</em>：多个 HAS 客户端必须能选择最可能的表示来提高 QoE。</p>
<p>$$ Inefficiency = \frac{|\sum_{x}b_x,_t - W|}{W} $$</p>
<p>低<em>Inefficiency</em>值表明多个客户端对带宽实现了最有效的利用。</p>
</li>
<li>
<p><em>Unfairness</em>：可用带宽应该被均等地分配。</p>
<p>$$ Unfairness = \sqrt{1-JainFair} $$</p>
<p>低<em>Unfairness</em>值表明多个客户端有相近的比特率。</p>
</li>
<li>
<p><em>Instability</em>：不必要的比特率切换会损害推流体验</p>
<p>$$Instability = \frac{\sum_{d=0}^{k-1}|b_{x,t-d} - b_{x,t-d-1}|*w(d)}{\sum_{d=1}^{k}b_{x,t-d} * w(d)}$$</p>
</li>
</ul>
<h5 id="probe-and-adaptpanda算法">Probe AND Adapt(PANDA)算法</h5>
<p>用于检测网络状况，考虑未来比特率选择的平均目标数据比特率。</p>
<p>目标是当多个 HAS 客户端共享一个拥塞带宽信道时，通过正确探测网络，进而最小化<strong>比特率震荡</strong>。</p>
<p>PANDA 算法在性能上击败了 FESTIVE 算法，并且 PANDA 算法在这些解决方案中表现出了最好的适应性，在不同带宽情况和播放器设置下实现了最优的<strong>效率、公平性和稳定性</strong>。</p>
<p>整体上的推流质量不只依赖于本地的吞吐量测量，还依赖服务端的网络容量。</p>
<ol>
<li>利用服务器发起的推送机制来降低 DASH 内容推流到移动客户端的端到端延迟。</li>
<li>利用<em>HTTP/2</em>的流终止特性来实现中间质量调整。</li>
<li>基于估计的用户 QoE，功耗和可用资源来改善用户端的推流体验。</li>
</ol>
<p>虽然有证据表明性能得到了提高，但是评估工作只是在受控的 LAN 环境下有效。</p>
<h5 id="cross-session-stateful-predictorcs2p方案">Cross Session Stateful Predictor(CS2P)方案</h5>
<p>一种数据驱动的吞吐量估计方案，以克服不准确的 HAS 流量预测问题。</p>
<p>将共享相似特性的推流会话分簇，然后对每个簇使用隐马尔科夫模型预测相应的吞吐量样本。</p>
<p>在一个大规模数据集上实验性的评估表明：CS2P 高效地估计了可用的网络吞吐量，进而改善了整体上的视频比特率的适应性。</p>
<p>CFA 和 Pytheas 等方案和 CS2P 类似，也使用数据驱动的控制器来估计可用的吞吐量。</p>
<p>但是这些工作<strong>不支持异构系统</strong>并且<strong>需要额外的训练复杂性</strong>，使其不够具有吸引力。</p>
<p>基于吞吐量的适应性方案主要的挑战在于对吞吐量的精确估计。</p>
<p>为 360 度视频采用一个没有经过精巧设计的吞吐量估计机制可能会导致不稳定性和较差的 QoE，在高度动态化的无线和蜂窝网络中尤甚。</p>
<h4 id="基于缓冲区的自适应方案">基于缓冲区的自适应方案</h4>
<p>客户端会在播放视频时根据当前缓冲区的占用情况请求将要到来的 segment。</p>
<h5 id="如何克服不完整的网络信息的限制">如何克服不完整的网络信息的限制</h5>
<ol>
<li>
<p>在多客户端启用缓存的环境中，结合客户端测量工具集和补偿算法构造模型。</p>
<p>这个模型可以高效探测比特率切换时间并通过选择切换适当的比特率来进行补偿，最终实现了可达 20%的比特率改善。</p>
</li>
<li>
<p>Buffer Based Adaptation(BBA)方法</p>
<p>应用于 Netfix 客户端时可以减少可达 20%的重新缓冲事件。</p>
<p>BBA 方法考虑的缓冲区较大，因此对于比较短的视频不一定有这样的性能。</p>
</li>
<li>
<p>Buffer Occupancy-based Lyapunov Algorithm(BOLA)</p>
<p>把比特率适应性问题看作是与播放质量和重新缓冲时间相关的最优化问题。</p>
<p>BOLA 旨在通过把缓冲区大小保持在设定的目标水平来避免重新缓冲。</p>
<p>对于缓冲区级别的突然下降，BOLA 通过请求最低可用视频比特率来避免停顿事件的频率。</p>
</li>
</ol>
<h5 id="如何优化缓冲区利用率">如何优化缓冲区利用率</h5>
<ol>
<li>
<p>Adaptation and Buffer Management Algorithm(ABMA+)</p>
<ul>
<li>基于重新缓冲事件的可能性确定未来 representation 的下载时间。</li>
<li>通过基于预先计算的缓冲区大小和 segment 下载时间选择最大比特率来确保流畅的播放。</li>
</ul>
<p>这样可以实现低计算开销的良好部署。</p>
</li>
<li>
<p>Scalable Video Coding(SVC)/Bandwidth Independent Efficient Buffering(BIEB)</p>
<ul>
<li>基于层分发获取视频块，进而维持稳定的缓冲区大小来避免频繁的中断。</li>
<li>没有考虑 QoE 模型中的卡顿和质量切换。</li>
<li>涉及额外的编码和处理开销。</li>
</ul>
</li>
<li>
<p>使用 PID 控制器的控制论方法</p>
<ul>
<li>强制执行缓冲区设置点来使缓冲区保持在最佳水平。</li>
<li>略微降低视频比特率，以防止不必要的视频比特率调整。</li>
<li>在多个客户端竞争的情况下，不能保证公平性。</li>
</ul>
</li>
</ol>
<h5 id="如何降低-dash-流的排队延迟">如何降低 DASH 流的排队延迟</h5>
<p>DASH 流会经历最长可达 1s 的排队延迟和严重拥塞，导致缓冲区膨胀问题，而这会严重损害实时多媒体服务的 QoE。</p>
<p>旨在减少网络拥塞的主动队列管理 (AQM) 策略并没有充分减少这种不必要的延迟。</p>
<ol>
<li>DASH 客户端根据网络设备的队列大小动态接收窗口大小可以显著减轻缓冲区膨胀效应。</li>
<li>由于长期的 viewport 预测的高度不确定性，充足的缓冲区空间对于 360 度视频的流畅播放来说并不可行。</li>
<li>通常小于 3s 的缓冲区大小对于短期的 viewport 预测来讲比较适合。</li>
<li>由于小缓冲区很有可能造成播放卡顿，因此较短持续时间的 segment 可以被用于基于 tile 的流中，但是相比于长持续时间的 segment，这样也会降低编码效率。</li>
</ol>
<h4 id="混合自适应方案">混合自适应方案</h4>
<p>客户端同时考虑吞吐量和播放缓冲信号来确定即将到来的 segments 的视频比特率。</p>
<h5 id="model-predictive-controlmpc">Model Predictive Control(MPC)</h5>
<p>利用良好定义的参数集合来估计可用的网络和缓冲区资源，进而为高 QoE 的比特率做出最优调整的控制论方法。</p>
<p>提出的 QoE 模型采用视频的平均质量$R_k$，平均比特率切换，重新缓冲事件，和初始延迟$T_s$作计算：</p>
<p>$$
QoE_1^K = \sum_{k=1}^{K}q(R_k) - \lambda\sum_{k=1}^{K-1}|q(R_{k+1}) - q(R_k)| - \mu\sum_{k=1}^{K}(d_k(R_k)/C_k - B_k)_+ - \mu_sT_s
$$</p>
<p>$C_k$：第 k 个块的可用带宽，$B_k$：第 k 个块的可用缓冲区大小</p>
<p>$\lambda, \mu, \mu_s$：可以根据用户兴趣进行调整的权重</p>
<ul>
<li>
<p>MPC 用调和平均的方法来估计吞吐量，并且能够明确管理复杂的控制对象。</p>
</li>
<li>
<p>只研究了单播放器的情况，因此没有公平性的考量。</p>
</li>
</ul>
<h5 id="throughput-and-buffer-occupancy-based-adaptationtboa">Throughput and Buffer Occupancy-based Adaptation(TBOA)</h5>
<p>选择合适的视频比特率来获得单个或多个客户端环境中改进的推流体验。</p>
<ul>
<li>
<p>激进地提高了比特率来最高效地利用可用的带宽。</p>
</li>
<li>
<p>等待缓冲区超过某个级别，然后降低比特率以获得稳定的性能。</p>
</li>
<li>
<p>为缓冲区等级设置三个阈值，例如：</p>
<p>$0 &lt; B_{min} &lt; B_{low} &lt; B_{high}$</p>
<p>目标区间在$B_{low}$和$B_{high}$之间。</p>
<p>算法努力使最优区间$B_{opt}满足$ $B_{opt} = B_{low} + B_{high} \over 2$。</p>
<p>通过控制$B_{low}$和$B_{high}$的阈值，使缓冲区和比特率的变化稳定来应对未知的 TCP 吞吐量。</p>
</li>
<li>
<p>算法表现的流畅而公平，但是没有把用户满意度的度量考虑在内。</p>
</li>
</ul>
<h5 id="fuzzy-logic-based-dash">fuzzy logic-based DASH</h5>
<p>控制重新缓冲事件和视频推流的质量。</p>
<ul>
<li>考虑了平均吞吐量的估计方法，获得了更高的视频比特率和更少的质量波动。</li>
<li>没有考虑 QoE 度量。</li>
</ul>
<p>为了更好地调整比特率做出的改进：</p>
<ul>
<li>用 Kaufman&rsquo;s Adaptive Moving Average/KAMA 测量法估计吞吐量。</li>
<li>用 Grey Prediction Model/GPM 来估计缓冲区等级。</li>
</ul>
<p>竞争流模拟环境中，改进所取得的效果：</p>
<ul>
<li>平均情况下达到 50%的公平性。</li>
<li>最好情况下达到 17%的更好的接收质量。</li>
</ul>
<h5 id="spectrum-based-quality-adaptationsquad算法">Spectrum-based Quality Adaptation(SQUAD)算法</h5>
<p>解决吞吐量预测和缓冲区等级估计的不连续性。</p>
<ul>
<li>吞吐量和缓冲区等级反馈信号都被用于选择恰当的质量。</li>
<li>在一开始获取最低质量的 segment 来减少启动时间。</li>
<li>在视频质量切换频率和幅度方面性能显著提高。</li>
</ul>
<p>尚未有方案讨论如何在视频质量和带宽利用率之间做出很好的平衡。</p>
<h5 id="throughput-friendly-dashtfdash">Throughput Friendly DASH/TFDASH</h5>
<p>获得多个竞争客户端情形下的公平性、稳定性和效率。</p>
<ul>
<li>通过避免 OFF 端获得了最大并且公平的带宽利用率。</li>
<li>双阈值的缓冲区保证播放时的稳定性。</li>
</ul>
<p>在单客户端的环境中，混合适应方案表现的很合理。</p>
<p>但是多个客户端一起竞争带宽时会迅速扼杀整个网络。</p>
<p>当客户端的缓冲区达到了最大阈值时，客户端进入了 ON-OFF 阶段，此时客户端只对自己的视频比特率做了调整而没有考虑其他客户端，因而不能正确地估计可用的带宽资源。</p>
<p>这会导致竞争客户端之间带宽利用不足以及带宽分配不均。</p>
<h4 id="基于多路径的自适应方案">基于多路径的自适应方案</h4>
<p>解决的主要问题是在异质网络之上，如何面对交付内容的增加。</p>
<h5 id="multipath-transmission-control-protocolmptcp">Multipath Transmission Control Protocol(MPTCP)</h5>
<ul>
<li>有用但是并不理想
<ul>
<li>因为需要发送端和接收端同时修改内核堆栈。</li>
<li>因为受到网络运营商的限制可能无法通过中间件。</li>
</ul>
</li>
</ul>
<h5 id="cmt-qa-方案">CMT-QA 方案</h5>
<p>采用多种特定的网络技术来实现并发的多路内容交付。</p>
<h5 id="multi-source-playermsplayer">Multi-source player(MSPlayer)</h5>
<p>实现多条链路之上的高质量视频传送和弹性的容错机制。</p>
<ul>
<li>
<p>客户端驱动的对未来视频 segment 的比特率分配依赖于估计的网络状况。</p>
</li>
<li>
<p>视频 segment 可以在两种可用网络之上进行下载，但是多路径的下载可能会造成交付顺序错乱。</p>
</li>
</ul>
<h5 id="cross-layer-fairness-solution">Cross-layer Fairness solution</h5>
<p>通过探索数据链路层和传输层之间的交互来分析数据传输路径的实时质量，提出了一个公平性驱动的高效流控机制。</p>
<p>在模拟环境中，相比于 CMT-QA 方案：</p>
<ul>
<li>获得了更高的公平性评级。</li>
<li>获得了更低的平均吞吐量和 PSNR（峰值信噪比）。</li>
</ul>
<h5 id="kim-and-chungs-work">Kim and Chung&rsquo;s work</h5>
<p>同时利用 WiFi 和 LTE 网络接口，从多个视频源下载部分 segment。</p>
<ul>
<li>对多路径的聚合带宽进行平滑处理以避免带宽波动。</li>
<li>实现了一种部分 segment 请求策略以避免乱序问题，经过各种路径传输的部分片段
在呈现给用户之前进行组合。</li>
</ul>
<h5 id="gos-work">Go&rsquo;s work</h5>
<p>在网络成本限制下，调度跨网络间相同比特率的视频块中的所有 segment。</p>
<h5 id="基于-mpeg-dash-的推流策略实验性评估">基于 MPEG-DASH 的推流策略实验性评估</h5>
<p>以低功耗为移动设备提供了 WiFi 和 LTE 网络下的无缝视频播放。</p>
<ul>
<li>没有分析感知视频质量的影响。</li>
</ul>
<h5 id="davvi">DAVVI</h5>
<p>基于 HTTP 的推流系统，为了实现 3G 和 WiFi 网络之上的多信道支持。</p>
<ul>
<li>基于每个信道的质量，视频 segment 被动态地划分成 subsegment，以便于最大负载可以被应用到每个信道上。</li>
</ul>
<p>为多媒体内容交付使用多个网络接口需要为路径质量测量和数据调度精心设计机制，来避免丢包和乱序交付的问题。</p>
<p>然而因为无线异质网络的高度动态性和复杂性，现有的方案在测量实时信息的时候是受限的。</p>
<h5 id="elgablis-work">Elgabli&rsquo;s work</h5>
<p>考虑了基于 SVC 的优先自适应视频传输的两条路径。</p>
<ul>
<li>属于每一层的段可以根据质量、块 deadline 和路径偏好从可用路由之一传输。</li>
<li>没有考虑在任何路径上应用最大贡献度。</li>
</ul>
<h5 id="zhangs-work">Zhang&rsquo;s work</h5>
<p>提出了一种基于两个流的优先级感知自适应解决方案，它为每个流使用不同的视频比特率。</p>
<ul>
<li>实现了一个集成带宽的方式来为高优先级流启用更高的视频比特率，并在没有足够的可用带宽时终止低优先级流。</li>
</ul>
<h5 id="yun-and-chungs-work">Yun and Chung&rsquo;s work</h5>
<p>为多视图视频提出了一种基于 DASH 的推流框架，它包括基于缓冲区的服务器推送方案和并行传输机制，以减少不同传输视图之间的切换时间。</p>
<ul>
<li>只有一种路径配置被应用。</li>
</ul>
<h5 id="rahman-and-chungs-work">Rahman and Chung&rsquo;s work</h5>
<p>介绍了基于 HAS 的多视图会议流解决方案，其中演示者、观众和演示屏幕的多个流通过多条路径同时传输。</p>
<ul>
<li>对所有的 3 个流分配相同的优先级。</li>
<li>采用统一带宽的方式，以便于统一的质量可以被分配到所有流的 segment 上。</li>
<li>对于多个流的每个 segment，其路径以通过考虑网络吞吐量和每个 segment 的比特率来决定。</li>
<li>没有考虑多信道的影响，这可能降低整体性能。</li>
</ul>
<p>利用多路径网络的特点和优先级特性可以为 360 度 tile 视频推流提供更高的推流性能。</p>
<p>提出的所有自适应策略都是通用的，目标是标准的视频交付，并没有对 360 度视频内容做出特别的考虑。</p>
]]></description>
</item>
</channel>
</rss>

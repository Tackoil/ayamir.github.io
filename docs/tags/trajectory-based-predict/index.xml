<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Trajectory-Based Predict - 标签 - Ayamir&#39;s Blog</title>
        <link>https://ayamir.github.io/tags/trajectory-based-predict/</link>
        <description>Trajectory-Based Predict - 标签 - Ayamir&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miracle_l@bupt.edu.cn (Ayamir)</managingEditor>
            <webMaster>miracle_l@bupt.edu.cn (Ayamir)</webMaster><lastBuildDate>Thu, 06 Jan 2022 15:17:33 &#43;0800</lastBuildDate><atom:link href="https://ayamir.github.io/tags/trajectory-based-predict/" rel="self" type="application/rss+xml" /><item>
    <title>Note for Content Assisted Prediction</title>
    <link>https://ayamir.github.io/posts/papers/note-for-content-assisted-prediction/</link>
    <pubDate>Thu, 06 Jan 2022 15:17:33 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-content-assisted-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://www.researchgate.net/publication/333971523_Content_Assisted_Viewport_Prediction_for_Panoramic_Video_Streaming" target="_blank" rel="noopener noreffer">Content Assisted Viewport Prediction for Panoramic Video Streaming</a></p>
<p>Level：IEEE CVPR 2019 CV4ARVR</p>
<p>Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion</p>
<h2 id="主要工作">主要工作</h2>
<h3 id="基于轨迹预测">基于轨迹预测</h3>
<p>输入：历史窗口轨迹</p>
<p>模型：64 个神经元的单层 LSTM，在输入层后面加上一个额外的减法层进行点归一化，以及一个加法层来恢复输出之前的值；用 ADAM 进行优化，MAE 作为损失函数。</p>
<h3 id="跨用户热图">跨用户热图</h3>
<p>除了观看者自己的历史 FOV 轨迹之外，其他观看者对同一视频帧的观看方向也有启发性。</p>
<p></p>
<p>对视频的每一帧，首先收集用户的观看方向（坐标使用原始的来自三个方向的欧拉角表示，而非经纬度）。</p>
<p>接着将坐标投影到用经纬度表示的 180x360 像素的平面图上，对于图中的每个像素点，可以数出其被看到的次数；并对周围像素应用二维高斯光滑。</p>
<p>上面的过程可以为视频生成热图：</p>
<p></p>
<h3 id="视频帧的显著图">视频帧的显著图</h3>
<p>鉴于观看相同的全景视频时跨用户行为的共性，进一步假设是内容促使多个观众观看公共区域，因此提取出每个帧的显著图可能会表明用户的 RoI。</p>
<p>对特定的视频帧，应用经典的特征密集型方法——Ittykoch，它首先根据强度、边缘、颜色和方向将图像分解为多个特征通道，然后将它们组合成识别显著区域。</p>
<p>除了在静态视频帧上检测显著性之外，进一步进行背景减法来减少不太可能感兴趣的区域：应用基于高斯混合的背景/前景分割算法，高级思想是在连续帧之间临时过滤变化的像素点。</p>
<p>结合上面这两个过程可以为视频帧提取时间显著图。</p>
<p></p>
<h3 id="多模态融合">多模态融合</h3>
<p></p>
<p>使用包含 3 个 LSTM 分支的深度学习模型来融合上述的几种预测方式的结果。</p>
<p>基于轨迹的 LSTM（图中绿色分支）从历史窗口 $hw$ 中接受 $n$ 个坐标的输入，接着预测未来窗口 $pw$ 中的 $m$ 个坐标，用 $trj_y_{i}$ 表示；</p>
<p>基于热图的 LSTM（图中蓝色分支）将每个预测步骤对应的视频帧的热图作为输入，并在 $pw$ 中输出第 2 组 $m$ 个坐标的预测，用 $ht_y_{i}$ 表示：</p>
<p>对于每个热图，让其通过 3 个卷积层，每个卷积层后面都有一个最大池化层。然后，在此图像特征提取之后，应用展平步骤和 1 个密集层来回归坐标（经纬度表示）。</p>
<p>基于显著图的 LSTM 采用与热图相似的架构，将显著图作为输入，在 $pw$ 中输出第 3 组 $m$ 个坐标的预测，用 $sal_y_{i}$ 表示。</p>
<p>对热图和显著图的分支，应用 <code>TimeDistributed</code>层，以便其参数在预测步骤中保持一致。</p>
<p>最终在每个预测步骤连接 $trj_y$ ， $ht_y$，和 $sal_y$ ，并产生一个最终输出 $y$ 。</p>
<p>每个模型的损失函数采用 MAE，优化函数采用 ADAM。</p>
<p>为每个分支的输出以及最终的输出都检查损失，单独和联合地去调整其参数。</p>
<h2 id="评估">评估</h2>
<p>使用 2 折的交叉验证。</p>
<h3 id="超参数">超参数</h3>
<ol>
<li>$pw$ 的大小：0.1s，1.0s，2.0s；</li>
<li>$hw$ 的大小：0.05s，0.6s，1.0s；（分别与上面的 $pw$ 对应）</li>
<li>用于训练的用户数：[3, 10, 30]</li>
</ol>
<p></p>
<h3 id="结果与分析">结果与分析</h3>
<ol>
<li>所有模型的预测精度随着 $pw$ 的增长而下降，表明长期预测问题更难解决；</li>
<li>所有模型的精度预测误差几乎是纬度预测误差的二倍，可能由于运动区域在水平方向的翻倍；</li>
<li>线性回归模型只有在 $pw$ 很短的时候预测精确，随着 $pw$ 的增长，其预测精度会迅速下降；</li>
<li>基于 LSTM 的轨迹模型始终优于所有 $pw$ 的基线模型，但更多的训练观众无助于显着提高准确性。</li>
<li>跨用户的热图和显著图可以帮助长期的预测，可以提供合理的离线全视频 FOV 预测，并具有一致的性能（因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入），当 $pw$ 增长时，其预测精度超过了基于历史轨迹的模型；</li>
<li>结合 3 种模型之后，可以平衡来自历史轨迹、跨用户兴趣和内容显著性的输入，不论 $pw$ 长或短都能产生优化的预测结果；</li>
</ol>
<h3 id="例外情况">例外情况</h3>
<p></p>
<p>M3 在经度上的表现并不适用于上面图中标示的两个视频（Mega.Coaster 和 GTR.Drives.First.Ever）</p>
<p>原因分析：</p>
<p>这两个视频的共同特点是在驾驶路径的一侧具有高运动内容的驾驶内容，因此用户在观看这些视频时，大多数 FOV 始终以行驶轨迹为中心。因此用户不太可能改变其观看方向，这导致即使 $pw = 2.0s$ 时，单一基于轨迹的模型的预测精度也更高。相比之下，从对内容角度出发的分析无济于事，但可能会引入观众可能会忽略的变道，进而造成预测误差。</p>
]]></description>
</item>
<item>
    <title>Note for RnnQoE</title>
    <link>https://ayamir.github.io/posts/papers/note-for-rnnQoE/</link>
    <pubDate>Thu, 16 Dec 2021 19:53:10 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-rnnQoE/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9580281" target="_blank" rel="noopener noreffer">QoE-driven Mobile 360 Video Streaming: Predictive
View Generation and Dynamic Tile Selection</a></p>
<p>Level：ICCC 2021</p>
<p>Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles</p>
<h2 id="系统建模与形式化">系统建模与形式化</h2>
<h3 id="视频划分">视频划分</h3>
<p>先将视频划分成片段：$\Iota = {1, 2, &hellip;, I}$表示片段数为$I$的片段集合。</p>
<p>接着将片段在空间上均匀划分成$M \times N$个 tile，FOV 由被用户看到的 tile 所确定。</p>
<p>使用 ERP 投影，$(\phi_i, \theta_i),\ \phi_i \in (-180\degree, 180\degree], \theta_i \in (-90\degree, 90\degree]$来表示用户在第$i$个片段中的视点坐标。</p>
<p>播放过程中记录用户头部运动的轨迹，积累的数据可以用于 FOV 预测。</p>
<p>跨用户之间的 FOV 轨迹可以用于提高预测精度。</p>
<h3 id="qoe-模型">QoE 模型</h3>
<ul>
<li>
<p>前提</p>
<p>视频编解码器预先确定，无法调整每个 tile 的码率。</p>
</li>
<li>
<p>实现</p>
<ol>
<li>每个 tile 都以不同的码率编码成不同的版本。</li>
<li>每个 tile 都有两种分辨率的版本。</li>
</ol>
</li>
<li>
<p>QoE 内容</p>
<p>客户端收到的视频质量和观看时的卡顿时间。</p>
</li>
<li>
<p>质量形式化</p>
<p>对于每个片段$i \in \Iota$，$S_i = {\tau_{i, j}}_{j=1}^{M \times N}$是用来表示用户实际看到的 tile 的集合的向量。</p>
<p>$\tau_{i, j} = 1$表示第$i$个段中的第$j$个 tile 被看到；$\tau_{i, j} = 0$表示未被看到。</p>
<p>同样的， $\tilde{S}_i = {\tilde{\tau}_{i, j}}_{j = 1}^{M \times N}$ 表示经过 FOV 预测和 tile 选择之后成功被传送到用户头戴设备上的 tile 集合的向量。</p>
<p>$\tilde{\tau}_{i, j} = 1$表示第$i$个段中的第$j$个 tile 被用户接收；$\tilde{\tau}_{i, j} = 0$表示未被接收。</p>
<p>第$i$个段的可感知到的质量可以表示为：</p>
<p>$$
Q_i = \sum_{j = 1}^{M \times N} p_{i, j}b_{i, j}\tau_{i, j}\tilde{\tau}_{i, j}
$$</p>
<p>$b_{i, j}$表示第$i$个片段的第$j$个 tile 的码率；$p_{i, j}$表示对不同位置 tile 所分配的权重；</p>
</li>
<li>
<p>关于权重$p_{i, j}$</p>
<p>研究表明用户在全景视频 FOV 中的注意力分配并不是均等的，越靠近 FOV 中心的 tile 对用户的 QoE 贡献越大。</p>
<p>下面讨论单个片段的情况：用$(\phi_j, \theta_j)$表示 tile 中心点的坐标，并映射到笛卡尔坐标系上$(x_j, y_j, z_j)$：</p>
<p>$$
x_j = cos\theta_jcos\phi_j,\ y_j = sin\theta_j,\ z_j = -cos\theta_jsin\phi_j
$$</p>
<p>则两个 tile 之间的半径距离$d_{j, j&rsquo;}$可以表示为：</p>
<p>$$
d_{j, j&rsquo;} = arccos(x_j x_{j&rsquo;} + y_j y_{j&rsquo;} + z_j z_{j&rsquo;})
$$</p>
<p>对于第$i$个片段，假设用户 FOV 中心的 tile 为$j^*$，那么第$j$个 tile 的权重可以计算出来：</p>
<p>$$
p_{i, j} = (1 - d_{j, j^*} / \pi) \tau_{i, j}
$$</p>
</li>
<li>
<p>卡顿时间形式化</p>
<p>当$\tilde{\tau}_{i, j}$与$\tau_{i, j}$出现分歧时，用户就不能成功收到请求的 tile，头戴设备中显示的内容就会被冻结，由此导致卡顿。</p>
<p>对于任意的片段$i \in \Iota$，相应的卡顿时间$D_i$可以计算出来：</p>
<p>$$
D_i = \frac{\sum_{j = 1}^{M \times N} b_{i, j} \cdot [\tau_{i, j} - \tilde{\tau}_{i, j}]^+}{\xi}
$$</p>
<p>$[x]^+ = max \lbrace x, 0 \rbrace $；$\xi$表示可用的网络资源（已知，并且在推流过程中保持为常数）</p>
<p>卡顿发生于在播放时，用户 FOV 内的 tile 还没有被传输到用户头戴设备中的时刻，终止于所有 FOV 内 tile 被成功传送的时刻。</p>
</li>
<li>
<p>质量与卡顿时间的结合</p>
<p>$$
max\ QoE = \sum_{i = 1}^I (Q_i - wD_i)
$$</p>
<p>$w$表示卡顿事件的惩罚权重。例如，w＝1000 意味着 1 秒视频暂停接收的 QoE 惩罚与将片段的比特率降低 1000 bps 相同。</p>
</li>
</ul>
<h2 id="联合-viewport-预测与-tile-选择">联合 viewport 预测与 tile 选择</h2>
<p>联合框架包括 viewport 预测和动态 tile 选择两个阶段。</p>
<p>viewport 预测阶段集成带有注意力机制的 RNN，接收用户的历史头部移动信息作为输入，输出每个 tile 出现在 FOV 中的可能性分布。</p>
<p>选择 tile 阶段为预测的输出建立的上下文空间，基于上下文赌博机学习算法来选择 tile 并确定所选 tile 的质量版本。</p>
<p></p>
<p></p>
<h3 id="viewport-预测">Viewport 预测</h3>
<p>FOV 预测问题可以看作是序列预测问题。</p>
<p>不同用户观看相同视频时的头部移动轨迹有强相关性，所以跨用户的行为分析可以用于提高新用户的 viewport 预测精度。</p>
<p>被广泛使用的 LSTM 的变体——Bi-LSTM（Bi-directional LSTM）用于 FOV 预测。</p>
<ol>
<li>
<p>输入参数构造</p>
<p>为了构造 Bi-LSTM 学习网络，需要对不同用户的 viewpoint 特性作表征。</p>
<ul>
<li>
<p>在用户观看事先划分好的$I$个片段时，记录每个片段对应的 viewpoint 坐标：</p>
<p>$\Phi_{1:I} = {\phi_i}^I_{i = 1},\ \Theta_{1:I} = {\theta_i}^I_{i=1}$</p>
</li>
<li>
<p>预测时使用的历史信息的窗口大小记为$k$；</p>
</li>
<li>
<p>对于第$i, (i &gt; k)$个片段，相应的 viewpoint 特性由$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$所给出；</p>
</li>
<li>
<p>列索引$m_i$和行索引$n_i$作为 viewpoint tile $(\phi_i, \theta_i)$的标签，由<a href="https://zh.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener noreffer">独热编码</a>表示；</p>
</li>
<li>
<p>通过滑动预测的窗口，所看到的视频片段的特性和标签可以被获取。</p>
</li>
</ul>
</li>
<li>
<p>LSTM 网络构造</p>
<p>整个网络包含 3 层：</p>
<ul>
<li>遗忘门层决定丢弃哪些信息；</li>
<li>更新门层决定哪类信息需要存储；</li>
<li>输出门层过滤输出信息。</li>
</ul>
<p>为了预测用户在第$i$个段的 viewpoint，LSTM 网络接受$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$作为输入；输出行列索引；</p>
<p>$$
m_i = LSTM(\theta_{i-k}, &hellip;, \phi_{i-1}; \alpha)
$$</p>
<p>$$
n_i = LSTM(\theta_{i-k}, &hellip;, \theta_{i-1}; \beta)
$$</p>
<p>$\alpha, \beta$是学习网络的参数；分类交叉熵被用作网络训练的损失函数。</p>
</li>
<li>
<p>Bi-LSTM 的特殊构造</p>
<ul>
<li>
<p>将公共单向的 LSTM 划分成 2 个方向。</p>
<p>当前片段的输出利用前向和反向信息，这为网络提供了额外的上下文，加速了学习过程。</p>
</li>
<li>
<p>双向的 LSTM 不直接连接，不共享参数。</p>
</li>
<li>
<p>每个时间槽的输入会被分别传输到前向和反向的 LSTM 中，并分别根据其状态产生输出。</p>
</li>
<li>
<p>两个输出直接连接到 Bi-LSTM 的输出节点。</p>
</li>
<li>
<p>引入注意力机制为每步时间自动分配权重，从大量信息中选择性地筛选出重要信息。</p>
</li>
<li>
<p>将 Softmax 层堆叠在网络顶部，以获取不同 tile 的 viewpoint 概率。</p>
</li>
</ul>
</li>
</ol>
<p></p>
<h3 id="动态-tile-选择">动态 tile 选择</h3>
<p>使用上下文赌博机学习算法来补偿 viewport 预测错误对 QoE 造成的影响。</p>
<ul>
<li>
<p>上下文赌博机学习算法概况</p>
<p>上下文赌博机学习算法是一个基于特征的 exploration-exploitation 技术。</p>
<p>通过在多条手臂上重复执行选择过程，可以获得在不同上下文中的每条手臂的回报。</p>
<p>通过 exploration-exploitation，目标是最大化累积的回报。</p>
</li>
<li>
<p>组成部分形式化</p>
<ol>
<li>
<p>上下文</p>
<p>直觉上讲，当预测的 viewpoint 不够精确时，需要扩大 FOV 并选择更多的 tile 进行传输。</p>
<p>为了做出第$i$个片段上的预测 viewpoint 填充决策，定义串联的上下文向量：</p>
<p>$c_i = [f^s_i, f^c_i]$，$f^s_i$表示自预测的上下文，$f^c_i$表示跨用户之间的预测上下文。</p>
<p>预测输出的用户$u$的 viewpoint tile 索引用$[\tilde{m}^u_{i-1}, \tilde{n}^u_{i-1}]$表示；</p>
<p>实际的用户$u$的 viewpoint tile 索引用$[m_{i-1}^u, n_{i-1}^u]$表示；</p>
<p>那么对第$i$个片段而言，自预测的上下文可以计算出来：</p>
<p>$$
f_i^s = [|m_{i-1}^u - \tilde{m}^u_{i-1}|, |n_{i-1}^u - \tilde{n}^u_{i-1}|]
$$
跨用户的上下文信息获取：使用 KNN 准则选择一组用户，其在前$k$个片段中的轨迹最接近用户$u$的轨迹。</p>
<p>使用$\zeta$表示获得的用户集合，使用</p>
<p>$$E_{\zeta_u}(m_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u} |m_i^u - \tilde{m}_i^u|$$</p>
<p>$$E_{\zeta_u}(n_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u}|n_i^u - \tilde{n}_i^u|$$</p>
<p>表示预测误差，则：</p>
<p>$$
f_i^u = [E_{\zeta_u}(m_i), E_{\zeta_u}(n_i)]
$$</p>
</li>
<li>
<p>手臂</p>
<p>根据从第一个阶段得到的每个 tile 的可能性分布，所有的 tile 可以用倒序的方式排列。</p>
<p>最高可能性的 tile 被看作 FOV 的中心，高码率以此 tile 为中心分配。</p>
<p>剩余的带宽用于扩展 FOV，低可能性的 tile 被顺序选择来扩展 FOV 直至带宽耗尽。</p>
<p>手臂的状态$a \in {0, 1}$表示 tile 选择的策略：</p>
<ul>
<li>
<p>$a = 0$表示 viewpoint 预测准确，填充 tile 分配了高质量；</p>
</li>
<li>
<p>$a = 1$表示 viewpoint 预测不准确，填充 tile 分配的质量较低，为了传送尽可能多的 tile 而减少卡顿；</p>
</li>
</ul>
</li>
<li>
<p>回报</p>
<p>给定上下文$c_i$，选择手臂$a$，预期的回报$r_{i, a}$建模为$c_i$和$a$组合的线性函数：</p>
<p>$$
\Epsilon[r_{i, a}|c_{i, a}] = c_{i, a}^T \theta_a^*
$$</p>
<p>未知参数$\theta_a$表示每个手臂的特性，目标是为第$i$个片段选择最优的手臂：</p>
<p>$$
a_i^* = \underset{a}{argmax}\ c_{i, a}^T \theta_a^*
$$</p>
<p>使用<a href="https://zhuanlan.zhihu.com/p/38875273" target="_blank" rel="noopener noreffer">LinUCB</a>算法做出特征向量的精确估计并获取$\theta_a^*$。</p>
<p></p>
</li>
</ol>
</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>评估准备</p>
<ul>
<li>使用现有的<a href="https://github.com/xuyanyu-shh/VR-EyeTracking" target="_blank" rel="noopener noreffer">viewpoint 轨迹数据集</a>，所有视频被编码为至少每秒 25 帧，长度为 20 到 60 秒；</li>
<li>视频每个片段被划分为$6 \times 12$的 tile，每个的角度是$30\degree \times 30\degree$；</li>
<li>初始 FOV 设定为$90\degree \times 90\degree$，在 viewpoint 周围是$3 \times 3$的 tile；</li>
<li>每个片段的长度为 500ms；</li>
<li>默认的预测滑动窗口大小$k = 5$；</li>
<li>HD 和 LD 版本分别以按照 HEVC 的$QP={32, 22}$的参数编码而得到；</li>
<li>训练集和测试集的比例为$7:3$；</li>
<li>Bi-LSTM 层配置有 128 个隐单元；</li>
<li>batch 大小为 64；</li>
<li>epoch 次数为 60；</li>
</ul>
</li>
<li>
<p>性能参数</p>
<ul>
<li>
<p>预测精度</p>
</li>
<li>
<p>视频质量</p>
<p>由传送给用户的有效码率决定：例如实际 FOV 中的 tile 码率总和</p>
</li>
<li>
<p>卡顿时间</p>
</li>
<li>
<p>规范化的 QoE</p>
<p>实际取得的 QoE 与在 viewpoint 轨迹已知情况下的 QoE 的比值</p>
</li>
</ul>
</li>
<li>
<p>对比目标</p>
<ul>
<li>预测阶段——预测精度
<ol>
<li>LSTM</li>
<li>LR</li>
<li>KNN</li>
</ol>
</li>
<li>取 tile 的阶段——规范化的 QoE
<ol>
<li>两个阶段都使用纯 LR</li>
<li>只预测而不做动态选择</li>
</ol>
</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for 360ProbDASH</title>
    <link>https://ayamir.github.io/posts/papers/note-for-360ProbDASH/</link>
    <pubDate>Thu, 09 Dec 2021 10:20:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/papers/note-for-360ProbDASH/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3123266.3123291" target="_blank" rel="noopener noreffer">360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming</a></p>
<p>Level: ACM MM 17</p>
<p>Keyword:</p>
<p>Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation</p>
<h2 id="工作范围与目标">工作范围与目标</h2>
<p>应用层-&gt;基于 tile-&gt;viewport 预测的可能性模型+预期质量的最大化</p>
<ul>
<li>
<p>针对小 buffer 提出了<code>target-buffer-based rate control</code>算法来避免重缓冲事件（避免卡顿）</p>
</li>
<li>
<p>提出 viewport 预测的可能性模型计算 tile 被看到的可能性（避免边缘效应）</p>
</li>
<li>
<p>形式化 QoE-driven 优化问题：</p>
<p>在传输率受限的情况下最小化 viewport 内的质量失真和空间质量变化（获取受限状态下最好的视频质量）</p>
</li>
</ul>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>形式化参数</p>
<p>$M*N$个 tile，M 指 tile 序列的序号，N 指不同的码率等级</p>
<p>$r_{i, j}$指比特率，$d_{i, j}$指失真，$p_{i}$指被看到的可能性（$\sum_{i=1}^{N}p_{i} = 1$）</p>
<p>$\Phi(X)$指质量失真，$\Psi(X)$指质量变化</p>
</li>
<li>
<p>目标</p>
<p>找到推流段的集合：$X = {x_{i, j}}$，其中${x_{i, j}} = 1$指被第$&lt;i, j&gt;$个 tile 被选中；$x_{i, j} = 0$则是未选中。
$$
\underset{X}{min}\ \Phi(X) + \eta \cdot \Psi(X) \
s.t. \sum_{i=1}^{N}\sum_{j=1}^{M}x_{i, j}\cdot r_{i, j} \le R, \
\sum_{j=1}^{M}x_{i, j} \le 1, x_{i, j} \in {0, 1}, \forall i.
$$
整个公式即为前所述的问题的形式化表达的公式化结果。</p>
</li>
<li>
<p>模型细节</p>
<ol>
<li>
<p>$\Phi(X)$和$\Psi(X)$的计算=&gt;通过考虑球面到平面的映射</p>
<p>通过计算球面上点的 Mean Squared Error 来得到 S-PSNR 进而评估质量：$d_{i, j}$来表示第${&lt;i, j&gt;}$个段的 MSE</p>
<p>
$$
\phi_i = \frac{\pi}{2} - h_i \cdot \frac{\pi}{H}, \Delta\phi = \Delta h \cdot \frac{\pi}{H}, \
\theta_i = w_i \cdot \frac{2\pi}{W}, \ \Delta\theta = \Delta w \cdot \frac{2\pi}{W},
$$
$H$和$W$分别指按照 ERP 格式投影之后的视频高度和宽度</p>
<p>第$i$个 tile 的空间面积用$s_i$表示：
$$
s_i\ =\ \iint_{\Omega_i}Rd\phi Rcos\phi d\theta \
=\Delta\theta R^2[sin(\phi_i + \Delta\phi) - sin\phi_i],
$$
$R$指球的半径（$R = W/2\pi$），所以整体的球面质量失真$D_{i, j}$可以计算出来：
$$
D_{i, j} = d_{i, j} \cdot s_i,
$$
结合每个 tile 被看到的概率$p_i$可以得出$\Phi(X)$和$\Psi(X)$
$$
\Phi(X)=\frac{\sum_{i=1}^N\sum_{j=1}^MD_{i, j}\cdot x_{i,j}\cdot p_i}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i},\
\Psi(X) = \frac{\sum_{i=1}^N\sum_{j=1}^Mx_{i, j}\cdot p_i \cdot\ (D_{i,j}-s_{i} \cdot \Phi(X))^2}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i}.
$$</p>
</li>
<li>
<p>Viewport 的可能性模型</p>
<ol>
<li>
<p>方向预测=&gt;<strong>线性回归模型</strong></p>
<p>将用户的欧拉角看作是$yaw(\alpha)$，$pitch(\beta)$和$rool(\gamma)$，应用线性回归做预测
$$
\begin{cases}
\hat{\alpha}(t_0 + \delta) = m_{\alpha}\delta+\alpha(t_0),\
\hat{\beta}(t_0 + \delta) = m_{\beta}\delta+\beta(t_0),\
\hat{\gamma}(t_0 + \delta) = m_{\gamma}\delta+\gamma(t_0).
\end{cases}
$$</p>
</li>
<li>
<p>预测错误的分布=&gt;<strong>高斯分布</strong>，根据公式均值和标准差都能从统计信息中计算出来</p>
<p>收集 5 名志愿者的头部移动轨迹并投影到 3 个方向上绘制成图，实验结果为预测错误呈现高斯分布（样本数可能不够？）</p>
<p>
$$
\begin{cases}
P_{yaw}(\alpha) = \frac{1}{\sigma_{\alpha}\sqrt{2\pi}}exp{-\frac{[\alpha-(\hat{\alpha}+\mu_{\alpha})]^2}{2\sigma_{\alpha}^2}},\
P_{pitch}(\beta) = \frac{1}{\sigma_{\beta}\sqrt{2\pi}}exp{-\frac{[\beta-(\hat{\beta}+\mu_{\beta})]^2}{2\sigma_{\beta}^2}},\
P_{roll}(\gamma) = \frac{1}{\sigma_{\gamma}\sqrt{2\pi}}exp{-\frac{[\gamma-(\hat{\gamma}+\mu_{\gamma})]^2}{2\sigma_{\gamma}^2}}.
\end{cases}
$$
3 个方向各自<strong>独立</strong>，因此最终的预测错误$P_E(\alpha,\beta,\gamma)$可以表示为：
$$
P_E(\alpha, \beta, \gamma) = P_{yaw}(\alpha)P_{pitch}(\beta)P_{roll}(\gamma).
$$</p>
</li>
<li>
<p>球面上点被看到的可能性</p>
<p>球面坐标为$(\phi, \theta)$点的可能性表示为$P_s(\phi, \theta)$</p>
<p>因为一个点可能在多个不同的 viewport 里面，所以定义按照用户方向从点$(\phi, \theta)$出发能看到的点集$L(\phi, theta)$</p>
<p>因此空间点$s$被看到的可能性可以表示为：
$$
P_s(\phi, \theta) = \frac{1}{|L(\phi, \theta)|}\sum_{(\alpha, \beta, \gamma) \in L(\phi, \theta)}P_E(\alpha, \beta, \gamma),
$$</p>
</li>
<li>
<p>球面上 tile 被看到的可能性</p>
<p>tile 内各个点被看到的可能性的<strong>均值</strong>即为 tile 被看到的可能性（可否使用其他方式？）
$$
p_i = \frac{1}{|U_i|} \sum_{(\phi, \theta) \in U_i} P_s(\phi, \theta).
$$
$U_i$表示 tile 内的空间点集</p>
</li>
</ol>
</li>
<li>
<p><code>Target-Buffer-based</code> Rate Control</p>
<p>因为长期的头部移动预测会产生较高的预测错误，所以不能采用大缓冲区（没有 cite 来证明这一点）</p>
<p></p>
<p>将处于相同时刻的段集合成一个块存储在缓冲区中。</p>
<p>在自适应的第 k 步，定义$d_k$作为此时的 buffer 占用情况（等到第 k 个块被下载完毕）
$$
b_k = b_{k-1} - \frac{R_k \cdot T}{C_k} + T
$$
$C_k$表示平均带宽，$R_k$表示总计的码率</p>
<p>为了避免重新缓冲设定目标 buffer 占用$B_{target}$，并使 buffer 占用保持在$B_{target}$（$b_k = B_{target}$）</p>
<p>因此总计的码率需要满足：
$$
R_k = \frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T),
$$
这里的$C_k$表示可以从历史的段下载信息中估计出来的带宽</p>
<p>设定$R$的下界$R_{min}$之后（没有说明为何需要设定下界），公式 12 可以修正为如下：
$$
R_k = max{\frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T), R_{min}}.
$$</p>
</li>
</ol>
</li>
</ol>
<h2 id="实现">实现</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>
<p>视频裁剪器</p>
<p>将视频帧切割成 tile</p>
</li>
<li>
<p>编码器</p>
<p>对 tile 进行划分并将其编码成多种码率的段</p>
</li>
<li>
<p>MPD 产生器</p>
<p>添加<strong>SRD 特性</strong>来表示段之间的空间关系</p>
<p>添加经度和<strong>纬度</strong>属性来表示</p>
<p>添加<strong>质量失真</strong>和<strong>尺寸</strong>属性</p>
</li>
<li>
<p>Apache HTTP 服务器</p>
<p>存储视频段和 mpd 文件，向客户端推流</p>
</li>
</ol>
<h3 id="客户端">客户端</h3>
<ol>
<li>
<p>基础：dash.js</p>
</li>
<li>
<p>额外的模块</p>
<ul>
<li>
<p><code>QoE-driver Optimizer</code>
$$
Output = HTTP\ GET请求中的最优段
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Target\ buffer\ based\ Rate\ Controller\
Viewport\ Probabilistic\ Model\
QR\ Map
\end{cases}
$$</p>
</li>
<li>
<p><code>Target-buffer-based Rate Controller</code>
$$
Output = 总计的传输码率，按照公式13计算而来
$$</p>
<p>$$
Input = Output\ of\ {Bandwidth\ Estimation\ module
$$</p>
</li>
<li>
<p><code>Viewport Probabilistic Model</code>
$$
Output = 每个tile被看到的可能性，按照公式10计算而来
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Orientation\ Prediction\ module\
SRD\ information
\end{cases}
$$</p>
</li>
<li>
<p><code>QR Map</code>QR=&gt;Quality-Rate
$$
Output = 所有段的QR映射
$$</p>
<p>$$
Input = MPD中的属性
$$</p>
</li>
<li>
<p><code>Bandwidth Estimation</code>（没有展开研究，因为不是关键？）
$$
Output = 前3秒带宽估计的平均值
$$</p>
<p>$$
Input = 下载段过程中的吞吐量变化
$$</p>
<p>可以通过<code>onProgess()</code>的回调函数<code>XMLHttpRequest API</code>获取</p>
</li>
<li>
<p><code>Orientation Prediction</code>
$$
Output = 用户方向信息的预测结果（yaw, pitch, roll）
$$</p>
<p>$$
Input = Web\ API中获取的DeviceOrientation信息，使用线性回归做预测
$$</p>
</li>
</ul>
</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>整体设定</p>
<ol>
<li>将用户头部移动轨迹编码进播放器来模拟用户头部移动</li>
<li>积极操控网络状况来观察不同方案对网络波动的反应</li>
</ol>
</li>
<li>
<p>详细设定</p>
<ul>
<li>
<p>服务端</p>
<ol>
<li>
<p>视频选择</p>
<p>2880x1440 分辨率、时长 3 分钟、投影格式 ERP</p>
</li>
<li>
<p>切分设置</p>
<p>每个块长 1s（$T=1$）、每个块被分成 6x12 个 tile（$N=72$）</p>
<p>每个段的码率设置为${20, 50, 100, 200, 300}$，单位 kpbs</p>
</li>
<li>
<p>视频编码</p>
<p><a href="http://www.videolan.org/developers/x264.html" target="_blank" rel="noopener noreffer">开源编码器 x264</a></p>
</li>
<li>
<p>视频分包</p>
<p><a href="https://gpac.wp.mines-telecom.fr/mp4box/" target="_blank" rel="noopener noreffer">MP4Box</a></p>
</li>
<li>
<p>注意事项</p>
<p>每个段的确切尺寸可能与其码率不同，尤其对于长度较短的块。</p>
<p>为了避免这影响到码率自适应，将段的确切尺寸也写入 MPD 文件中</p>
</li>
</ol>
</li>
<li>
<p>客户端</p>
<ol>
<li>
<p>缓冲区设定（经过实验得出的参数）</p>
<p>$B_{max}=3s$，$B_{target}=2.5s$，$R_{min}=200kbps$，$权重\eta=0.0015$</p>
</li>
</ol>
</li>
<li>
<p>高斯分布设定</p>
<table>
<thead>
<tr>
<th style="text-align:center">Yaw</th>
<th style="text-align:center">Pitch</th>
<th style="text-align:center">Roll</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\mu_{\alpha}=-0.54,\ \sigma_{\alpha}=7.03$</td>
<td style="text-align:center">$\mu_{\beta}=0.18,\ \sigma_{\beta}=2.55$</td>
<td style="text-align:center">$\mu_{\gamma}=2.16,\ \sigma_{\gamma}=0.15$</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li>
<p>比较对象</p>
<ul>
<li>ERP：原始视频格式</li>
<li>Tile：只请求用户当前 viewport 的 tile，不使用 viewport 预测，作为 baseline</li>
<li>Tile-LR：使用线性回归做预测，每个 tile 的码率被平均分配</li>
</ul>
</li>
<li>
<p>性能指标</p>
<ul>
<li>卡顿率：卡顿时间占播放总时长的比例</li>
<li>Viewport PSNR：直接反应 Viewport 内的视频质量</li>
<li>空间质量差异：Viewport 内质量的协方差</li>
<li>Viewport 偏差：空白区域在 Viewport 中的比例</li>
</ul>
</li>
</ul>
]]></description>
</item>
</channel>
</rss>

[{"categories":["knowledge"],"content":"Jitter Buffer 是什么 在了解 Jitter Buffer 之前，我们应该先来看一下整个 webrtc 会话中数据传输的完整流程。 与传输相关的部分主要出现在 pacer 和 Jitter Buffer 这两个部分，从图中可以清晰的看到这两者处于编解码和网络传输之间，考虑到编解码可能会引入突变的帧大小（比如 I 帧），而在网络传输的过程又受到网络传输速率和排队延迟的影响，所以它们的作用其实就比较显而易见了。Pacer 在发送端负责平滑编码后的码流打包成 rtp 包之后，发送到网络上的速率；Jitter Buffer 在接收端负责平滑接收到的 rtp 包到组成解码所需的码流的过程。 当然，这个传输的过程中离开不了拥塞控制算法如 gcc 和各种抗丢包的技术如 nack，fec 等来保障实时通信质量。在这里我们主要关注传输的流程，相关算法和机制之后再研究。 Jitter Buffer 可以理解为有两部分的功能，一部分是 Buffer 的功能，也就是作为 rtp 包的缓冲区，并且将 rtp 包恢复成表示可解码帧的码流；另一部分是 Jitter 的功能，也就是通过引入延迟来平滑因帧大小和网络状况而造成的接收帧不均匀的情况。 ","date":"2024-04-18","objectID":"/posts/knowledge/webrtc/jitter-buffer/:1:0","tags":["WebRTC"],"title":"Jitter Buffer学习理解（上）","uri":"/posts/knowledge/webrtc/jitter-buffer/"},{"categories":["knowledge"],"content":"buffer 的工作流程 正如前面所说，网络传输的是 rtp 包，而解码器的输入是可以解码的码流，所以需要一个将 rtp 包转换成可以解码的帧的过程。因为一个帧由多个 rtp 包组成，所以肯定需要缓冲区来存放前面收到但是还不足以组成一个帧的 rtp 包，这个缓冲区在 webrtc 中其实就是PacketBuffer。此外，考虑到编解码原理，接收到的 P 帧还需要等它所依赖的 I 帧/P 帧被解码，它才能被解码，所以在PacketBuffer之外还需要一个FrameBuffer来缓存可以解码的一个 GOP 中的各个帧。而负责寻找当前帧所依赖帧的是RtpFrameReferenceFinder，因为这个寻找依赖帧的过程实际上是递归依赖的，直到找到一个 GOP 的 IDR 帧才算结束，这样就能得到按照解码依赖顺序排列的一个 GOP。而最后，因为不同 GOP 的解码是独立的，所以 GOP 之间实际上就直接按照时间顺序排列就完成了 GOP 的排序。 总体上来看，buffer 负责的工作就是 3 个：rtp 包的排序并组成帧、帧的排序并组成 GOP、GOP 的排序并组成视频。 ","date":"2024-04-18","objectID":"/posts/knowledge/webrtc/jitter-buffer/:2:0","tags":["WebRTC"],"title":"Jitter Buffer学习理解（上）","uri":"/posts/knowledge/webrtc/jitter-buffer/"},{"categories":["knowledge"],"content":"代码实现 结合代码逻辑来看一下实际的工作流程，最核心的类是RtpVideoStreamReceiver2，这个类负责完成上面所说的工作内容的实现与逻辑的拆分，不过除了上面这些逻辑之外，这个类还需要向发送端发送 rtcp 反馈包的工作，比如 nack, pli, fir。 class RtpVideoStreamReceiver2 : public LossNotificationSender, public RecoveredPacketReceiver, public RtpPacketSinkInterface, public KeyFrameRequestSender, public NackSender, public OnDecryptedFrameCallback, public OnDecryptionStatusChangeCallback, public RtpVideoFrameReceiver { public: // A complete frame is a frame which has received all its packets and all its // references are known. class OnCompleteFrameCallback { public: virtual ~OnCompleteFrameCallback() {} virtual void OnCompleteFrame(std::unique_ptr\u003cEncodedFrame\u003e frame) = 0; }; RtpVideoStreamReceiver2( TaskQueueBase* current_queue, Clock* clock, Transport* transport, RtcpRttStats* rtt_stats, // The packet router is optional; if provided, the RtpRtcp module for this // stream is registered as a candidate for sending REMB and transport // feedback. PacketRouter* packet_router, const VideoReceiveStreamInterface::Config* config, ReceiveStatistics* rtp_receive_statistics, RtcpPacketTypeCounterObserver* rtcp_packet_type_counter_observer, RtcpCnameCallback* rtcp_cname_callback, NackPeriodicProcessor* nack_periodic_processor, // The KeyFrameRequestSender is optional; if not provided, key frame // requests are sent via the internal RtpRtcp module. OnCompleteFrameCallback* complete_frame_callback, rtc::scoped_refptr\u003cFrameDecryptorInterface\u003e frame_decryptor, rtc::scoped_refptr\u003cFrameTransformerInterface\u003e frame_transformer, const FieldTrialsView\u0026 field_trials, RtcEventLog* event_log); ~RtpVideoStreamReceiver2() override; void AddReceiveCodec(uint8_t payload_type, VideoCodecType video_codec, const webrtc::CodecParameterMap\u0026 codec_params, bool raw_payload); // Clears state for all receive codecs added via `AddReceiveCodec`. void RemoveReceiveCodecs(); void StartReceive(); void StopReceive(); // Produces the transport-related timestamps; current_delay_ms is left unset. absl::optional\u003cSyncable::Info\u003e GetSyncInfo() const; bool DeliverRtcp(const uint8_t* rtcp_packet, size_t rtcp_packet_length); void FrameContinuous(int64_t seq_num); void FrameDecoded(int64_t seq_num); void SignalNetworkState(NetworkState state); // Returns number of different frames seen. int GetUniqueFramesSeen() const { RTC_DCHECK_RUN_ON(\u0026packet_sequence_checker_); return frame_counter_.GetUniqueSeen(); } // Implements RtpPacketSinkInterface. void OnRtpPacket(const RtpPacketReceived\u0026 packet) override; // Public only for tests. // Returns true if the packet should be stashed and retried at a later stage. bool OnReceivedPayloadData(rtc::CopyOnWriteBuffer codec_payload, const RtpPacketReceived\u0026 rtp_packet, const RTPVideoHeader\u0026 video, int times_nacked); // Implements RecoveredPacketReceiver. void OnRecoveredPacket(const RtpPacketReceived\u0026 packet) override; // Send an RTCP keyframe request. void RequestKeyFrame() override; // Implements NackSender. void SendNack(const std::vector\u003cuint16_t\u003e\u0026 sequence_numbers, bool buffering_allowed) override; // Implements LossNotificationSender. void SendLossNotification(uint16_t last_decoded_seq_num, uint16_t last_received_seq_num, bool decodability_flag, bool buffering_allowed) override; // Returns true if a decryptor is attached and frames can be decrypted. // Updated by OnDecryptionStatusChangeCallback. Note this refers to Frame // Decryption not SRTP. bool IsDecryptable() const; // Implements OnDecryptedFrameCallback. void OnDecryptedFrame(std::unique_ptr\u003cRtpFrameObject\u003e frame) override; // Implements OnDecryptionStatusChangeCallback. void OnDecryptionStatusChange( FrameDecryptorInterface::Status status) override; // Optionally set a frame decryptor after a stream has started. This will not // reset the decoder state. void SetFrameDecryptor( rtc::scoped_refptr\u003cFrameDecryptorInterface\u003e frame_decryptor); // Sets a frame transformer after a stream has started, if no transformer // has previously been set. Does not reset the decoder state. void SetDepack","date":"2024-04-18","objectID":"/posts/knowledge/webrtc/jitter-buffer/:3:0","tags":["WebRTC"],"title":"Jitter Buffer学习理解（上）","uri":"/posts/knowledge/webrtc/jitter-buffer/"},{"categories":["knowledge"],"content":"概念 同步和异步、阻塞和非阻塞这两组概念经常出现，并且人们往往会有如下认知： 同步就是程序发出同步调用之后就需要等待调用返回一个结果，然后才能继续指令的执行流。 异步就是程序发出异步调用之后能直接得到返回，程序可以继续执行，至于调用发起者想要得到的结果会在未来的某个时刻获取。 阻塞就是在调用结果返回之前，当前线程会被挂起。 非阻塞就是再不能立刻得到结果之前，当前线程并不会被挂起。 那么这样来看的话，同步调用就是阻塞调用，异步调用就是非阻塞调用，这个认知是有些狭隘的。 ","date":"2024-04-13","objectID":"/posts/knowledge/os/sync-async-block-nonblock/:1:0","tags":["OS"],"title":"同步、异步、阻塞、非阻塞","uri":"/posts/knowledge/os/sync-async-block-nonblock/"},{"categories":["knowledge"],"content":"同步和异步 同步和异步主要 focus 的是调用者和被调用者双方消息通信的机制。 同步是调用者等待被调用者返回结果，异步则是调用被直接返回，调用者不会等待被调用者。 以例子来说明的话就是：假如你打开了崩铁想玩，但是却发现需要下载更新客户端： 如果采用同步的方式就是你一直等着下载安装完成，期间什么都不做。 不过我相信正常人都不会在这个过程中干等着什么都不做，而是会在点击下载按钮之后玩会儿手机或者干点别的事，这就是异步的方式。 在这个例子中我们可以发现： 如果采用同步的方式，我们一定能在更新完成之后的第一时间立刻玩到游戏，但是在苦苦等待的过程中我们的时间被浪费掉了。 如果采用异步的方式，我们在等游戏更新完成的过程中做了其他事情，时间没有被浪费掉，但是我们需要一种机制来知道什么时候游戏就更新好了。假如在下载过程中我们去做了别的事情，那么就可能不会第一时间知道它什么时候更新完成。 如果把我们自己比作 CPU 的话，并且假设目前 OS 上面只有这一个任务，同步的方式会浪费 CPU 时间，而采用异步的方式可以让我们多做一些别的事情，不过异步需要一些消息通知的方式来告诉我们等待的任务什么时候会有结果。假如崩铁下载器在下载完成之后没法通知我们，那么我们可能需要隔一段时间检查一下有没有更新完成。 这么看来，其实同步就是 OS/函数调用 默认支持的通信方式（无非就是等呗），而异步虽然可以解决同步会浪费时间的问题，但是需要引入 消息通知（下载器窗口变成启动游戏的窗口，并且置于最前）/注册回调函数（假如可以派个人替我玩的话）/轮询（隔几分钟看看有没有更新完）这些机制才能保证完成任务。 从线程/协程的角度来看同步和异步的话，其实同步就是完完全全的单线程模式，而异步可以利用协程的特性在单线程中完成异步任务，从而避免大量使用回调函数带来的“回调地狱”。 以实际的例子来说明，在使用 neovim 写代码的时候会使用代码格式化的功能，默认的代码格式化的同步完成的，也就是说我们需要等格式化完成才能执行别的任务（从阻塞的角度看就是，neovim 被格式化的过程阻塞了，这种方式就是同步且阻塞的方式）。在文件很小的时候，因为格式化很快所以以同步的方式进行格式化并不会有太多的影响。但是如果需要进行大文件的格式化，同步的方式会阻塞很久，严重影响体验。从更高的角度来看，格式化器影响的主要是代码的位置（可能也会影响代码的内容例如 goimports ），那么理论上我们不进行与代码内容和代码位置相关的写入操作就不会造成写冲突。但是这种同步的方式就是一种一刀切，使我们只能等格式化完成，这其实不太合理。 为什么说这个例子可以用协程的方式实现异步呢？其实原理就是局部性 + 协程特性。因为我们在写代码的时候通常只是会编辑一处的内容，如果我们下达了对整个大文件的格式化操作，那么理论上是可以按照不同的小部分（比如一个函数）来完成格式化过程的，而在完成格式化一个函数的过程中，CPU 的执行权可以交给格式化器，而在用户需要进行一些别的操作的时候，格式化协程可以挂起(yield)并将 CPU 让给用户操作的协程，而当用户的操作完成之后，格式化协程可以恢复(resume)并获取 CPU 继续执行。这样来看，通过对任务的分割和对协程的交替切换，就实现了异步的机制。 ","date":"2024-04-13","objectID":"/posts/knowledge/os/sync-async-block-nonblock/:2:0","tags":["OS"],"title":"同步、异步、阻塞、非阻塞","uri":"/posts/knowledge/os/sync-async-block-nonblock/"},{"categories":["knowledge"],"content":"阻塞和非阻塞 阻塞和非阻塞主要 focus 的是调用者在等待调用结果时候的状态。 还是以上面的例子来说： 阻塞描述的是我们在等待游戏更新完毕的过程中，处于什么都干不了的状态（我只想玩崩铁，我啥都不想干！）， 非阻塞描述的是在游戏更新的时候，我们可以干点别的，比如看一集《葬送的芙莉莲》（这个时间正好能多看一集番，美滋滋~）。 对于实际的编程场景而言，阻塞和非阻塞这组概念常常在 Socket 编程中出现，我们可以利用 fcntl 把 socket 置为阻塞或者非阻塞的状态（默认是非阻塞） 对于 TCP 而言，其对应的发送和接收的 API 是 send/recv，而 send/recv 其实并不是真的直接向网络上发数据/直接从网络上接收数据，而是将数据写入到内核发送缓冲区/从内核接收缓冲区读取数据。 如果发送端一直往发送缓冲区写数据而接收端不读数据的话（其实就是流量的滑动窗口不滑动了），当缓冲区满了之后： 如果 socket 是阻塞模式，继续调用 send 会将程序阻塞在 send 处，不会执行之后的逻辑。 如果 socket 是非阻塞模式，继续调用 send 会直接返回错误，然后执行之后的逻辑（通常使用非阻塞模式我们会获取 send 调用的返回值并在循环中判断）。 ","date":"2024-04-13","objectID":"/posts/knowledge/os/sync-async-block-nonblock/:3:0","tags":["OS"],"title":"同步、异步、阻塞、非阻塞","uri":"/posts/knowledge/os/sync-async-block-nonblock/"},{"categories":["knowledge"],"content":"总结 其实总的来看，在实际的编码过程中我们没必要严格区分这两种概念，因为它们之间的区别并不是左与右，正与负这种关系。概念还是需要与实际的例子相结合才有相辅相成的意义。 ","date":"2024-04-13","objectID":"/posts/knowledge/os/sync-async-block-nonblock/:4:0","tags":["OS"],"title":"同步、异步、阻塞、非阻塞","uri":"/posts/knowledge/os/sync-async-block-nonblock/"},{"categories":["knowledge"],"content":"进程 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:1:0","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"是什么 学操作系统课的时候学过一句话叫做：进程是操作系统资源分配的最小单位，进程的资源直接由 OS 分配，并存储在进程控制块 PCB 中： 进程标识符 PID 进程状态：就绪、运行、阻塞 内存资源： 代码段、数据段、堆和栈 文件描述符 fd ： stdin、stdout、stderr、以及进程打开的文件描述符列表比如本地文件以及网络连接等的 fd 寄存器： PC、SP、还有其他的通用寄存器 进程控制信息： 父进程 ID ，子进程 ID ，以及信号处理器这些 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:1:1","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"有什么用 在拿进程和程序做对比的时候我们知道，进程就是运行着的程序（这里的运行指的是程序被加载到内存空间中然后开始按照程序指令执行，而不是指进程状态中的运行状态），受 OS 的调度，可以说我们写程序的目的就是要让 CPU 可以按照磁盘上的代码指令来执行操作，进程就是实现这一目的的过程。 因为 OS 使用了虚拟内存这一概念，使得每个进程都认为自己是独占 OS 的，所以一个进程是不知道其他进程的存在的。因而如果面对需要多个进程协作完成一项任务的时候（其实这种情况的描述从逻辑上应该是自上到下的，先有的是一项任务，我们通过分析发现这两个任务需要写多个程序来完成），就会不可避免地引入进程间通信 IPC 。 常用的进程间通信手段大概有 6 种：消息队列、共享内存、匿名管道、命名管道、信号量、Socket，这几种方式根据需求的不同都有自己的用武之地，不过我个人最习惯用的还是 Socket ，因为它具有最优的可扩展性（跨主机、跨语言），可记录性（可以使用 tcpdump/wireshark 抓包），也完美符合我对于通信这一名词想象（明确的通信双方、全双工的信道）。 从我的实际项目经历中来看，我的 Unity 客户端实例需要把游戏运行过程中产生的 2D 轨迹数据输入给 Python 端的 AI 模型，并获取模型输出。对于这一场景，我的首选就是 Socket 通信，首先是因为 Socket 具备全双工的特性可以满足需求，其次是使用 Socket 可以在 AI 模型部署到其他主机上的时候也能正常运行。 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:1:2","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"线程 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:2:0","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"是什么 上面说到进程是 OS 资源分配的最小单位，这句话的下半句是：线程是操作系统调度的最小单位，这句话其实暗示了，线程和进程的概念对于单线程的进程而言是相同的。 OS 在调度 CPU 的时候是以线程为单位的，也就说明线程其实也是一种 OS 级别的概念。对于 Linux 而言，线程和进程使用的是相同的数据结构 task_struct 来表示的，不过进程的创建使用的是 fork() 这一系统调用，而线程的创建用的是 clone() 这一系统调用。 结合前半句话，说明 OS 在分配资源的时候分配不到线程这个层面上（单线程进程是特例），对于同一个进程的多个线程，他们之间共享进程的代码段、数据段和 fd 这些，不过每个线程都拥有自己独立的堆、栈空间。 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:2:1","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"有什么用 因为每个进程都会拥有上面列出的这些资源，直接受到 OS 的控制，所以进程的创建和销毁不可避免地会涉及到相对比较大的时间开销。 相比之下，线程因为可以直接继承并共享进程的部分资源，所以线程的创建和销毁要更加轻量。 也正因如此，同一进程之间的多个线程之间只需要使用一些编程上的技法就可以完成通信，常用的就是各种锁、条件变量以及阻塞队列。 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:2:2","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"什么时候用多线程 首先需要考虑的是能不能使用多线程。多线程的执行过程是 OS 调度 CPU 的多个核心来分别执行多个线程的过程，因而最适合使用多线程的任务一定具备：划分给各个线程之间的任务没有重叠、也无需通信（或者说没有依赖关系）的特性，每个任务都是 Compute-Intensive 的。 从我的实际项目经历中来看，在把 GPU 显存中的 yuv 图像数据回读到内存中的时候，图像的不同部分之间是相互独立的，因而这个过程天然适合使用多线程来完成，主线程只需要等待多个线程读完数据之后执行下一步操作即可。 其次需要考虑的是多线程能带来多大的收益。单线程和多线程的区别其实就是可以占有并利用的 CPU 核心数的区别，因此当任务的瓶颈不在于 CPU 的时候就需要考虑是否有使用多线程的必要。根据 Amdahl’s law ，$S(n)=\\frac{1}{(1-P)+\\frac{P}{n}}$，当处理单元数趋向于无穷的时候，并行化所带来的加速比 $S(n)$ 将趋近于 $\\frac{1}{1-P}$​ 。如果任务中可并行的部分比较小的情况下，可能就没有并行化的必要了。 实际任务中使用的线程数量应该结合可以利用的系统资源（CPU 和内存）和相应任务的性能瓶颈两方面考虑，这里之所以要强调是可以利用的资源是因为我们需要从 OS 的角度来考虑，如果当前进程使用了 CPU 的全部核心，那么 OS 上的其他任务就不能得到及时响应，当然因为线程本身需要独立的堆栈空间，所以线程的理论上限需要考虑内存大小。另一方面，多线程本质上只是在提高当前任务对于 CPU 的使用率，但是一个任务的执行不可能只用到 CPU ，还会用到内存以及可能涉及 IO 操作。就算是不涉及锁的并行读操作，也需要考虑 IO 可以利用的总线带宽大小，所以对于这样的任务使用多线程并行化带来的性能提升曲线应该会随着线程数的增多而呈现先升后降的趋势。还是以我上面遇到的问题为例，从 GPU 显存回读数据到内存中的操作就会受到 PCIe 总线带宽的限制，如果线程数过多就会造成多个线程对于总线的争用，这样就会导致性能下降。 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:2:3","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"怎么用多线程 默认的多线程使用方式就是在需要多线程执行任务的时候创建线程，在任务执行完毕之后销毁线程（就是直接交给 OS 来进行线程的创建和销毁）。 而实际上，我们常用的是线程池的方式，也就是在任务开始的时候我们就创建多个线程并且保存在一个线程池中，通过任务队列的形式确定将那个任务分配给哪个线程。这样做的方式其实就是把对线程的控制权从 OS 转移到程序员，避免了重复的线程创建和销毁带来的开销。 下面是使用 C++ 实现的一个简单的线程池： #include \u003ccondition_variable\u003e #include \u003cfunctional\u003e #include \u003ciostream\u003e #include \u003cmutex\u003e #include \u003cqueue\u003e #include \u003cthread\u003e #include \u003cvector\u003e class Task { public: Task(std::function\u003cvoid()\u003e f) : func(f) {} void operator()() const { func(); } private: std::function\u003cvoid()\u003e func; }; class ThreadPool { public: ThreadPool(size_t numThreads) : stop(false) { for (size_t i = 0; i \u003c numThreads; ++i) { workers.emplace_back([this] { for (;;) { std::function\u003cvoid()\u003e task; { std::unique_lock\u003cstd::mutex\u003e lock(queueMutex); condition.wait(lock, [this] { return stop || !tasks.empty(); }); if (stop \u0026\u0026 tasks.empty()) return; task = std::move(tasks.front()); tasks.pop(); } task(); } }); } } template \u003cclass F\u003e void enqueue(F \u0026\u0026f) { { std::unique_lock\u003cstd::mutex\u003e lock(queueMutex); tasks.emplace(std::forward\u003cF\u003e(f)); } condition.notify_one(); } ~ThreadPool() { { std::unique_lock\u003cstd::mutex\u003e lock(queueMutex); stop = true; } condition.notify_all(); for (std::thread \u0026worker : workers) { worker.join(); } } private: std::vector\u003cstd::thread\u003e workers; std::queue\u003cTask\u003e tasks; std::mutex queueMutex; std::condition_variable condition; bool stop; }; // 示例任务函数 void printHello(int num) { std::cout \u003c\u003c \"Hello from thread \" \u003c\u003c num \u003c\u003c std::endl; } int main() { ThreadPool pool(4); // 创建线程池，包含4个线程 // 向线程池添加任务 for (int i = 0; i \u003c 8; ++i) { pool.enqueue([i] { printHello(i); }); } return 0; } ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:2:4","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"协程 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:3:0","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"是什么 协程这一概念可以理解为“函数plus”，普通的函数只有两种行为：调用(Invoke)和返回(Return)。协程比函数多了一种行为：挂起(Suspend/Yield)。 在只使用函数的情况下，程序的执行流可以只用一个栈就能模拟（调用函数时 push ，函数返回时 pop ），而引入协程之后，因为其具有挂起这一行为，所以需要额外的空间（比如堆）来暂存协程的上下文。 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:3:1","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"有什么用 协程的作用可以用一句话来描述，即：协程就是用单线程的方式完成并发的任务逻辑。 协程其实与进程和线程没有太近的“亲缘关系”，只是在作用上有着相近的效果，即宏观上看是并发执行的。以经典的生产者-消费者任务为例： 多进程/多线程：至少需要一个生产者进程/线程，消费者进程/线程，两者之间可能需要使用一个单向管道作为数据缓冲区来提高性能（或者说控制管道大小来实现不同的任务逻辑）。 协程：只需要一个线程，通过两个分别负责生产和消费的协程来完成，即：生产者协程生产一定数量之后挂起，并调用消费者协程。消费者协程消费完之后挂起，并调用生产者协程，如此交替往复进行。 协程相比于上面提到的进程和线程，区别在于协程是运行在用户态的，或者说协程的控制权是掌握在程序员手中的，由程序员负责控制在什么时候把 CPU 的使用权交给哪个协程。 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:3:2","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"什么时候用协程 协程的使用情形其实很简单，即：你需要在只利用一个 CPU 核心的情况下完成并发任务的时候就是使用协程的时候。所以其实协程这个概念诞生的很早（1960年 Melvin Conway 解决COBOL 编译器的问题，使用协程技术只需要遍历一遍源代码） ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:3:3","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"怎么用 目前 C++ 的协程只是在 C++20 中提供了机制，标准库的实现可能会在下一个版本 C++23 中提供。 Go 中的 Goroutine 其实并不受程序员调度，其挂起行为由 Go runtime 调度。 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:3:4","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"总结 从进程到线程再到协程的概念，其使用层级是逐级向上的。 如果希望程序可以充分利用多核资源来实现 CPU 密集型操作的并行加速，那可以使用多线程，通过使用锁/条件变量等方式来完成线程之间的协作。 如果不满 OS 的任务/线程调度策略，那可以在程序中使用并调度协程，用单线程+异步的逻辑来完成宏观上的并发操作。 ","date":"2024-04-06","objectID":"/posts/knowledge/os/prcess-thread-coroutine/:4:0","tags":["os"],"title":"进程、线程和协程","uri":"/posts/knowledge/os/prcess-thread-coroutine/"},{"categories":["knowledge"],"content":"是什么 RPC 全名即 Remote Procedure Call：远程过程调用，本质上是一种设计/概念，它允许在一台机器上的 Client 调用运行在另一台机器上的 Server 上的程序接口。 ","date":"2024-03-29","objectID":"/posts/knowledge/backend/what-is-rpc/:1:0","tags":["backend"],"title":"什么是 RPC ？","uri":"/posts/knowledge/backend/what-is-rpc/"},{"categories":["knowledge"],"content":"为什么 RPC 的出现主要是为了满足现实世界中多机集群的业务分离， Client 端的业务和 Server 端的业务相互分离，目的是更强的性能、可扩展性和可维护性。 RPC 在我看来就是传统的前后端 http RESTful 框架的更加 general 的版本，两者在思想上是一致的，只不过 RESTful 框架是把现实业务中的 前端的显示 和 后端的数据处理 进行分离，而 RPC 则是更为通用的一种考虑，只要项目的设计者认为某个功能使用 RPC 进行分离会带来如性能、可靠性、可维护性等非功能特性上的收益，那其实就可以引入 RPC 。RPC 能完成的功能性需求不使用 RPC 一般来说也能实现，RPC 的收益主要体现在非功能性需求上。 ","date":"2024-03-29","objectID":"/posts/knowledge/backend/what-is-rpc/:2:0","tags":["backend"],"title":"什么是 RPC ？","uri":"/posts/knowledge/backend/what-is-rpc/"},{"categories":["knowledge"],"content":"怎么做 RPC 的核心是面向接口编程的思想，Server 端和 Client 端可以通过定义好语言无关的接口（函数签名），双方的过程调用就可以像调用同一文件中的不同函数一样进行。 既然涉及到了不同主机，那么不可避免地会引入网络通信，而网络通信的本质其实就是需要规定好：消息如何编解码（或者说如何序列化和反序列化）、消息如何通过网络传输。 因而 RPC 在实现上主要需要考虑两部分，第一个部分是通信协议，第二部分是编码协议， 通信协议：HTTP/TCP/UDP 编码协议：xml/json/protobuf 目前主流的 RPC 框架在编码协议上基本上都使用 protobuf ，因为 protobuf 作为一种二进制数据可以带来比 xml/json 这种文本数据更高的压缩效率（当然，更加重要的前提条件是 RPC 传输的消息其实不太需要跟人打交道，也就无需可读性）。 对于通信协议，不同的 RPC 框架可能根据自己的用途有着不同的选择，比如 gRPC 使用的是 HTTP/2，而 tRPC 则根据不同的传输形式（unary和stream）设计了不同的自定义的协议格式。 ","date":"2024-03-29","objectID":"/posts/knowledge/backend/what-is-rpc/:3:0","tags":["backend"],"title":"什么是 RPC ？","uri":"/posts/knowledge/backend/what-is-rpc/"},{"categories":["knowledge"],"content":"实际操作 以 tRPC 的 helloworld 为例，首先需要做的是写 IDL(Interface Defined Language) ，也就是前面提到的 proto 文件： service Greeter { rpc Hello (HelloRequest) returns (HelloReply) {} } message HelloRequest { string msg = 1; } message HelloReply { string msg = 1; } 这里首先定义了一个名为 Greeter 的服务，服务包含一个名为 Hello 的函数，其入参为 HelloRequest ，出参为 HelloReply。 后面则定义了 HelloRequest 和 HelloReply 这两种消息的格式，需要注意的是这里的 string msg = 1 并不是将 msg 初始化为 1 的意思（当然，string 类型的数据就算要初始化也应该是 \"1\" 而不是 1），而是说 msg 这个成员是 HelloReply 和 HelloRequest 的第 1 个成员。 之后可以使用 tRPC 的命令行工具或者是 Makefile 来生成相应的桩代码，供实现实现逻辑的客户端和服务端代码进行调用。 这里体现的其实就是前面所说的，RPC 只是一种对原本功能的一种分离，通过 IDL 确定好 C-S 之间的接口之后，C-S 都只需要调用协商好的接口，而原来的业务逻辑该怎么实现就怎么实现。 ","date":"2024-03-29","objectID":"/posts/knowledge/backend/what-is-rpc/:4:0","tags":["backend"],"title":"什么是 RPC ？","uri":"/posts/knowledge/backend/what-is-rpc/"},{"categories":["development"],"content":"TaskQueue TaskQueue也即任务队列，不过这个类本身并没有与队列相关的任何代码，所以它是用来干什么的呢？ 我们直接来读代码（为了方便，我这里直接把方法的实现代码贴了出来）： class RTC_LOCKABLE RTC_EXPORT TaskQueue { public: // TaskQueue priority levels. On some platforms these will map to thread // priorities, on others such as Mac and iOS, GCD queue priorities. using Priority = ::webrtc::TaskQueueFactory::Priority; explicit TaskQueue(std::unique_ptr\u003cwebrtc::TaskQueueBase, webrtc::TaskQueueDeleter\u003e task_queue) : impl_(task_queue.release()) {} ~TaskQueue() { impl_-\u003eDelete(); } // Used for DCHECKing the current queue. bool IsCurrent() const { impl_-\u003eIsCurrent(); } // Returns non-owning pointer to the task queue implementation. webrtc::TaskQueueBase* Get() { return impl_; } // TODO(tommi): For better debuggability, implement RTC_FROM_HERE. // Ownership of the task is passed to PostTask. void PostTask(std::unique_ptr\u003cwebrtc::QueuedTask\u003e task) { return impl_-\u003ePostTask(std::move(task)); } // Schedules a task to execute a specified number of milliseconds from when // the call is made. The precision should be considered as \"best effort\" // and in some cases, such as on Windows when all high precision timers have // been used up, can be off by as much as 15 millseconds (although 8 would be // more likely). This can be mitigated by limiting the use of delayed tasks. void PostDelayedTask(std::unique_ptr\u003cwebrtc::QueuedTask\u003e task, uint32_t milliseconds) { return impl_-\u003ePostDelayedTask(std::move(task), milliseconds); } // std::enable_if is used here to make sure that calls to PostTask() with // std::unique_ptr\u003cSomeClassDerivedFromQueuedTask\u003e would not end up being // caught by this template. template \u003cclass Closure, typename std::enable_if\u003c!std::is_convertible\u003c Closure, std::unique_ptr\u003cwebrtc::QueuedTask\u003e\u003e::value\u003e::type* = nullptr\u003e void PostTask(Closure\u0026\u0026 closure) { PostTask(webrtc::ToQueuedTask(std::forward\u003cClosure\u003e(closure))); } // See documentation above for performance expectations. template \u003cclass Closure, typename std::enable_if\u003c!std::is_convertible\u003c Closure, std::unique_ptr\u003cwebrtc::QueuedTask\u003e\u003e::value\u003e::type* = nullptr\u003e void PostDelayedTask(Closure\u0026\u0026 closure, uint32_t milliseconds) { PostDelayedTask(webrtc::ToQueuedTask(std::forward\u003cClosure\u003e(closure)), milliseconds); } private: webrtc::TaskQueueBase* const impl_; RTC_DISALLOW_COPY_AND_ASSIGN(TaskQueue); }; private部分： TaskQueue只有一个私有变量，也就是使用TaskQueueBase的裸指针指向的常量impl_，并且TaskQueue禁用了拷贝构造函数和赋值运算符。 这里为什么存放的是裸指针呢，我猜主要是出于性能的考虑。 public部分： TaskQueue的构造函数接受 1 个参数，也就是使用unique_ptr管理生命周期的对象，这个对象需要是实现了TaskQueueBase这一接口的对象。 TaskQueue有 2 个重要的方法，也就是PostTask和PostDelayedTask。 PostTask：将task加入到任务队列中进行即时处理； PostDelayedTask：将task加入到任务队列中，过milliseconds毫秒处理。 这 2 种方法都有 2 种重载形式。在 webrtc 的代码中，经常使用的是接受一个闭包也就是 lambda 表达式作为参数的这一重载形式。 比如在call/rtp_transport_controller_send.cc中： void RtpTransportControllerSend::OnSentPacket( const rtc::SentPacket\u0026 sent_packet) { task_queue_.PostTask([this, sent_packet]() { RTC_DCHECK_RUN_ON(\u0026task_queue_); absl::optional\u003cSentPacket\u003e packet_msg = transport_feedback_adapter_.ProcessSentPacket(sent_packet); pacer()-\u003eUpdateOutstandingData( transport_feedback_adapter_.GetOutstandingData()); if (packet_msg \u0026\u0026 controller_) PostUpdates(controller_-\u003eOnSentPacket(*packet_msg)); }); } void RtpTransportControllerSend::OnReceivedPacket( const ReceivedPacket\u0026 packet_msg) { task_queue_.PostTask([this, packet_msg]() { RTC_DCHECK_RUN_ON(\u0026task_queue_); if (controller_) PostUpdates(controller_-\u003eOnReceivedPacket(packet_msg)); }); } TaskQueue的创建基本上都是采用工厂模式完成。 从TaskQueue的实现代码中可以发现，它其实只是为实现了TaskQueueBase这一接口类的子类封装了一个统一的调用接口，实际上起到的是代理的作用。 真正做事的或者说真正核心的代码应该是TaskQueueBase这个接口类的定义以及实现其纯虚函数的子类。 ","date":"2024-03-19","objectID":"/posts/development/webrtc-task-queue/:1:0","tags":["WebRTC"],"title":"WebRTC任务队列学习笔记","uri":"/posts/development/webrtc-task-queue/"},{"categories":["development"],"content":"TaskQueueBase TaskQueueBase是 WebRTC 中用来实现异步执行任务的类，保证队列中的任务按照 FIFO 的顺序执行，不同任务的执行时间不会重叠。 不过，同一个任务队列中的不同任务并不一定总是在相同的 worker 线程上执行。 class RTC_LOCKABLE RTC_EXPORT TaskQueueBase { public: // Starts destruction of the task queue. // On return ensures no task are running and no new tasks are able to start // on the task queue. // Responsible for deallocation. Deallocation may happen syncrhoniously during // Delete or asynchronously after Delete returns. // Code not running on the TaskQueue should not make any assumption when // TaskQueue is deallocated and thus should not call any methods after Delete. // Code running on the TaskQueue should not call Delete, but can assume // TaskQueue still exists and may call other methods, e.g. PostTask. virtual void Delete() = 0; // Schedules a task to execute. Tasks are executed in FIFO order. // If |task-\u003eRun()| returns true, task is deleted on the task queue // before next QueuedTask starts executing. // When a TaskQueue is deleted, pending tasks will not be executed but they // will be deleted. The deletion of tasks may happen synchronously on the // TaskQueue or it may happen asynchronously after TaskQueue is deleted. // This may vary from one implementation to the next so assumptions about // lifetimes of pending tasks should not be made. virtual void PostTask(std::unique_ptr\u003cQueuedTask\u003e task) = 0; // Schedules a task to execute a specified number of milliseconds from when // the call is made. The precision should be considered as \"best effort\" // and in some cases, such as on Windows when all high precision timers have // been used up, can be off by as much as 15 millseconds. virtual void PostDelayedTask(std::unique_ptr\u003cQueuedTask\u003e task, uint32_t milliseconds) = 0; // Returns the task queue that is running the current thread. // Returns nullptr if this thread is not associated with any task queue. static TaskQueueBase* Current() { return current; } bool IsCurrent() const { return Current() == this; } protected: class CurrentTaskQueueSetter { public: ABSL_CONST_INIT thread_local TaskQueueBase* current = nullptr; explicit CurrentTaskQueueSetter(TaskQueueBase* task_queue) : previous_(current) { current = task_queue; } ~CurrentTaskQueueSetter() { current = previous_; } CurrentTaskQueueSetter(const CurrentTaskQueueSetter\u0026) = delete; CurrentTaskQueueSetter\u0026 operator=(const CurrentTaskQueueSetter\u0026) = delete; private: TaskQueueBase* const previous_; }; // Users of the TaskQueue should call Delete instead of directly deleting // this object. virtual ~TaskQueueBase() = default; }; 从代码中的注释可以看明白PostTask这一方法的作用，也就是我们上面所说的：把task加入到事件队列中，按照 FIFO 的顺序进行处理。 需要注意的是，这里还有一个可访问性为protected的类：CurrentTaskQueueSetter，这个类的作用就像它的命名一样，用于设置当前的任务队列，也就是把任务队列绑定到当前线程上。 构造时，用传入构造函数的任务队列更新当前线程存放的任务队列，并将更新前的任务队列暂存到当前线程的 TLS(Thread Local Storage)中。 析构时，用构造时暂存的任务队列更新当前线程存放的任务队列。 WebRTC 中有好几个实现了TaskQueueBase这一接口的类如TaskQueueStdlib, TaskQueueLibevent, TaskQueueWin, SimulatedTaskQueue等，它们的作用也各不相同。 下面我们以TaskQueueStdlib为例，对实际的PostTask等函数是如何运行的一探究竟。 ","date":"2024-03-19","objectID":"/posts/development/webrtc-task-queue/:2:0","tags":["WebRTC"],"title":"WebRTC任务队列学习笔记","uri":"/posts/development/webrtc-task-queue/"},{"categories":["development"],"content":"TaskQueueStdlib ","date":"2024-03-19","objectID":"/posts/development/webrtc-task-queue/:3:0","tags":["WebRTC"],"title":"WebRTC任务队列学习笔记","uri":"/posts/development/webrtc-task-queue/"},{"categories":["knowledge"],"content":"什么是虚拟地址空间？ 虚拟地址空间就是每个程序在运行起来之后所独占的内存空间，也就是进程自己的地址空间。 虚拟地址空间的大小由地址总线的宽度也就是计算机的字长决定： 对于 32 位系统，进程的虚拟地址空间大小为： $$ 2^{32} bit = 4^{30} Byte = 4 GiB $$ 对于 64 位系统，进程的虚拟地址空间大小为： $$ 2^{64}bit = 16^{30} GiB = 16 ^{20} TiB = 16^{10} PiB= 16 EiB $$ 不过理论是理论，实际是实际。 对于 32 位的linux系统而言，操作系统占用了空间中上面的 1GiB（从0xC0000000到0xFFFFFFFF），程序可以使用的虚拟空间原则上只有 3GiB（从0x00000000到0xBFFFFFFF），对于 64 位的 OS 跟进程各自占用 128T 的空间，分别在最高处和最低处。 对于 32 位的windows系统而言，操作系统 2GiB，程序 2GiB（不过windows系统可以设置启动参数来将 OS 占用的虚拟地址空间大小缩小到 1GiB）. 进程的虚拟地址空间用于存放进程运行所必不可少的数据，内存地址从低到高生长，各个区域分别为： 代码段(.text)：程序代码段 数据段(.data)：已初始化的静态常量、全局变量 BSS 段(.bss)：未初始化的静态变量、全局变量 堆：动态分配的内存，从低地址开始向上增长； 文件映射段：动态库、共享内存等，从高地址开始向下增长； 栈：局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB，从高地址开始向下增长。 ","date":"2024-02-07","objectID":"/posts/knowledge/os/virtual-memory-space/:1:0","tags":["OS"],"title":"虚拟地址空间","uri":"/posts/knowledge/os/virtual-memory-space/"},{"categories":["knowledge"],"content":"为什么需要虚拟地址空间？ 虚拟地址空间其实是一种应对多进程环境下的策略，这种对程序员透明的抽象方式可以使每个进程都无法感知到其他进程的存在，让各个进程之间的内存空间相互隔离，程序员也无需关心进程运行的物理地址的事情，极大地降低了程序员的心智负担。 ","date":"2024-02-07","objectID":"/posts/knowledge/os/virtual-memory-space/:2:0","tags":["OS"],"title":"虚拟地址空间","uri":"/posts/knowledge/os/virtual-memory-space/"},{"categories":["knowledge"],"content":"32 位的机器，程序使用的空间大小能超过 4GiB 吗？ 如果指的是虚拟地址空间，那么答案是“否”。因为 32 位的 CPU 只能使用 32 位的指针，最大的寻址范围就到 4GiB。 如果指的是计算机的内存空间，答案为“是”。Intel 从 95 年推出的 Pentium Pro CPU 开始采用 36 位的物理地址，可以访问达 64GiB 的物理内存。同时，Intel 修改了页映射的方式，使得新的映射方式Physical Address Extension, PAE可以访问到更多的物理内存。 在windows下，进程可以拿一段连续的内存地址作为窗口，然后从高于 4GiB 的物理空间中申请多个大小等于窗口大小的物理空间并进行编号 A, B, C 等，用到哪部分就把窗口映射到哪部分。这一操作也叫做AWE(Address Windowing Extensions)。 在linux下则使用mmap系统调用来实现。mmap系统调用的主要作用是使进程之间通过映射同一个普通文件来实现共享内存(IPC)。普通文件被映射到地址空间之后，进程可以像访问普通内存一样对文件进行访问，而不需要调用write, read函数。mmap本质上并不分配空间，只是将文件映射到进程地址空间（当然，会占掉虚拟内存空间），映射成功后就可以直接用memcpy等操作来写文件，因而用户对这段内存区域的修改就可以直接反映到内核空间（当然反过来也一样）。 void * mmap(void *start, size_t length, int prot , int flags, int fd, off_t offset) 对映射空间所写的内容并不会立刻更新到文件中，而是有一段时间的延迟，内核会挑个时间进行写入操作。如果需要即使写入可以调用msync来强制同步。 ","date":"2024-02-07","objectID":"/posts/knowledge/os/virtual-memory-space/:3:0","tags":["OS"],"title":"虚拟地址空间","uri":"/posts/knowledge/os/virtual-memory-space/"},{"categories":["knowledge"],"content":"ABI 是什么？ ABI: Application Binary Interface（应用二进制接口）。 其实就是针对 编译器 和 链接器 的二进制级别的一些规范和约束，主要规范的内容有： 规定函数的调用顺序，也称为“调用约定”，规定了如何将“函数”转换成汇编代码。 规定库函数如何表示，主要对链接过程有指导作用。 规定可以使用什么类型的数据，这些数据如何对齐以及其他低级细节。 ABI 还涉及到 OS 的内容，包括可执行文件的格式、虚拟地址空间布局等细节。 ","date":"2024-02-07","objectID":"/posts/knowledge/cpp/abi/:1:0","tags":["ABI","C++"],"title":"ABI是什么？","uri":"/posts/knowledge/cpp/abi/"},{"categories":["knowledge"],"content":"为什么会有 ABI ？ 原因其实很简单，硬件架构、OS、编译工具链以及编程语言的发展和逐层抽象让大部分程序员可以不太在意底层程序的执行过程，而只需要负责编写表明业务逻辑的源代码。大部分程序员不需要在意并不意味着这部分不存在，实际上，这部分内容是通向二进制文件执行的必经之路。 通过上面的分析可以知道， ABI 这个概念基本上是由(硬件架构, OS, 编译工具链, 编程语言)这个四元组决定的。 架构兼容性：amd64架构和arm64架构对应的指令集不同，因而一个可执行文件要想在这两个架构上成功运行，就需要编译这两个架构的二进制文件（也就是交叉编译）。 OS 兼容性：windows(PE-COFF), linux(ELF)和macos(MACH-O)上规定的程序二进制文件格式不同，因而也需要为不同的 OS 编译不同的二进制文件。 编译工具链兼容性：这个我们平时遇到的比较多，常见原因是不同的编译器或不同的编译器版本的名字修饰规则不同，导致链接器在链接时找不到对应名字的库函数。 编程语言兼容性：C 语言中的一些基本内容如不同类型数据在内存中存放的形式，寄存器的使用形式等，以及 C++的众多特性：虚函数如何调用、虚表的内容和分布形式、template 如何实例化等等，都是 ABI 所需要规定的内容。 ","date":"2024-02-07","objectID":"/posts/knowledge/cpp/abi/:2:0","tags":["ABI","C++"],"title":"ABI是什么？","uri":"/posts/knowledge/cpp/abi/"},{"categories":["knowledge"],"content":"ABI-Compatible ? ABI-compatible 允许编译好的目标代码可以无需修改或重新编译链接就能直接运行，而从上面举的例子就可以发现，ABI 兼容是一件很难做到的事情，光是架构和 OS 的不同就需要不同的目标文件了。 而编译工具链的兼容性容易做到吗？其实也不容易。目前主流的 C++编译工具链有gcc, llvm(clang)和msvc，这三者之间对于名字修饰的规定都不同，因而一个用clang编译的库函数是无法被一个用msvc编译的main文件调用的。当然，这里指的是默认进行名字修饰的情况，如果使用extern \"C\"对函数进行修饰，从而要求编译器使用 C 语言的编译和链接规范进行处理就可以解决这个问题。 C++一直被诟病的原因之一就是二进制兼容性不好，对于小型项目而言使用同一种编译器进行编译可能可行，但是对于大型项目而言不太现实，库代码的提供者通常只是提供编译链接好的库，并不提供源代码，所以要想做到对于所有的编译器（的所有版本）都进行支持是一件困难且不太现实的事情。 ","date":"2024-02-07","objectID":"/posts/knowledge/cpp/abi/:3:0","tags":["ABI","C++"],"title":"ABI是什么？","uri":"/posts/knowledge/cpp/abi/"},{"categories":["development"],"content":"问题背景 前两天室友问我，怎么 kill 掉在 Shell 脚本中调用的 Python 进程，我第一时间想到的是：打开 htop，把它调整成树形布局，然后搜索 Shell 脚本，选中之后把它 kill 掉，Python 进程应该也会被 kill 掉。 但是结果是 Python 进程并没有变红，而是成为了 init 进程的子进程。 ","date":"2024-01-29","objectID":"/posts/development/orphan-process/:1:0","tags":["OS","Shell","Python"],"title":"孤儿进程","uri":"/posts/development/orphan-process/"},{"categories":["development"],"content":"孤儿进程是怎么产生的 大二学 OS 学到父进程和子进程的概念的时候，还是只是以为父进程和子进程之间应该存在牢固的控制关系，父进程退出时子进程也应该默认退出。 但是 OS 的实际行为不是这样，子进程和父进程只是说明了二者之间存在谁创建谁的关系，并不存在牢固的控制关系（而是类似于现实中的父子关系）。 父进程结束时子进程并没有结束，子进程成为孤儿进程，会被 init 进程收养 父进程崩溃或异常终止 并发和竞争条件导致父子进程的结束顺序错误 ","date":"2024-01-29","objectID":"/posts/development/orphan-process/:2:0","tags":["OS","Shell","Python"],"title":"孤儿进程","uri":"/posts/development/orphan-process/"},{"categories":["development"],"content":"如何避免孤儿进程的产生 其实就是需要在程序设计时，考虑到上述的这几种可能导致孤儿进程产生的原因，然后对异常情况进行注册和处理。对于开始时的这个引入问题而言，答案可以写成以下两个脚本： #!/bin/bash # 定义一个函数来处理信号 cleanup() { echo \"捕捉到终止信号，正在终止 Python 进程...\" kill $PYTHON_PID exit } # 在接收到 SIGINT || SIGTERM || SIGKILL 时执行 cleanup 函数 trap 'cleanup' SIGINT SIGTERM # 启动 Python 脚本并获取其进程 ID python example_python.py \u0026 PYTHON_PID=$! # 等待 Python 进程结束 wait $PYTHON_PID import time import signal import sys # 定义信号处理函数 def signal_handler(signum, frame): print(\"Python 脚本接收到终止信号，正在退出...\") sys.exit() # 设置 SIGINT SIGTERM 的处理器 signal.signal(signal.SIGINT, signal_handler) signal.signal(signal.SIGTERM, signal_handler) # Python 脚本的主逻辑 try: while True: print(\"Python 脚本正在运行...\") time.sleep(1) except KeyboardInterrupt: pass 通过在父进程和子进程中都注册相应的事件，就可以保证 kill 作为父进程的 Shell 进程之后，作为子进程的 Python 进程也会终止。 实际演示：chmod +x example.sh example_python.py \u0026\u0026 bash example.sh 执行 SIGTERM 信号的 kill 之后，父子进程都被终止。 需要注意的是，如果使用 kill -9 $PARENT_PID 的形式来杀死父进程的话，子进程并不会被杀死。 因为 9 这个编号对应的是 SIGKILL 信号，SIGKILL 信号被设计为不能被捕捉、阻塞或忽略的。SIGKILL 的主要用途是允许操作系统或用户强制终止一个进程，即使该进程处于非响应状态。（类似的还有 SIGSTOP 信号，用于暂停一个进程的执行，也不能被捕捉、阻塞或忽略。） 所以我们也无法在 Python 脚本中注册监听这个信号（强行注册 Python 脚本会无法运行）。 ","date":"2024-01-29","objectID":"/posts/development/orphan-process/:3:0","tags":["OS","Shell","Python"],"title":"孤儿进程","uri":"/posts/development/orphan-process/"},{"categories":["development"],"content":"这篇博客用来记录平时用到的一些 Git 操作，用到之后会不定时更新。 ","date":"2024-01-23","objectID":"/posts/development/git-usage/:0:0","tags":["git"],"title":"Git 常用用法记录","uri":"/posts/development/git-usage/"},{"categories":["development"],"content":"clone 相关 克隆指定 branch ： git clone --branch \u003cbranch-name\u003e \u003cremote-repo-url\u003e 递归克隆（包括 submodule ）：git clone --recursive 已经 clone 完的仓库：git submodule update --init --recursive ","date":"2024-01-23","objectID":"/posts/development/git-usage/:1:0","tags":["git"],"title":"Git 常用用法记录","uri":"/posts/development/git-usage/"},{"categories":["development"],"content":"checkout 相关 切换分支：git checkout \u003cbranch-name\u003e / git switch \u003cbranch-name\u003e 新建分支：git checkout -b \u003cbranch-name\u003e / git switch -c \u003cbranch-name\u003e 切换到一个 tag ：git fetch --all --tags --prune -\u003e git tag -\u003e 使用 / 快速搜索 -\u003e git checkout tags/\u003ctag-name\u003e -b \u003cbranch-name\u003e ","date":"2024-01-23","objectID":"/posts/development/git-usage/:2:0","tags":["git"],"title":"Git 常用用法记录","uri":"/posts/development/git-usage/"},{"categories":["development"],"content":"commit 相关 undo 本地改动（还未 commit）：git restore \u003cfile-path\u003e 修改 commit 消息（还未 push）：git commit --amend undo 前 1 次 commit（还未 push）：git reset --soft HEAD~ undo 前 2 次 commit（还未 push）：git reset --soft HEAD~2 undo 某次 commit（已经 push）：git revert \u003ccommit-hash\u003e undo 某个区间内的 commit（已经 push）： git revert --no-commit \u003cleft-commit-hash\u003e..\u003cright-commit-hash\u003e （左开右闭） -\u003e git commit ","date":"2024-01-23","objectID":"/posts/development/git-usage/:3:0","tags":["git"],"title":"Git 常用用法记录","uri":"/posts/development/git-usage/"},{"categories":["development"],"content":"协作相关 Review 并且 Commit 别人提出的 PR 的流程： git remote add \u003cremote-name\u003e \u003cremote-repo-url\u003e git remote -v git fetch \u003cremote-name\u003e git checkout -b \u003clocal-branch-name\u003e \u003cPR-branch-name\u003e git commit -sm \"\u003ccommit-message\u003e\" git push \u003cremote-name\u003e HEAD:\u003cPR-branch-name\u003e ","date":"2024-01-23","objectID":"/posts/development/git-usage/:4:0","tags":["git"],"title":"Git 常用用法记录","uri":"/posts/development/git-usage/"},{"categories":["knowledge"],"content":"编码框架 编码器包含两个方向的码流分支： 从左到右的前向码流分支为编码过程； 从右到左的反向码流分支为重建过程。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:1:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"前向编码分支 以 16x16 像素的 MB 为单位进行处理，首先从当前输入的视频图像(Frame or Field)中取一个待编码宏块$F_n$，该宏块以帧内或者帧间的模式进行编码，生成一个预测宏块$P$。 如果是帧内编码，$P$由当前 Slice 里面已经编码、解码、重构并且还没进行去块滤波的宏块 $μF_n’$ 使用帧内预测得到。当前宏块 $μF_n’$ 减去预测宏块 $P$，得到残差块$D_n$，对残差块 $D_n$ 进行整数变换（一般是 4x4，或者 8x8）、量化后得到一组系数 $X$ ，再对 $X$ 进行重排序和熵编码，就完成了一个宏块的编码过程。对于 P 帧和 B 帧，如果 ME 时候找不到最佳匹配块那也会使用帧内预测编码。 经过熵编码的码流加上宏块解码所需的一些信息，如预测模式、量化步长、描述宏块运动预测补偿的运动矢量信息等，就组成了该宏块压缩后的码流，Slice 中所有 MB 的码流加上 Slice 头信息就组成了 Slice 的编码码流，再通过 NAL 层进行传输或存储。图像参数集 PPS 和序列参数集 SPS 则由 NAL 单独进行传输。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:1:1","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"后向重建分支 在后向重建分支中，对量化后的宏块系数 $X$ 进行解码从而得到重建宏块，后续宏块进行编码需要从已重建的宏块中寻找参考块。宏块重建过程如下： 宏块系数 $X$ 经过反量化和反变换之后，得到残差宏块 $D_n$ 的近似值 $D_n’$ ，预测块 $P$ 加上 $D_n’$ 得到未滤波的重构宏块 $μF_n’$ ，再做环路滤波来减少块效应，即得到了最终的重构宏块 $F_n’$ ，当图像中所有宏块都重建完成后，就形成了重建图像。 后向重建分支其实就是包含在编码中的完整解码流程，与真正解码器的唯一区别是： 其预测块 P 直接从前向编码分支中得到，而真正的解码器需要利用码流中解出的预测块信息获得预测块 P。当前图像的已重建宏块会被用做帧内预测的参考，而完整的重建图像会被加入参考帧列表，作为未来编码图像帧预测的参考图像。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:1:2","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"预测编码 预测编码（Prediction Coding）利用相邻像素之间的空间和时间相关性，用已传输的像素对当前正在编码的像素进行预测，然后对预测值和真实值的差值——预测误差进行编码和传输。 消除空间冗余：利用一帧图像已经编码的部分来预测还没有被编码的部分 消除时间冗余：利用之前编码过的图像来预测当前图像需要编码的部分 通过预测可以得到预测值，预测值通常不等于实际值，所以用实际值减去预测值可以得到预测残差。预测方法越好，残差越小，因而对残差进行编码得到的码流要比对实际值直接进行编码的码流要小 在解码端可以解码出残差，使用与编码端相同的预测过程来获取预测值，加上残差便可以得到相应的实际值。 目前使用较广的预测方法是线性预测。用已传像素的线性组合对正在编码的像素进行预测。 编码器输出预测值和实际值之间的差值，选择最优的预测系数，使预测误差的分布在 0 附近。经过非均匀量化之后，最终传输的是量化之后的预测误差。 重建的图像与编码前的图像之间的区别是量化带来的。 如果取消量化操作，那么预测编码和解码就是一个无失真的编解码系统，但是这样的压缩率要远远低于使用量化的压缩率。 量化的事实依据是图像中存在人眼感知并不明显的区域，通过非均匀量化可以过滤掉这部分数据（也被称为量化噪声），能达到提高压缩率但不降低主观质量的效果。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:2:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"帧间预测 帧间预测消除的是时间冗余，主要利用的是运动估计和运动补偿。 运动估计 ME 的过程其实就是计算运动向量 MV 的过程： 寻找当前编码的块在已编码图像中的最佳对应快，并且计算出对应块的偏移量：运动向量 假设当前编码块是 $B$ ，在参考帧 $P_r$ 中寻找与 $B$ 块相减残差最小的块 $B_r$ ， $B_r$ 就是 $B$ 块的最佳匹配块。 $$ MV=B_r-B=(x_r - x,y_r - y) $$ $B_r$ 块就是 $B$ 块的参考块， $B_r$ 的像素值就作为 $B$ 块像素值的预测值。 运动向量 MV 也需要用合适的方式编码到码流中。 运动补偿 MC：根据 MV 和帧间预测方法，求出当前帧的估计值的过程。 当前帧的估计值是对当前图像的描述，用来说明当前图像的每一个像素怎么由它的参考图像的像素块得到。 ME 是动态的过程，MC 是静态的描述。 ME 设计很多算法和技巧。 MC 可以看作是索引表，一个描述像素块的最佳匹配块分布情况的索引表。 实际计算过程中会对树状结构分块的 8 种模式都尝试一遍。 H264 支持亮度 1/4 像素、色度 1/8 像素的运动估计，在亚像素 ME 之前需要先用插值法得到亚像素值。 在噪声大的视频中，提高搜索精度没法提高预测精确度。 在噪声小的视频中，1/4 像素精度可以达到比较好的预测效果。 视频会议里 1/2 像素精度基本可以满足需求。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:3:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"运动估计算法 假设匹配误差随着离全局误差最小点的距离增加而单调增加，从原点开始，采用固定的搜索模板和搜索策略得到最佳匹配块。 理论最优的算法是全搜索法，但是计算量巨大，效率低, 一般作为其他搜索算法的一种效率参考。 还有其他的快速法：X264 里面主要使用的是钻石搜索法 DIA、六边形搜索法 HEX 和 UMH。 钻石搜索法以搜索起点为中心，先用 LDSP 进行搜索，直到最佳匹配点位于大菱形的中心位置，然后再用小菱形搜索，直至最佳匹配点位于小菱形的中心位置。 六边形搜索法采用 1 个大模板（六边形模板）和 2 个小模板（小菱形模板和小正方形模板）。 步骤 1：以搜索起点为中心，采用图中左边的六边形模板进行搜索。计算区域中心及周围 6 个点处的匹配误差并比较，如最小 MBD 点位于模板中心点，则转至步骤 2；否则以上一次的 MBD 点作为中心点，以六边形模板为模板进行反复搜索。 步骤 2：以上一次的 MBD 点为中心点，采用小菱形模板搜索，计算各点的匹配误差，找到 MBD 点。然后以 MBD 点为中心点，采用小正方形模板搜索，得到的 MBD 点就是最优匹配点。 UMH 是基于 MV 具有时空相关性，所以可以结合上一帧和上一步中 MV 的方向和角度，来修改多层六边形的形状，UMH 算法包含四中搜索模式:不均匀交叉搜索、多六边形网格搜索、迭代六边形搜索、菱形搜索。 步骤 1：进行一次小菱形搜索，根据匹配误差值和两个门限值（对于一种尺寸的宏块来说是固定大小的 threshold1 和 threshold2）之间的关系作相应的处理，可能用到中菱形模板或者正八边形模板，也有可能直接跳到步骤 1。 步骤 2：使用非对称十字模板搜索。“非对称”的原因是一般水平方向运动要比垂直方向运动剧烈，所以将水平方向搜索范围定为 W，垂直方向搜索范围定为 W/2。 步骤 3：使用 5x5 逐步搜索模板搜索。 步骤 4：使用大六边形模板搜索。 步骤 5：使用六边形搜索算法找到最优匹配点。 码率不变的前提下，“Dia”、“HEX”、“UMH”编码获得的质量依次提高，速度依次降低。快速算法（“Dia”、“HEX”、“UMH”）的编码质量比全搜索算法低不了太多，但是速度却高了很多倍。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:3:1","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"多参考帧预测 编码端存储参考帧的缓冲区就是 DPB，主要有三种参考帧：短期参考帧、长期参考帧和非参考帧。 短期参考帧就是和当前帧相邻的帧，按照从近到远的顺序排序。 长期参考帧是较早之前的帧，按照从远到近的方式排列。 不使用的参考帧是因为某些原因废弃了的参考帧，并且没有被新的参考帧替换掉。比如遇到 IDR 帧的时候 DPB 里面的所有参考帧都会标记成非参考状态。所以 I 帧之后的 P 帧也可以参考这个 I 帧之前的图像。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:3:2","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"MV 预测和 Skip 模式 需要对 MV 进行压缩，方式是使用临近分块 MV 之前的相关性对当前块的 MV 进行预测，只对预测残差 MVD 进行编码。 Skip 模式：Skip 模式只针对宏块编码，也就是完全不用编码只需要在码流里面标明是 SKIP 宏块就行。P_Skip 宏块就是 COPY 宏块，既没有 MVD，也不编码量化残差，解码时候直接用 MVp 作为运动向量得到像素的预测值作为像素重建值。B_Skip 宏块也是既没有 MVD 也没有量化残差，解码时候通过 Direct 预测模式计算出前向和后向 MV，然后得到像素预测值作为重建值。 P_Skip：最佳模式是 Inter16x16、参考帧是 List0 里面的第一个参考帧、MVD=0、变换系数被量化成 0，或者在 RDO 模型中被抛弃。 B_Skip：最佳模式是 B_direct_16x16，变换系数要么全是 0 要么被算法抛弃。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:3:3","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"加权预测 用来应对明暗/亮度变化的场景，使用两种预测模式：显式模式(P 帧、B 帧)和隐式模式(B 帧)。 显式模式：加权系数由编码器决定并且在 Slice Header 里面传输。 隐式模式：加权系数由参考图像的时间位置推算，越接近当前图像，加权系数越大。 步骤： 亮度变化检测： 使用直方图计算前后两幅图像在各个灰度级别 SAD，然后采用阈值法判断有没有发生亮度变化。 计算加权系数： 第一种方法：计算参考帧和当前帧的亮度均值比值，利用比值作为加权系数（全局加权补偿的效果有限） 第二种方法：全局补偿，但是使用使前后图像 MSE 和偏移量最小的加权系数：使 MSE 的表达式（加权系数$W_1$和偏移量$W_2$）分别基于$W_1$和$W_2$的偏微分等于 0。 亮度补偿： 原则上需要做到宏块级别的亮度补偿，但是为了降低复杂度只做帧级亮度补偿。 根据求出的加权系数和偏移量进行全局亮度补偿得到新图像。 把新图像存到一个列表中作为带亮度补偿运动估计的参考帧。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:3:4","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"帧内预测 帧内预测实质是消除空间冗余，利用已编码的块的像素的来预测未编码的像素值。 未编码的块像素的实际值-预测值=残差，传输只需要传输残差。 H264 引入了基于空域的帧内预测技术，在空域中利用当前块的相邻像素直接对每个像素做预测，并对预测残差进行变换、量化。 H264 帧内预测中，色度和亮度信息是被分开预测的。 对于亮度待编码块，可以按照 4x4 块方式预测(I4MB)或 16x16 宏块方式预测(I16MB)。 4x4 预测时有 9 种模式（水平、垂直、DC、6 个方向），用于图像细节部分的预测。 16x16 预测时有 4 种模式（水平、垂直、DC、平面），用于图像平坦区域的预测。 对于色度待编码块，基于 8x8 块进行预测。 8x8 预测有 4 种模式（水平、垂直、DC、平面）。 亮度和色度的最佳帧内预测模式相互独立： 色度的只需要比较 4 种模式的代价，选择最小的。 亮度的需要：算出代价最小的 Intra4x4 模式、算出代价最小的 Intra16x16 模式、取两者最小的。 Intra4x4：用 RDO 模型也就是拉格朗日模型计算代价。 Intra16x16：用 SATD，变换使用哈达马变换（看作是简单的时频变换，可以反映生成码流的大小） ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:4:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"变换编码 变换编码（Transform Coding）将空间域描述的图像，经过某种变换形成变换域中的数据，达到改变数据分布，减少有效数据量的目的。 变换编码中主要使用方式是正交变换。正交变换不会改变信源的熵值，变换之后图像的信息量并没有损失，完全可以通过反变换得到原来的图像值。 正交变换可以改变数据的分布，将数据集中分布之后就可以使用进一步的量化操作来去除大部分的 0 值和接近 0 的值。 正交变换中的理论最优变换是 K-L 变换。 实际中常用的正交变换有 DCT 变换，DFT 变换(离散傅里叶变换)，Hadamard 变换。 因为 DCT 系数主要集中在低频区域，越是高频区域系数值越小，通过设置不同的视觉阈值的量化电平，将许多能量较小的高频系数量化为 0，可以增加变换系数中 0 的个数，同时保留能量较大的系数分量，对量化之后的系数进行熵编码可以获得进一步的压缩。 H264 使用的是整数变换，变换核只用加减法和左移操作实现，不需要乘法器。 各种变换的比较： 压缩比和重建质量： 较小分块时，DCT 的 MSE 接近 K-L 变换。 块大小超过 16x16 时，除傅里叶变换之外，其他几种变换的 MSE 下降很慢。 大方块尺寸时，傅里叶变换趋向于 K-L 变换。 计算复杂度： Hadamard 计算复杂度最小。 其次是 DFT 和 DCT，具有固定的核函数。 K-L 变换的核函数与输入相关，计算量很大，不实用。 变换编码中的失真还是由量化器引起，正反变换和变长编解码都是无损处理。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:5:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"DCT 变换 离散余弦变换与 DFT 变换相似，但是 DCT 变换只使用实数。 DCT 变换具有能量集中性，大多数的声音和图像信号的能量都集中在 DCT 变换之后的低频部分。 当信号具有接近马尔可夫过程的统计特性时，DCT 变换的去相关性接近于 K-L 变换。 图像处理领域中，DCT 变换的效果要强于 DFT 变换，因而图像处理中更多应用的是 DCT 变换。 DCT 产生的系数很容易被量化，因而可以获得较好的块压缩。 DCT 算法的性能好，有快速算法，采用快速傅里叶变换可以进行高效的运算。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:5:1","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"量化 量化的思想就是映射一个输入间隔到一个整数，减少信源编码的 bit 数。 量化器的设计就是率失真优化问题，在允许一定失真的条件下，获得尽可能高的压缩比。 量化步长决定量化器的编码压缩率和图像精度。 量化最简单的方法就是均匀（线性）量化，但均匀量化的效果往往并不好，因为它没有考虑到量化对象的概率分布。 对 DCT 系数这样的数据而言，其分布大部分集中在直流和低频附近，如果采用非均匀量化，对低频区域进行细量化，对高频区域进行粗量化，在相同的量化步长的条件下，非均匀量化比均匀量化所造成的量化误差要小得多。 量化之后，熵编码之前，可以根据从高到低的统计特性，对系数进行 Zigzag 锯齿扫描和游程长度编码。这样做的原因在于：量化之后的 DCT 系数更为稀疏，只有少数的 AC 系数不为 0，Zigzag 扫描能增加连 0 的长度，减少统计事件的个数，从而进一步增加对 DCT 系数熵编码的压缩率。 量化区间上的最优量化值应该是区间的期望值，所以需要知道残差变换系数的统计分布。 引入量化偏移量 f 来进行非均匀量化，在帧内预测时 f=Qstep/3，帧间预测时 f=Qstep/6. f 可以控制量化死区大小，f 变大，量化死区减少，f 变小，量化死区增加。死区大小直接影响图像的主观质量。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:6:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"去块滤波（不是很了解细节） 环路滤波器是被放置在编解码的图像重建环路当中。 在启用了环路滤波的编解码环境中，无论是编码器还是解码器，都是在图像被重建后才进行滤波。 在编码器中，滤波后的图像会作为后续编码运动补偿的参考图像； 在解码器中，滤波后的图像会被输出显示并且作为后续图像解码重建的参考图像。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:7:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"块效应出现的原因 基于块的量化会破坏相邻块之间的相关性，并且在低码率情况下会放大这种误差。 运动补偿加剧了由变换量化导致的块效应。因为运动补偿块的匹配不可能绝对准确，各个块的残差大小程度存在差异，尤其是当相邻两个块用的参考帧不同、运动矢量或参考块的差距过大时，块边界上产生的数据不连续就更加明显。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:7:1","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"过程 估算边界强度、区分真假边界、滤波运算 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:7:2","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"熵编码 主要利用信源的统计特性进行码率压缩，是无损压缩编码方法。 H264 支持 CAVLC（变长编码）和 CABAC（二进制算术编码）。 CAVLC 本质是哈夫曼编码，所以必须为所有可能的长度为 N 的序列设计和存储编码表，复杂度随 N 指数增长。 CABAC 的思想是用 0 到 1 区间上的一个数来表示整个字符输入流，而不是为输入流中的每个字符分别指定码字。 算术编码用区间递进的方法为输入流寻找码字，从第一个符号确定的初始区间开始，逐个读入输入流，在每个新的字符出现后递归地划分当前区间，划分的依据是各个字符的概率，将当前区间按照各个字符的概率划分成若干子区间，将当前字符对应的子 2 区间取出，作为处理下一个字符时的当前区间。到处理完最后一个字符后，得到了最终区间，在最终区间中任意挑选一个数作为输出。在解码时候也采用相同的方法和步骤，但是解码器每划分一个子区间就能得到输入流中的一个字符。 在实际过程中，输入流中字符的概率分布是动态改变的，这需要维护一个概率表去记录概率变化的信息。在作递进计算时，通过对概率表中的值估计当前字符的概率，当前字符处理后，需要重新刷新概率表。这个过程表现为对输入流字符的自适应。编码器和解码器按照同样的方法估计和刷新概率表，从而保证编码后的码流能够顺利解码。 一般来讲，只要计算量允许，就应该选择使用算术编码。 H264 对不同的数据采用不同的熵编码模式，对于宏块和子块的残差数据经过变换之后的系数采用 CAVLC，对于其他相对重要的语法元素使用指数哥伦布编码。在 CABAC 方案里，对不同的语法元素也使用了不同的编码树结构。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:8:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"GOP/Frame/Slice/I/B/P 帧 Frame 包含切片 Slice，Slice 包含宏块 Macroblock(MB)。 一个 Frame 至少包含一个 Slice，一个 Slice 至少包含一个 MB。 Slice 是 MB 的载体，出现的原因是为了防止误码的扩散和传播。 每个 Slice 都是互相独立被传输的，某个 Slice 不能以其他 Slice 中的 MB 为参考。 Slice 存储在 NAL 单元 NALU 中，是 NALU 的有效载荷 Payload。 Slice 中包含 Slice Header 和 Slice Payload。 Slice Header 中存放的是 Slice 类型、Slice 中的宏块类型、Slice 属于哪个 Frame、对应的帧的设置和参数等信息，Slice Payload 中存放的是 MB MB 中包含了 MB 类型、预测类型、Coded Block Pattern(CBP)、量化参数 QP、像素的亮度和色度数据等信息 GOP 由 IDR 帧开始，中间存在多个 P 帧或 B 帧（基本档次 Baseline Profile 不存在） I 帧只进行帧内编码，IDR 帧是每个 GOP 的第一个帧，IDR 帧是 I 帧，I 帧不一定是 IDR 帧 P 帧参考前面的 I 帧或者 P 帧进行编码； B 帧参考前向、后向的 I 帧或 P 帧进行编码； P/B 帧进行编码时候只能参考当前 GOP 的 I/P 帧，不能越过当前 GOP 开始时的 IDR 帧，但是 I 帧之后的 P 帧也可以参考这个 I 帧之前的图像。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:9:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"SPS/PPS/DTS/PTS SPS：序列参数集，描述的是整个视频序列的参数信息，如图像的宽度、高度、帧率、色度空间等，一个视频序列只有一个 SPS，用于描述整个视频序列的基本特性。SPS 一般在视频码流的开头发送，是一种全局静态的描述方式。当然，如果编码器在编码过程中改变了 SPS 中描述的参数如分辨率时也需要发送新的 SPS。 PPS：图像参数集，描述的是一个序列中一个或多个图像的参数，例如编码图像的配置、QP 这些，可以在视频序列的任何时刻发送，PPS 相对与 SPS 来说比较灵活动态。 发 I 帧之前至少要发一次 SPS 和 PPS DTS：解码时间序列，编码帧时的顺序 PTS：展示时间序列，展示帧时的顺序，当有 B 帧时 PTS 不等于 DTS 编码之后的 VCL 数据被封装进 NALU 中，构成了 h264 原始码流 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:10:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"NALU NALU 是视频编码的基本单位，同时也是后续进行视频传输的基本单位。 不同应用需求采用不同的传输方式，NALU 根据传输方式可以以两种方式应用于传输业务。 一类是字节流，即把 NALU 按照解码顺序生成连续的比特流进行传输和处理。 一类是分组流应用，也是本文使用的实时视频通信的应用场景，网络可以根据不同网络分组的重要性优化视频流的服务质量，分组流通过将编码后得到的 NALU 作为网络传输的载荷。 NALU 由 NAL Header 和 RBSP 组成，Header 占一个字节，分为 3 个部分。 第 1 个部分是第 0 位，禁止位，值为 0，值为 1 表示语法错误。 第 2 个部分是第 1~2 位，表示当前 NAL 的优先级。值越高表示当前 NAL 越重要，越需要优先保护。SPS/PPS/IDR 帧非常重要，I/P 帧重要，B 帧/SEI 不重要 第 3 个部分是第 3~7 位，表示当前 NALU 的类型： RBSP：原始字节序列载荷，是 NALU 数据部分的封装格式，封装的数据来自于 SODB（原始数据比特流）。 SODB 是编码后的原始数据。SODB 到 RBSP 的过程： 如果 SODB 是空的，生成的 RBSP 也是空的。 否则： RBSP 的第一个字节直接取自 SODB 的第 1~8 个 bit，（RBSP 字节内的 bit 按照从左到右对应位从高到低的顺序排列），RBSP 其余的每个字节都直接取自 SODB 的相应 bit。RBSP 的最后一个字节包含 SODB 的最后几个 Bit 和 rbsp_trailing_bits() rbsp_trailing_bits 的第一个 bit 是 1，接下来填充 0 直到字节对齐。 最后添加几个 cabac_zero_word，值为 0x0000. NALU 主要涉及到 SPS/PPS/SEI 和 Slice 这几种类型。 SEI 是补充增强信息，提供了向视频码流中加入额外信息的方法，不是解码过程中的必须选项，可能对解码过程有帮助，集成在视频码流中。 RBSP 字节流加上 0x0300 就得到 NALU 载荷字节流： ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:11:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"Profile/Level Profile 主要对视频编码的特性做了差异化支持。 H264 中的常用 Profile 有 Baseline，Extended，Main 和 High Baseline Profile：基本画质，只支持 I/P 帧和 CAVLC 和无交错 Extended Profile：进阶画质，支持 I/P/B/SP/SI，只支持 CAVLC 和无交错 Main Profile：主流画质，支持 I/P/B 帧和，交错和无交错、CAVLC 和 CABAC High Profile：高级画质：在 Main 的基础上增加了 8x8 内部预测、自定义量化和更多的 yuv 格式。 Baseline Profile 主要用于实时视频通信，Main Profile 和 High Profile 主要用于流媒体领域。 Level 主要根据设备能力来确定编解码时的码率/分辨率上限支持 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:12:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"CBR/VBR/ABR/CRF CBR：恒定码率，每秒传输的 bit 数固定，每个视频帧都被分配相同数量的 bit，和复杂度无关，适用于要求网络或者存储带宽具有固定容量的场景，但可能导致复杂场景下的帧质量下降。WebRTC 中使用的就是 CBR。 VBR：可变码率，允许每个帧使用不同的比特数，根据图像复杂度和需要进行动态分配。适合于在不同场景下需要保持一致质量的情况，但是消耗的带宽会有较大波动。适合用于视频存储，不适合网络传输。 ABR：平均码率，允许在整个视频序列中有一定的变化，但是在一个时间窗口内保持一定的平均码率。在此时间内，对简单、静态的图像分配低于平均码率的码率，对于复杂的、大量运动的图像分配高于平均码率的码流。码率分配比较均衡，比较适合网络传输。 CRF：恒定质量因子，追求的是恒定的视觉质量，编码器根据图像内容自动调整码率，以保持相对恒定的质量。适合于追求质量而不在乎码率的场景如视频剪辑和存档。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:13:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"OpenH264 码控 编码第一个 IDR 帧时使用固定的 QP，具体的值使用视频分辨率作为判断依据并查表得出并将其初始化为 initialQP： 将 QP 查表转换为 QStep 完成初始化： 之后进行 IDR 帧的量化及后续编码操作，并得到 frameDqBits（已编码的比特数），由此可以计算 intraCmplx： 对于第一个 PFrame，linearCmplx 也使用下面的公式计算，其中 QStep 使用 initialQP 查表。 之后计算 QStep\u0026QP 时就使用一阶 RQ 模型计算，frameComplexity 在预处理阶段得出。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:14:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"RTP RTP（Real-time Transport Protocol，实时传输协议）是一种应用层协议，通常基于 UDP 协议，但也支持 TCP 协议。 它提供了端到端的实时传输数据的功能，但不包含资源预留存 1（resource reservation）、不保证实时传输质量，这些功能都需要 WebRTC 自己实现。 RTP 协议分为两种子协议，分别是 RTP Data Transfer Protocol 和 RTP Control Protocol。 前者顾名思义，是用来传输实时数据的；后者则是我们常说的 RTCP 协议，可以提供实时传输过程中的统计信息（如网络延迟、丢包率等），WebRTC 正是根据这些信息处理丢包。 RT(D)P 包分为两部分，分别是 header 和 payload，header 包含了实时音视频的同步信息（和一些额外参数），payload 则承载了具体的音视频数据。这里我们只需要关注 header 结构就好，payload 是编解码器关心的。 如图所示，RT(D)P header 最小为 12 bytes；红色部分为可选字段。字段的含义分别如下： Version 表示 RTP 协议的版本，目前版本为 2。 P (Padding) 表示 RT(D)P 包末尾是否有 padding bytes，且 padding bytes 的最后一个 byte 表示 bytes 的数量。Padding 可以被用来填充数据块，比如加密算法可能会用到。 X (Extension) 表示是否有头部扩展，头部扩展可以用于存储信息，比如视频旋转角度。 CC (CSRC count) 表示红色部分的 CSRC（参见下文）数量，显然最多只能有 15 个 CSRC。 M (Marker) 表示当前数据是否与应用程序有某种特殊的相关性。比如传输的是一些私有数据，或者数据中的某些标志位具有特殊的作用。 PT (Payload type) 表示 payload 的数据类型，音视频的默认映射格式可参见 RFC 3551。 Sequence number 是递增的序列号，用于标记每一个被发送的 RT(D)P 包。接收方可以根据序列号按顺序重新组包，以及识别是否有丢包。序列号的初始值应当是随机的（不可预测的），从而增加明文攻击的难度。 Timestamp 即时间戳，接收方根据其来回放音视频。时间戳的间隔由传输的数据类型（或具体的应用场景）确定，比如音频通常以 125µs（8kHz）的时钟频率进行采样，而视频则以 90kHz 的时钟频率采样。这里时间戳的初始值也是随机选取的，是一种相对时间戳。 SSRC (Synchronization source) 即同步源标识符。相同 RTP 会话中的 SSRC 是唯一的，且生成的 SSRC 也需要保持随机。尽管多个源选中同一个标识符的概率很低，但具体实现时仍然需要这种情况发生，即避免碰撞。 CSRC (Contributing source) 在 MCU 混流时使用，表示混流出的新的音视频流的 SSRC 是由哪些源 SSRC 贡献的。根据上述 CC 得知，我们最多可以同时混 15 路音视频流。 Extension header 即头部扩展，包含了音视频的一些额外信息，比如视频旋转角度。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:15:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"RTP 与 NALU 分组 RFC3984 给出了 3 中不同的 RTP 打包方案： Single NALU Packet：在一个 RTP 包中只封装一个 NALU，对于小于 1400 字节的 NALU 便采用这种打包方案。 Aggregation Packet：在一个 RTP 包中封装多个 NALU，对于较小的 NALU 可以采用这种打包方案，从而提高传输效率。 Fragmentation Unit：一个 NALU 封装在多个 RTP 包中，在本文中，对于大于 1400 字节的 NALU 便采用这种方案进行拆包处理。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:15:1","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"RTCP RTCP 协议提供实时传输过程中的统计信息，如网络延迟、丢包率等。 在传统的实时通讯过程中，RT(D)P 协议占用偶数位的端口，而 RTCP 协议占用随后的奇数位端口。 不过如果接收方的 SDP 中包含 rtcp-mux 字段 6，即表明接收方支持 RT(D)P 协议和 RTCP 协议共用同一个端口，即多路复用。在 Chrome 57 版本已经强制开启了 rtcp-mux 。 对于 RTCP 包而言，我们不只要关注 header 的结构，还要关注具体的 report block 内容。不过我们先来看一个典型的 RTCP header 结构，如下图所示： RTCP header 的固定大小为 8 bytes，其中 Version、P、SSRC 的含义同上述 RTP header 相同，在此不与赘述。其他几个字段的含义分别如下： RC (Reception report count) 表示当前 RTCP 包有几个 block，显然最多只能有 32 个。 PT (Packet type) 表示 RTCP 包的类型，比如 SR=200、RR=201（SR、RR 参见下文）。 Length 等于整个 RTCP 包的长度减一（使得 Length = 0 是合法的），其值包含 header 的长度和所有 padding 占用的空间长度。值的单位是以 32 位字长（32-bit words）描述的。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:16:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"PSNR/SSIM/VMAF/BD-Rate ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:17:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"PSNR 峰值信噪比，基于均方误差（MSE）计算。 公式： $$ PSNR = 10 \\cdot log_{10}(\\frac{MAX_I^2}{MSE}) $$ $$ MSE = \\frac{1}{MN}\\sum_{i=0}^{m-1}\\sum_{j=0}^{n-1}[I(i,j) - K(i,j)]^2 $$ 对于用 8bit 表示的视频图像而言，MAX 就是 255 PSNR 的物理含义就是信号的峰值与平均误差的比值，如果误差越小，那么 PSNR 值越高。如果完全没有误差，那么 PSNR 值就是无穷大。 对于图像数据来说，通常有 Y、U、V 三个分量，可以对三个分量各自计算 PSNR。 也可以把三个分量的 PSNR 值以一定的权重加起来作为总的 PSNR 值。 对于整个视频来说，可以计算单帧的 PSNR 值，然后平均，也可以计算整个视频的 Overall PSNR。 一般，psnr 值高于 40dB，表示画面质量极好，（非常接近于原始图像）； psnr 值在 30dB-40dB 之间，表示画面质量较好（有失真但可接受）； psnr 值在 20dB-30dB 之间，表示画面质量差； ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:17:1","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"SSIM PSNR 指标比较常用，但是不能体现编码前后图像之间的相关性，SSIM 可以从亮度，对比度和结构三个方面来描述编码前后图像之间的相似性。 亮度维度的相似性用均值计算，𝝁𝒙为均值： 对比度维度的相似性用方差计算，𝝈𝒙 为方差： 结构维度的相似性用协方差计算， 𝝈𝒙𝒚 表示协方差： SSIM 指标和 SSIM 计算公式： SSIM 性质： ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:17:2","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"VMAF VMAF 是一种 Full reference 的视频质量评估方法，适用于视频流媒体质量评估。 主要包括三种指标：视觉信息保真度 (VIF: visualquality fidelity) 、细节损失指标 (DLM: detail loss measure) 、时域运动指标/平均相关位置像素差 (Tl: temporal information) 。 其中 VIF 和 DLM 是空间域的一个画面之内的特征。 TI 是时间域的，多个画面之间相关性的特征。 这些特性之间融合计算总分的过程使用了训练好的 SVM 来预测。 VMAF 基于 SVM 的 nuSvr 算法，在运行的过程中，根据事先训练好的 model，赋予每种视频特征以不同的权重。对每一帧画面都生成一个评分。最终以均值算法进行汇总，算出该视频的最终评分 使用方式： 计算时需要保证编码后图像的分辨率与原始图像分辨率一致，如果不一致需要向上或者向下缩放编码图像，而不能缩放原始图像。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:17:3","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"BD-Rate 比较两个编码器的 RD 曲线（Rate-Distortion）的差异： 一种是相同质量下的码率差异，指标为 BD-Rate。 一种是相同码率下的质量差异，指标为 BD-PSNR。 BD-Rate 是选取一个范围的的多个采样点（通常是 4 个），然后进行曲线拟合插值，最后计算出平均的指标差异。目前大部分测试数据的对比，都是基于 BDRate 指标的。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:17:4","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"视频封装 视频的封装格式定义的是多媒体数据的存储结构，包括如何组织和存储音频、视频、字幕、元数据等信息，以及怎么进行同步和时间标记。 感觉封装其实可以理解为是一个柜子，并且对柜子里的不同抽屉里面能存放哪些数据以及怎么存放数据做了规定。\\ 常见的封装格式有 Mp4、Mkv，微软的 AVI、苹果的 MOV、Adobe 的 FLV、Google 的 Webm。 不同的封装格式提供了一种统一的方式来存储和传输各种编码格式的多媒体数据。 H264 支持 AnnexB 和 AVCC 两种封装模式。 AnnexB 模式是传统模式，有 startcode，SPS 和 PPS 在码流中分别作为一个 NALU。 AVCC 模式没有 startcode，SPS 和 PPS 以及其它信息被封装在 container 中，每个 frame 前面 4 个字节是这个 frame 的长度。一般在 mp4 和 mkv 中使用 AVCC。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:18:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"硬件编码和软件编码 硬编码是通过专用的硬件编码器，比如 GPU 或者专用的视频编码芯片来进行的，硬件编码可以利用专为编码设计的电路从而提供更高的编码速度和更低的功耗，适用于实时性要求较高的应用比如云游戏、视频会议这些场景。硬编码也通常会在移动设备比如手机或者其他的专业视频设备中使用 软编码是通过通用的计算设备，一般来讲就是用 CPU 编码。软编码的性能和功耗一般来讲要比硬件编码差，但是可以跨平台运行，容易升级和更新，编码的画质也会更好。一般用于需要更大灵活性和可定制性的应用，比如短视频录制、非实时的转码等。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:19:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"码率和分辨率 码率提高和分辨率提高都会增大视频的体积，两者之间需要平衡。 在码率一定的情况下，提高分辨率可能会导致每个像素获得的比特数减少，从而降低整体感知质量。 在分辨率一定的情况下，提高码率可以分配更多的比特给每个像素，从而提高每个像素的清晰度。 高分辨率主要是为了适应更大的屏幕尺寸，或者需要更多图像细节的任务。 其实屏幕尺寸和分辨率之间的关系有点类似于分辨率和码率之间的关系。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:20:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"一个视频文件能否倒放 一个文件不行，至少需要两个文件才可以。 理论上方式有两种： 第一种是先顺序解码视频到一个 yuv 文件中，然后倒序读入内存进行编码。 第二种是先遍历视频，获取一共有多少个 GOP，跳到最后一个 GOP 的 IDR 帧，对这个 GOP 进行解码输出到 yuv 文件中，再逆序读出这个解码之后的 yuv 文件然后编码，这样最后一个 GOP 就变成了第一个 GOP，按照从后往前的顺序依次类推就可以。 第一种方式简单粗暴好实现，但是对于磁盘存储空间的要求比较高。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:21:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"H265(HEVC)比 H264(AVC)做了哪些改进 H265 针对编码的各个环节都引入各自对应的单元。 与 H264 中宏块类似的是，在 H.265 里面用的是一系列互不重叠的编码树单元 CTU 处理信息，CTU 内部可以以四叉树结构递归向下划分成更小的正方形编码单元 CU。CU 可以支持最大 64x64 的尺寸，因而可以对高分辨率视频中的平坦和复杂区域做有针对性的 CTU 划分。 预测单元 PU 是定义在 CU 上的一个矩形区域，用来存储和预测相关的所有信息如帧内预测方向、帧间预测的参考帧、和 MV 等。 变换单元 TU 是变换和量化的基本单位，支持 4 种正方形的尺寸大小(4/8/16/32)。变换时采用 RQT 技术，基于四叉树结构进行自适应变换。大块的 TU 模式能够将能量更好地集中，小块的 TU 模式能够保存更多的图像细节。根据当前 CU 内残差特性，自适应选择变换块大小，可以在能量集中和细节保留两者做最优的折中。与传统的固定块大小变换相比，RQT 对编码效率贡献更大。 CU 划分成 PU 和 TU，PU 和 TU 之间存在交叉重叠关系，Inter 预测时允许 CU 内的 TU 跨越 PU 边界，Intra 预测时，TU 不能跨越 PU 边界。 在帧内预测模块，H265 支持更多的帧内预测模式。H265 的亮度分量支持 35 种帧内预测模式包括平面模式、DC 模式和 33 种角度模式，色度分量有 5 种帧内预测模式包括平面模式、DC 模式、水平、垂直方向模式和对应于亮度分量的帧内预测模式。 在帧间预测模块，H265 引入了更加复杂的 ME 方式，主要包括 Merge 和 AMPV 以及基于 Merge 的 Skip 模式。 Merge：取相邻 PU 的运动参数作为当前 PU 的运动参数（利用空域相关性和时域相关性） AMVP 得到的 MV 一方面为 ME 提供了搜索起点，另一方面也用于预测 MV。AMVP 根据周围块预测 MV，MV=MVP+MVD(矢量差值) H265 把变换和量化模块结合了起来，降低了计算复杂度，支持加权量化矩阵。 在环路滤波模块，H265 新增了采样点自适应偏移滤波 SAO，通过解析去方块滤波后的像素的统计特性，为像素添加相应的偏移值，削弱振铃效应。 因为 H265 的解码要比 h264 的解码复杂很多，所以提供了很多可以并行优化的思路。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:22:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"SIMD SIMD 是一种并行计算技术，允许单一指令处理多个数据元素。SIMD 指令集通常由处理器提供，用于加速向量化计算。视频编码中，SIMD 可以用于加速压缩和解压算法。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:23:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"H264 中的差错控制 Slice 分割、Data Partition (DPA \u003e DPB \u003e DPC)、对 SPS 和 PPS 提供使用高传输优先级、差异化的熵编码（对重要的 SPS 和 PPS 采用指数哥伦布编码） FMO 通过宏块分配映射把同一帧里的不同宏块划分到不同的 Slice Group 里，在同一个 Slice Group 里的 MB 按照普通的光栅扫描顺序编码。 因为不管是 Intra Coding 还是 Inter Coding 都必须使用同一个 Slice group 的宏块数据，这样当一个 Slice group 里的某一个或者某几个宏块发生错误时候，因为相邻的宏块可能分布在不同的 Slice group，就可以从其他正确接收的 Slice group 里拿到和丢失宏块相邻的宏块信息来进行错误掩盖。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:24:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"SVC SVC：(Scalable Video Coding)可伸缩视频编码，编码器产生的码流包含一个或者多个子码流或者层，子码流可以有不同的码流、帧率和分辨率。基本层编码最低层的时域、空域和质量流；增强层以基本层作为起始点，对附加信息进行，从而在解码过程中重构更高层的质量、分辨率和时域层。通过解码基本层和相邻增强层，解码器能生成特定层的视频流。 时域分层：从码流中提取出有不同帧频的码流。 空域分层：从码流中提取出有不同分辨率的码流。 质量分层：从码流中提取出有不同质量的码流。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:25:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"应用场景 监控：监控视频流产生两路，一路质量好的用于存储，一路低码率的用于预览。 视频会议：会议终端利用 SVC 编出多种分辨率、分层质量的码流，会议中心替代传统的 MCU 二次编解码方法改成视频路由分解转发。也可以在丢包环境下利用时域分级，抛弃一些时域级别实现网络适应性。 流媒体 IPTV：服务器可以根据不同的网络情况丢弃质量层，保证视频的流畅。兼容不同网络环境和终端。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:25:1","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"优缺点 优点：分级码流优点是应用非常灵活，因为能根据需要产生不同的码流或者提取出不同的码流。使用 SVC 实现一次分层编码比用 AVC 编多次更高效。 SFU 从发布客户端复制音视频流的信息，然后分发到多个订阅客户端。典型的应用场景是 1 对多的直播服务。SFU 是解决服务端性能问题的好方法，因为它不涉及视频解码和编码的计算费用，用最低的开销来转发各路媒体流，能实现海量的客户端接入。重终端，轻平台。 缺点：因为 SVC 解码控制复杂不利于流式处理，硬件编解码器支持差，协议协商细节复杂，业界标准不统一。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:25:2","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"RTC 应用中提高实时性 因为编码主要的时间开销在运动预测过程中，如果在云游戏的场景下，可以得到游戏画面中物体的运动信息然后考虑用来辅助运动预测，或者说对搜索过程进行剪枝。 ","date":"2024-01-23","objectID":"/posts/knowledge/h264-encode/:26:0","tags":["Multimedia"],"title":"H264 Encode","uri":"/posts/knowledge/h264-encode/"},{"categories":["knowledge"],"content":"关于远程桌面 远程桌面是一种将一台计算机的桌面控制权限交给网络上另一台计算机的技术，两台计算机之间建立连接之后，可以进行音视频以及控制信令的相互传输，从而实现远程控制的功能。 远程桌面技术的实现 基于远程桌面要完成的任务目标，其需要实现以下两个核心功能： 音视频的传输，即需要让控制机收到受控机的音频跟视频。 控制信令的传输，即鼠标键盘的控制信号等 目前主流的远程桌面技术主要有 2 种： 基于VNC(Virtual Network Computing)的远程桌面技术 基于RDP(Remote Desktop Protocol)的远程桌面技术 ","date":"2023-06-15","objectID":"/posts/knowledge/webrtc/remote-desktop-with-webrtc/:0:0","tags":["WebRTC"],"title":"远程桌面与WebRTC","uri":"/posts/knowledge/webrtc/remote-desktop-with-webrtc/"},{"categories":["knowledge"],"content":"VNC VNC 使用远程帧缓冲协议即(RFB, Remote FrameBuffer)来远程控制另一台计算机，将控制机的键盘和鼠标事件传输到被控制机，同时将被控制机的屏幕图像传输到控制机。 基于其技术原理，VNC 有以下优点： 跨平台，可以在不同的操作系统上运行，VNC 技术本身也有多个客户端和服务端的实现版本，如 RealVNC、TightVNC、UltraVNC 等 开源，VNC 的源代码及其很多现代衍生品都是在 GNU 许可证之下发布的 轻量级，VNC 的客户端和服务端都是非常轻量级的程序，可以在低配置的计算机上运行 但因为 VNC 本身的设计时间很早，因此在 2023 年的今天暴露出了很多的时代局限性： 因为其基于像素方块的传输原理，就算是采用部分更新传输的方式，在大量像素变化的情况下会消耗大量的带宽。特别是对于现在的高分屏，其传输的数据量会更大。 VNC 在设计之初被用于局域网内使用，因此没有考虑太多的安全性，虽然密码并不以明文发送，但是如果从网络中嗅探出加密密钥和编码之后的密码，也可能成功破解出密码。 ","date":"2023-06-15","objectID":"/posts/knowledge/webrtc/remote-desktop-with-webrtc/:1:0","tags":["WebRTC"],"title":"远程桌面与WebRTC","uri":"/posts/knowledge/webrtc/remote-desktop-with-webrtc/"},{"categories":["knowledge"],"content":"RDP RDP 是微软提出的一种专有协议，扩展了 T-120 系列协议标准，最早专用于 Windows 系统的终端和服务器之间的远程桌面连接，之后微软也实现了RDP 的 MacOS 客户端，现在也有很多第三方的实现版本实现了其功能的子集，为 GNU/Linux 做了适配如xrdp。因此，可以说 RDP 也一定程度上具有跨平台的性质。 相比于 VNC，RDP 的实现原理还是比较复杂的： 首先，RDP 的最底层是 TCP，TCP 之上是各层的协议和服务。 TPKT：是 TCP 之上的 ISO 传输服务，允许两个组交换 TPDU（传输协议数据单元）或 PDU（协议数据单元）的信息单元。 X.224：连接传输协议，主要用于 RDP 初始连接请求和响应。 T.125 MCS：多点通信服务，允许 RDP 通过多个通道进行通信和管理。 RDP 的工作原理是通过 TPKT 实现信息单元的交换，通过 X.224 建立连接，使用 T.125 MCS 打开两个通道来完成两个设备之间的来回数据传输。 RDP 的特点功能比较丰富，比如： 支持共享剪切板。 支持多个显示器。 支持虚拟化 GPU。 支持 32 位彩色和 64000 个独立的数据传输通道。 通过 RC4 对称加密算法使用 128 位密钥对数据进行加密。 可以在使用远程计算机时参考本地计算机上的文件系统。 远程计算机的应用程序可以在本地计算机上运行。 当然，事物都有两面性，RDP 拥有这么多强大功能，也有一些难以避免的缺点： 网络速度较慢时，远程连接容易出现延迟。 两台计算机在不同的网络上时，其配置过程相当复杂。 固定使用 3389 端口监听，可能成为攻击的目标。 RDP 整体上还是受到微软控制，定制性比较差。 WebRTC 和远程桌面 远程桌面的核心需求和 WebRTC 的核心功能完美契合。 WebRTC 基于 ICE/STUN/TURN 的 NAT 穿透方案可以很方便地解决不同网络情况下主机连接的问题， WebRTC 基于 SRTP 的传输方式天然提供了实时特征、端到端的加密的数据传输服务。 WebRTC 针对各种网络情况做了音视频传输的大量优化，可以保证各种网络条件下的可用性。 WebRTC 本身其实是 Chromium 浏览器的一部分，天然具备跨平台的性质。 WebRTC 完全开源，定制性极强，不少公司都基于 WebRTC 来做自家的直播、云游戏业务。 整体上来讲，WebRTC 的优势使其很适合用于远程桌面业务，当然，目前市面上已经有 App 基于 WebRTC 实现了远程桌面的功能，比如Chrome Remote Desktop和ToDesk。前者可以理解为是 Google 用自己 WebRTC 推出的远程桌面服务，体验了一下，整体上功能比较少，但是连接比较稳定，不过受 GFW 影响，这玩意在国内应该是处于没法用的状态；后者则是国产远程桌面软件，目前已经比较成熟，提供了企业版、个人版、专业版和游戏版四个版本，从其官网上提供的信息来看，应该是做出了一定成绩。 从技术上讲，基于 WebRTC 开发远程桌面应用相当合理，开源可控，还有谷歌背书，WebRTC 本身在不停地与时俱进，作为上层应用开发的远程桌面也可以及时享受到 WebRTC 带来的改进。 从业务上讲，WebRTC 本身具有的功能可以解决上面所说的 VNC 和 RDP 的诸多问题，不过就功能的丰富性而言，可能跟微软的 RDP 还差一些，但是 WebRTC 基于音视频的解决方案本身可以优化的上限还是挺高的，毕竟随着人们需求的上升，高分辨率、高帧率也会成为未来远程桌面应用必不可少的功能需求。 本篇博客从非技术层面探讨了远程桌面技术的当下两大主流技术，以及 WebRTC 应用于远程桌面业务下的可行性。下篇博客将从技术层面详细分析 WebRTC 与远程桌面业务的契合程度及可能的解决方案，就先从核心功能开始吧！ ","date":"2023-06-15","objectID":"/posts/knowledge/webrtc/remote-desktop-with-webrtc/:2:0","tags":["WebRTC"],"title":"远程桌面与WebRTC","uri":"/posts/knowledge/webrtc/remote-desktop-with-webrtc/"},{"categories":["development"],"content":"本文主要记录笔者在 Gentoo Linux 下面搭建 WebRTC 开发环境的过程。 ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:0:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["development"],"content":"准备工作 网络：可以科学上网的梯子 IDE：VSCode 或者 CLion ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:1:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["development"],"content":"安装depot_tools Google 有自己的一套用于管理 Chromium 项目的工具，名叫depot_tools，其中有包括git在内的一系列工具和脚本。 # 创建google目录用于存储google相关的代码 mkdir ~/google cd ~/google # clone depot_tools git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git 克隆完成之后需要将depot_tools的路径加到PATH中，Linux 上添加环境变量最简单的方式是修改~/.profile，这种方式与你的登录 shell 是什么没有关系，不管是fish还是bash还是zsh都会吃这种方式： # ~/.profile export GOOGLE_BIN=$HOME/google/depot_tools export PATH=$GOOGLE_BIN:$PATH 但是这种方式需要你注销重新登录。 ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:2:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["development"],"content":"克隆代码 mkdir webrtc-checkout cd webrtc-checkout fetch --nohooks webrtc gclient sync 整个 WebRTC 的项目代码大小约 20G，克隆过程中需要保证网络畅通顺畅，如果你的梯子有大流量专用节点最好，否则可能克隆完你的流量就用光了。 克隆期间可能会因为网络问题中断，重新执行gclient sync即可，直到所有的模块都克隆完毕。 按照官方的建议，克隆完成之后创建自己的本地分支，因为官方分支更新很快，不 checkout 的话，可能你的 commit 还没写完，就被 Remote 的 change 给覆盖了，还要手动处理冲突。 cd src git checkout master git new-branch \u003cbranch-name\u003e ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:3:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["development"],"content":"编译 WebRTC 关于 WebRTC 的版本可以在Chromium Dash查到： 如上图所示，113 分支是当前的稳定分支，对应的 tag 是branch-heads/5672： cd ~/google/webrtc-checkout/src git checkout branch-heads/5672 git switch -c m113 创建本地分支之后就可以用gn生成ninja文件了： gn gen out/Default --root=\".\" --args='is_debug=true target_os=\"linux\" target_cpu=\"x64\" rtc_include_tests=false rtc_use_h264=true rtc_enable_protobuf=false is_clang=true symbol_level=0 enable_iterator_debugging=false is_component_build=false use_rtti=true rtc_use_x11=true use_custom_libcxx=false treat_warnings_as_errors=false use_ozone=true' 这里使用了clang并且启用了h264，详细的gn参数可以参考gn-build-configuration和项目根目录下的webrtc.gni文件。 之后使用autoninja进行编译，编译时会吃满你 PC 的所有核心，编译时间取决于你 PC 的配置： autoninja -C out/Default 可以看到默认生成了几个样例的可执行文件。 cd 到obj目录下可以看到libwebrtc.a文件，就是编译链接之后最终生成的可以引用的库文件。 ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:4:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["development"],"content":"搭建开发环境 Google 官方给出了 Chromium 项目的CLion 配置指南，所以只需要照猫画虎给 WebRTC 配置一下。 ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:5:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["development"],"content":"配置 CLion 属性 因为整个项目比较大，所以需要调大 CLion 的 VM 内存和 intellisence 支持的文件大小： Help-\u003e Edit Custom VM Options，在文件的末尾添加： -Xmx8g 表示给 VM 设定8G的可用内存，这样基本上不用担心使用过程因为内存不足而 CLion 性能不够了。 Help-\u003eEdit Custom Properties，在文件的末尾添加： idea.max.intellisense.filesize=12500 表示为大小为12500KB也就是12M以下的文件提供 intellisense 支持。 ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:5:1","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["development"],"content":"配置 gdb vim ~/.gdbinit # 添加下面一行 source ~/google/webrtc-checkout/src/tools/gdb/gdbinit 之后在 CLion 中的Settings-\u003eToolchain-\u003eDebugger选择系统自带的 gdb：/usr/bin/gdb即可。 ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:5:2","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["development"],"content":"配置 intellisense 因为 WebRTC 用的是gn+ninja作为构建工具，而CLion目前只支持cmake，所以当要求配置CMakeLists.txt时直接无视即可。网络上有说用gn_to_cmake.py这个脚本的，但是我没看懂这东西的功能，反正是不能生成CMakeLists.txt，只是生成一个json文件，并不能用于 CLion 的索引。 我这边成功开启 IDE 语法高亮和索引的姿势是这样的： cd webrtc-checkout/src python3 ./tools/clang/scripts/generate_compdb.py -p ./out/Default -o ./compile_commands.json --target_os=linux 这一步会生成 CLion 可以自动识别的compile_commands.json文件，从而可以正确索引项目的代码并提供代码补全功能。 之后每次启动项目 CLion 就会自动索引项目文件，就可以愉快地看代码和写代码啦！ ","date":"2023-04-23","objectID":"/posts/development/webrtc-development-prepare/:5:3","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/development/webrtc-development-prepare/"},{"categories":["knowledge"],"content":"概况 WebRTC提供了视频自适应机制，其目的主要是通过降低编码的视频的质量来减少带宽和 CPU 消耗。 视频自适应发生的情形：带宽或 CPU 资源发出信号表明自己未被充分使用或被过度使用时，进行视频自适应。过度使用则降低质量，否则提高质量。 视频自适应调整的对象：帧率与分辨率。 资源 Resources监测指标来自于系统或视频流。例如，一个资源可以监测系统温度或者视频流的带宽使用率。 资源实现了Resource接口： 当资源检测到被过度使用则调用SetUsageState(kOveruse)； 当资源不再被过度使用则调用SetUsageState(kUnderuse)。 对所有的视频而言，默认有两种类型的资源： 质量标量资源 编码过度使用资源 ","date":"2022-09-15","objectID":"/posts/knowledge/webrtc/note-for-webrtc-1/:0:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/knowledge/webrtc/note-for-webrtc-1/"},{"categories":["knowledge"],"content":"QP 标量资源 质量标量资源监测发送视频流中编码之后的帧的量化参数（QP），确保视频流的对于当前的分辨率而言可以接受。 每一帧被编码之后，QualityScaler就能获得相应的 QP。 过度使用或者未被充分使用的信号在平均 QP 脱离 QP 阈值之后发出。 QP 阈值在EncoderInfo中的scaling_settings属性中设置。 需要注意的是 QP 标量只在降级偏好设置为MAINTAIN_FRAMERATE或BALANCED时启用。 ","date":"2022-09-15","objectID":"/posts/knowledge/webrtc/note-for-webrtc-1/:1:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/knowledge/webrtc/note-for-webrtc-1/"},{"categories":["knowledge"],"content":"编码使用资源 编码使用资源监测编码器需要花多长时间来编码一个视频帧，实际上这是 CPU 使用率的代理度量指标。 当平均编码使用超过了设定的阈值，就会触发过度使用的信号。 ","date":"2022-09-15","objectID":"/posts/knowledge/webrtc/note-for-webrtc-1/:2:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/knowledge/webrtc/note-for-webrtc-1/"},{"categories":["knowledge"],"content":"插入其他资源 自定义的资源可以通过Call::AddAdaptationResource方法插入。 自适应 资源发出过度使用或未充分使用的信号之后，会发送给ResourceAdaptationProcessor，其从VideoStreamAdapter中请求Adaptation提案。这个提案基于视频的降级偏好设置。 ResourceAdaptationProcessor基于获得的提案来确定是否需要执行当前的Adaptation。 ","date":"2022-09-15","objectID":"/posts/knowledge/webrtc/note-for-webrtc-1/:3:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/knowledge/webrtc/note-for-webrtc-1/"},{"categories":["knowledge"],"content":"降级偏好设置 有 3 种设置，在RtpParameters的头文件中定义： MAINTAIN_FRAMERATE: 自适应分辨率 MAINTAIN_RESOLUTION: 自适应帧率 BALANCED: 自适应帧率或分辨率 降级偏好设置在RtpParameters中的degradation_perference属性中设置。 VideoSinkWants和视频流自适应 自适应完成之后就会通知视频流，视频流就会转换自适应为VideoSinkWants。 这些接收器需求向视频流表明：在其被送去编码之前需要施加一些限制。 对于自适应而言需要被设置的属性为： target_pixel_count: 对于每个视频帧要求的像素点总数，为了保持原始的长宽比，实际的像素数应该接近这个值，而不一定要精确相等， max_pixel_count: 每个视频帧中像素点的最大数量，不能被超过。 max_framerate_fps: 视频的最大帧率，超过这个阈值的帧将会被丢弃。 VideoSinkWants可以被任何视频源应用，或者根据需要可以直接使用其基类AdaptationVideoTraceSource来执行自适应。 ","date":"2022-09-15","objectID":"/posts/knowledge/webrtc/note-for-webrtc-1/:4:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/knowledge/webrtc/note-for-webrtc-1/"},{"categories":["paper"],"content":"整体概况 Link：Modeling the Perceptual Quality for Viewport-Adaptive Omnidirectional Video Streaming Considering Dynamic Quality Boundary Artifact Level：IEEE TCSVT 2021 DQB: Dynamic Quality Boundary，指在基于分块的 FoV 自适应全景视频推流过程中低质量分块区域的暴露和质量切换现象。 DQB 现象实际上就是 FoV 内分块间的质量差异和随时间变化的分块质量变化。 这篇论文主要的贡献在于深入研究了这种现象，并且针对此提出了可以利用现存的 QoE 评估指标的模型，并且可以实际应用。 Model 的建立 执行一系列主观评估，由低质量分块的比例和质量导致的感知质量的降低可以基于主观实验结果完成建模。 结合剩下分块的感知质量可以完成单帧质量模型的建模。 最后将一段时间内的所有帧的感知质量池化，就完成了整个的模型。 ","date":"2022-03-20","objectID":"/posts/papers/note-for-dqb/:0:0","tags":["QoE"],"title":"Note for DQB","uri":"/posts/papers/note-for-dqb/"},{"categories":["paper"],"content":"主观实验的设定 获得 FoV 内帧的感知质量（低质量分块和高质量分块同时存在） 获取整个视频的感知质量（与上面的实验过程相近，只是过程中没有暂停） 获取整个视频的感知质量（没有引入 DQB，所有分块质量相同） 实验结果 ","date":"2022-03-20","objectID":"/posts/papers/note-for-dqb/:1:0","tags":["QoE"],"title":"Note for DQB","uri":"/posts/papers/note-for-dqb/"},{"categories":["paper"],"content":"帧质量感知模型 从上面的实验结果可以看出来高质量区域与低质量区域的质量差距 $d_n$ 越大，DQB 效应越显著（符合直觉）。将这部分影响因素看作是感知质量的主要影响因素： $$ d_n = Q_{H, n} - Q_{L, n} $$ $Q_{H, n}$ 和 $Q_{L, n}$ 分别表示第 $n$个 帧高质量分块和低质量分块的感知质量。 这两个质量从主观实验 3 的主观质量获得，在之后的训练过程中可以被客观质量评估的结果所替换。 为了调查质量差异 $d_n$ 和感知质量降低 $D_n$ 之间的关系，通过使用实验 1 的帧质量分数计算得出第$n$个帧的感知质量降低： $$ D_n = Q_{H, n} - Q_{HL, n} $$ $Q_{HL, n}$是实验 1 中评分得到的第$n$个帧的 FoV 内感知质量。 在 6 个视频上的实验结果如下图： 可以看到二者的关系可以近似为线性相关，即： $$ D_n = k_1 d_n $$ $k_1$ 作为线性回归的参数，可以计算出来。 但是对于不同取值的 $p_n$ ， $k_1$ 的取值也相当不同，两者之间的关系可以见下图： 数学表示可以建模为： $$ k_1 = a_1 \\cdot ln(a_2 \\cdot p_n + a_3) \\cdot sgn(p_n - P) $$ $sgn$ 是符号函数，$a_1, a_2, a_3$ 可以从回归中计算出来， $P$ 表示低质量分块的比例。按照图中的回归结果，$P = 0.118$ 时，用户几乎没办法注意到低质量区域的存在。 最终，由低质量区域暴露引起的感知质量降低 $D_n$ 可以计算为： $$ D_n = a_1 \\cdot ln(a_2 \\cdot p_n + a_3) \\cdot (Q_{H, n} - Q_{L, n}) \\cdot sgn(p_n - P) $$ 那么实际的感知质量 $Q_n$ 可以计算为： $$ Q_n = Q_{H, n} - D_n $$ ","date":"2022-03-20","objectID":"/posts/papers/note-for-dqb/:2:0","tags":["QoE"],"title":"Note for DQB","uri":"/posts/papers/note-for-dqb/"},{"categories":["paper"],"content":"时间池化 可以采用下面两种方式之一完成 ","date":"2022-03-20","objectID":"/posts/papers/note-for-dqb/:3:0","tags":["QoE"],"title":"Note for DQB","uri":"/posts/papers/note-for-dqb/"},{"categories":["paper"],"content":"Exp Minkowski-Based 单个帧的感知质量由衰减指数加权，衰减指数表示在主观评估中观察到的近因效应。 最终整个视频的感知质量 $PQ$ 可以计算为： $$ PQ = \\Big[\\frac{1}{N} \\sum_{n=1}^{N} exp\\big( \\frac{n-N}{\\delta} \\big) \\cdot {Q_n}^p \\Big]^{1/p} $$ $N$ 是整个视频的帧数。 $p$ 是 Minkowski指数，高 $p$ 值强调了最高质量帧的影响。 $\\delta$ 是控制近因效应强度的指数时间常数，以帧的数量的形式给出，高 $\\delta$ 值对应较弱的近因效应。 $p$ 和 $\\delta$ 的值可以通过对主观帧质量和视频序列的整体质量进行回归得到。 ","date":"2022-03-20","objectID":"/posts/papers/note-for-dqb/:3:1","tags":["QoE"],"title":"Note for DQB","uri":"/posts/papers/note-for-dqb/"},{"categories":["paper"],"content":"Quality Contribution-Based 之前的研究表明，传统视频在时间维度上的感知质量降低主要与每帧的显示时长相关。 FoV 自适应的全景视频也与之类似，感知质量与降低质量帧和高质量帧的持续时间相关。因此采用Quality Contribution的概念来描述每帧对视频感知质量的影响（考虑每帧的空间感知质量和显示时长）。 时间池化是由相应的显示时长加权的每帧的质量贡献的函数，特别的，质量贡献是从 MOS 和显示持续时间之间初步找到的对数关系所导出的： $$ C_n = Q_n \\cdot (p_1 + p_2 \\cdot log(T)) $$ $C_n$ 是第 $n$ 帧的贡献， $T$ 是每帧的显示时长， $T = Max(T, 33.3ms)$，即当帧率不低于 30fps 时，时间不连续性可以忽略。 接着，二级时间池化法用于池化单帧的分布。这种方法将 FoV 内的帧以注视水平划分为短时帧组(GoFs)，并以 GoF 的质量作为长期时间池化的基本单位来评估感知质量。 给出每帧的质量贡献之后，每个 GoF 的质量可以计算为 $$ Q_{GoF} = \\frac{\\sum_{n \\in N} \\big( C(n) \\cdot T(n) \\big)}{\\sum_{n \\in N} T(n)} $$ 接下来组合 GoF 的质量得到长期时间池化，即可以获得感知质量。 质量严重受损的帧会影响相邻帧的感知质量，视频中质量最差的部分主要决定整个视频的感知质量。因此提出选择计算出的质量低于平均值 75%的 GoF，以此计算平均质量并作为整个视频的感知质量。 ","date":"2022-03-20","objectID":"/posts/papers/note-for-dqb/:3:2","tags":["QoE"],"title":"Note for DQB","uri":"/posts/papers/note-for-dqb/"},{"categories":["paper"],"content":"Overview Link: Toward Immersive Experience: Evaluation for Interactive Network Services Level: IEEE Network 2022 Keywords: QoE Metrics Background Compared with traditional QoE for regular video/audio services, the existing work on IE is still in its infancy. This work aims at providing systematic and comprehensive research on IE for interactive network services, mainly studying the following three fundamental and challenging issues. What is the essential difference between IE and traditional QoE? Which categories of factors mainly influence IE? How to evaluate IE in an efficient and intelligent manner? IE versus traditional QoE ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:0:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Theoretical definitions Existing concepts of IE can be classified into two categories. The subjective sense of being surrounded or experiencing multi-sensory stimulation when interacting with the virtual environment. The user’s psychological state of deep involvement, engagement, absorption, or engrossment. Traditional QoE: A subjective measure from the user perspective of the overall value of the provided service and application. We can summary two significant points as follows to distinguish IE and traditional QoE: Both IE and traditional QoE are devoted to characterizing user’s subjective experience for network services. In terms of application scenarios, IE concentrates on the evaluation of network services equipped with interactive characteristics while traditional QoE is generally appropriate for regular audio/video services. IE is much more complex, fine-grained and multi-dimensional perception, which is produced through the interplay between multi-sensory data and diverse cognitive processes. ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:1:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Technical challenges Growing data volume Stricter delay constraint Increasing data dimension ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:2:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"IFs on IE ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:3:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Network-aware IFs Actually, when heterogeneous streams are delivered to the network, their transmission quality is dependent on the outside network conditions(e.g., delay, jitter, throughput, and so on), as well as the streaming strategy (e.g., encoding, transmission protocol, and so on) inside streams, which ultimately impact end users’ IE. To this end, we can further subdivide this category into two classes including network QoS and stream-related IFs. QoS: low latency high throughput high reliability temporal synchronization among heterogeneous streams stream-related IFs the form of data compression strategy resource scheduling scheme ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:4:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"User-aware IFs IE may be influenced by human users while human users can perceive IE, for which we can subdivide this category into three classes based on such correlations. User profile Physiological IFs Psychological IFs It is obvious that users with diverse user profiles have distinctive influences on IE. The psychology and physiology of users can highly reflect the IE for the application. For psychological IFs, they are able to directly demonstrate a user’s positive or negative feedback for interactive network services. However, this can hardly be simply measured. For physiological IFs, some of them(e.g., heart rate, blood pressure) can be objectively measured by affordable medical sensors. ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:5:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Device-aware IFs With regard to device-aware IFs, two broad classes can be gotten according to internal systems(e.g., CPU) and external specifications(e.g., screen size, FOV) of the device. IE management in the device level mainly lies in two aspects. The selection of terminal type(e.g., mobile phone, laptop, VR/AR glasses) The corresponding possession of hardware(e.g., CPU, GPU, battery). ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:6:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Context-aware IFs Typically, IE for interactive network services is generated by interacting with the virtual environment. To this end, we can derive two primary classes. Virtual context: focuses on the specific virtual application scenario. Physical context: focuses on its surrounding physical environment. We can provide constructive suggestions for different contexts. For example, online virtual games are appropriate to play outside for the broad horizon, but watching a 3D film is more proper inside the home. We can suggest appropriate application types with different technical requirement to guarantee users’ IE according to existing network resources and the surrounding environment. Light-weight IE evaluation We proposed two light-weight IE evaluation approaches by respectively exploiting the AI technology and exploring the mathematical relationship among IFs and IE, which are appropriate for different cases according to the data amount. ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:7:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"AI-based Existing popular studies focusing on DL-based models(e.g., DNNs, LSTMs) can hardly satisfy the stringent delay requirement. We employ a multi-view learning combining with lightweight ML methods(e.g., SVM, decision tree) for fast and accurate IE evaluation. The raw data through multi-view learning is first represented by multiple feature extractors according to their heterogeneous properties. Each modality is regraded as a particular view for multi-modal applications. Motivations are: It can provide efficient dimension reduction via subspace mapping. Subspace learning-based approaches can map the high-dimensional raw data to a latent subspace, in which its dimensionality is lower than that of raw data. Multi-view learning is more applicable to the IE context with abundant infomation, which can overcome the weakness of ML-based methods regarding evaluation accuracy for interactive network services. Multi-view learning can take full advantage of the associated and complementary features from redundant views for evaluation performance improvement. ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:8:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Statistical function-based AI-based approach may achieve better evaluation performances under large amounts of data, they lack strong interpretability and cannot explicitly explain the inherent relations among IFs and IE. We introduced statistical function-based approach to analyze the mathematical relationship among IFs and IF under limited data. Existing statistical function-based approaches for user experience evaluation are broadly divided into three categories: Exponential model Logarithmic model Linear regression model Notably, in order to further improve evaluation performance for interactive network services via statistical function-based approaches, two fundamental and significant issues need to be concerned as follows: How to comprehensively explore diverse and various IFs for accurate IE evaluation? How to conduct an efficient dimension reduction method for fast IE evaluation? Case study ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:9:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Multi-view generation We can construct multiple views from expert prior knowledge or via the random subspace method, which is a random sampling algorithm for automatic feature set partitioning. Here we partition multi-modal data into three specific views according to different modalites.(e.g., audio, video, and haptic signals). ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:10:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"View combination Then we adopt subspace learning-based approaches to obtain an appropriate subspace from the above-mentioned multiple views. Importantly, canonical correlation analysis in subspace learning plays a significant role in dimension reduction, and outputs the optimal projection for each view. ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:11:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"IE evaluation Finally, based on the optimal and combined projection subspace, decision tree is deployed here to evaluation IE. The key point is find a general and robust evaluation approach: $$ f: X \\rarr Y $$ Result is: $$ Y = X^{\\top} {\\beta} + {\\epsilon} $$ ${\\epsilon}$ is the noise, ${\\beta}$ can be considered as influencing degree of various IFs to the IE. IE evaluation for multi-modal applications must satisfy more stringent delay requirements in the context of higher-dimensional data. So we apply the LASSO estimation, which is equipped with sparse solutions for the linear regression model, is incorporated to alleviate the issue of high-dimensional data for fast IE evaluation. Dataset: VisTouch Compare obejcts: Ridge regression Exponential model Performance metric: MAE Test result: ","date":"2022-03-09","objectID":"/posts/papers/note-for-toward-immersive-experience/:11:1","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/papers/note-for-toward-immersive-experience/"},{"categories":["development"],"content":"Overview MLflow是一个用于管理机器学习全生命周期的框架。 其主要的作用是： 完成训练和测试过程中不同超参数的结果的记录、对比和可视化——MLflow Tracking 以一种可复现重用的方式包装 ML 代码——MLflow Projects 简化模型部署的难度——MLflow Models 提供中心化的模型存储来管理全生命周期——MLflow Model Registry 现在主要用到的是第三个，所以先记录Models的用法 MLflow Models MLflow Models本质上是一种格式，用来将机器学习模型包装好之后为下游的工具所用。 这种格式定义了一种惯例来让我们以不同的flavor保存模型进而可以被下游工具所理解。 ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:0:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"存储格式 每个MLflow Model是一个包含任意文件的目录，根目录之下有一个MLmodel文件，用于定义多个flavor。 flavor是MLflow Model的关键概念，抽象上是部署工具可以用来理解模型的一种约定。 MLflow定义了其所有内置部署工具都支持的几种标准flavor，比如描述如何将模型作为Python函数运行的python_function flavor。 目录结构示例如下： MLmode文件内容示例如下： 这个模型可以用于任何支持pytorch或python_function flavor的工具，例如可以使用如下的命令用python_function来 serve 一个有python_function flavor的模型： mlflow models serve -m my_model ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:1:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"Model Signature 模型的输入输出要么是column-based，要么是tensor-based。 column-based inputs and outputs can be described as a sequence of (optionally) named columns with type specified as one of the MLflow data type. tensor-based inputs and outputs can be described as a sequence of (optionally) named tensors with type specified as one of the numpy data type. ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:2:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"Signature Enforcement Schema enforcement checks the provided input against the model’s signature and raises an exception if the input is not compatible. It only works when using MLflow model deployment tools or loading models as python_function. It has no impact on native model. Name Ordering Enforcement The input names are checked against the model signature. If there are any missing inputs, MLflow will raise an exception. Extra inputs will be ignored. Prioritized method is matching by name if provided in input schema, then according to position. Input Type Enforcement For column-based signatures, MLflow will perform safe type conversions if necessary. Only lossless conversions are allowed. For tensor-based signatures, type checking is strict(any dismatch will throw an exception). Handling Integers With Missing Values Integer data with missing values is typically represented as floats in Python. Best way is to declare integer columns as doubles whenever there can be missing values. Handling Data and Timestamp Python has precision built into the type for datatime values. Datetime precision is ignored for column-based model signature but is enforced for tensor-based signatures. ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:2:1","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"Log Models with Signatures Pass signature object as an argument to the appropriate log_model call to include a signature with model. The model signature object can be created by hand or inferred from datasets with valid model inputs and valid model outputs. Column-based example The following example demonstrates how to store a model signature for a simple classifier trained on the Iris dataset: import pandas as pd from sklearn import datasets from sklearn.ensemble import RandomForestClassifier import mlflow import mlflow.sklearn from mlflow.models.signature import infer_signature iris = datasets.load_iris() iris_train = pd.DataFrame(iris.data, columns=iris.feature_names) clf = RandomForestClassifier(max_depth=7, random_state=0) clf.fit(iris_train, iris.target) signature = infer_signature(iris_train, clf.predict(iris_train)) mlflow.sklearn.log_model(clf, \"iris_rf\", signature=signature) The same signature can be created explicitly as follows: from mlflow.models.signature import ModelSignature from mlflow.types.schema import Schema, ColSpec input_schema = Schema([ ColSpec(\"double\", \"sepal length (cm)\"), ColSpec(\"double\", \"sepal width (cm)\"), ColSpec(\"double\", \"petal length (cm)\"), ColSpec(\"double\", \"petal width (cm)\"), ]) output_schema = Schema([ColSpec(\"long\")]) signature = ModelSignature(inputs=input_schema, outputs=output_schema) Tensor-based example from keras.datasets import mnist from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten from keras.optimizers import SGD import mlflow import mlflow.keras from mlflow.models.signature import infer_signature (train_X, train_Y), (test_X, test_Y) = mnist.load_data() trainX = train_X.reshape((train_X.shape[0], 28, 28, 1)) testX = test_X.reshape((test_X.shape[0], 28, 28, 1)) trainY = to_categorical(train_Y) testY = to_categorical(test_Y) model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1))) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(100, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(10, activation='softmax')) opt = SGD(lr=0.01, momentum=0.9) model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY)) signature = infer_signature(testX, model.predict(testX)) mlflow.keras.log_model(model, \"mnist_cnn\", signature=signature) The same signature can be created explicitly as follows: import numpy as np from mlflow.models.signature import ModelSignature from mlflow.types.schema import Schema, TensorSpec input_schema = Schema([ TensorSpec(np.dtype(np.uint8), (-1, 28, 28, 1)), ]) output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 10))]) signature = ModelSignature(inputs=input_schema, outputs=output_schema) ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:2:2","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"Model Input Example Model inputs can be column-based (i.e DataFrame) or tensor-based (i.e numpy.ndarrays). A model input example provides an instance of a valid model input which can be stored as separate artifact and is referenced in the MLmodel file. ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:3:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"Log Model with column-based example An example can be a single record or a batch of records. The sample input can be passed in as a Pandas DataFrame, list or dict. The given example will be converted to a Pandas DataFrame and then serialized to json using the Pandas split-oriented format. input_example = { \"sepal length (cm)\": 5.1, \"sepal width (cm)\": 3.5, \"petal length (cm)\": 1.4, \"petal width (cm)\": 0.2 } mlflow.sklearn.log_model(..., input_example=input_example) ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:3:1","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"Log Model with Tensor-based example An example must be a batch of inputs. The axis 0 is the batch axis by default unless specified otherwise in the model signature. The sample input can be passed in as a numpy ndarray or a dict mapping a string to a numpy array. # each input has shape (4, 4) input_example = np.array([ [[ 0, 0, 0, 0], [ 0, 134, 25, 56], [253, 242, 195, 6], [ 0, 93, 82, 82]], [[ 0, 23, 46, 0], [ 33, 13, 36, 166], [ 76, 75, 0, 255], [ 33, 44, 11, 82]] ], dtype=np.uint8) mlflow.keras.log_model(..., input_example=input_example) ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:3:2","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"Model API MLflow includes integrations with several common libraries. For example, mlflow.sklearn contains save_model, log_model, and load_model functions for scikit-learn models. Additionally, we can use mlflow.models.Model class to create and write models which has 4 key functions: add_flavor to add a flavor to the model. Each flavor has a string name and a dict of key-value attributes, where the values can be any object that can be serialized to YAML. save to save the model to a local directory. log to log the model as an artifact in the current run using MLflow tracking. load to load a model from a local directory or from an artifact in a previous run. ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:4:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["development"],"content":"Pytorch mlflow.pytorch module defines utilities for saving and loading MLflow Models with the pytorch flavor. We can use mlflow.pytorch.save_model() and mlflow.pytorch.log_model() methods to save pytorch models in MLflow format. We can use mlflow.pytorch.load_mode() to load MLflow Models with pytorch flavor as pytorch model objects. This loaded PyFunc model can be scored with both DataFrame input and numpy array input. ","date":"2022-03-07","objectID":"/posts/development/note-for-mlflow/:4:1","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/development/note-for-mlflow/"},{"categories":["knowledge"],"content":"Context Create an HTML5 canvas Get the canvas id Obtain WebGL Context The parameter WebGLContextAttributes is not mandatory. Attributes Description Default value alpha true: provide an alpha buffer to the canvas; true depth true: drawing buffer contains a depth buffer of at least 16 bits; true stencil true: drawing buffer contains a stencil buffer of at least 8 bits; false antialias true: drawing buffer performs anti-aliasing true premultipliedAlpha true: drawing buffer contains colors with pre-multiplied alpha true preserveDrawingBuffer true: buffers will not be cleared and will preserve their values until cleared or overwritten by the author false let canvas = document.getElementById(\"my_canvas\"); let context = canvas.getContext(\"webgl\", { antialias: false, stencil: true }); Geometry ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:0:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Definition A 2D or 3D model drawn using vertices is call a mesh. Each facet in a mesh is called a polygon and a polygon is made of 3 or more vertices. // create a 2D triangle which lies on the coordinates {(-5, -5), (5, -5), (5, 5)} let vertices = [ -0.5, -0.5 // Vertex 0 0.5, -0.5, // Vertex 1 0.5, 0.5, // Vertex 2 ]; Similarly, we can create an array for the indices follow the sequence. let indices = [0, 1, 2]; drawArrays(): pass the vertices of the primitive using JavaScript arrays. drawElements(): pass both vertices and indices of the primitive using JavaScript arrays. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:1:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Buffer Objects A buffer object indicates a memory area allocated in GPU. We can store data of the models corresponding to vertices, indices, color and etc. There are 2 types of buffer objects: Vertex Buffer Object(VBO): It holds the per-vertex data of the graphical model that is going to be rendered. Index Buffer Object(IBO): It holds the indices of the graphical model that is going to be rendered. After defining the required geometry and storing them in JavaScript arrays, we need to pass these arrays to the buffer objects, from where the data will be passed to the shader programs. Create an empty buffer. let vertex_buffer = gl.createBuffer(); let index_buffer = gl.createBuffer(); Bind an appropriate array object to the empty buffer. void bindBuffer(enum target, Object buffer); // ARRAY_BUFFER represents vertex data gl.bindBuffer(gl.ARRAY_BUFFER, vertex_buffer); // ELEMENT_ARRAY_BUFFER represent index data gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, index_buffer); Pass the data (vertices/indices) to the buffer using one of the typed arrays. void bufferData(enum target, Object data, enum usage); // Usage specifies how to use the buffer object data to draw shapes // gl.STATIC_DRAW -- Data will be specified once and used many times. // gl.STREAM_DRAW -- Data will be specified once and used a few times. // gl.DYNAMIC_DRAW -- Data will be specified repeatedly and used many times. // vertex buffer gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW); // index buffer gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(indices), gl.STATIC_DRAW); Unbind the buffer (Optional/Recommended). gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, null); Shader Shaders are written in ES SL which has variables of its own data types, qualifiers, built-in inputs and outputs. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:2:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Data Types Type Description void empty value bool true or false int signed integer float floating scalar vec2, vec3, vec4 n-component floating point vector bvec2, bvec3, bvec4 boolean vector ivec2, ivec3, ivec4 signed integer vector mat2, mat3, mat4 2x2, 3x3, 4x4 float matrix sampler2D access a 2D texture samplerCube access cube mapped texture ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:3:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Qualifiers Qualifier Description attribute acts as a link between a vertex shader and OpenGL ES for per-vertex data. Its value changes for every execution of the vertex shader. uniform links shader programs and the WebGL application. Its value is read-only. It can be used for to declare a variable with any basic data types: uniform vec4 lightPosition;. varying forms a link between a vertex shader and fragment shader for interpolated data. It can be used with the following data types: float, vec2, vec3, vec4, mat2, mat3, mat4, arrays like: varying vec3 normal; ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:4:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Vertex Shader Vertex shader is a program code, which is called on every vertex. Programmer have to define attribute in code of vertex shader to handle data. The attribute point to a VBO written in JavaScript. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:5:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Predefined Variables OpenGL ES SL provides the following predefined variables for every vertex shader Variables Description highp vec4 gl_Position Holds the position of the vertex mediump float gl_PointSize Holds the transformed point size attribute vec2 coordinates; void main(void) { gl_Position = vec4(coordinates, 0.0, 1.0); } gl_Position is the predefined variable which is available only in the vertex shader. It contains the vertex position. As vertex shader is a per-vertex operation, the gl_Position value is calculated for each vertex. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:5:1","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Fragment Shader A mesh is formed by multiple triangles, and the surface of each triangle is known as a fragment. Fragment shader is the code that runs on every pixel on each fragment. This is written to calculate and fill the color on individual pixels. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:6:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Predefined Variables Variables Description mediump vec4 gl_FragCoord; Holds the fragment position within the frame buffer bool gl_FrontFacing; Holds the fragment that belongs to a front-facing primitive mediump vec2 gl_PointCoord; Holds the fragment position within a point mediump vec4 gp_FragColor; Holds the output fragment color value of the shader mediump vec4 gl_FragData[n]; Holds the fragment color for color attachment n void main(void) { gl_FragColor = vec4(0, 0.8, 0, 1); } ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:6:1","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Store and Compiling let vertCode = \"attribute vec2 coordinates;\" + \"void main(void) {\" + \"gl_Postion = vec4(coordinates, 0.0, 1.0);\" + \"}\"; let fragCode = \"void main(void) {\" + \"gl_FragColor = vec4(0, 0.8, 0, 1);\" + \"}\"; Compilation involves following 3 steps Creating the shader object Attaching the source code to the created shader object Compiling the program let vertShader = gl.createShader(gl.VERTEX_SHADER); gl.shaderSource(vertShader, vertCode); gl.compileShader(vertShader); Same process for fragment shader let fragShader = gl.createShader(gl.FRAGMENT_SHADER); gl.shaderSource(fragShader, fragCode); gl.compileShader(fragShader); ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:7:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Combined Program Create a program object Attach both the shaders Link both the shaders Use the program let shaderProgram = gl.createProgram(); gl.attachShader(shaderProgram, vertShader); gl.attachShader(shaderProgram, fragShader); gl.linkProgram(shaderProgram); gl.useProgram(shaderProgram); Associating Attributes \u0026 Buffer Objects Get the attribute location Point the attributes to a vertex buffer object Enable the attribute // ulong getAttribLocation(Object program, string name) let coordinatesVar = gl.getAttribLocation(shaderProgram, \"coordinates\"); // void vertexAttribPointer(location, int size, enum type, bool normalized, long stride, long offset) gl.vertexAttribPointer(coordinatesVar, 3, gl.FLOAT, false, 0, 0); gl.enableVertexAttribArray(coordinatesVar); Drawing a Model ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:8:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"drawArrays() void drawArrays(enum mode, int first, long count); mode: gl.POINTS, gl.LINE_STRIP, gl.LINE_LOOP, gl.LINES, gl.TRIANGLE_STRIP, gl.TRANGLE_FAN, gl.TRIANGLES. first: specified the starting element in the enabled arrays. (Non-negative) count: specifies the number of elements to be rendered. WebGL will create the geometry in the order in which the vertex coordinates while rendering the shapes. draw a triangle: let vertices = [-0.5, -0.5, -0.25, 0.5, 0.0, -0.5]; gl.drawArrays(gl.TRIANGLES, 0, 3); draw two contiguous triangles: let vertices = [ -0.5, -0.5, -0.25, 0.5, 0.0, -0.5, 0.0, -0.5, 0.25, 0.5, 0.5, -0.5, ]; gl.drawArrays(gl.TRIANGLES, 0, 6); ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:9:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"drawElements() void drawElements(enum mode, long count, enum type, long offset); mode: same as drawArrays(); count: same as drawArrays(); type: specifies the data type of the indices which must be UNSIGNED_BYTE or UNSIGNED_SHORT; offset: specifies the starting point for rendering, usually the first element (0); If use drawElements() to draw a model, then index buffer object should also be created along with the vertex buffer object. The vertex data will be processed once and used as many time as mentioned in the indices. draw a triangle: let vertices = [-0.5, -0.5, 0.0, -0.25, 0.5, 0.0, 0.0, -0.5, 0.0]; let indices = [0, 1, 2]; gl.drawElements(gl.TRIANGLES, indices.length, gl.UNSIGNED_SHORT, 0); draw two contagious triangles: let vertices = [ -0.5, -0.5, 0.0, -0.25, 0.5, 0.0, 0.0, -0.5, 0.0, 0.25, 0.5, 0.0, 0.5, -0.5, 0.0, ]; let indices = [0, 1, 2, 2, 3, 4]; gl.drawElements(gl.TRIANGLES, indices.length, gl.UNSIGNED_SHORT, 0); ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples-explanation/:10:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/knowledge/webgl/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Structure of WebGL Application WebGL application code is a combination of JavaScript and OpenGL Shader Language. JavaScript is required to communicate with the CPU. OpenGL Shader Language is required to communicate with the GPU. Samples ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples/:0:0","tags":["WebGL"],"title":"WebGL 样例","uri":"/posts/knowledge/webgl/webgl-samples/"},{"categories":["knowledge"],"content":"2D coordinates \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003cbody\u003e \u003ccanvas width=\"300\" height=\"300\" id=\"my_canvas\"\u003e\u003c/canvas\u003e \u003cscript\u003e // 1. Prepare the canvas and get context let canvas = document.getElementById(\"my_canvas\"); let gl = canvas.getContext(\"experimental-webgl\"); // 2. Define the geometry and store it in buffer objects let vertices = [ -0.5, 0.5, // vertex 1 -0.5, -0.5, // 0.0, -0.5, ]; // Create buffer object let vertex_buffer = gl.createBuffer(); // Bind an empty array buffer to it gl.bindBuffer(gl.ARRAY_BUFFER, vertex_buffer); // Pass the vertices data to the buffer gl.bufferData( gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW ); // Unbind the buffer gl.bindBuffer(gl.ARRAY_BUFFER, null); // 3. Create and compile Shader programs // Vertex shader source code let vertCode = \"attribute vec2 coordinates;\" + \"void main(void) {\" + \" gl_Position = vec4(coordinates, 0.0, 1.0);\" + \"}\"; // Create a vertex shader object let vertShader = gl.createShader(gl.VERTEX_SHADER); // Attach vertex shader source code gl.shaderSource(vertShader, vertCode); // Compile the vertex shader gl.compileShader(vertShader); // Fragment shader source code let fragCode = \"void main(void) {\" + \"gl_FragColor = vec4(0.0, 0.0, 0.0, 0.1);\" + \"}\"; let fragShader = gl.createShader(gl.FRAGMENT_SHADER); gl.shaderSource(fragShader, fragCode); gl.compileShader(fragShader); // Create a shader program object to store combined shader program let shaderProgram = gl.createProgram(); // Attact vertex and fragment shader gl.attachShader(shaderProgram, vertShader); gl.attachShader(shaderProgram, fragShader); // Link both programs gl.linkProgram(shaderProgram); // Use the combined shader program object gl.useProgram(shaderProgram); // 4. Associate the shader programs to buffer objects // Bind vertex buffer object gl.bindBuffer(gl.ARRAY_BUFFER, vertex_buffer); // Get the attribute location let coord = gl.getAttribLocation(shaderProgram, \"coordinates\"); // Point an attribute to the currently bound VBO gl.vertexAttribPointer(coord, 2, gl.FLOAT, false, 0, 0); // Enable the attribute gl.enableVertexAttribArray(coord); // 5. Drawing the required object (triangle) // Clear the canvas gl.clearColor(0.5, 0.5, 0.5, 0.9); // Enable the depth test gl.enable(gl.DEPTH_TEST); // Clear the color buffer bit gl.clear(gl.COLOR_BUFFER_BIT); // Set the view port gl.viewport(0, 0, canvas.width, canvas.height); // Draw the triangle gl.drawArrays(gl.TRIANGLES, 0, 3); \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-samples/:1:0","tags":["WebGL"],"title":"WebGL 样例","uri":"/posts/knowledge/webgl/webgl-samples/"},{"categories":["knowledge"],"content":"Overview JavaScript JavaScript is used to write the control code of the program, which includes the following actions: Initialization: initialize WebGL context. Arrays: create arrays to hold the data of the geometry. Buffer objects: create buffer objects by passing the arrays as parameters. Shaders: create, compile and link the shaders. Attributes: create attributes, enable them and associate them with buffer objects. Uniforms: associate the uniforms. Transformation matrix: create transformation matrix. Vertex Shader The vertex shader is executed for each vertex provided in the vertex buffer object when start the rendering process by invoking the methods drawElements() and drawArrays(). It calculates the position of each vertex of a primitive polygon and stores it in the varying gl_position It calculates the other attributes such as color, texture coordinates and vertices that are normally associated with a vertex. Primitive Assembly Here the triangles are assembled and passed to the rasterizer. Resterization The pixels in the final image of the primitive are determined. Culling: Initially the orientation of the polygons is determined. All those triangles with improper orientation that are not visible in view area are discarded. Clipping: If a triangle is partly outside the view area, then the part outside the view area is removed. Fragment Shader The fragment shader gets: data from the vertex shader in varying variables primitives from the rasterization stage then: calculates the color value for each pixel between the vertices stores the color values of every pixel in each fragment Fragment Operations The fragment operations may include: Depth Color buffer blend Dithering Once all the fragments are processed, a 2D image is formed and displayed on the screen. Frame Buffer Frame buffer is the final destination of the rendering pipeline. Frame buffer is a portion of graphics memory that hold the scene data. This buffer contains details such as width and height of the surface (in pixels), color of each pixel and depth and stencil buffers. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-pipeline/:0:0","tags":["WebGL"],"title":"WebGL 中的管线","uri":"/posts/knowledge/webgl/webgl-pipeline/"},{"categories":["knowledge"],"content":"Coordinate System There are x, y, z axes in WebGL, where the z axis signifies depth. The coordinates in WebGL are restricted to (1, 1, 1) and (-1, -1, -1). Positive value meaning: z: near viewer. x: near right. y: near top. Graphics System ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:0:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"Vertices To draw a polygon, we need to mark the points on the plane and join them to form a desired polygon. A vertex is a point which defines the conjunction of the edges of a 3D object. Use javascript arrays to stores points’ coordinates like [0.5, 0.5, 0.5]. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:1:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"Indices The numerical values which are used to identify the vertices is call Indices. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:2:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"Arrays There are no predefined methods in WebGL to render the vertices directly. let vertices = [0.5, 0.5, 0.1, -0.5, 0.5, -0.5]; ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:3:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"Buffers Buffers are the memory areas of WebGL that hold the data. There are various buffers: drawing buffer frame buffer vertex buffer index buffer The vertex buffer and index buffer are used to describe and process the geometry of the model, stores data about vertices and indices respectively. The frame buffer is a portion of graphics memory that hold the scene data. This buffer contains details such as width and height of the surface (in pixels), color of each pixel, depth and stencil buffers. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:4:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"Mesh The WebGL API provides two methods to draw 2D or 3D objects: drawArrays() drawElements() They accept a parameter called mode using which you can select the object you want to draw. mode: points or lines or triangles We can construct primitive polygons using points, lines and triangles. Thereafter, we can form a mesh using these polygons. A 3D object drawn using primitive polygons is called a mesh. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:5:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"Shader Programs Since WebGL uses GPU accelerated computing, the information about these triangles should be transferred from CPU to GPU which takes a lot of communication overhead. WebGL provides a solution to reduce the communication overhead. Since it uses ES SL(Embedded System Shader Language) that runs on GPU, we write all the required programs to draw graphical elements on the client system using shader programs(OpenGL ES Shader Language). Shader is a snippet that implements algorithms to get pixels for a mesh. There are two types of shaders: Vertex Shader and Fragment Shader. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:6:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"Vertex Shader called on every vertex. used to transform the geometry from one place to another. handle the data of each vertex such as vertex coordinates, normals, colors, and texture coordinates. vertex transformation normal transformation and normalization texture coordinate generation texture coordinate transformation lighting color material application ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:6:1","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"Fragment Shader(Pixel Shader) A mesh is formed by multiple triangles. The surface of each of the triangles is known as a fragment. Fragment shader is the code that runs on all pixels of every fragment. It is written to calculate and fill the color on individual pixels. operations on interpolated values texture access texture application fog color sum ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:6:2","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["knowledge"],"content":"OpenGL ES SL Variables To handle the data in the shader programs, ES SL provides three types of variables. Attributes: hold the input values of the vertex shader program. Attributes point to the vertex buffer objects that contains per-vertex data. Uniforms: hold the input data that is common for both vertex and fragment shaders, such as light position, texture coordinates and color. Varyings: used to pass the data from the vertex shader to the fragment shader. ","date":"2022-03-03","objectID":"/posts/knowledge/webgl/webgl-basics/:7:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/knowledge/webgl/webgl-basics/"},{"categories":["paper"],"content":"Bitrate Adaptation Schemes ","date":"2022-02-27","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:0:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Client-based Recently, most of the proposed bitrate adaptation schemes reside at the client side, according to the specifications in the DASH standard. Purposes: Minimal rebuffering events when the playback buffer depletes. Minimal startup delay especially in case of live video streaming. A high overall playback bitrate level with respect to network resources. Minimal video quality oscillations, which occur due to frequent switching. ","date":"2022-02-27","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Available bandwidth-based The client makes its representation decisions based on the measured available network bandwidth, which is usually calculated as the size of the fetched segment(s) divided by the transfer time. This scheme suffers from poor QoE due to a lack of a reliable bandwidth estimation methods, which results in frequent buffer underruns. General context Based on segment fetch time(SFT) measures the time starting from sending the HTTP GET request to receiving the last byte of the segment. Sequential and parallel segment fetching method in CDNs, by using metric that compares the expected segment fetch time(ESFT) with the measured SFT to determine if the selected segment bitrate matches the network capacity. Based on the bitrate observed for the last segment downloaded and the estimated throughput that was calculated during the previous estimation. Probe AND Adapt tries to eliminate the ON-OFF steady state issue as well as reduce bitrate oscillations when multiple clients share the same bottleneck link. piStream enables clients to estimate bandwidth based on a resource monitor module that act as a physical-layer daemon. SVC with DASH prefetches base layers of future segments or downloads enhancement layers for existing segments using a bandwidth-sloping-based heuristic. Mobile context Static DASH2M uses HTTP/2 server push and stream terminate properties to reduce the battery consumption of the mobile device. Adaptive k-push scheme propose to increase/decrease k according to a bandwidth increase/decrease while keeping in mind the overall power consumption in a push cycle. LOw-LatenceY Prediction-based adaPtation(LOLYPOP) leverages TCP throughput predictions on multiple times scales to achieve low latency and improve QoE. Motive GeoStream: introduce the use of geostatistics to estimate future bandwidth in unknown locations. ","date":"2022-02-27","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:1","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Playback Buffer-Based The client uses the playout buffer occupancy as a criterion to select the next segment bitrate during video playback. This scheme suffers from many limitations including low overall QoE and instability issues, especially in the case of long-term bandwidth fluctuations. SVC-based approaches have limitations related to the complexity of SVC. Base combines the buffer size with a tool-set of client metrics for accurate rate selection and smooth switching. BBA aims to maximize the average video quality and avoid unnecessary rebuffering events, but suffers from QoE degradation during long-term bandwidth fluctuations. BOLA uses online control algorithm that treats bitrate adaptation as a utility maximization problem. Provide strong theorectical proof that it is near optimal, design a QoE model that incorporates both the average playback quality and the rebuffering time. It is implemented and available in the dash.js player. BIEB maximizes video quality based on SVC priority while reducing the number of quality oscillations and avoiding stalls and frequent bitrate switching. it maintains a stable buffer occupancy before increasing the quality. QUEuing Theory approach to DASH Rate Adaptation(QUETRA) allows to calculate the expected buffer occupancy given a bitrate choice, network throughput, and buffer capacity. ","date":"2022-02-27","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:2","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Mixed Adaptation The client makes its bitrate selection based on a combination of metrics including available bandwidth, buffer occupancy, segment size and/or duration. Simple client Control-theoretic based: FastMPC, Another Optimization problem: Streaming video over HTTP with Consistent Quality Towards agile and smooth video adaptation in dynamic HTTP streaming aims to balance bandwidth utilization and smoothness in DASH in both single and multiple CDN(s) scenarois. SQUAD is a lightweight bitrate adaptation algorithm that uses the available bandwidth and buffer information to increase the average bitrate while minimizing the number of quality switches. Multi-path solution for abr in wireless networks avoids the problems of TCP congestion control by using parallel TCP streams. SARA is Segment-Aware Rate Adaptation algorithm based on the segment size variation, the available bandwidth estimate and the buffer occupancy. It extends MPD file to include the size of every segment. ABMA+ selects the highest segment representation based on the estimated probability of video rebuffering. It makes use of buffer maps, which define the playout buffer capacity that is required under certain conditions to satisfy a rebuffering threshold and to avoid heavy online calculations. GTA uses a cooperative game in coalition formation then formulates the bitrate selection problem as a bargaining process and consensus mechanism. GTA improves QoE and video stability without increasing the stall rate or startup delay. Multiple clients ELASTIC generates a long-lived TCP flow and avoids the ON-OFF steady state behavior which leads to bandwidth overestimations. Ensure bandwidth fairness between competing clients based on network feedback assistance, but without taking the QoE into consideration. In addition, it ignores quality oscillations in its bitrate decisions. Adaptation algorithm for HAS uses current buffer occupancy level to estimate available bandwidth and average bitrate of the different bitarte levels from MPD as metrics in its bitrate selection. FESTIVE contains: a bandwidth estimator module a bitrate selection and update method with stateful player a randomized scheduler which incorporates the buffer size to schedule the download of the next segment. TSDASH uses a logarithmic-increase-multiplicative-decrease (LIMD) based bandwidth probing algorithm to estimate the available bandwidth and a dual-threshold buffer for the bitrate adaptation. ","date":"2022-02-27","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:3","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"MDP-Based The video streaming process is formulated as a finite MDP to be able to make adaptation decisions under fluctuating network conditions. This scheme may suffer from instability, unfairness and underutilization when the number of clients increases, probably because such factors are not taken into account in the MDP models and due to clients’ decentralized ON-OFF patterns. Real-time best-action search algorithm over multiple access networks uses both Bluetooth and WiFi links to simultaneously download video segments. However, this scheme shows limitations during user mobility which negatively affect QoE. Optimizing in Vehicular environment introduces a three-variant of RL-based algorithms which take advantage of the historical bandwidth samples to build an accurate bandwidth estimation model. Multi-agent Q-Learning-based for fairness uses a central manager in charge of collecting QoE statistics and coordination between the competing clients. The algorithm ensures a fair QoE distribution and improves QoE while avoiding suboptimal decisions.(without considering stalls and quality switches) Online learning adaptation aims to select the optimal representation and maximize the long-term expected QoE. The reward function is calculated from a combination of quality oscillations, segment quality and stalls experienced by the client. It exploits a parallel learning technique to avoid slow convergence and suboptimal solutions. mDASH aims to improve QoE during long-term bandwidth variations. It takes buffer size, bandwidth conditions and bitrate stability as Markov state variables. Pensive does not rely on pre-programmed models or assumptions about the environment, but gradually learns the best policy for bitrate decisions through observation and experience. D-DASH combines DL and RL to improve QoE, achieves a good trade-off between policy optimality and convergence speed during the decision process. ","date":"2022-02-27","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:4","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Server-Based Server-based schemes use a bitrate shaping method at the server side and do not require any cooperation from the client. The switching between the bitrates is implicitly controlled by the bitrate shaper. The client still makes its own decisions, but the decisions are more or less determined by the shaping method on the server. Traffic shaping analyzes instability and unfairness issues in the presence of multiple HAS players competing for the available bandwidth. This method can be deployed at a home gateway to improve fairness, stability and convergence delay, and to eliminate the OFF periods during the steady states. Tracker-assisted adaptation uses a architecture which consists of clients communicating with a server through a shared proxy and a server having a tracker functionality that manages the clients’ statuses and helps them share knowledge about their statues. Quality Adaptation Controller aims to control the size of the server sending buffer in order to adjust and select the most appropriate bitrate level for each DASH player. It maintains the playback buffer occupancy of each player as stable as possible and to match bitrate level decisions with the available bandwidth. Multi-Source Stream system: the client fetches the segments from multi-source stream servers. Cons: Produce high overhead on the server side with a high complexity These schemes also need modifications to the MPD or a custom server software to implement the bitrate adaptation logic.(a violation of the DASH-standard design principles) ","date":"2022-02-27","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:2:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Paper Overview Link: https://ieeexplore.ieee.org/document/8424813 Level: IEEE Communications Surveys \u0026 Tutorials 2019 Background ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:0:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Traditional non-HAS IP-based streaming The client receives media that is typically pushed by a media server using connection-oriented protocol such as Real-time Messaging Protocol(RTMP/TCP) or connectionless protocol such as Real-time Transport Protocol(RTP/UDP). Real-time Streaming Protocol(RTSP) is a common protocol to control the media servers, which is responsible for setting up a streaming session and keeping the state information during this session, but is not responsible for actual media delivery(task for protocol like RTP). The media server performs rate adaption and data delivery scheduling based on the RTP Control Protocol(RTCP) reports sent by the client. When it comes to NAT and firewall, additional protocols or configurations are needed during the session establishment. The characteristics result in complex and expensive servers. These scalability and vendor dependency issues as well as high maintenance costs have resulted in deployment challenges for protocols like RTSP. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:1:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"HAS Around 2005, HTTP adaptive streaming(HAS) became popular and dominant, which treated the media content like regular Web content and delivered it in small pieces over HTTP protocol. HTTP as application and TCP as the transport-layer protocol. Client pull the data from a standard HTTP server, which simply hosts the media content. HAS solutions employ dynamic adaptation with respect to varying network conditions to provide a seamless streaming experience. The original file/stream is partitioned into segments (also called chunks) of equi-length playback time. Multiple versions(also called representations) of each segment are generated that vary in bitrate/resolution/quality using an encoder or a transcoder. The server generates an index file, which is a manifest that lists the available representations including HTTP urls to identify the segments along with their availability times. The client first receives the manifest that contains the metadata for video, audio, subtitles and other features, then constantly measures certain parameters: available network bandwidth, buffer status, battery and CPU levels, etc. According to these parameters, the HAS client repeatedly fetches the most suitable next segment among the available representations from the server. Advantages: It use HTTP to deliver video segments, which simplifies the traversal through NATs and firewalls. At the server side, it use conventional Web servers or caches available within the networks of ISPs and CDNs. At the client side, it requests and fetches each segment independently from others and maintains the playback session state, whereas the server is not required to maintain any state. It doesn’t require a persistent connection between the client and server, which improves system scalability and reduces implementation and deployment costs. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:2:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Comparison Summary Challenges ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:3:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Multi-Client Competition/Stability Issues A centralized management controller can enhance the overall video quality, while improve QoE. A robust HAS scheme should achieve 3 main objectives: Stability: HAS clients should avoid frequent bitrate switching. Fairness: Multiple HAS clients competing for available bandwidth should equally share network resources based on viewer, content and device characteristics. High Utilization: While the clients attempt to be stable and fair, network resources should be used as efficiently as possible. A streaming session consists of 2 states: buffer-filling state and steady state. The buffer-filling state aims to fill the playback buffer and reach a certain threshold where the playback can be initiated or resumed. The steady state is to keep the buffer level above a minimum threshold despite bandwidth fluctuation or interruptions. The steady state consists of 2 activity periods referred to as ON and OFF. The client requests a segment every $T_s$ time units, where $T_s$ represents the content time duration of each segment, and sum of ON and OFF period durations equals $T_s$. ON period: client downloads the current segment and notes the achieved throughput value that will be later used in selecting the appropriate bitrate for future segments. OFF period: client becomes idle temporarily. There are different cases during competition process. The ON periods of clients don’t overlap during the current segment download, each client will overestimate the available bandwidth. So longer download time will cause the initially non-overlapping ON periods to eventually start overlapping. As the amount of overlap increases, the clients will have lower bandwidth estimations and start selecting segments that have lower bitrate. These segment will take less time to download, causing the amount of overlap among the ON periods to precedurally shorten, until the process reverts to its initial situation. The cycle repeats itself, causing periodic up and down shift in the selected bitrates, leading to unstable video quality, unfairness, and underutilization. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:4:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Consistent-Quality Streaming The correlation between video bitrate and its perceptual quality is non-linear. Different video content types have unique characteristics. Differences of inter-stream and intra-stream video scene complexity across content. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:5:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"QoE Optimization and Measurement HAS scheme uses application control loop, which also interacts with a lower-layer control loop(such as TCP congestion control). It plays a key role in determining the viewer QoE. Factors influencing QoE are categorized as: Perceptual, directly perceived by the viewer. Technical, indirectly affecting the QoE. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:6:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Perceptual Perceptual factors include the video image quality, initial delay, stalling duration and frequency. The impact of these factors differs depending on the users subjectivity. Most users consider initial delays less critical than stalling. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:6:1","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Technical Technical factors include the algorithms, parameters, and hardware/software used in streaming system. Specifically, factors are: Server side: encoding parameters, video qualities and segment size. Client side: adaptation parameters and environment that clients reside in. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:6:2","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"QoE measurement Objective matrics: Peak Signal-to-Noise Ratio(PSNR), Structural SIMilarity(SSIM and SSIMplus), Perceived Video Quality(PVQ) and Statistically Indifferent Quality Variation(SIQV). Subjective matrics: Mean Opinion Score(MOS). Quality-of-Service (QoS)-derived matrics: startup delay, average video bitrate, quality switches and rebuffering events. Try to optimize each metric is difficult because it may result in conflicts. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:6:3","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Inter-Destination Multimedia Synchronization Online communities are drifting towards watching online videos together in a synchronized manner. Having Multiple streaming clients distributed in different geographical locations poses challenges in delivering video content simultaneously, while keeping the playback state of each client the same. Typically, IDMS solutions involve a master node to which clients synchronize their playout to. Rainer et proposed an IDMS architecture for DASH by using a distribute control scheme where peers can communicate and negotiate a reference placback timestamp in each session. In another work, Rainer et provided a crowdsourced subjective evaluation to find a asynchronism threshold at which QoE was not significantly affected. ","date":"2022-02-26","objectID":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:7:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["development"],"content":"最近几天一直在用WebXR的技术重构目前的基于分块的全景视频自适应码率播放客户端，下面简述一下过程。 首先结论是：分块播放+自适应码率+完全的沉浸式场景体验=Impossible（直接使用 WebXR 提供的 API） ","date":"2022-02-25","objectID":"/posts/development/webxr-for-panoramic-video/:0:0","tags":["WebXR"],"title":"使用 WebXR 完成基于分块的全景视频自适应码率播放器","uri":"/posts/development/webxr-for-panoramic-video/"},{"categories":["development"],"content":"分块播放 分块播放的本质是将一整块的全景视频从空间上划分成多个小块，各个小块在时间上与原视频的长度是相同的。 在实际播放的时候需要将各个小块按照原有的空间顺序排列好之后播放，为了避免各个分块播放进度不同的问题，播放时还需要经过统一的时间同步。 对应到 web 端的技术实现就是： 一个分块的视频\u003c-\u003e一个\u003cvideo\u003eh5 元素\u003c-\u003e一个\u003ccanvas\u003eh5 元素 视频的播放过程就是各个分块对应的\u003ccanvas\u003e元素不断重新渲染的过程 各个分块时间同步的实现需要一个基准视频进行对齐，大体上的原理如下： let baseVideo = null; let videos = []; initBaseVideo(); initVideos(); for (video in videos) { video.currentTime = baseVideo.currentTime; } ","date":"2022-02-25","objectID":"/posts/development/webxr-for-panoramic-video/:1:0","tags":["WebXR"],"title":"使用 WebXR 完成基于分块的全景视频自适应码率播放器","uri":"/posts/development/webxr-for-panoramic-video/"},{"categories":["development"],"content":"自适应码率 自适应码率的方案使用dashjs库实现，即对每个分块\u003cvideo\u003e元素的播放都用dashjs的方案控制： import { MediaPlayer } from \"dashjs\"; let videos = []; let dashs = []; let mpdUrls = []; initVideos(); initMpdUrls(); for (let i = 0; i \u003c tileNum; i++) { let video = videos[i]; let dash = MediaPlayer().create(); dash.initialize(video, mpdUrls[i], true); dash.updateSettings(dashSettings); dashs.push(dash); } 通过对dashSettings的调整的可以设置各种可用的 dash 参数如不同质量版本下的缓冲区长度，播放暂停时是否终止后台下载等。 ","date":"2022-02-25","objectID":"/posts/development/webxr-for-panoramic-video/:2:0","tags":["WebXR"],"title":"使用 WebXR 完成基于分块的全景视频自适应码率播放器","uri":"/posts/development/webxr-for-panoramic-video/"},{"categories":["development"],"content":"沉浸式场景体验 全景视频的完全的沉浸式体验目前在Oculus Browser上有两种实现方式： 直接使用浏览器默认的全屏功能之后选择视频为：普通视频或 180 度视频或 360 度视频。 使用最新的WebXR session的layers特性，手动代码实现。 第 1 种方式因为并没有给出实际的API，所以不可能与分块传输的视频相结合，所以只能使用第 2 种方式手动实现。 其对应的草案标准地址：https://www.w3.org/TR/webxrlayers-1/ 可以看到目前最新的开发标准刚在 1 个月前完成。 WebXR中的开发流程如下： 判断浏览器是否支持immersive-vr，如果支持就请求xrSession，所需的特性为layers： import { WebXRButton } from \"webxr-button.js\"; let xrButton = new WebXRButton({ onRequestSession: onRequestSession, onEndSession: onEndSession, }); let xrSession = null; function onRequestSession() { if (!xrSession) { navigator.xr .requestSession(\"immersive-vr\", { requiredFeatures: [\"layers\"], }) .then(onSessionStarted); } else { onEndSession(); } } function onEndSession() { xrSession.end(); } if (navigator.xr) { navigator.xr.isSessionSupported(\"immersive-vr\").then((supported) =\u003e { if (supported) { xrButton.enabled = supported; } }); } 获取到需要的xrSession之后请求ReferenceSpace，并创建会话中需要的对象，之后用创建的图层更新会话的渲染器状态，并设置requestAnimationFrame需要的回调函数： let xrRefSpace = null; let xrMediaFactory = null; function onSessionStarted(session) { xrSession = session; xrButton.textContent = \"Exit XR\"; xrMediaFactory = new XRMediaBinding(session); session.requestReferenceSpace('local').then((refSpace) = { xrRefSpace = refSpace; let baseLayer = xrMediaFactory.createEquirectLayer(baseVideo, { space: refSpace, centralHorizontalAngle: Math.PI * 2 }); session.updateRenderState({layers: [baseLayer]}); session.requestAnimationFrame(onXRFrame); }); } 最后设置每次xrSession要求渲染新帧的函数，并设定渲染循环： let xrViewerPose = null; function onXRFrame(time, frame) { let session = frame.session; session.requestAnimationFrame(onXRFrame); xrViewerPose = frame.getViewerPose(xrRefSpace); console.log(xrViewerPose); } onXRFrame函数在每次渲染新帧时调用，其中每帧对应的观看者的相对位置以及头戴设备的线速度和角速度等变量可以从xrViewerPose中取得。 这么看WebXR的完全沉浸式体验是可行的，但是问题出在需要与分块结合。 xrMediaFactory作为XRMediaBinding绑定到当前xrSession的实例对象，可以用来创建采用等距长方形投影的方式的图层XREquirectLayer： 虽然这里出现了可以创建采用Equirectangular方式投影的图层，并可以通过指定其初始化参数完成不同大小的偏移创建，但是这里的处理方式还是将一个完整视频从映射到球面上的方式，即不管怎么改变参数，创建出来的总是有 4 条曲边的球面块： 并不能实现每个分块以特定的映射逻辑将其不重不漏的铺到球面上的功能。 不过就算可以实现这样的功能，因为 1 个图层与 1 个视频块相绑定，在实际创建中发现： 在一个xrSession中最多只能创建 16 个图层，并不能与MxN的分块逻辑相对应； 创建 16 个图层之后整个xrSession会变得异常卡顿，视频已无法正常播放； 那么是否可以先将多个分块的视频从空间上拼接好，将最终拼接好的视频进行等距长方投影？ 首先从实际的实现上没法完成，因为每个视频在 h5 中本质是\u003cvideo\u003e元素，多个\u003cvideo\u003e元素并不能在DOM的基础上实现空间的复原，就算有办法做到，最后在与layer绑定时也必须是 1 个\u003cvideo\u003e元素而这 1 个\u003cvideo\u003e元素还需要实现各个部分的自适应码率变化，这完全是不可行的。 测试的代码地址：media-layer-sample 进一步的解决办法是存在的： 因为目前的WebXR不能够满足需求，所以需要深入WebGL的层面，手动设计一套将各个分块以等距长方投影的方式映射到球面上的逻辑，同时还要与WebXR上层的处理 API 相对应，任务工作量和难度还需要进一步评估。 ","date":"2022-02-25","objectID":"/posts/development/webxr-for-panoramic-video/:3:0","tags":["WebXR"],"title":"使用 WebXR 完成基于分块的全景视频自适应码率播放器","uri":"/posts/development/webxr-for-panoramic-video/"},{"categories":["development"],"content":" 远程启动jupyter notebool： jupyter notebook --no-browser --ip=\"\u003cserver-ip\u003e\" --port=\"\u003cserver-port\u003e\" 激活预先配置好的conda环境，这里假设环境名为keras-tf-2.1.0： conda activate keras-tf-2.1.0 安装ipykernel： pip3 install ipykernel --user 为ipykernel安装环境： python3 -m ipykernel install --user --name=keras-tf-2.1.0 打开notebook更改服务之后刷新即可： ","date":"2022-02-15","objectID":"/posts/development/use-jupyter-notebook-in-conda-env/:0:0","tags":["Conda","Jupyter"],"title":"在 Jupyter Notebook 中设置 Conda 环境","uri":"/posts/development/use-jupyter-notebook-in-conda-env/"},{"categories":["paper"],"content":"LiveObj LiveDeep方法利用卷积层从视频内容中提取深层特征，不受动态背景的影响。然而在整个推流会话中需要更新一个带有大量权重的巨大的神经网络模型。同时因为没有历史视频和用户的轨迹的数据，模型需要在运行时从随机权重开始训练。而这会导致两个问题： 模型需要花很长时间从一次预测错误中恢复； 在初始化的阶段预测率成功率很低； 为了解决这两个问题，提出预训练的模型来分析视频内容，对视频的语义进行层次化。 基于对内容的分析，进一步设计了一个轻量级用户模型，将用户偏好映射到不同的视频内容。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:0:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"用户观看行为分析 在直播推流中，不能通过分析其他用户的行为模式来得到特定用户的ROI，因此只能直接从视频内容本身入手。 通过对视频内容从空间和时间两个维度的分析得出结论：用户的ROI与物体的大小无关，而是很大程度上依赖于物体在视频中的语义，即用户倾向于观看有意义的事物。 这一结论可以给出推断FoV的直觉：基于检测视频中有意义的物体。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:1:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Methods 首先提出两种直观的通过分析视频内容的视点预测方法，进一步总结这些方法的局限性，并逐步切换到对LiveObj的讨论。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:2:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Basic method Basic方法检测视频中所有的对象并使用其中心作为预测的中心。 给出每个帧中的 $k$ 个物体， $\\vec{O} = [o_1, o_2, o_3, …, o_k]$ ，其中每个 $o_i(i = 1, …, k)$ 表示物体的中心坐标： $o_i = \u003co^{(x)}_i, o^{(y)}_i\u003e$ 。 最终的预测中心点坐标可以计算出来： $$ C_x = \\frac{1}{k} \\sum^{k}_{i=1} o^{(x)}_i;\\ C_y = \\frac{1}{k} \\sum^{k}_{i=1} o^{(y)}_i $$ ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:2:1","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Over-Cover method 受LiveMotion方法的启发，其创建了不规则的预测FoV来覆盖更多的潜在的区域，Over-Cover的方式预测的FoV会覆盖所有包含物体的区域。 采用YOLOv3来处理帧并检测物体，接着每个检测到的对象生成与该对象共享相同中心的预测子视图，所有子视图的聚合形成最终的预测视口。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:2:2","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Summary for intuitive methods Basic方式可能会在多个物体的场景中无法正确选择目标； Over-Cover方式覆盖所有可能的目标来满足较高的精度，但会导致更高的带宽使用量； Velocity方式能很快的适应用户偏好的变化，但是预测精度在长期预测的情况下会显著下降； ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:2:3","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"LiveObj Method Over-Cover方法将所有检测到的目标合并到预测的FoV中而导致冗余问题，而用户一次只能观看其中的几个。 为了解决这个问题，提出基于用户的反馈选择最吸引人的目标，例如用户当前的FoV来形成预测的FoV。 基于这种想法而提出LiveObj，一种基于轨迹的 VP 方式，通过从Over-Cover方法的结果中过滤掉用户更小可能性看到的目标来缩小最终的FoV。 Object Detection：处理视频帧并检测目标； User View Estimation：分析用户反馈并用Velocity的方式估计FoV； Object tracking：追踪用户观看的目标； RL-based modeling：接受估计出的FoV和被追踪的目标，最终更新每个分块的状态（选中或未选中） ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:3:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Object Detection and Tracking Detection：YOLOv3； Tracking：追踪的基本假设是用户会在接下来的一段时间内接着观看当前看着的目标。追踪任务在直播推流的运行时完成。因此每隔几秒收集用户反馈，并进一步推断用户之前正在观看的目标，然后据此更新追踪目标。 追踪算法： ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:3:1","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"User View Estimation 分析用户的反馈处于两个目的： 估计未来的用户的FoV； 校准当前用户FoV以及要跟踪的对象； 给出用户反馈（即过去片段中实际的FoV），首先更新用户FoV并分析用户的行为模式，并根据此模式计算出下一帧中的预期用户速度。然后识别更新后的FoV中的对象，这些对象确定为ROI，对象追踪步骤将这些更新用于未来的片段来提高预测精度。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:3:2","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"RL-based Modeling 因为预测的误差和用户实际FoV的变化，可能会导致追踪的目标从FoV中消失，而这会使整个预测算法完全失效。所以提出一个基于 RL 的模型来为每个分块建立用户行为模型，旨在最小化预测误差。 出发点是不同的分块有不同的概率包含有意义的目标，并且更可能包含有意义目标的分块通常对目标检测错误更敏感。 将上面的观察形式化为一个策略学习过程 $M$： $$ M = \u003cS, A, P_{s, a, s’}, R\u003e $$ 其中 $S$ 和 $A$ 表示状态和动作， $P_{s, a, s’}$ 是给定状态 $s$ 的情况下选择动作 $a$ 的概率，转移之后的状态为 $s’$ ，$R$ 表示奖励函数。 系统的目标是通过设定不同的 $P_{s, a, s’}$ 的值，来学习每个分块对目标检测误差的不同的敏感性。 状态-价值函数用于估计在为所有可能的状态 $s \\in S$ 选择动作 $a$ 时的价值，形式化为： $$ v = E[Q_{s, a} | S_t = s] $$ $$ Q_{s, a} = R^a_s + \\gamma \\sum_{s’ \\in S} P_{s, a, s’} v $$ 其中：$\\gamma$ 是奖励参数。 最终的目标是通过计算每个 $P_{s, a, s’}$ 找到最大的 $max(Q_{s, a})$。 而这一过程很耗费时间，因此使用修改之后的Q-learning过程，用贪心的方式来解决最优化问题。 Q-learning过程在直播推流中有别于传统点播中的应用： 预测同时基于当前的输入（目标追踪和FoV估计的结果）和历史状态（分块是否被选择）； 奖励基于用户的反馈在线生成，并且会在整个推流会话中变化，而不是预先设定好的奖励矩阵 $R$ ； 由于直播推流中内容的不可提前获取性， $Q$ 表必须在每次预测中更新； 特别的，为每个分块都创建一个 $Q$ 表，对于每个 $Q$ 表有 4 种类型： object only; object and viewport; viewport only; no objects or viewport; 将这 4 种类型和 2 种中历史状态（选中或未选中）组合之后，得到每个表中状态 $s$ 的 8 个选项组合； 对每个状态而言，有 2 种动作（选中或不选中），因此每个表有 8 个状态和 2 个动作。 对每个表的奖励基于用户是否看到了分块而更新。 基于状态 $s$ 的对动作 $a$ 的选择转化成了：在相同输入的情况下找到 $max(Q(s, s’))$； LiveROI LiveObj的基础是对象检测算法，用于分析视频内容的敏感性。但是其检测性能可能会受到算法、对象的缩放程度和全景视频导致的扭曲失真的影响，进而引起预测误差。类似于LiveObj的出发点，LiveROI的目标是通过使用动作识别来对视频内容进行分析，这会降低预测性能与前面所提因素的敏感性。 使用3D-CNN等预先训练的模型来分析每个分块上的视频内容，以完成动作识别。同时基于NLP技术，使用轻量级用户模型将用户偏好映射到不同的视频内容。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:3:3","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"用户对视频内容的偏好 最基本的研究问题是：找到直播视频内容中的有效特征和信号或用户的行为，这些与用户的未来的FoV有强相关关系，因此可以将其作为预测因子。 通过对两个固定主题的视频的实验可以得出： 用户花绝大多数的时间在视频中有意义的部分； ROI在空间上只占整个帧很小的部分； ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:4:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"LiveROI Method 融合视频内容感知和用户偏好反馈（即以用户头部运动轨迹的形式）来预测实时 VR 视频流中的FoV。 主要想法是使用 CV 算法去理解每个分块的内容，除此之外，采用实时的用户反馈方便分块的选择。 需要满足的条件是：所有分块上的视频处理开销应该保持较小，以避免视频冻结和累计的实时延迟。 使用3D-CNN进行视频理解，重点是识别视频中隐含的有意义的动作，动作识别结果用于以自然语言的格式描述视频内容。这种 3D-CNN 模型可以在公共数据集上进行训练，因此具有通用性，以适应各种类型的动作和视频，这使得它可以用于实时 VR 流传输，因为在流传输会话之前没有关于视频内容的先验知识。 但是具有有意义动作的区域可能不是用户最后会确定的FoV，尤其是在目标视频中存在多个有意义动作的情况下。 为了解决这一问题，通过收集用户关于偏好视频内容的实时描述，进一步设计了基于“词/短语”的用户偏好模型。 采用词语嵌入的方法，通过比较两个来源短语的语义相似度，确定最佳匹配区域作为预测FoV，以此来桥接动作识别结果和用户偏好模型。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:5:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Workflow 3D-CNN的输入数据包含一批 $T$ 张图像，因此统一在一个视频片段中子采样 $T$ 帧。 每个子采样的帧都划分成 $M \\times N$ 个分块，VP 问题定义为确定要包含在FoV中的分块。 为了避免由于分块带来的潜在的信息损失（有意义的动作被划分成多个分块），每个用于动作识别的输入图像是从比原始分块边界更大的区域中所提取出来的，但是将共享与原始分块相同的中心。 3D-CNN模块的输出是动作识别结果，即结果矩阵。 面对 $M \\times N$ 个分块，为了满足性能要求，将每个分块的动作识别过程视为相互独立的过程，创建 $m \\times n$ 个线程来实现并行识别，每个线程向结果矩阵输出对应分块的结果向量。 在预测的最后一步，生成包含所有分块的预测分数的得分向量。进一步对所有的分数向量进行排序，并定位第 $M$ 个值，该值设定为选择分块进入预测FoV中的阈值。通过控制 $M$ 的大小可以控制预测的FoV的大小，分数向量中的分数表示用户对分块内容的感兴趣程度。 为了计算分数向量，进一步设计用户向量，其中包含描述用户偏好的词或短语。考虑到推流过程中用户可能会改变兴趣，用户向量会基于用户实时轨迹更新。 在给定用户向量和结果矩阵中的词或短语的情况下，考虑到非自然语言中的两个不同的词可能具有相近的含义，不直接进行词比较，而是使用词分析来计算其相关性。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:5:1","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"CNN Model 采用ECO lite模型完成 VR 直播推流中的动作识别。所有来自同一视频片段的图像都被储存在一个缓冲帧集合中。 ECO lite模型为 2D CNN 提取特征图的任务收集工作帧集合（分别由前一视频片段和当前视频片段的缓冲帧集合的后半部分和前半部分组成），在下一个阶段，从每个片段获得的特征图被堆叠到更高的表示中，之后被送到之后的 3D CNN 中用于最终的动作预测。具体的识别过程中同样使用多线程并行处理，处理 1 帧图像是每次创建和分块数相同的线程，为每个分块都初始化一个ECO lite模型。 显然预训练的模型不能为直播推流提供正确的推理结果，但是它可以看作是对视频内容的验证，即：给定一种类型的视频内容，其实其本身被误分类了，但在同一个模型之下它总是会被分类进在整个推流过程中都有相近分数的簇中。 利用这个特性，基于动作识别模型提供的对视频内容的描述，进一步设计动态的用户模型来映射用户偏好到不同的视频内容上。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:5:2","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"NLP Model 为了桥接动作识别和用户偏好向量，必须分析词/短语之间的相似性。 然而现有的 ML 算法不能直接处理生数据，因为输入必须是数值。为了解决这个问题，采用单词嵌入技术，使用多种语言模型以数值向量的形式来表示单词，以此来确保有相近意义的词有相近密度的表示。 具体处理时使用Phrase2Vec作为 NLP 模块的模型（作为Word2Vec的扩展，能更好的分析两个短语之间的相似性）。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:5:3","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"用户模型与预测 图 5.3 阐明了基于结果向量和用户向量的预测过程。由动作识别得出的结果向量，包括一个动作向量 $A$ 和一个权重向量 $W$ 。用户向量包括偏好向量 $P$ 和可能性向量 $L$ 。$A$ 和 $P$ 包含词和短语，描述了视频内容和用户偏好。 $W$ 和 $L$ 分别由表示神经网络对动作结果的置信度和用户对视频内容的参考可能性的值组成。 假设每帧 25 个分块，CNN 模块的输出结果是 25 个 $A$ 向量和 25 个 $W$ 向量；对与用户偏好，只使用 1 个 $P$ 向量和 1 个 $L$ 向量。 最终的分数向量 $S$ 计算为每个 $A$ 和 唯一的 $P$ 之间的相关性。结果也受相应的 $W$ 和 $L$ 的影响而调整。 假设余弦相似性函数为 $\\rho$ ，那么 $A$ 和 $P$ 中的每个 $a_i$ 和 $p_i$ 的计算可以表示为： $$ {\\rho}_i (a_i, p_i) = Phrase2Vec(a_i, p_i) $$ 设定每个向量中包含 5 个元素，分数向量 $S$ 计算为： $$ S = L \\cdot W \\cdot \\sum {\\rho} (A, P) $$ 对应于 25 个分块，最终的分数向量中包含 25 个元素。 $s_k$ 表示 $k_{th}$ 分块的分数值，详细算法： 分数向量更新完毕之后就可以获得每个分块内容和用户偏好之间的相关性，用帧上每个分块的亮度来做可视化： 将分数向量中的元素从高到低排序，选定 $\\frac{1}{3}$ 作为阈值，将前 $\\frac{1}{3}$ 的分块看作相同的分数等级作为最后的预测区域。 为了应对推流过程中用户偏好的变化，为分数向量的计算设计动态加权的用户偏好向量。 设定用户偏好向量 $P$ 的大小与动作向量 $A$ 的大小相同，一旦系统获取到用户实际的FoV位置，就计算其视野中心并定位到相应的分块，使用前一视频片段中该选中分块的动作向量 $A’$ 来更新用户的偏好向量。 ","date":"2022-01-25","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/:6:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"LiveMotion ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:0:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Motivation 基于视频中物体的运动模式来做对应的FoV预测。 将用户的FoV轨迹与视频内容中运动物体的轨迹结合到一起考虑： 细节可以参见：note-for-content-motion-viewport-prediction. LiveDeep 受限于Motion识别算法，前面提出的LiveMotion只能作用于有清晰并且容易分别的前景背景边界的视频，其健壮性并不能满足全景直播推流的场景。 ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:1:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Method LiveDeep处理问题的场景为： 视频内容在线生成； 没有历史用户数据； 预测需要满足实时性的要求； LiveDeep的设计原则： online：在线训练在线预测； lifelong：模型在整个视频播放会话中更新； real-time：预测带来的处理延迟不能影响推流延迟； CNN的设计： 在推流会话的运行时收集并标注训练数据； 以交替迭代的方式进行基于当前视频片段的推理和基于之前视频片段的训练； 子采样少部分的代表帧来运行 VP 以满足实时性的要求； ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:2:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Framework ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:3:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Setup 分包器将视频按照 DASH 标准将视频分段，每个段作为训练模型和预测的单元； 考虑到不同的视频可能具有不同的帧速率，在每个单元中统一采样 $k$ 帧而非以固定的采样率采样； 将每帧图像划分成 $x \\times y$ 个分块，最终每个单元中要处理的分块数为 $k \\times x \\times y$ ； 训练集来自于用户的实时反馈，根据实际FoV和预测FoV之间的差距来标注数据； 用户的轨迹数据来自于用户的实时头部轨迹，采样的帧与CNN模块采样的帧同步； ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:3:1","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Details 在用于训练的图像还没有被标注之前并不能直接预测，所以 CNN 模块只能以随机的权重给出预测结果。用预测结果与实际结果计算出损失值之后以此来更新 CNN 模型； LSTM 模型只能以用户观看到视频之后的实际轨迹作为训练的输入输入数据； 对下一个片段而言，首先使用两个模块独立做出预测。每个模块的预测都基于子采样之后的 $k$ 个帧； 为了产生对整个片段的预测结果，假设相邻的帧之间共享相同的视野中心（时空局部性）； 取两个模块预测输出的共同的部分作为最终的预测结果； ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:3:2","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"CNN Module 使用经典的 CNN：VGG 作为骨干网络，修改最后一层，只输出两类：感兴趣的和不感兴趣的。 ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:4:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"推理和视口生成 直观上的想法是选择被分类为感兴趣的部分，并且这些所选部分在原始帧中的位置将指示其他帧中可能感兴趣的FoV。 实际上存在的问题是：几乎所有的部分都被分类为感兴趣的一类，最终结果是整个帧被选择作为预测的结果。 所以不直接使用 CNN 网络的输出，而是在被分类为感兴趣的部分中进一步细分。通过对输出的分数排序并选择前 $M$ 份比例的输出作为最终的结果，这样通过控制 $M$ 的大小可以调整精度和消耗的带宽。 ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:4:1","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"训练过程 在传统的监督训练中，训练时间取决于可接受的最低损失值和 epoch 的值。为了满足实时性，LiveDeep采用较高的最低损失值和较低的最大 epoch 值。 High acceptable loss value：因为直接对从被分类为感兴趣的部分中去获取最终结果，所以通过实验证明，损失值应该要比常规的 CNN 更高：设定为 0.2。 The number of epochs：因为直播推流的特殊性，重复的训练并不能持续降低损失，所以采用较小的值：10。 The batch size：受限于训练的图像，将其设定为训练图像的个数即： $k \\times x \\times y$。 Dynamic learning rate： ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:4:2","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"LSTM Module 单纯的CNN模型可能会导致对视频内容有强记忆性，而这会使模型在面对新视频内容时需要花较长的时间去接受用户偏好，即对于用户偏好的快速切换不能做出即时响应。而LSTM的模块用于弥补这一缺陷； 采用与原始的LSTM模型相同的训练过程：先用收集的训练数据训练模型然后推断未来的数据。 收集用户在过去的视频片段中的用户轨迹，包括从 $k$ 个子采样帧中的 $k$ 个采样点，因此作为训练数据，同时将每个采样点中每个帧的索引指定为时间戳。最终模型的输出是预测出的分块的索引。 ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:5:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"混合模型 将CNN模块得到的输出作为主要的结果，接着结合LSTM模块的输出结果作为最终的预测结果。 ","date":"2022-01-22","objectID":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/:6:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/papers/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"论文概况 Link：Popularity-Aware 360-Degree Video Streaming Level：IEEE INFOCOM 2021 Keywords：Dynamic tiling, Cross-user division, Heuristic QoE optimization ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:1:0","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"Motivation 将视频划分成分块进行编码之后，会降低编码效率，并增大服务端的存储压力。（细节可以参见Optile） 而分块时根据用户的 ROI 来确定不同的大小，并在客户端预取，这可以节省带宽。 用户的 ROI 推断利用跨用户的偏好来确定，即所谓的Popularity-Aware。 ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:2:0","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"Model and Formulation ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:3:0","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"Video Model 视频从时间上被分成固定长度的片段，接着每个片段被从空间上划分成 $C$ 个分块。 除了常规的分块之外， $M$ 个宏块也被建构出来。 每个常规分块和宏块都被编码成 $V$ 个不同的码率质量等级并存储在服务端。 整个推流过程可以看作是一系列连续的下载任务。 客户端在每次下载任务中的目标是：选择恰当分块（宏块或者常规分块的集合）的恰当质量。 用 $L$ 表示客户端请求分块时，缓冲区中已经下载但还没有查看的视频的视频长度，为了避免缓冲事件，分块需要在缓冲区被清空即 $L = 0$ 之前被下载完毕。 ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:3:1","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"QoE Model $$ Q(V_k) = Q_{0}(V_k) - {\\omega}_v I_v (V_k) - {\\omega}_r I_r (V_k) $$ $V_k$ 表示下载的第 $k$ 段视频质量； $Q_0$ 表示平均质量； $I_v$ 表示由质量变化导致的质量损害； $I_r$ 表示由缓冲事件导致的质量损害； ${\\omega}_v$ 和 ${\\omega}_r$ 分别表示质量变化和缓冲的加权因子； 平均质量： $$ Q_0(V_k) = q(\\overline{V_k}) $$ $\\overline{V_k}$ 表示FoV内的平均视频质量； $q(\\cdot)$ 表示视频质量和用户实际感知质量之间的映射函数； 质量变化：两个连续段之间的质量差异和FoV内不同空间位置 tile 的质量差异会导致用户不适。 $$ I_v(V_k) = |Q_0(V_k) - Q_0(V_{k-1})| + \\widehat{V_k} $$ $|Q_0(V_k) - Q_0(V_{k-1})|$ 表示连续段间的FoV内时间质量差异； $\\widehat{V_k}$ 表示一个视频段的FoV内空间质量差异； 缓冲： $$ L_r(V_k) = {(\\frac{S(V_k)}{R} - L, 0)}_+ $$ $S(V_k)$ 表示段数据量大小； $R$ 表示下载吞吐量； ${(x)}_+ = max \\lbrace x, 0 \\rbrace$ ； ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:3:2","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"Formulation 用 ${\\beta}^v_m ({\\beta}^v_c)$ 表示对应的宏块或常规块是否被下载： ${\\beta}^v_m = 1$ 表示下载编码的质量等级为 $v$ 的宏块，消耗的带宽为 $B^v_m$ ，反之 $ {\\beta}^v_m = 0$ 表示不下载； ${\\beta}^v_c = 1$ 表示下载编码的质量等级为 $v$ 的常规块，消耗的带宽为 $B^v_c$，反之 ${\\beta}^v_m = 0$ 表示不下载； 客户端应该优先下载覆盖用户FoV 的宏块，如果没有这样的宏块则去下载对应的常规块的集合。 优化目标： $$ max\\ Q(\\lbrace v | {\\forall}_{m, v} {\\beta}^v_m = 1 \\rbrace) + Q(\\lbrace v | {\\forall}_{c, v} {\\beta}^v_c = 1 \\rbrace) $$ 同时需要满足以下 3 个约束： $$ \\sum^{M}_{m=1} \\sum^{V}_{v=1} {\\beta}^v_m + 1(\\sum^{C}_{c=1} \\sum^{V}_{v=1} {\\beta}^v_c) = 1 $$ $$ \\sum^{V}_{v=1} {\\beta}^v_c \\le 1,\\ for\\ c = 1, …, C $$ $$ \\sum^{M}_{m=1} \\sum^{V}_{v=1} {\\beta}^v_m B^v_m + \\sum^{C}_{c=1} \\sum^{V}_{v=1} {\\beta}^v_c B^v_c \\le R \\cdot L $$ $Q(\\cdot)$ 是公式 1 中定义的质量； $R$ 是网络带宽； $1(x) = 1 \\iff x \u003e 0$ ；$1(x) = 0 \\iff x \\le 0$ ； 约束 1 强制为观看区域下载宏块或常规块的集合，只下载宏块的一个质量版本； 约束 2 规定只下载常规块的一个质量版本； 约束 3 保证视频数据可以在开始播放之前被完全下载； 给出用户的观看区域之后，候选的宏块或对应的常规块集合也可以求出。 将QoE最大化的问题分解成两个子问题： 确定宏块的质量等级； 确定常规块的质量等级； 最后的解取这两种方案能取得更大QoE的那种。 如果QoE模型不考虑常规块之间的质量差异，则整体的QoE等价于下载的常规块的平均质量等级。 确定常规块质量等级的问题则可以简化为： $$ max\\ \\sum_{c \\in C} \\sum^{V}_{v=1} Q({\\beta}^v_c v) $$ 需要满足以下 2 个约束： $$ \\sum^{V}_{v=1} {\\beta}^v_c = 1,\\ for\\ c \\in C $$ $$ \\sum_{c \\in C} \\sum^{V}_{v=1} {\\beta}^v_c B^v_c \\le R \\cdot L $$ $C$ 表示覆盖观看区域的常规块集合。 简化之后的子问题可以通过对多项选择背包问题的简化，证明为是NP-hard问题，基于此提出启发式算法。 ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:3:3","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"基于宏块的流行性感知推流 ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:4:0","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"基于观看区域确定宏块 不同用户对相同视频的观看有着相似的 ROI，其视野中心是相近的，因此首先确定其视野中心并聚类到一起。 不能直接应用的知名聚类算法： 需要事先确定簇（即宏块）数量的算法（事先并不能确定需要多少宏块）：K-means 簇会越聚越大的算法（这样会失去节约带宽的优点）：DBSCAN 提出的算法用 2 个参数 $\\lambda$ 和 $\\gamma$ 来保证彼此相近的两个视野中心被归入同一簇，同时基于簇的宏块不至于太大。 被归入同一簇的视野中心之间的距离应该小于等于 $\\lambda$； 同一个簇的任意两个视野中心之间的距离应该小于等于 $\\gamma$； 为了确定这两个参数，还需要考虑常规块的大小带来的影响。 算法描述： 给出用 $P$ 表示的点集，其中每个点表示一个用户的视野中心位置； 用 $N_p = \\lbrace q | q \\in P \\land q \\neq p \\land dist(p, q) \\le \\lambda \\rbrace$ 来表示与点 $p$ 之间欧式距离小于 $\\lambda$ 的点集（即为临近点集）； 初始化拥有最多临近点的点所在的簇，例如： $p = {argmax}_{p \\in P} |N_p|$； 添加临近簇内任何点的点到簇中，扩张过程直到找不到符合条件的点位置； 检查簇中任意两个点之间的距离是否大于 $\\gamma$ ，如果存在这种情况就使用K-means算法将这个簇分成两个子簇； 从 $P$ 中移除簇中的点； 重复 1-4 的过程直到 $P = \\empty$； ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:4:1","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"宏块优化 通过简单地覆盖簇中用户的所有观看区域来为每个簇建构宏块可能会导致建构出不必要的大宏块，因此需要确定恰当的宏块大小。 首先需要确定哪些用户的观看区域应该被用于构建宏块，这样用户下载宏块时的带宽使用率小于下载一组常规块时的带宽使用率：$B_m$ 和 $B_c$ 分别表示覆盖相同观看区域的宏块和常规块的数据量大小。 为了解决用户头部运动的随机性，宏块应该在覆盖用户观看区域之外加上一些边界区域。边界区域可以基于用户观看中心的变化来确定，变化通过在推流观看过程中以固定采样率记录。 一个视频片段中 $x(y)$ 坐标的变化定义为 $x(y)$ 坐标的标准差。 实验发现：在一个视频片段中，用户的 $x(y)$ 坐标的变化很小。 分别用 $A_x$ 和 $A_y$ 表示 $x$ 和 $y$ 方向上的变化，构建的宏块应该覆盖用户的观看区域，并为 $x(y)$ 方向加上 $\\frac{A_x}{2}(\\frac{A_y}{2})$ 的边缘区域。 宏块构造问题的形式化： 为每个用户 $i$ 引入二元变量 ${\\alpha}_i$ ，${\\alpha}_i = 1$ 表示此用户的观看区域用于构建宏块，反之则没有； 实际应用中即为：如果 ${\\alpha}_i = 1$ ，则用户 $i$ 可以下载宏块；否则用户只能下载对应的常规块集合。 问题的目标是：在下载宏块或相同质量等级的常规块集合时，最小化所有用户的总带宽使用量。 $$ \\underset{\\lbrace {\\alpha}_i \\rbrace}{min}\\ \\sum^{N_j}_{i=1} {\\alpha}_i B_m + (1-{\\alpha}_i) B_c $$ $N_j$ 表示在 $j^{th}$ 簇中的用户数量；解决问题之后，可以用所有 ${\\alpha}_i = 1$ 的用户观看区域构建宏块； 尽管暴力枚举法可以完成最优求解，但是其时间复杂度为 $O(2^{N_j})$ ，为了减少实际建构宏块的时间，提出一种类似于随机采样一致性算法的迭代算法，每次迭代中，所做工作如下： 随机选取用户观察区域的子集。 编码宏块，用 $B_m$ 表示构建的宏块的带宽使用量。 检查建构的宏块是否覆盖用户 $i \\in \\lbrace 1, …N_j \\rbrace$ ，是则${\\alpha}_i = 1$；否则 ${\\alpha}_i = 0$。 检查总共的带宽使用量是否比之前迭代的更小，是则用当前迭代建构的宏块更新最终的宏块；否则继续迭代。 为了避免预测失败时用户看到空白区域，在下载观看区域的高质量宏块或常规块集合之外，也以最低质量下载其余的常规块。 ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:4:2","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"流行性感知推流 服务端基于多个用户的历史观看信息建构宏块，同时也使用常规块的划分方案编码视频。 客户端在推流过程中选择恰当块（宏块或常规块集）的恰当的质量等级来最大化用户的QoE。 流行性感知的推流算法首先为每个视频段预测用户的观看区域，之后预取相应的宏块或常规块集。 使用岭回归做 VP，输入用户在一系列历史帧中的观看区域中心坐标，输出未来帧中用户的观看区域位置。 基于预测的观看区域，算法确定是否存在覆盖预测区域及其边缘区域的宏块，是则搜索并下载满足条件的最高质量的宏块；否则下载相应区域的常规块集。 选择常规块集时首先为所有要选择的块确定满足贷款限制的最高质量等级，分配完之后如果还有剩余的带宽，算法会根据常规块与视野中心距离的远近程度提高一个质量等级，越近越优先提高。同时考虑到空间质量差异会降低QoE，所以提高质量的行为只有在超过半数的常规块满足条件时才会执行。 ","date":"2022-01-18","objectID":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/:4:3","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/papers/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["knowledge"],"content":"VR 和 360 度全景视频都是获得沉浸式体验的重要途径，除此之外，AR（Argmented Reality）和 MR（Mixed Reality）也是比较火的概念，可以用来对比学习。 ","date":"2022-01-17","objectID":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/:0:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/"},{"categories":["knowledge"],"content":"全景视频 全景视频实际上事先通过特殊的全景摄像机录制好视频，之后可以在HMD中观看。虽然看到的图像相对于用户当前环境而言是虚拟的，但是终归是从实际环境中录制而来的，本质上更贴近普通视频的全景推广。 在全景视频的观看过程中，用户只有 3DoF 的自由度，即只能完成头部的 3 个角度的运动，同时手柄实际上并不能和视频中的内容进行交互。 全景视频的主要应用在于实景导览，通过事先由拍摄者带着全景录像设备行走拍摄，用户观看时实际是将自己带入到全景设备的位置上，同时移动头部来观察不同角度的视频。 ","date":"2022-01-17","objectID":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/:1:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/"},{"categories":["knowledge"],"content":"VR VR 主要做的工作是创造出一个完全虚拟的环境，用户戴上HMD之后可以通过其看到虚拟环境中的事物，同时也可以使用HMD配套的手柄等设备进行操作，完成与虚拟环境之间的交互； VR 支持的是 6DoF 的自由度，即除了头部的运动之外也支持身体的前后、左右、上下的移动，手柄； VR 的主要应用在于游戏，比如广受好评的Beat Saber（又称节奏光剑），用户根据音乐节奏通过挥动手柄（在虚拟环境中被建模成光剑）来准确地按照提示的方向去砍击方块； ","date":"2022-01-17","objectID":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/:2:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/"},{"categories":["knowledge"],"content":"AR 和 MR AR 主要做的工作是将虚拟世界中的事物投影到现实世界中，主体是现实世界，虚拟事物用于增强现实世界。 MR 主要做的工作是将现实世界中的事物虚拟化进入虚拟世界中，主体是虚拟世界，现实事物混合进虚拟世界中。 AR 实现起来比较简单，只需要将计算机产生的图像投影显示在现实中即可，目前的应用比如游戏Pokémon GO里面的AR-mode，启用之后游戏中遇到的Pokémon就可以投影在现实中。 MR 实现起来比较复杂，首先需要用摄像头扫描物体，得到的 2D 图像再交给计算机采用算法进行 3D 重建，最后将虚拟化建模好的物体展示到虚拟世界中，目前的应用比如Meta推出的Workrooms，线上的远距离视频会议在虚拟世界中可以变成虚拟人物之间面对面的交流。 ","date":"2022-01-17","objectID":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/:3:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/"},{"categories":["knowledge"],"content":"总结 全景视频侧重于对虚拟环境的观察，而 VR 侧重于对虚拟环境的交互。 全景视频实际上是将用户带入到全景摄像机的位置上，让用户产生自己身临拍摄的环境中的感觉，本质上是对传统视频的推广； VR 实际上是将用户完全带入到虚拟的环境中，用户可以和虚拟环境中的事物进行交互，而虚拟环境中发生的一切都和现实无关，本质上是对传统游戏的推广； 全景视频实际上和 VR、AR、MR 这种概念距离比较远，实际上只是因为全景摄像机相较于普通摄像机的 360 度视角的特殊性，这能让用户产生沉浸感。 VR 相比于 AR、MR 而言，是纯粹的虚拟环境，并不涉及到现实事物（除了HMD配套的手柄等设备），而纯粹的虚拟环境将人带入到了一个完全不同的世界，也是 VR 沉浸式体验的来源。 AR 和 MR 是虚拟和现实交融的技术，前者主体是现实，后者主体是虚拟环境。 ","date":"2022-01-17","objectID":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/:4:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/knowledge/360video/summary-for-vr-and-panoramic-video/"},{"categories":["paper"],"content":"论文概况 Link：Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network Level：IEEE Transactions on Broadcasting 2021 Keywords：Cross-user vp, Sequetial RL ABR ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:1:0","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"主要工作 使用跨用户注意力网络CUAN来做 VP； 使用360SRL来做 ABR 将上面两者集成到了推流框架中； ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:2:0","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"VP ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:3:0","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"Motivation 形式化 VP 问题如下： 给出 $p^{th}$ 用户的 $1-t$ 时间内的历史视点坐标 $L^{p}_{1:t} = \\lbrace l^p_1, l^p_2, …, l^p_t \\rbrace$ ，其中 $l^p_t = (x_t, y_t), x_t \\in [-180, 180]; y_t \\in [-90, 90]$ ； 同一视频的不同用户视点表示为 $L^{1:M}_{1:t+T}$ ， $M$ 表示其他用户的数量； 目标是预测未来的 $T$ 个时刻的视点位置 $L^p_i, i = t+1, …, t+T$ ； 最终可以用数学公式表达为： $$ \\underset{F}{min} \\sum^{t+T}_{k = t+1} {\\parallel l^p_k - \\hat{l}^p_k \\parallel}_1 $$ 现有的用KNN做的跨用户预测基于 LR 的模型，而 LR 的模型很容易产生偏差，所以为了增强KNN的性能，同时考虑单用户的历史视点轨迹和跨用户的视点轨迹。 提出一种注意力机制来自动提取来自其他用户视口的有用信息； 对于与当前用户有相似偏好的用户轨迹信息给与更多的注意； 相似性通过基于过去时间段内其他用户的轨迹计算出来； ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:3:1","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"Design 轨迹编码器模块从用户的历史视点位置提取时间特征； 使用LSTM来编码用户的观看路径； 为了预测 ${(t+1)}^{th}$ 帧的视点位置，首先向LSTM输入 $p^{th}$ 用户的历史视点坐标： $$ f^{p}_{t+1} = h(l^p_1, l^p_2, …, l^p_t) $$ $h(\\cdot)$ 是LSTM的输入输出函数； 接着使用相同的LTSM编码其他用户的观看轨迹： $$ f^{i}_{t+1} = h(l^i_1, l^i_2, …, l^i_{t+1}), i \\in \\lbrace 1, …, M \\rbrace $$ 注意力模块从其他用户的视点轨迹中提取与 $p^{th}$ 用户相关的信息 首先推导出 $p^{th}$ 用户和其他用户之间的相关系数： $$ s^{pi}_{t+1} = z(f^{i}_{t+1}, l^{p}_{t+1}), i \\in \\lbrace 1, …, M \\rbrace \\cup \\lbrace p \\rbrace; $$ $s^{th}_{t+1}$ 表示 $p^{th}$ 用户和 $i^{th}$ 用户之间的相似性；$z()$ 由内积运算建模（还可用其他方式建模比如多个 FC 层）； 接着将相关系数规范化： $$ {\\alpha}^{pi}_{t+1} = \\frac{e^{s^{pi}_{t+1}}}{\\sum_{i \\in \\lbrace 1,… M \\rbrace \\cup {\\lbrace p \\rbrace}^{e^{s^{pi}_{t+1}}}}} $$ 最后得到融合特征： $$ g^{p}_{t+1} = \\sum_{i \\in {\\lbrace 1,…M \\rbrace \\cup \\lbrace p \\rbrace}} {\\alpha}^{pi}_{t+1} \\cdot f^{i}_{t+1} $$ 融合特征被最后用于 VP。 VP 模块预测 ${(t+1)}^{th}$ 帧的视点位置 $$ \\hat{l}^{p}_{t+1} = r(g^{p}_{t+1}) $$ 函数 $r(\\cdot)$ 由一层 FC 建模。值得注意的是，对应于未来 T 帧的视点是以滚动方式预测的。 ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:3:2","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"Loss 损失函数定义为预测的视点位置和实际视点位置之间的所有绝对差异的总和： $$ L = \\sum^{t+T}_{i=t} {|\\hat{l}^p_i - l^p_i|}_1 $$ ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:3:3","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"Details 使用PyTorch实现； 函数 $h(\\cdot)$ 由两个堆叠的LSTM层组成，两者都有 32 个神经元； 函数 $r(\\cdot)$ 包含一个带有 32 个神经元的 FC 层，接着是Tanh函数； 历史视点和未来视点的长度设定为 1 秒和 5 秒； 每次迭代从数据集中随机产生 2048 个样本； 所有训练变量的优化函数采用Adam； $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$； $learning\\ rate = 10^{-3}, training\\ epoch = 50$； ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:3:4","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"ABR ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:4:0","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"Formulation 全景视频被切分成 $m$ 个长度为 $T$ 秒的视频片段，每个视频片段空间上划分成 $N$ 个分块，分别以 $M$ 个不同的码率等级编码。因此对于每段有 $N \\times M$ 个可选的编码块。 ABR 的目标是为每个片段找到最优的码率集 $X = \\lbrace x_{i, j} \\rbrace \\in Z^{N \\times M}$ （ $x_{i, j} = 1$ 意味着为 $i^{th}$ 块选择 $j^{th}$ 的码率等级）： $$ \\underset{X}{max} \\sum^{m}_{t=1} Q_t $$ $Q_t$ 表示 $t^{th}$ 段的 QoE 分数，与以下几个方面有关： VIewport Quality： $$ Q^1_t = \\sum^{N}_{i=1} \\sum^{M}_{j=1} x_{i,j} \\cdot p_i \\cdot r_{i,j} $$ $p_i$ 表示 $i^{th}$ 分块的规范化观看概率； $r_{i,j}$ 记录块 $(i, j)$ 的码率； Viewport Temporal Variation： $$ Q^2_t = |Q^1_t - Q^{1}_{t-1}| $$ Viewport Spatial Variation： $$ Q^3_t = \\frac{1}{2} \\sum^{N}_{i=1} \\sum_{u \\in U_i} p_i \\cdot p_u \\sum^{M}_{j=1} |x_{i,j} \\cdot r_{i,j} - x_{u,j} \\cdot r_{u,j}| $$ $U_i$ 表示 $i^{th}$ 个分块的 1 跳邻居中的 tile 索引[1]； Rebuffering： $$ Q^4_t = max(\\frac{\\sum^{N}_{i=1} \\sum^{M}_{j=1} x_{i,j} \\cdot r_{i,j} \\cdot T}{\\xi_t} - b_{t-1}, 0) $$ $\\xi_t$ 表示网络吞吐量； $b_{t-1}$ 表示播放器的缓冲区占用率； 最终的 QoE 可以由上面的指标定义： $$ Q_t = Q^1_t - \\eta_1 \\cdot Q^2_t - \\eta_2 \\cdot Q^3_t - \\eta_3 \\cdot Q^4_t $$ $\\eta_*$ 是可调节的参数，与不同的用户偏好对应。 ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:4:1","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"Sequential RL-Based ABR 假设基于 tile 的全景推流 ABR 过程也是 MDP。 细节在360SRL中已经说明清楚。 ","date":"2022-01-15","objectID":"/posts/papers/note-for-srlABR-cross-user/:4:2","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/papers/note-for-srlABR-cross-user/"},{"categories":["paper"],"content":"论文概况 Link：360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming Level：ICME 2019 Keywords：ABR、RL、Sequential decision ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:1:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"创新点 在 MDP 中，将 N 维决策空间内的一次决策转换为 1 维空间内的 N 次级联顺序决策处理来降低复杂度。 ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:2:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"问题定义 原始的全景视频被划分成每段固定长度为 $T$ 的片段， 每个片段包含 $N$ 个分块，并以 $M$ 的码率等级独立编码， 因此对每个片段，有 $N \\times M$ 种可选的编码块。 为了保证播放时的流畅性，需要确定最优的预取集合： ${a_0, …, a_i, …, a_{N-1}}, i \\in \\lbrace 0, …, N-1 \\rbrace, a_i \\in \\lbrace 0, …, M-1 \\rbrace $ 分别用 $q_{i, a_i}$ 和 $w_{i, a_i}$ 表示码率选择为 $a^{th}_i$ 的 $i^{th}$ 分块的质量和相应的分块片段大小。 用 $p_i \\in [0, 1]$ 表示 $i^{th}$ 块的被看到的可能性。 ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:3:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"顺序 ABR 决策 ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:4:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"代理设计 ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:5:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"状态 对于 $i^{th}$ 维，输入状态包括原始的环境状态 $s_t$ ； 与之前维度的动作集合相关的信号： $u^{i}_{s_t} = \\lbrace Th, C_i, p_{0:i-1}, q_{0:i-1}, b_t, p_i, S_i, Q_{t-1} \\rbrace$ $Th$ ：表示过去 m 次下载一个段的平均吞吐量； $C_i \\in R^M$ ：表示 $i^{th}$ 个分块的可用块大小向量； $p_{0:i-1}$ 和 $q_{0:i-1, a^{0:i-1}_{t}}$ 分别表示选中的码率集合和看到之前 $i-1$ 个分块的概率集； $b_t$ 是缓冲区大小； $p_i$ 是 $i^{th}$ 个分块被看到的可能性； $S_i$ 是之前选择的 $i-1$ 个分块的块大小之和： $S_i = \\sum^{i-1}_{h=0} C_{h, a^h_t}$ ； $Q_{t-1}$ 记录了最后一个段中 $N$ 个分块的平均视频质量； ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:5:1","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"动作 动作空间离散，代理输出定义为价值函数：$f(u^i_{s_t}, a^i_t)$ 表示所选状态的价值 $a^i_t \\in \\lbrace 0, …, M-1 \\rbrace$ 处于状态 $u_{s_t}^i$ . ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:5:2","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"回报 回报定义为下列因素的加权和： 平均视频质量 $q^{avg}_t$，空间视频质量方差 $q^{s_v}_t$，时间视频质量方差 $q^{t_v}_t$ ，重缓冲时间 $T^r_t$ $$ q^{avg}_t = \\frac{1}{\\sum^{N-1}_{i=0} p_i} \\cdot \\sum^{N-1}_{i=0} p_i \\cdot q_{i, a_i} $$ $$ q^{s_v}_t = \\frac{1}{\\sum^{N-1}_{i=0} p_i} \\cdot \\sum^{N-1}_{i=0} p_i \\cdot |q_{i, a_i} - q^{avg}_t| $$ $$ q^{t_v}_t = |q^{avg}_{t-1} - q^{avg}_t| $$ $$ T^r_t = max \\lbrace T_t - b_{t-1}, 0 \\rbrace $$ $$ R_t = w_1 \\cdot q^{avg}_t - w_2 \\cdot q^{s_v}_t - w_3 \\cdot q^{t_v}_t - w_4 \\cdot T^r_t $$ ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:5:3","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"训练方法 使用DQN作为基本的算法来学习动作-价值函数 $Q(s_t, a_t; \\theta)$ ，其中 $\\theta$ 作为参数，对应的贪心策略为 $\\pi(s_t; \\theta) = \\underset{\\theta}{argmax} Q(s_t, a_t; \\theta)$ 。 DQN网络的关键想法是更新最小化损失函数的方向上的参数： $$ L(\\theta) = E[y_t - Q(s_t, a_t; \\theta)] $$ $$ y_t = r(s_t, a_t) + \\gamma Q(s_{t+1}, \\pi(s_{t+1}; {\\theta}’); {\\theta}’) $$ ${\\theta}’$ 表示固定且分离的目标网络的参数； $r(\\cdot)$ 是即时奖励函数，即上面公式 5 中的 $R_t$ ； $\\gamma \\in [0, 1]$ 是折扣因子； 为了缓解过拟合，引入 double-DQN 的结构，所以公式 7 被重写为： $$ y_t = r(s_t, a_t) + \\gamma Q(s_{t+1}, {\\pi}(s_{t+1}; \\theta); {\\theta}’) $$ 利用公式 6 和公式 8 可以得出 $i^{th}$ 维的暂时损失函数： $$ l^i_t = Q_{target} - Q(u^i_{s_t}, a^i_t; \\theta), \\forall i \\in [0, …N-1] $$ 其中 $Q_{target}$ 满足： $$ Q_{target} = r_t + {\\gamma}_u \\cdot Q(u^0_{s_{t+1}}, \\pi(u^0_{s_{t+1}}; 0); {\\theta}’) $$ ${\\gamma}_u$ 和 ${\\gamma}_b$ 分别代表”Top MDP“和”Bottom MDP“的折扣因子，训练中设定 ${\\gamma}_b = 1$ 。 观察公式 9 和公式 10 可以看出每维都有相同的目标函数，意味着无法区别每个独立维度的动作 $a^i_t$ 对 $r_t$ 的贡献。 为了克服限制，根据某个分块的动作 $a^i_t$ 与其观看概率成正比的先验知识，向 $l^i_t$ 添加一个额外的 $r^i_{extra}$ ： $$ l^i_t = r^i_{extra} + Q_{target} - Q(u^i_{s_t}, a^i_t; \\theta), \\forall i \\in [0, …N-1] $$ $$ r^i_{extra} = \\begin{cases} 0, p_i \u003e P ; \\ -a^i_t, p_i \\le P \\end{cases} $$ 通过设定一个观看概率的阈值 $P$ ，对观看概率低于 $P$ 但选择了高码率的分块施加 $-a^i_t$ 的奖励。 因此最终的平均损失可以形式化为： $$ l^{avg}_t = \\frac{1}{N} \\sum^{N-1}_{i=0} l^i_t $$ 接着使用梯度下降法来更新模型，学习率设定为 $\\alpha$： $$ \\theta \\larr \\theta + \\alpha \\triangledown l^{avg}_t $$ 同时，在训练阶段利用经验回放法来提高360SRL的泛化性。 ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:6:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["paper"],"content":"实现细节 特征从输入状态中通过特征提取网络提取出来。 初始的 4 个输入通过带有 128 个过滤器的 1 维卷积层被传递，4 个输入核心大小分别为 $1 \\times m$ 、 $1 \\times M$ 、 $1 \\times N$ 、 $1 \\times M$ ，后续这 4 个输入被喂给有 128 个神经元的全连接层； 随后特征映射被连接成一个张量，接着是具有 1024 个神经元和 256 个神经元的前向网络； 整个动作-价值网络的输出是 M 维的向量。 特征提取层和前向网络层都使用 Leaky-ReLU作为激活函数，最后是层归一化层。 ","date":"2022-01-13","objectID":"/posts/papers/note-for-360srl/:7:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/papers/note-for-360srl/"},{"categories":["knowledge"],"content":"视口预测是什么？ 视口预测 (Viewport Predict) 是全景视频中特有的一种用于进一步优化码率自适应的方式。 相较于全景视频 360 度无死角的特性，用户实际上能看到的内容其实只是全景视频中的一个小窗口，这个小窗口就是视口 (Viewport) 。 因为用户在观看全景视频时会在 3DoF 的自由度下转动头部去观看全景视频在空间上的不同部分，所以视口预测做的事情就是在用户的观看过程中预测相较于预测执行时刻的下一时刻的视口位置。 ","date":"2022-01-07","objectID":"/posts/knowledge/360video/summary-for-vp/:1:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/knowledge/360video/summary-for-vp/"},{"categories":["knowledge"],"content":"VP 在传输中所处的作用 基于 tile 的全景视频传输方式之所以热门，就是因其可以通过只传输用户 FoV 内的分块而大幅减少观看过程中消耗的带宽。 所以对用户 FoV 的预测是首先要处理的因素，如果 VP 精度很高，那么所有的带宽都可以用很高的码率去传输 FoV 内的分块。 ","date":"2022-01-07","objectID":"/posts/knowledge/360video/summary-for-vp/:2:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/knowledge/360video/summary-for-vp/"},{"categories":["knowledge"],"content":"两种方式的基本假设 基于轨迹的方法的基本假设 相对于当前时刻，前 $hw$ (history window)内用户的 FoV 位置对未来可预测的 $pw$ (predict window)内用户的 FoV 位置有影响，比如用户只有很小可能性会在很短的一段单位时间内做 180 度的转弯，而更小角度的调整则更可能发生。 基于内容的方法的基本假设 用户的 FoV 变化是因为对视频内容感兴趣，即 ROI 与 FoV 之间有相关关系，比如在观看篮球比赛这样的全景视频时，用户的 FoV 更可能专注于篮球。 按照提取 ROI 的来源不同可以分为两种类型： 从视频内容本身出发，使用 CV 方法去猜测 ROI； 从用户观看视频的热图出发，相当于得到了经过统计之后的平均 FoV 分布，以此推测其他用户的 ROI； 基于轨迹的方式是要在最表层的历史和预测的轨迹之间学习，即假设两者之间只有时空关系。 跨用户的方式则假设由用户群体所得出的热图可以用来预测单个用户的 FoV，即利用共性来推断个性。 基于内容的方式直接提取视频显著图来推断 FoV，即进一步假设共性与视频内容本身有关系。 ","date":"2022-01-07","objectID":"/posts/knowledge/360video/summary-for-vp/:3:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/knowledge/360video/summary-for-vp/"},{"categories":["knowledge"],"content":"跨用户预测的概念 基本假设 就单个用户而言，在观看视频过程中其 FoV 的变化看似随机，但是其行为可能从用户群体的角度去看是跨用户相通的，即多个用户在观看视频时可能会表现出相似的，可以学习的行为模式，这种行为模式可以帮助提高 VP 的精度。 实际应用 基于轨迹的跨用户：如果训练的模型是基于轨迹的离线模型如 LSTM，那么实际上训练好的模型已经学习到了这种跨用户的行为模式；而如果采用的是边训练边预测的模型如 LR（输入历史窗口的经纬度数据，输出预测窗口的经纬度数据），那么这样的模型就是纯粹的单用户模型。 基于内容的跨用户：将用户在观看视频帧时的注意点作为研究对象，找到用户群体在面对同一帧视频时共同关注的空间区域，而这就是用户间相似的行为模式。这种与内容相结合的跨用户方式即为实际研究中所指的跨用户的研究方式。（实际上就是基于内容的研究方法，只不过出发点不是视频本身，而是用户在观看视频时的 FoV） ","date":"2022-01-07","objectID":"/posts/knowledge/360video/summary-for-vp/:4:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/knowledge/360video/summary-for-vp/"},{"categories":["knowledge"],"content":"实际应用 图中 3 个黄色矩形表示 3 种方法： ROI extract：基于内容的预测 Multiple watchers’ FoV：跨用户的预测 Multiple watchers’ trajectories：基于轨迹的预测 绿色渐变矩形表示直接使用用户当前的历史轨迹数据去训练模型，接着做出预测。 ","date":"2022-01-07","objectID":"/posts/knowledge/360video/summary-for-vp/:5:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/knowledge/360video/summary-for-vp/"},{"categories":["knowledge"],"content":"研究方法 基于轨迹的方法 在线训练：输入历史窗口的位置信息，不断迭代修正模型，输出预测窗口的位置信息。 离线训练：输入任何采样条件下的多对 hw 和 pw 信息来拟合模型。 跨用户的方法 求出多个用户在同一帧上的热图，以此作为 FoV 预测的依据。 基于内容的方法 提取视频帧中的显著图，以此作为 FoV 预测的依据。 ","date":"2022-01-07","objectID":"/posts/knowledge/360video/summary-for-vp/:6:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/knowledge/360video/summary-for-vp/"},{"categories":["knowledge"],"content":"优点 使用回归实现的在线训练模型实现简单，反应迅速，有优秀的短期预测精度。 因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入，跨用户的热图可以帮助长期的预测，可以提供合理的离线全视频 FOV 预测，并具有一致的性能。 显著图对于 ROI 集中突出的预测效果较好。 ","date":"2022-01-07","objectID":"/posts/knowledge/360video/summary-for-vp/:7:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/knowledge/360video/summary-for-vp/"},{"categories":["knowledge"],"content":"缺点 使用回归实现的在线训练模型在预测窗口增大时，性能会显著下降。 提取显著图的方式一方面训练开销比较大，另一方面对于 ROI 不够集中突出的视频效果并不好。 ","date":"2022-01-07","objectID":"/posts/knowledge/360video/summary-for-vp/:8:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/knowledge/360video/summary-for-vp/"},{"categories":["paper"],"content":"论文概况 Link：Content Assisted Viewport Prediction for Panoramic Video Streaming Level：IEEE CVPR 2019 CV4ARVR Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:1:0","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"主要工作 ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:2:0","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"基于轨迹预测 输入：历史窗口轨迹 模型：64 个神经元的单层 LSTM，在输入层后面加上一个额外的减法层进行点归一化，以及一个加法层来恢复输出之前的值；用 ADAM 进行优化，MAE 作为损失函数。 ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:2:1","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"跨用户热图 除了观看者自己的历史 FOV 轨迹之外，其他观看者对同一视频帧的观看方向也有启发性。 对视频的每一帧，首先收集用户的观看方向（坐标使用原始的来自三个方向的欧拉角表示，而非经纬度）。 接着将坐标投影到用经纬度表示的 180x360 像素的平面图上，对于图中的每个像素点，可以数出其被看到的次数；并对周围像素应用二维高斯光滑。 上面的过程可以为视频生成热图： ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:2:2","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"视频帧的显著图 鉴于观看相同的全景视频时跨用户行为的共性，进一步假设是内容促使多个观众观看公共区域，因此提取出每个帧的显著图可能会表明用户的 RoI。 对特定的视频帧，应用经典的特征密集型方法——Ittykoch，它首先根据强度、边缘、颜色和方向将图像分解为多个特征通道，然后将它们组合成识别显著区域。 除了在静态视频帧上检测显著性之外，进一步进行背景减法来减少不太可能感兴趣的区域：应用基于高斯混合的背景/前景分割算法，高级思想是在连续帧之间临时过滤变化的像素点。 结合上面这两个过程可以为视频帧提取时间显著图。 ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:2:3","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"多模态融合 使用包含 3 个 LSTM 分支的深度学习模型来融合上述的几种预测方式的结果。 基于轨迹的 LSTM（图中绿色分支）从历史窗口 $hw$ 中接受 $n$ 个坐标的输入，接着预测未来窗口 $pw$ 中的 $m$ 个坐标，用 $trj_y_{i}$ 表示； 基于热图的 LSTM（图中蓝色分支）将每个预测步骤对应的视频帧的热图作为输入，并在 $pw$ 中输出第 2 组 $m$ 个坐标的预测，用 $ht_y_{i}$ 表示： 对于每个热图，让其通过 3 个卷积层，每个卷积层后面都有一个最大池化层。然后，在此图像特征提取之后，应用展平步骤和 1 个密集层来回归坐标（经纬度表示）。 基于显著图的 LSTM 采用与热图相似的架构，将显著图作为输入，在 $pw$ 中输出第 3 组 $m$ 个坐标的预测，用 $sal_y_{i}$ 表示。 对热图和显著图的分支，应用 TimeDistributed层，以便其参数在预测步骤中保持一致。 最终在每个预测步骤连接 $trj_y$ ， $ht_y$，和 $sal_y$ ，并产生一个最终输出 $y$ 。 每个模型的损失函数采用 MAE，优化函数采用 ADAM。 为每个分支的输出以及最终的输出都检查损失，单独和联合地去调整其参数。 ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:2:4","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"评估 使用 2 折的交叉验证。 ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:3:0","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"超参数 $pw$ 的大小：0.1s，1.0s，2.0s； $hw$ 的大小：0.05s，0.6s，1.0s；（分别与上面的 $pw$ 对应） 用于训练的用户数：[3, 10, 30] ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:3:1","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"结果与分析 所有模型的预测精度随着 $pw$ 的增长而下降，表明长期预测问题更难解决； 所有模型的精度预测误差几乎是纬度预测误差的二倍，可能由于运动区域在水平方向的翻倍； 线性回归模型只有在 $pw$ 很短的时候预测精确，随着 $pw$ 的增长，其预测精度会迅速下降； 基于 LSTM 的轨迹模型始终优于所有 $pw$ 的基线模型，但更多的训练观众无助于显着提高准确性。 跨用户的热图和显著图可以帮助长期的预测，可以提供合理的离线全视频 FOV 预测，并具有一致的性能（因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入），当 $pw$ 增长时，其预测精度超过了基于历史轨迹的模型； 结合 3 种模型之后，可以平衡来自历史轨迹、跨用户兴趣和内容显著性的输入，不论 $pw$ 长或短都能产生优化的预测结果； ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:3:2","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"例外情况 M3 在经度上的表现并不适用于上面图中标示的两个视频（Mega.Coaster 和 GTR.Drives.First.Ever） 原因分析： 这两个视频的共同特点是在驾驶路径的一侧具有高运动内容的驾驶内容，因此用户在观看这些视频时，大多数 FOV 始终以行驶轨迹为中心。因此用户不太可能改变其观看方向，这导致即使 $pw = 2.0s$ 时，单一基于轨迹的模型的预测精度也更高。相比之下，从对内容角度出发的分析无济于事，但可能会引入观众可能会忽略的变道，进而造成预测误差。 ","date":"2022-01-06","objectID":"/posts/papers/note-for-content-assisted-prediction/:3:3","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/papers/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"Dash 客户端自适应逻辑 tile priority setup：根据定义的规则对 tile 进行优先级排名。 rate allocation：收集网络吞吐量信息和 tile 码率信息，使用确定的 tile 优先级排名为其分配码率，努力最大化视频质量。 rate adaption：在播放过程中，执行码率自适应算法，基于播放速度、质量切换的次数、缓冲区占用情况等。 ","date":"2021-12-30","objectID":"/posts/papers/note-for-gpac/:1:0","tags":["Immersive Video","DASH"],"title":"Note for GPAC","uri":"/posts/papers/note-for-gpac/"},{"categories":["paper"],"content":"tile priority setup Dash 客户端加载带有 SRD 信息的 MPD 文件时，首先确定使用 SRD 描述的 tile 集合。 确定 tile 之间的编码依赖（尤其是使用 HEVC 编码的 tile 时） 为每个独立的 tile 向媒体渲染器请求一个视频对象，并向其通知 tile 的 SRD 信息。 渲染器根据需要的显示大小调整 SRD 信息之后，执行视频对象的最终布局。 一旦 tile 集合被确定，客户端向每个 tile 分配优先级。（每次码率自适应执行的时候都需要分配 tile 优先级） ","date":"2021-12-30","objectID":"/posts/papers/note-for-gpac/:1:1","tags":["Immersive Video","DASH"],"title":"Note for GPAC","uri":"/posts/papers/note-for-gpac/"},{"categories":["paper"],"content":"Rate allocation 首先需要估计可用带宽（tile 场景和非 tile 场景的估计不同） 在一个视频段播放过程中，客户端需要去下载多个段（并行-HTTP/2） 带宽可以在下载单个段或多个段的平均指标中估计出来。 一旦带宽估计完成，码率分配将 tile 根据其优先级进行分类。 一开始所有的 tile 都分配成最低的优先级对应的码率，然后从高到低依次增长优先级高的 tile 的码率。 一旦每个 tile 的码率分配完成，将为目标带宽等于所选比特率的每个 tile 调用常规速率自适应算法 ","date":"2021-12-30","objectID":"/posts/papers/note-for-gpac/:1:2","tags":["Immersive Video","DASH"],"title":"Note for GPAC","uri":"/posts/papers/note-for-gpac/"},{"categories":["paper"],"content":"论文概况 Link：A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP Level：ACM SIGCOMM 15 Keywords：Model Predictive Control，ABR，DASH ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:1:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"Motivation 关于码率自适应的逻辑，现有的解决方案还没有形成清晰的、一致的意见。不同类型的方案之间优化的出发点并不相同，比如基于速率和基于缓冲区，而且没有广泛考虑各方面的因素并形成折中。 文章引入了控制论中的方法，将各方面的影响因素形式化为随机优化控制问题，利用模型预测控制 MPC将两种不同出发点的解决方案结合到一起，进而解决其最优化的问题。而仿真结果也证明，如果能运行一个最优化的 MPC 算法，并且预测误差很低，那么 MPC 方案可以优于传统的基于速率和基于缓冲区的策略。 ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:2:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"背景 播放器端为 QoE 需要考虑的问题： 最小化冲缓冲事件发生的次数； 在吞吐量限制下尽可能传输码率较高的视频； 最小化播放器开始播放花费的时间（启动时间）； 保持播放过程平滑，尽可能避免大幅度的码率变化； 这些目标相互冲突的原因： 最小化重缓冲次数和启动时间会导致只选择最低码率的视频； 尽可能选择高码率的视频会导致很多的重缓冲事件； 保持播放过程平滑可能会与最小的重缓冲次数与最大化的平均码率相冲突； ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:3:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"控制论模型 ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:4:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"视频推流模型 参数形式化 将视频建模成连续片段的集合，即：$V = \\lbrace 1, 2, …, K \\rbrace$，每个片段长为$L$秒； 每个片段以不同码率编码，$R$ 作为所有可用码率的集合； 播放器可以选择以码率$R_k \\in R$ 下载第$k$块片段，$d_k(R_k)$ 表示以码率$R_k$编码的视频大小； 对于恒定码率 CBR 的情况，$d_k(R_k) = L \\times R_k$； 对于变化码率 VBR 的情况，$d_k \\sim R_k$； 选择的码率越高，用户感知到的质量越高： $q(\\cdot):R \\rightarrow \\R_+$ 是一个不减函数，是选择的码率 $R_k$ 到用户感知到的视频质量 $q(R_k)$ 的映射； 片段被下载到回访缓冲中，其中包含下载了的但还没看过的片段。 $B(t) \\in [0, B_{max}]$ 表示 $t$ 时刻缓冲区的占用， $B_{max}$ 表示内容提供商的策略和播放器的存储限制； 播放过程形式化 在 $t_k$ 时刻，视频播放器开始下载第 $k$ 个块，这个块的下载时间可以计算为： $d_k(R_k) / C_k$； $C_k$ 表示下载过程中经历的平均下载速度； 一旦第 $k$ 个块下载完毕，播放器等待 $\\Delta t_k$ 时间并在 $t_{k+1}$ 时刻下载下一个块 $k+1$ ； 假设等待时间 $\\Delta t_k$ 很短并且不会导致重缓冲事件，用 $C_t$ 表示 $t$ 时刻的网络吞吐量： $$ t_{k+1} = t_k + \\frac{d_k(R_k)}{C_k} + \\Delta t_k $$ $$ C_k = \\frac{1}{t_{k+1} - t_k - \\Delta t_k} \\int_{t_k}^{t_{k+1} - \\Delta t_k} C_t dt $$ $B(t)$ 的变化取决于下载的块和播放的块的数量： 在第 $k$ 个块下载完毕之后缓冲区占用增长 $L$ 秒；用户观看一个块之后缓冲区占用减少 $L$ 秒； $B_k = B(t_k)$ 表示播放器开始下载第 $k$ 个块时的缓冲区占用； 缓冲区占用的动态变化可以表示为： $$ B_{k+1} = \\big( (B_k - \\frac{d_k(R_k)}{C_k})_+ + L - \\Delta t_k \\big)_+ $$ 其中 $(x)_+ = max\\lbrace x, 0 \\rbrace $ 确保其非负； 如果 $B_k \u003c d_k(R_k) / C_k$ ，表示缓冲区在播放器还在下载第 $k$ 个块时变空，而这会导致重缓冲事件； 等待时间 $\\Delta t_k$ 的确定也称为块调度问题，本文中假设播放器在第 $k$ 个块下载完毕之后尽可能快地去下载第 $k+1$ 个块（除了缓冲区满了的情况，播放器等待缓冲区中的块被消耗之后再下载新的块）： $$ \\Delta t_k = \\Big( \\big( B_k - \\frac{d_k(R_k)}{C_k} \\big)_+ + L - B_max \\Big)_+ $$ ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:4:1","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"QoE 最大化问题 QoE 的组成部分： 平均视频质量：在所有块中每个块平均的质量，计算为： $$ \\frac{1}{K} \\sum^K_{k=1} q(B_k) $$ 平均质量变化：相邻块之间质量变化的平均值，计算为： $$ \\frac{1}{K-1} \\sum^{K-1}_{k=1} | q(R_{k+1}) - q(R_k) | $$ 重缓冲总计时间：对每个块而言，当轮到其被消耗时但下载块的过程还没完成即出现了重缓冲，总时间计算为： $$ \\sum^K_{k=1} (\\frac{d_k(R_k)}{C_k} - B_k)_+ $$ 启动延迟 $T_s$ ，假设 $T_s \\ll B_{max}$ 。 对不同用户而言，上述 4 种因素的重要程度不同。使用上述分量的加权，定义视频块 $1$ 到 $K$ 的 QoE： $$ QoE^K_1 = \\sum^K_{k=1} q(R_k) - \\lambda \\sum^K_{k=1} | q(R_{k+1}) - q(R_k) | - \\mu \\sum^K_{k=1} (\\frac{d_k(R_k)}{C_k} - B_k)_+ - \\mu_s T_s,\\ \\lambda, \\mu, \\mu_s \\nless 0 $$ 相对较小的 $\\lambda$ 表示用户不太关心视频质量变化； $\\lambda$ 越大表明越需要使视频质量变得光滑。 相对较大的 $\\mu$ 表示用户很在意重缓冲； 在这里文章倾向于启动延迟很低，所以采用大 $\\mu_s$ ； QoE 的最大化： 输入：吞吐量迹 ${C_t, t \\in [t_1, t_{K+1}]}$ 输出：码率选择 $R_1, …, R_K$；启动时间 $T_s$ ； 需要注意：当最大化的决策发生在播放过程中时，启动时间便不再存在； ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:4:2","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"算法 上图中的 QoE 最大化问题是一种随机优化控制问题，随机性源自可获得的吞吐量 $C_t$ 。 $t_k$ 时刻播放器选择码率 $R_k$ ，只有过去的吞吐量 $\\lbrace C_t, t \\le t_k \\rbrace$ 可知，未来的值 ${C_t, t \u003e t_k}$ 未知。 但是，吞吐量预测器可以用于获取对吞吐量的预测，定义其为 $\\lbrace \\hat{C_t}, t \u003e t_k \\rbrace$ 。 基于这样的预测和缓冲区的信息（精确可知），码率选择器对下个块 $k$ 的码率选择可以表示为： $$ R_k = f \\big( B_k, \\lbrace \\hat{C_t}, t \u003e t_k \\rbrace, \\lbrace R_i, i \u003c k \\rbrace \\big) $$ 文章只关注码率自适应算法，假设已经得到了预测值，并根据预期预测误差对其进行了表征，即： 我们着重于 $f(\\cdot)$ 的设计以及预测误差对比较控制算法性能的影响。 现有的两类自适应算法：基于速率和基于缓冲区，分别可以表示为： $$ R_k = f \\big( \\lbrace \\hat{C_t}, t \u003e t_k \\rbrace, \\lbrace R_i, i \u003c k \\rbrace \\big) $$ $$ R_k = f(B_k, \\lbrace R_i, i \u003c k \\rbrace) $$ 前者只基于吞吐量的预测结果而不管缓冲区状况；后者只基于缓冲区而不管未来的吞吐量可能状况； 这两种方法在原则上都只是次优的，理想情况下我们想要同时考虑缓冲区占用和吞吐量预测结果。 ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:4:3","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"MPC for Optimal Bitrate Adaptation ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:5:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"Why MPC MPC 天然适合码率自适应问题。 Strawman solutions 码率自适应问题本质是随机控制优化问题，就这一点而言，有两个知名控制算法： Proportional-integral-derivation(PID) control. Markov Decision Process(MDP) based control. PID 相较 MDP 而言计算起来更加简单，只能用于使系统稳定，不能显式地优化 QoE 目标；此外 PID 被设计用于有连续的时间和连续的状态空间的问题中，用于当前这种高度离散化的问题中会导致性能亏损和不稳定。 应用 MDP 的话可以将吞吐量和缓冲区状态形式化为马氏过程，然后使用诸如值迭代和策略迭代等标准算法求出最优解。 （然而，这有一个很强的假设，即吞吐量动态遵循马尔可夫过程，不清楚这在实践中是否成立。我们将 MDP 的潜在用途和吞吐量动态分析作为未来的工作。） Case for MPC 理想情况下，如果给出未来吞吐量的完美数据，那么启动时间 $T_s$ 和最优码率选择 $R_1, … R_K$ 可以一下子就计算出来； 实际情况中，虽然不能得到未来吞吐量的完美预测，但是我们可以假设吞吐量在较短的时间段 $[t_k, t_{k+N}]$ 内不会剧烈变化。 基于此，可以使用当前视界中的预测来应用第 1 个码率 $R_k$ ，之后将视界向前移动到 $[t_{k+1}, t_{k+N+1}]$ 。 而这种方案就称为 MPC。MPC 的一般好处在于，MPC 可以利用预测在约束条件下在线优化动态系统中的复杂控制目标。 ","date":"2021-12-23","objectID":"/posts/papers/note-for-mpc/:5:1","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/papers/note-for-mpc/"},{"categories":["paper"],"content":"论文概况 Link：TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming Level：ACM MM 21 Keywords：Adaptive tiling and bitrate，Mobile streaming ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:1:0","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"创新点 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:2:0","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"背景 现有的固定的 tile 划分方式严重依赖 viewport 预测的精度，然而 viewport 预测的准确率往往变化极大，这导致基于 tile 的策略实际效果并不一定能实现其设计初衷：保证 QoE 的同时减少带宽浪费。 考虑同样的 viewport 预测结果与不同的 tile 划分方式组合的结果： 从上图可以看到： 如果采用$6 \\times 6$的分块方式，就会浪费 26，32 两个 tile 的带宽，同时 15，16，17 作为本应在实际 viewport 中的 tile 并没有分配最高的优先级去请求。 如果采用$5 \\times 5$的分块方式，即使预测的结果与实际的 viewport 有所出入，但是得益于 tile 分块较大，所有应该被请求的 tile 都得到了最高的优先级，用户的 QoE 得到了保证。 另一方面，基于 tile 的方式带来了额外的编解码开销（可以看这一篇论文：note-for-optile），而这样的性能需求对于移动设备而言是不可忽略的。 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:2:1","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"创新 除了考虑常见的因素如带宽波动和缓冲区占用之外，提出同时自适应分块策略和码率分配以应对变化的 viewport 预测性能和受限的移动设备的解码能力。 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:2:2","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"论文组织 首先使用现实世界的轨迹分析了典型的 viewport 预测算法并确定了其性能的不确定性。 接着讨论了不同的分块策略在 tile 选择和解码效率上的影响。 自适应的分块策略可以适应 viewport 预测的错误，并能保证 tile 选择的质量。 为解码时间建构了分析模型，可以在给定受限的计算资源时用于选择恰当的分块策略和码率。 形式化了优化模型，讨论了自适应算法的细节。 评估证明了方案的优越性。 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:2:3","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"Motivation ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:3:0","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"分块策略对 tile 选择的影响 实现 4 种轻量的 viewport 预测算法：线性回归 LR、岭回归 RR、支持向量回归、长短期记忆 LSTM。 设置历史窗口大小为 2s，预测窗口大小为 1s；viewport 的宽度和高度分别为 100°和 90°。 默认的分块策略为$6 \\times 6$；头部移动数据集来自公开数据集。 viewport 预测的不准确性 研究表明，用户的头部运动主要发生在水平方向而较少发生在垂直方向，所以只分析水平方向的预测。 实际的商业移动终端只有有限的传感和处理能力，并不能支持高频的 viewport 预测采样。 视频内容的不同类型会显著影响预测的精度，基于录像环境（室内或户外）和相机的运动状态分类。 改变采样频率会直接影响 viewport 预测的精度，频率越低，精度越低。 相机运动的 viewport 预测错误率比相机静止的明显更高。 通过分块容忍预测错误 因为不管 tile 的哪个部分被包含在预测的 viewport 中，只要包含一部分就会请求整个 tile，所以增大每个 tile 的尺寸能吸收预测错误。 实验验证： 设定从$4 \\times 4$到$10 \\times 10$的分块方式，使用不同的预测误差来检查分块设定可以容纳的最大预测误差，同时保持 tile 选择结果的相同质量。 用$F_1$分数来表示 tile 选择的质量：$F_1 = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}$。 实验结果表明更大的 tile 尺寸更能容忍预测错误。 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:3:1","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"分块策略对解码复杂性的影响 虽然当前的移动设备硬件性能发展迅速，但是实时的高码率高分辨率全景视频的解码任务还是充满挑战。 分块对于编码的影响： tile 越小，帧内和帧间内容的相关区域就越小，编码效率越低。 直接影响解码复杂性的因素： tile 的数量。 视频的分辨率。 用于解码的资源。 固定其中 1 个因素改变另外 2 个因素来检查其对解码的影响： 根据对图的观察可以得出这 3 个因素在经验上是相互独立的，因为这三幅图之中的图像几乎相同。 分别用$F_n(x), F_r(x), F_c(x)$表示 tile 数量、分辨率、线程数量为$x$时，解码时间与基线时间的比值。 将这 3 个比值作为 3 个乘子建立分析模型： $$ D = D_0 \\cdot F_n(x_1) \\cdot F_r(x_2) \\cdot F_c(x_3) $$ 上式表示计算整体的解码时间，其中 tile 数量为$x_1$、分辨率为$x_2$、线程数量为$x_3$；$D_0$时解码的基线时间。 这个模型将用于帮助做出分块和码率适应的决策。 注意在实际情况中，可供使用的计算资源（线程数）是受限的，需要根据设备当前可用的计算资源来分配。 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:3:2","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"TBRA 的设计 $S = \\lbrace s_1, s_2, … \\rbrace$ 表示 360°视频分块方式的集合； 对于分块方式$s_i$，$|s_i|$ 表示这种方案中 tile 的数量； 当 $i \u003c j$ 时，假设 $|s_i| \u003c |s_j|$； 对于分块方式$s$， $b_{i, j}$ 表示第 $i$ 块的 tile $j$，$i \\le 块的数量, j \\le |s|$； 目标是确定分块方式$s$，并为每个 tile 确定其码率$b_{i, j}$； ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:4:0","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"分块自适应 自适应的概念 分块尺寸大小会导致 viewport 容错率和传输效率的变化。 分块尺寸小，极端情况下每个像素点作为一个 tile，viewport 容错率最小，但是传输效率达到 100%； 分块尺寸大，极端情况下整个视频帧作为一个 tile，viewport 容错率最大，但是传输效率最小； 优化的目标就是在这两种极端条件中找到折中的最优解。 分块选择 以$\\overline{r_d}, d \\in \\lbrace left, right, up, down \\rbrace$为半径扩大预测区域；$e_d$表示过去 n 秒中方向 $d$ 的预测错误平均值； $$ \\overline{r_d} = (1-\\alpha) \\cdot \\overline{r_d} + \\alpha \\cdot e_d $$ 预测区域的扩展被进一步用于 tile 选择，受过去预测精度的动态影响。 下一步检查不同分块方式，进而找到 QoE 和传输效率之间的折中。 对于每个分块方式，比较基于扩展的预测区域的 tile 选择的质量。使用 2 个比值作为 QoE 和传输效率的度量： $$ Miss\\ Ratio = \\frac{of\\ missed\\ pixels\\ in\\ expanded\\ prediction}{of\\ viewed\\ pixels} $$ $$ Waste\\ ratio = \\frac{of\\ unnecessary\\ pixels\\ in\\ expanded\\ prediction}{of\\ viewed\\ pixels} $$ 这 2 个比值的 tradeoff 可以在上图中清晰地看出。 使用分块方式对应的惩罚$Tiling\\ i_{penalty}$来评估其性能： $$ Tiling\\ i_{penalty} = \\beta \\cdot Miss\\ Ratio + |1/cos(\\phi_i)| \\cdot Waste\\ Ratio $$ $\\phi_i$ 是 viewport $i$ 的中心纬度坐标，它表明随着 viewport 的垂直移动，浪费率的权重会发生变化。（因为投影方式是 ERP） 检查完所有的方式之后，最终选择惩罚最小的分块方式。 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:4:1","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"码率自适应 视频质量 $w_{i, j}$表示在第 $i$ 个视频块播放时，tile $j$ 的权重；在当前方案中 $w_{i, j} = 0\\ or\\ 1$ 取决于 tile 是否在预测的 viewport 中。 $q(b_{i, j})$ 是 tile 比特率选择 $b_{i, j}$ 与用户实际感知到的质量之间的非递减映射函数。 第 $i$ 个视频块的质量等级可以定义为： $$ Q^{(1)}_i = \\sum^n_{j=1} w_{i, j} q(b_{i, j}) $$ 使用最新研究的主观视频质量模型： $$ subjective\\ PSNR:\\ q_i = PSNR_i \\cdot [M(v_i)]^{\\gamma} [R(v_i)]^{\\delta} $$ $M(v_i)$ 是检测阈值；$R(v_i)$ 是视网膜滑移率；$v_i$ 是第播放 $i$ 个视频块时 viewport 的移动速度；$\\gamma = 0.172, \\delta = -0.267$ 质量变化 连续视频块之间的强烈质量变化会损害 QoE，定义质量变化作为响铃两个视频块之间质量的变化： $$ Q^{(2)}_i = |Q^{(1)}_1 - Q^{(1)}_{i-1}|,\\ i \\in [2, m] $$ 重缓冲时间 参数设置： $C_i$ 表示下载视频块 $i$ 的预计吞吐量； $B_i$ 表示客户端开始下载视频块 $i$ 时缓冲区的占用率； $B_{default}$ 表示在启动阶段默认的缓冲区填充等级，记 $B_{default} = B_1$； 下载第 $i$ 个视频块需要时间 $\\sum^n_{j=1} b_{i, j} / C_i$ ； 每个视频块的长度为 $L$ ； 缓冲区的状态应该在每次视频块被下载的时候都得到更新，则下一个视频块 $i+1$ 的缓冲区占用情况可以计算为： $$ B_{i+1} = max\\lbrace B_1 - \\sum^n_{j=1} b_{i, j} / C_i,\\ 0\\rbrace + L $$ 下载第 $i$ 个视频块时的重缓冲时间可以计算为： $$ Q^{(3)}_i = max \\lbrace \\sum^n_{j=1} b_{i, j} / C_i - B_i,\\ 0 \\rbrace + t_{miss} $$ 第一部分是下载时间过长且缓冲区耗尽，视频无法播放情况下的重新缓冲时间； 第二部分 $t_{miss}$ 表示下载缺失的 tile 所花费的时间（在视频块播放过程中被看到但是之前没有分配码率的 tile）。 优化目标 第 $i$ 个视频块的整体优化目标可以定义为前述 3 个指标的加权和： $$ Q_i = pQ^{(1)}_i - qQ^{(2)}_i - rQ^{(3)}_i $$ 各个系数的符号分配表示：最大化视频质量、最小化块间质量变化、最小化重缓冲时间。 传统意义上使用所有视频块的平均 QoE 作为优化对象，但实际上很难获得从块 $1$ 到块 $m$ 的整个视界的完美的未来信息。 为了处理预测长期吞吐量和用户行为的难度，采用基于 MPC 的框架，在有限的范围内优化多个视频块的 QoE，最终的目标函数可以形式化为： $$ \\underset{b_{i, j}, i \\in [t, t+k-1], j \\in [1, n]}{max} \\sum^{t+k-1}_{i=t} Q_i $$ 因为短期内的 viewport 预测性能和网络状况可以很容易得到，QoE 优化可以通过使用窗口 $[t, t+k-1]$ 内的预测信息； 接着将视界向前移动到 $[t+1, t+k]$ ，更新新的优化窗口的信息，为下一个视频块执行 QoE 优化，直到最后一个窗口。 使用基于 MPC 的公式的优点：由于受限的问题规模，每个优化问题的实例都是实际可解的。 高效求解 提出的公式天然适合在线求解，得益于短窗口的实例问题规模很小，QoE 优化可以通过详尽搜索定期解决。 但是因为优化过程需要高频调用，所以对于大的搜索空间还是充满挑战。 为了支持实时优化，需要对搜索空间进行高效剪枝，确定几点约束： 解码时间需要被约束； 解码时间应该短于回放长度。 给定移动设备上可用的计算资源，可以得到支持的最大解码线程数。 基于解码时间的分析模型，由于解码复杂度和分辨率的单调性，可以找到设备能够限定时间内解码的最大质量水平，这会将码率选择限制在有界搜索空间内。 码率选择应该考虑吞吐量的限制：$\\sum^n_{j=1} b_{i, j} \\le LC_i$ ； 不会主动耗尽缓冲区，无需让其处理吞吐量的波动。 码率选择应该考虑 tile 的分类； tile 的码率不应该低于同一个视频块中更低权重 tile 的码率： $b_{i, j} \\ge b_{i, j’}, \\forall w_{i, j} \u003e w_{i, j’}$ 。 属于相同类别的 tile 比特率选择应该是同一个等级； 这使码率自适应在 tile 类的级别上执行而非单个 tile 的级别，大大减小了搜索空间的规模。 当优化窗口中的吞吐量和用户行为保持稳定时，同一个窗口中的 tile 应该有相同的结果。 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:4:2","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"TBRA workflow 这样的方式需要在服务端存储大量的按照不同分块方式划分的不同码率版本的视频块，这一点可以进一步研究。 但是对于移动终端设备而言，这样的解决方案只引入了可以忽略不计的开销。 观察到 tile 自适应问题具有全局最优通常就是局部最优的特点，因此可以大大减少计算量。 基于 MPC 的优化 workflow 还可以有效地解决码率自适应问题。 ","date":"2021-12-21","objectID":"/posts/papers/note-for-tbra/:4:3","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/papers/note-for-tbra/"},{"categories":["paper"],"content":"论文概况 Link：Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019 Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:1:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"Workflow Tracking：VR motion 追踪算法：应用了高斯混合模型来检测物体的运动。 Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户 viewport 来自动更正潜在的预测错误。 Update：viewport 动态更新算法：动态调整预测的 viewport 大小去覆盖感兴趣的潜在 viewport，同时尽可能保证最低的带宽消耗。 Evaluation：经验用户/视频评估：构建 VR viewport 预测方法原型，使用经验 360°视频和代表性的头部移动数据集评估。 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:2:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"全景直播推流的预备知识 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:3:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"VR 推流直播 相比于传统的 2D 视频推流的特别之处： VR 系统是交互式的，viewport 的选择权在客户端； 呈现给用户的最终视图是整个视频的一部分； ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:3:1","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"用户头部移动的模式 在大量的 360°视频观看过程中，用户主要的头部移动模式有 4 种，使用$i-j\\ move$来表示； 其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。 $1-1\\ move$：单个物体以单一方向移动； $1-n\\ move$：单个物体以多个方向移动； $m-n\\ move$：多个物体以多个方向移动； $Arbitrary\\ move$：用户不跟随任何感兴趣的物体而移动，viewport 切换随机； 现有的直播 VR 推流中的 viewport 预测方法是基于速度的方式，这种方式只对$1-1\\ move$这一种模式有效。 本方案的目标是提出对 4 种模式都有效的预测策略。 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:3:2","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"系统架构 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:4:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"理论创新 核心功能模块： motion detection：区分运动物体与静止的背景。 feature selection：选择代表性的特征并对运动物体做追踪。 这两个模块使系统能识别用户可能感兴趣的 viewport。 使用贝叶斯方法分析用户观看行为并形式化用户的兴趣模型。 使用错误恢复机制来使当预测错误被检测到时的预测 viewport 去适应实际的 viewport，尽管不能消除预测错误但是能避免在此基础上进一步的预测错误。 使用动态 viewport 更新算法来产生大小可变的 viewport，通过同时考虑跟踪到的 viewport 轨迹和用户当前的速度（矢量）。 这样，即使用户的运动模式很复杂也能有更高的概率去覆盖潜在的视图。 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:4:1","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"具体实施 虽然提出的运动追踪和错误处理机制是计算密集型的任务，但是这些组件都部署在 video packager 中，运行在服务端。 将生成 VR 视图的工作负载移动到服务端，进一步减少了客户端的计算开销以及网络开销。 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:4:2","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"形式化 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:5:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"基于运动轨迹的 viewport 预测 使用GMM完成运动检测，使用Shi-Tomasi algorithm解决运动轨迹跟踪问题。 运动检测 GMM 前景提取 特征选取与过滤 采用 Shi-Tomasi algorithm 从视频中检测代表性的特征，直接检测得到的代表性特征数量较多而难以追踪。 采用两种过滤的方法来减少要追踪的特征数量。 比较当前帧和前一帧的特征，只保留其共有的部分。 采用第 1 步中运动检测的方式，只保留运动的部分。 viewport 生成 经过选择和过滤之后的特征通常分布在不能被单一用户视图所覆盖的广阔区域中。 在整个 360°视频中可能存在多个运动的物体，即$m-n\\ move$。 提出一种系统的方式来产生用户最可能跟随观看的 viewport。 直觉是用户更可能将大部分注意力放在两种类型的物体上： 离用户更近的物体。 就物理形状而言更“重要”的物体。 这两种类型的物体大多包含最密集和最大量的特征，因此通过所有特征的重心来计算预测用户视图的中心。 对于剩余的特征列表：$\\vec{F} = [f_1, f_2, f_3, …, f_k]$，其中$f_i(i = 1 … k)$表示特征$f_i = \u003cf^{(x)}_i, f^{(y)}_i\u003e$的像素点坐标，则预测出的 viewport 中心坐标可以计算出来： $$ l_x = \\frac{1}{k} \\sum^k_{i=1} f^{(x)}_i;\\ l_y = \\frac{1}{k} \\sum^k_{i=1} f^{(y)}_i. $$ 考虑到即使预测的 viewport 中包含用户观看的物体，预测得到的 viewport 也可能会与实际的 viewport 存在差异。 所以预测的 viewport 可能比实际的 viewport 要大，所以使用缩放因子$S_c$来产生预测的 viewport。 给出用户 viewport 的大小$S_{user}$，预测的 viewport 可以通过$S_{pre} = S_c \\cdot S_{user}$计算出来。 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:5:1","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"基于用户反馈的错误恢复 video packager 可以通过 HMD 和 web 服务器通过反向路径从用户处检索用户实际视图的反馈信息。 基于反馈的错误恢复机制在以下两种场景中表现良好： 没有运动的物体 如果没有检测到运动的物体，则用户很可能是在观看静止的物体，这会导致基于运动目标的 viewport 预测失败。 在这种场景中，可以认为视频内容已经不再是决定用户 viewport 的因素，而只取决于用户自身的行为。 因此采用基于速度的方式来预测 viewport。（这样的决策可以在运动检测模块没有检测到运动物体时就做出） 一旦从反馈路径上得到用户信息，可以产生用户 viewport 位置向量：$\\vec{L} = [l_1, l_2, l_3, …, l_M]$，其中$l_i$表示第$i$个帧中用户 viewport 的位置，$M$表示视频播放缓冲区中的帧数。那么可以计算 viewport 速度： $$ \\vec{V} = \\frac{\\vec{(l_2 - l_1)} + \\vec{(l_3 - l_2)} ….(l_M - l_{M-1})}{M-1} = \\frac{(\\vec{l_M - l_1})}{M-1} $$ 下一帧的预测位置$L_{M=1}$也可以计算出来： $$ l_{M+1} = l_M + \\vec{V} $$ 预测视图与实际视图的不匹配 一旦运动追踪策略检测到用户实际的视图和预测的视图不同，就会触发恢复机制去追踪用户实际在看着的物体。 可以使用运动追踪方式确定用户实际观察的物体的速度。 给出前一帧匹配的特征$\\vec{FA} = [fA_1, fA_2, fA_3, …, fA_p]$和当前帧的特征$\\vec{FB} = [fB_1, fB_2, fB_3, …, fB_p]$，可以计算出速度： $$ V_x = \\frac{1}{p} (\\sum^p_{i=1} fB^{(x)}_i - \\sum^p_{i=1}fA^{(x)}_i),\\ V_y = \\frac{1}{p} (\\sum^p_{i=1} fB^{(y)}_i - \\sum^p_{i=1}fA^{(y)}_i), $$ 假设预测的 viewpoint 是$(l_x, l_y)$，修改之后的 viewpoint 是$(l_x + V_x,\\ l_y + V_y)$。 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:5:2","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"动态 viewport 更新 前述的错误恢复机制发生在 viewport 预测错误出现之后，任务是避免未来更多的错误。 动态的 viewport 更新则努力避免 viewport 预测错误。 关键思想是扩大预测的 viewport 大小，以高概率去覆盖$m-n\\ move$和$arbitrary\\ move$下所有潜在的运动目标；更重要的是动态调整视图的大小去获得更高效的带宽利用率。 对于一个 360°全景视频，将 360°的帧均分为$N = n \\times n$个网格，每个网格看作是一个 tile，预测的 viewport 即为$N$个 tile 的子集。 使用贝叶斯方法分析用户的观看行为，每个 tile 分配一个独立的贝叶斯模型，所以每个 tile 可以独立更新。 设$X$表示用户 viewport，$Y$表示静态内容，$Z$表示运动物体。 未来的用户 viewport 可以以条件概率计算为$P(X|Y,\\ Z)$，$Y$与$Z$相互独立。 用户的 viewport 可以通过反馈信息得出$P(X)$；用户观看静态特征可以表示为$P(X|Y)$；用户观看动态特征可以表示为$P(X|Z)$。 $P(X|Y, Z)$可以计算为： $$ P(X|Y, Z) = \\frac{P(Y|X) \\cdot P(Z|X) \\cdot P(X)}{P(Y, Z)} $$ 只要用户开始观看，对于 tile $T_i$，就能得到其先验概率$P(Y_i|X_i)$和$P(Z_i|X_i)$，进而根据贝叶斯模型计算出$P(X|Y, Z)$。 为每个 tile 定义两种属性： 当前状态：表示此 tile 是否属于预测的 viewport（属于标记为$PREDICTED$，不属于标记为$NONPREDICTED$）。 生存期：表示此 tile 会在 view port 中存在多长时间（例如定义 3 种等级：$ZERO$，$MEDIUM$，$HIGH$，实际的定义划分可以根据具体的用户和视频设定）。 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:5:3","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"预测步骤 按照形式化中提出的 3 步，分为系统初始化、帧级别的更新、缓冲区级别的更新。 系统初始化 初始化阶段中，view 更新算法将所有的$N$个 tile 标注为$PREDICTED$，并将生存期设置为$MEDIUM$，即系统向用户发送完整的一帧作为自举。 这样设定的原因在于：当用户第一次启动视频会话时，允许“环视”类型的移动，这可能会覆盖 360°帧的任意 viewport。 帧级别的更新 给定一帧，应用修改后的 motion 追踪算法在运动区域中选择特征，而不使用特征的密度做进一步的过滤。 使用有多个 tile 的多个视图来覆盖一个放大的区域，该区域包含作为预测 viewport 的移动对象上的所有特征，这样就能适应$m-n\\ move$中的用户行为。 设计帧级别的算法标记选择的 tile 作为$PREDICTED$并设置其生存期为$HIGH$（直觉上讲运动中的物体或用户所感兴趣的静态特征会更以长时间保留在 viewport 之中）。 缓冲区级别的更新 以缓冲区长度为间隔检索用户的实际视图，基于此可以对 tile 的两种属性做出调整。 对于与用户实际视图重叠的 tile，设置为$PREDICTED$和$HIGH$。 对于用户实际视图没有出现但出现在预测的视图中的 tile，生存期减 1，如果生存期减为$ZERO$，就重设其状态为$NONPREDICTED$，将其从预测的 viewport 中移除。 ","date":"2021-12-20","objectID":"/posts/papers/note-for-content-motion-viewport-prediction/:6:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/papers/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"论文概况 Link：QoE-driven Mobile 360 Video Streaming: Predictive View Generation and Dynamic Tile Selection Level：ICCC 2021 Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles ","date":"2021-12-16","objectID":"/posts/papers/note-for-rnnQoE/:1:0","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/papers/note-for-rnnQoE/"},{"categories":["paper"],"content":"系统建模与形式化 ","date":"2021-12-16","objectID":"/posts/papers/note-for-rnnQoE/:2:0","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/papers/note-for-rnnQoE/"},{"categories":["paper"],"content":"视频划分 先将视频划分成片段：$\\Iota = {1, 2, …, I}$表示片段数为$I$的片段集合。 接着将片段在空间上均匀划分成$M \\times N$个 tile，FOV 由被用户看到的 tile 所确定。 使用 ERP 投影，$(\\phi_i, \\theta_i),\\ \\phi_i \\in (-180\\degree, 180\\degree], \\theta_i \\in (-90\\degree, 90\\degree]$来表示用户在第$i$个片段中的视点坐标。 播放过程中记录用户头部运动的轨迹，积累的数据可以用于 FOV 预测。 跨用户之间的 FOV 轨迹可以用于提高预测精度。 ","date":"2021-12-16","objectID":"/posts/papers/note-for-rnnQoE/:2:1","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/papers/note-for-rnnQoE/"},{"categories":["paper"],"content":"QoE 模型 前提 视频编解码器预先确定，无法调整每个 tile 的码率。 实现 每个 tile 都以不同的码率编码成不同的版本。 每个 tile 都有两种分辨率的版本。 QoE 内容 客户端收到的视频质量和观看时的卡顿时间。 质量形式化 对于每个片段$i \\in \\Iota$，$S_i = {\\tau_{i, j}}_{j=1}^{M \\times N}$是用来表示用户实际看到的 tile 的集合的向量。 $\\tau_{i, j} = 1$表示第$i$个段中的第$j$个 tile 被看到；$\\tau_{i, j} = 0$表示未被看到。 同样的， $\\tilde{S}_i = {\\tilde{\\tau}_{i, j}}_{j = 1}^{M \\times N}$ 表示经过 FOV 预测和 tile 选择之后成功被传送到用户头戴设备上的 tile 集合的向量。 $\\tilde{\\tau}_{i, j} = 1$表示第$i$个段中的第$j$个 tile 被用户接收；$\\tilde{\\tau}_{i, j} = 0$表示未被接收。 第$i$个段的可感知到的质量可以表示为： $$ Q_i = \\sum_{j = 1}^{M \\times N} p_{i, j}b_{i, j}\\tau_{i, j}\\tilde{\\tau}_{i, j} $$ $b_{i, j}$表示第$i$个片段的第$j$个 tile 的码率；$p_{i, j}$表示对不同位置 tile 所分配的权重； 关于权重$p_{i, j}$ 研究表明用户在全景视频 FOV 中的注意力分配并不是均等的，越靠近 FOV 中心的 tile 对用户的 QoE 贡献越大。 下面讨论单个片段的情况：用$(\\phi_j, \\theta_j)$表示 tile 中心点的坐标，并映射到笛卡尔坐标系上$(x_j, y_j, z_j)$： $$ x_j = cos\\theta_jcos\\phi_j,\\ y_j = sin\\theta_j,\\ z_j = -cos\\theta_jsin\\phi_j $$ 则两个 tile 之间的半径距离$d_{j, j’}$可以表示为： $$ d_{j, j’} = arccos(x_j x_{j’} + y_j y_{j’} + z_j z_{j’}) $$ 对于第$i$个片段，假设用户 FOV 中心的 tile 为$j^*$，那么第$j$个 tile 的权重可以计算出来： $$ p_{i, j} = (1 - d_{j, j^*} / \\pi) \\tau_{i, j} $$ 卡顿时间形式化 当$\\tilde{\\tau}_{i, j}$与$\\tau_{i, j}$出现分歧时，用户就不能成功收到请求的 tile，头戴设备中显示的内容就会被冻结，由此导致卡顿。 对于任意的片段$i \\in \\Iota$，相应的卡顿时间$D_i$可以计算出来： $$ D_i = \\frac{\\sum_{j = 1}^{M \\times N} b_{i, j} \\cdot [\\tau_{i, j} - \\tilde{\\tau}_{i, j}]^+}{\\xi} $$ $[x]^+ = max \\lbrace x, 0 \\rbrace $；$\\xi$表示可用的网络资源（已知，并且在推流过程中保持为常数） 卡顿发生于在播放时，用户 FOV 内的 tile 还没有被传输到用户头戴设备中的时刻，终止于所有 FOV 内 tile 被成功传送的时刻。 质量与卡顿时间的结合 $$ max\\ QoE = \\sum_{i = 1}^I (Q_i - wD_i) $$ $w$表示卡顿事件的惩罚权重。例如，w＝1000 意味着 1 秒视频暂停接收的 QoE 惩罚与将片段的比特率降低 1000 bps 相同。 ","date":"2021-12-16","objectID":"/posts/papers/note-for-rnnQoE/:2:2","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/papers/note-for-rnnQoE/"},{"categories":["paper"],"content":"联合 viewport 预测与 tile 选择 联合框架包括 viewport 预测和动态 tile 选择两个阶段。 viewport 预测阶段集成带有注意力机制的 RNN，接收用户的历史头部移动信息作为输入，输出每个 tile 出现在 FOV 中的可能性分布。 选择 tile 阶段为预测的输出建立的上下文空间，基于上下文赌博机学习算法来选择 tile 并确定所选 tile 的质量版本。 ","date":"2021-12-16","objectID":"/posts/papers/note-for-rnnQoE/:3:0","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/papers/note-for-rnnQoE/"},{"categories":["paper"],"content":"Viewport 预测 FOV 预测问题可以看作是序列预测问题。 不同用户观看相同视频时的头部移动轨迹有强相关性，所以跨用户的行为分析可以用于提高新用户的 viewport 预测精度。 被广泛使用的 LSTM 的变体——Bi-LSTM（Bi-directional LSTM）用于 FOV 预测。 输入参数构造 为了构造 Bi-LSTM 学习网络，需要对不同用户的 viewpoint 特性作表征。 在用户观看事先划分好的$I$个片段时，记录每个片段对应的 viewpoint 坐标： $\\Phi_{1:I} = {\\phi_i}^I_{i = 1},\\ \\Theta_{1:I} = {\\theta_i}^I_{i=1}$ 预测时使用的历史信息的窗口大小记为$k$； 对于第$i, (i \u003e k)$个片段，相应的 viewpoint 特性由$\\Phi_{i-1:i-k}$和$\\Theta_{i-1:i-k}$所给出； 列索引$m_i$和行索引$n_i$作为 viewpoint tile $(\\phi_i, \\theta_i)$的标签，由独热编码表示； 通过滑动预测的窗口，所看到的视频片段的特性和标签可以被获取。 LSTM 网络构造 整个网络包含 3 层： 遗忘门层决定丢弃哪些信息； 更新门层决定哪类信息需要存储； 输出门层过滤输出信息。 为了预测用户在第$i$个段的 viewpoint，LSTM 网络接受$\\Phi_{i-1:i-k}$和$\\Theta_{i-1:i-k}$作为输入；输出行列索引； $$ m_i = LSTM(\\theta_{i-k}, …, \\phi_{i-1}; \\alpha) $$ $$ n_i = LSTM(\\theta_{i-k}, …, \\theta_{i-1}; \\beta) $$ $\\alpha, \\beta$是学习网络的参数；分类交叉熵被用作网络训练的损失函数。 Bi-LSTM 的特殊构造 将公共单向的 LSTM 划分成 2 个方向。 当前片段的输出利用前向和反向信息，这为网络提供了额外的上下文，加速了学习过程。 双向的 LSTM 不直接连接，不共享参数。 每个时间槽的输入会被分别传输到前向和反向的 LSTM 中，并分别根据其状态产生输出。 两个输出直接连接到 Bi-LSTM 的输出节点。 引入注意力机制为每步时间自动分配权重，从大量信息中选择性地筛选出重要信息。 将 Softmax 层堆叠在网络顶部，以获取不同 tile 的 viewpoint 概率。 ","date":"2021-12-16","objectID":"/posts/papers/note-for-rnnQoE/:3:1","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/papers/note-for-rnnQoE/"},{"categories":["paper"],"content":"动态 tile 选择 使用上下文赌博机学习算法来补偿 viewport 预测错误对 QoE 造成的影响。 上下文赌博机学习算法概况 上下文赌博机学习算法是一个基于特征的 exploration-exploitation 技术。 通过在多条手臂上重复执行选择过程，可以获得在不同上下文中的每条手臂的回报。 通过 exploration-exploitation，目标是最大化累积的回报。 组成部分形式化 上下文 直觉上讲，当预测的 viewpoint 不够精确时，需要扩大 FOV 并选择更多的 tile 进行传输。 为了做出第$i$个片段上的预测 viewpoint 填充决策，定义串联的上下文向量： $c_i = [f^s_i, f^c_i]$，$f^s_i$表示自预测的上下文，$f^c_i$表示跨用户之间的预测上下文。 预测输出的用户$u$的 viewpoint tile 索引用$[\\tilde{m}^u_{i-1}, \\tilde{n}^u_{i-1}]$表示； 实际的用户$u$的 viewpoint tile 索引用$[m_{i-1}^u, n_{i-1}^u]$表示； 那么对第$i$个片段而言，自预测的上下文可以计算出来： $$ f_i^s = [|m_{i-1}^u - \\tilde{m}^u_{i-1}|, |n_{i-1}^u - \\tilde{n}^u_{i-1}|] $$ 跨用户的上下文信息获取：使用 KNN 准则选择一组用户，其在前$k$个片段中的轨迹最接近用户$u$的轨迹。 使用$\\zeta$表示获得的用户集合，使用 $$E_{\\zeta_u}(m_i) = \\frac{1}{|\\zeta_u|}\\sum_{u \\in \\zeta_u} |m_i^u - \\tilde{m}_i^u|$$ $$E_{\\zeta_u}(n_i) = \\frac{1}{|\\zeta_u|}\\sum_{u \\in \\zeta_u}|n_i^u - \\tilde{n}_i^u|$$ 表示预测误差，则： $$ f_i^u = [E_{\\zeta_u}(m_i), E_{\\zeta_u}(n_i)] $$ 手臂 根据从第一个阶段得到的每个 tile 的可能性分布，所有的 tile 可以用倒序的方式排列。 最高可能性的 tile 被看作 FOV 的中心，高码率以此 tile 为中心分配。 剩余的带宽用于扩展 FOV，低可能性的 tile 被顺序选择来扩展 FOV 直至带宽耗尽。 手臂的状态$a \\in {0, 1}$表示 tile 选择的策略： $a = 0$表示 viewpoint 预测准确，填充 tile 分配了高质量； $a = 1$表示 viewpoint 预测不准确，填充 tile 分配的质量较低，为了传送尽可能多的 tile 而减少卡顿； 回报 给定上下文$c_i$，选择手臂$a$，预期的回报$r_{i, a}$建模为$c_i$和$a$组合的线性函数： $$ \\Epsilon[r_{i, a}|c_{i, a}] = c_{i, a}^T \\theta_a^* $$ 未知参数$\\theta_a$表示每个手臂的特性，目标是为第$i$个片段选择最优的手臂： $$ a_i^* = \\underset{a}{argmax}\\ c_{i, a}^T \\theta_a^* $$ 使用LinUCB算法做出特征向量的精确估计并获取$\\theta_a^*$。 ","date":"2021-12-16","objectID":"/posts/papers/note-for-rnnQoE/:3:2","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/papers/note-for-rnnQoE/"},{"categories":["paper"],"content":"实验评估 评估准备 使用现有的viewpoint 轨迹数据集，所有视频被编码为至少每秒 25 帧，长度为 20 到 60 秒； 视频每个片段被划分为$6 \\times 12$的 tile，每个的角度是$30\\degree \\times 30\\degree$； 初始 FOV 设定为$90\\degree \\times 90\\degree$，在 viewpoint 周围是$3 \\times 3$的 tile； 每个片段的长度为 500ms； 默认的预测滑动窗口大小$k = 5$； HD 和 LD 版本分别以按照 HEVC 的$QP={32, 22}$的参数编码而得到； 训练集和测试集的比例为$7:3$； Bi-LSTM 层配置有 128 个隐单元； batch 大小为 64； epoch 次数为 60； 性能参数 预测精度 视频质量 由传送给用户的有效码率决定：例如实际 FOV 中的 tile 码率总和 卡顿时间 规范化的 QoE 实际取得的 QoE 与在 viewpoint 轨迹已知情况下的 QoE 的比值 对比目标 预测阶段——预测精度 LSTM LR KNN 取 tile 的阶段——规范化的 QoE 两个阶段都使用纯 LR 只预测而不做动态选择 ","date":"2021-12-16","objectID":"/posts/papers/note-for-rnnQoE/:4:0","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/papers/note-for-rnnQoE/"},{"categories":["paper"],"content":"论文概况 Link：OpTile: Toward Optimal Tiling in 360-degree Video Streaming Level：ACM MM 17 Keyword：Dynamic tile division, Optimize encoding efficiency, Optimize tile size ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:1:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"背景知识 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:2:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"编码过程概述 对一帧图像中的每一个 block，编码算法在当前帧的已解码部分或由解码器缓冲的临近的帧中搜索类似的 block。 当编码器在邻近的帧中找到一个 block 与当前 block 紧密匹配时，它会将这个类似的 block 编码进一个动作向量中。 编码器计算当前 block 和引用 block 之间像素点的差异，通过应用变换（如离散余弦变换），量化变换系数以及对剩余稀疏矩阵系数集应用无损熵编码（如 Huffman 编码）对计算出的差异进行编码。 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:2:1","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"对编码过程的影响 基于 tile 的方式会减少可用于拷贝的 block 数量，增大了可供匹配的 tile 之间的距离。 不同的投影方式会影响编码变换输出的系数稀疏性，而这会降低视频编码效率。 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:2:2","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"投影过程 因为直接对 360 度图像和视频的编码技术还没有成熟，所以 360 度推流系统目前还需要先将 3D 球面投影到 2D 平面上。 目前应用最广的投影技术主要是 ERP 和 CMP，分别被 YouTube 和 Meta 采用。 ERP 投影 基于球面上点的左右偏航角$\\theta$与上下俯仰角$\\phi$将其映射到宽高分别为$W$和$H$的矩形上。 对于平面坐标为$(x, y)$的点，其球面坐标分别为： $$ \\theta = (\\frac{x}{W} - 0.5) * 360 $$ $$ \\phi = (0.5 - \\frac{y}{H}) * 180 $$ CMP 投影 将球面置于一个立方体中，光线从球心向外发射，并分别与球面和立方体相交于两点，这两点之间便建立了映射关系。 之后将立方体 6 个平面拼接成矩形，就可以使用标准的视频编码方式进行压缩。 关于投影方式还可以参考这里的讲解：谈谈全景视频投影方式 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:2:3","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"tile 方式的缺点 降低编码效率 tile 划分越细，编码越低效 增加更大的整体存储需求 可能要求更多的带宽 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:2:4","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"OpTile 的设计 直觉上需要增大一些 tile 的大小来使与这些 tile 相关联的片段能捕获高效编码所需的类似块。 同时也需要 tile 来分割视频帧来减少传输过程中造成的带宽浪费。 为了明白哪些片段的空间部分可以被高效独立编码，需要关于 tile 的存储大小的不同维度的信息。 为了找到切分视频的最好位置，需要在片段播放过程中用户 viewport 运动轨迹的偏好。 将编码效率和浪费数据的竞争考虑到同一个问题之中，这个问题关注的是一个片段中所有可能的视图的分布。 片段的每个可能的视图可以被 tile 的不同组合所覆盖。 目标是为一个片段选择一个 tile 覆盖层，以最小化固定时间段内视图分布的总传输带宽。 目标分离的部分考虑整个固定时间段的表示（representation）的存储开销。 目标的存储部分与下载的带宽部分相竞争。例如，如果一个不受欢迎的视频一年只观看一次，那么我们更喜欢一个紧凑的表示，我们可以期望向用户发送更多未观看的像素。 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:3:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"问题形式化 segment/片段 推流过程中可以被下载的连续播放的视频单元 basic sub-rectangle/基本子矩形 推流过程中可以被下载的片段中最小的空间划分块 solution sub-rectangle/解子矩形 片段中由若干基本子矩形组成的任何矩形部分 $x$ 用于表示子矩形在解中的存在的二元向量 $c^{(stor)}$ 每个子矩形存储开销相关的向量 $c^{(view)}$ 给定一个 segment 中用户 viewport 的分布，$c^{(view)}$指相关子矩形的预期下载字节 $\\alpha$ 分配到$c^{(view)}$的权重，以此来控制相对于传输一个片段的存储开销 考虑将 1 个矩形片段划分成 4 个基本子矩形，其对应的坐标如下： 4 个基本子矩形可以有 9 种分配方式，成为解子矩形，如下（因为需要保持对应的空间关系，所以只有 9 种）： $x$的形式化 可以用向量$x$来分别表示上图中子矩形在解中的存在： $$ [1 \\times 1\\ at\\ (0, 0),\\ 1 \\times 1\\ at\\ (0, 1),\\ 1 \\times 1\\ at\\ (1, 0), \\ 1 \\times 1\\ at\\ (1, 1),\\ 1 \\times 2\\ at\\ (0, 0),\\ 1 \\times 2\\ at\\ (1, 0), \\ 2 \\times 1\\ at\\ (0, 0),\\ 2 \\times 1\\ at\\ (0,1),\\ 2 \\times 2\\ at\\ (0,0).] $$ （$x$中每个二元变量的的组成：$1 \\times 1$表示子矩形的形状，$(0,0)$表示所处的位置） 要使$x$有效，每个基本子矩形必须被$x$中编码的子矩形精确覆盖一次。例如： $[0, 0, 0, 0, 1, 1, 0, 0, 0]$=\u003e有效（第 5 和第 6 次序的位置分别对应$e$和$f$子矩形，恰好覆盖了所有基本子矩形 1 次） $[0,0,0,1,1,0,0,0,0]$=\u003e无效（第 4 和第 5 次序的位置分别对应$d$和$e$子矩形，没有覆盖到$(1,0)$基本子矩形） $[0,0,0,1,1,1,0,0,0]$=\u003e无效（第 4、第 5 和第 6 次序的位置分别对应$d$、$e$和$f$子矩形，$(1,1)$基本子矩形被覆盖了两次） $c^{(stor)}$的形式化 与每个$x$相对应的向量$c^{(stor)}$长度与$x$相等，其中每个元素是$x$中对应位置的子矩形的存储开销的估计值。 $c^{(view)}$的形式化 考虑分发子矩形的网络带宽开销时，需要考虑所有可能被分发的 360 度表面的视图。 为了简化问题，将片段所有可能的视图离散化到一个大小为$V$的集合中。 集合中每个元素表示一个事件，即向观看 360 度视频片段的用户显示基本子矩形的唯一子集。 注意到片段中被看到的视频区域可以包含来自多个视角的区域。 将之前离散化好的大小为$V$的集合中每个元素与可能性相关联：$[p_1, p_2, …, p_V]$。 考虑为给定的解下载视图$V$的开销，作为需要为该视图下载的数据量： $$ quantity = x^{\\top}diag(d_v)c^{(stor)} $$ $d_v$是一个二元向量，其内容是按照$x$所描述的表示方案，对所有覆盖视图的子矩形的选择。 例如对于 ERP 投影中位置坐标为$yaw = 0, pitch = 90$即处于等矩形顶部的图像，对应的$d_{view-(0, 90)} = [1, 1, 0, 0, 1, 0, 1, 1, 1]$ （即上面图中$a, b, e, g, h, i$位置的子矩形包含此视图所需的基本子矩形）。 给出一个片段中的用户 viewport 分布，$c^{(view)}$的元素是相关联的子矩形预期的下载字节。 $$ c^{(view)} = \\sum_v p_v diag(d_v) c^{(stor)} $$ 最后，将优化问题的基本子矩形覆盖约束编码为矩阵$A$。 $A$是一个列中包含给定子矩形解所覆盖的基本子矩形信息的二元矩阵。 对于$2 \\times 2$的矩形片段，其$A$有 4 行 9 列，例子如下： 因此最终的问题可以形式化为一个整数线性程序： $c^{(stor)}$ 可以理解为存储一段$\\Delta t$时间长的片段的子矩形的存储开销； $c^{(view)}$ 可以理解为传输一个视图所需要的所有的子矩形的传输开销。 $\\alpha$ 控制相比于传输一个片段的相对存储开销，同时应该考虑片段的流行度。 即$\\alpha$应该与所期望的片段在$\\Delta t$的时间间隔内的下载次数成比例，$\\alpha$应该可以通过经验测量以合理的精度进行估计。 可以通过将$x$的二元离散限制放松到$0 \\le x_i \\le 1\\ \\forall i$构成一个线性程序，其解为整数。 （对于有 33516 个变量的$x$，其解可以在单核 CPU 上用 7~10 秒求出） ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:4:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"开销向量建构 首先需要建构出存储开销向量$c^{(stor)}$，但是对于有$n$个基本子矩形的子矩形，其建构复杂度为$O(n^2)$。 因此对每个子矩形进行编码来获得存储开销并不可行，所以利用视频压缩与运动估计之间的强相关性来预测$c^{(stor)}$的值。 给定一个视频，首先暂时将其分成长度为 1 秒的片段，每个片段被限定为只拥有 1 个 GOP，片段的大小表示为$S_{orig}$。 接着抽取出每个片段中的动作序列用于之后的分析。 将片段从空间上划分成基本子矩形，每个基本子矩形包含$4 \\times 4 = 16$个宏块（例如：$64 \\times 64$个像素点）。 独立编码每个基本子矩形，其大小表示为$S_i$。 通过分析动作向量信息，可以推断出如果对基本子矩形$i$进行独立编码，指向基本子矩形$i$的原始运动向量应该重新定位多少。 将其表示为$r_i$。 每个运动向量的存储开销可以计算为： $$ o = \\frac{\\sum_i S_i - S_{orig}}{\\sum_i r_i} $$ 即：存储开销的整体增长除以被基本子矩形边界所分割的运动向量数。 如果基本子矩形被融合进更大的子矩形$t$，使用$m_t$来表示由于融合操作而无须再进行重定位的运动向量的数量： $$ m_t = \\sum_{i \\in t} r_i - r_t $$ $i \\in t$表示基本子矩形位于子矩形$t$中。 为了估计任意子矩形$t$的大小，使用下面 5 个参数： $$ \\sum_{i \\in t} S_i,\\ \\sum_{i \\in t} r_i,\\ m_t,\\ o,\\ n $$ $n$表示$t$中基本子矩形的数量。 实际操作： 创建了来自 4 个单视角 360 度视频的 6082 个 tile 数据集。4 个视频都以两种分辨率进行编码：$1920 \\times 960$和$3980 \\times 1920$。 为了产生 tile，从视频中随机选取片段，随机选取 tile 的位置，宽度和高度。 设置 tile 的 size 最大为$12 \\times 12$个基本子矩形。 对于每个选择的 tile，为其建构一个数据集元素： 计算上面提到的 5 参数的特性向量。 使用 FFmpeg 编码 tile 的视频段来得到存储该段需要的空间。 使用多层感知机 MLP 来估计 tile 的大小。 MLP 中包含 50 个节点的单隐层，激活函数为 ReLU 函数，300 次迭代的训练过程使用L-BFGS 算法。 为了评估 MLP 的预测效果，使用 4 折的交叉验证法。 每次折叠时先从 3 个视频训练 MLP，接着使用训练好的模型去预测第 4 个视频的 tile 大小。 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:5:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"实现 将视频划分成 1 秒长的片段，之后为每个片段解决整数线性问题来确定最优的 tile 划分策略。 使用 MLP 模型估计每个 tile 的存储开销。 根据视图的集合$d$及其对应的可能性分布$p$，来估计视图的下载开销$c^{(view)}$。 构造矩阵$A$时，限制最大的 tile 大小为$12 \\times 12$的基本子矩形（如果设置每个基本子矩形包含$64 \\times 64$的像素，tile 的最大尺寸即为$768 \\times 768$的像素）。 使用GNU Linear Programming Kit来解决问题。 将所有可能的解子矩形编码进一个二元向量$x$中来表示解。 GLPK 的解表明一个可能的解子矩形是否应该被放入解中。 基于最终得到的解，划分片段并使用 ffmepg 以同样参数的 x264 格式进行编码。 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:6:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"评估 度量指标 服务端存储需求。 客户端需要下载的字节数。 数据来源 数据集：dash.ipv6.enstb.fr 评估准备 下载 5 个使用 ERP 投影的视频，抽取出测试中用户看到的对应部分。 每个视频都有$1920 \\times 960$和$3840 \\times 1920$的两种分辨率的版本。 $1920 \\times 960$视频的基本子矩形尺寸为$64 \\times 64$的像素。 $3840 \\times 1920$视频的基本子矩形尺寸为$128 \\times 128$的像素。 将视频划分成 1 秒长的片段，对每个片段都产生出 MLP 所需的 5 元组特性。 之后使用训练好的 MLP 模型来预测所有可能的 tile 的大小。 数据选择 从数据集中随机选择出 40 个用户的集合。 假设 100°的水平和垂直 FOV，并使用 40 个用户的头部方向来为每个片段产生$p_v$和$d_v$。 即：分块的决策基于每个片段的内容特征信息与用户的经验视图模式。 参数设定：$\\alpha = 0,1,1000$. 对比实验： 一组使用由 ILP 得出的结构进行分块； 另外一组： $1920 \\times 960$的视频片段分别使用$64 \\times 64$，$128 \\times 128$，$256 \\times 256$，$512 \\times 512$的方案固定大小分块。 $3840 \\times 1920$的视频片段分别使用$128 \\times 128$，$256 \\times 256$，$512 \\times 512$，$1024 \\times 1024$的方案固定大小分块。 划分结果对比 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:7:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"服务端的存储大小 按照$\\alpha = 0$方案分块之后的视频大小几乎与未分块之前的视频大小持平，有时甚至略微小于未分块前的视频大小。 因为所有分块方案都使用相同的编码参数，所以重新编码带来的有损压缩并不会影响竞争的公平性。 如果将$\\alpha$的值调大，存储的大小会略微增大；固定分块大小的方案所得到的存储大小也会随 tile 变小而变大。 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:7:1","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["paper"],"content":"客户端的下载大小 预测完美的情况——下载的 tile 没有任何浪费 $\\alpha= 1000$的情况下，OpTile 的表现总是最好的。 正常预测的情况 预测的方法：假设用户的头部方向不会改变，预测的位置即为按照当前方向几秒之后的位置。 相比于完美假设的预测，所有分块方案的下载大小都增大了。 $\\alpha = 1000$的方案在两个视频的情况下都取得了最小的下载大小。在剩下的 3 个视频中，OpTile 方案的下载大小比起最优的固定分块大小方案不超过 25%。 尽管固定分块大小的方案可能表现更好，但是这种表现随视频的改变而变化显著。 因为固定分块的方案没有考虑视频内容的特性与用户的观看行为。 ","date":"2021-12-13","objectID":"/posts/papers/note-for-optile/:7:2","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/papers/note-for-optile/"},{"categories":["knowledge"],"content":"媒体处理过程 ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:0:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["knowledge"],"content":"解协议 将流媒体传输方案中要求的数据解析为标准的相应封装格式数据。 音视频在网络中传播时需要遵守对应的传输方案所要求的格式，如 DASH、HLS 将媒体内容分解成一系列小片段，每个片段有不同的备用码率版本。 同时应用层的协议会要求在媒体文件本身之外，传输信令数据（如对播放的控制或网络状态的描述） 解协议的过程会去除信令数据并保留音视频内容，需要的话还要对视频段进行拼接，最终将其还原成传输之前的媒体格式如 MP4，FLV 等。 ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:1:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["knowledge"],"content":"封装格式 封装格式如 AVI、MPEG、Real Video 将音频和视频组合打包成一个完整的文件. 封装格式不会影响视频的画质，影响画质的是视频的编码格式。 解封装过程就是将打包好的封装格式分离成某种编码的音频压缩文件和视频压缩文件，有时也包含字幕和脚本。 比如 FLV 或 TS 格式数据，解封装之后得到 H.264-AVC 编码的视频码流和 AAC 编码的音频码流。 ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:2:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["knowledge"],"content":"编码 视频的本质是一帧又一帧的图片。 所以对于一部每秒 30 帧，90 分钟，分辨率为 1920x1080，24 位的真彩色的视频，在压缩之前的大小$S$满足： $$ 一帧大小s = 1920 * 1080 * 24 = 49766400(bit) = 6220800(Byte) \\ 总帧数n = 90 * 60 * 30 = 162000 \\ 总大小S = s * n = 6220800 * 162000 = 1.0077696*10^{12}(Byte) \\approx 939(GB) $$ 因为未经压缩的视频体积过于庞大，所以需要对其进行压缩，而压缩就是通常所说的编码。 视频编码方式：H.264-AVC，H.265-HEVC，H.266-VVC 音频编码方式：MP3，AAC 压缩比越大，解压还原之后播放的视频越失真，因为压缩过程中不可避免地丢失了视频中原有图像的数据信息。 ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:3:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["knowledge"],"content":"解码 解码就是解压缩过程。 解码之后能够得到系统音频驱动和视频驱动能识别的音频采样数据（如 PCM 数据）和视频像素数据（如 YUV420，RGB 数据）。 ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:4:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["knowledge"],"content":"音视频同步 根据时间，帧率和采样率采用一定的算法，同步解码出来的音频和视频数据，将其分别送至声卡和显卡播放。 视频质量指标 ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:5:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["knowledge"],"content":"分辨率 分辨率指的是视频图像在一个单位尺寸内的精密度。 将视频放大足够大的倍数之后就能看到组成影像的基本单元：像素。 视频的分辨率从数值上描述了像素点的个数，如 1920x1080：视频在水平方向有 1920 个像素，垂直方向有 1080 个像素。 常见的描述方式： 1080P：指视频有1080 行像素，P=\u003eProgressive（逐行扫描） 2K：指视频有2000 列像素 MP：像素总数，指像素的行数 P 与列数 K 乘积的结果（百万像素） 1080P 的分辨率为 1920x1080=2073600，所以 1080P 通常也称为 200 万像素分辨率 通常视频在同样大小的情况下，分辨率越高，所包含的像素点越多，画面就越细腻清晰 参考链接： 科普：视频分辨率是什么？ 「1080p」和「2k、4k」的关系与差别在哪里？ ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:6:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["knowledge"],"content":"视频帧率 帧率的单位 FPS(Frame Per Second)或 Hz，即每秒多少帧，决定视频画面的流畅程度。 低帧率会导致播放卡顿，镜头移动不顺畅，并伴随画面模糊的主观体验； 帧率过高则会造成眩晕的感觉。 不同帧率的视频在支持不同帧率的设备上播放： 若设备最高支持 60fps，则播放 120fps 视频的时候，设备会每隔一帧删除一帧，被删除的帧即成为无效帧。 所以高帧率的视频在低帧率设备上播放时会导致播放卡顿。 若设备最高支持 120fps，则播放 60fps 视频的时候，设备会每隔一帧复制一帧，来填补空缺的帧位置。 但是效果和在 60fps 上的设备播放一样，不能提升播放流畅度。 关于显卡对帧率的影像： 显示器帧率低而显卡输出帧率高时，会导致画面撕裂：显示器同时将两帧或几帧显示在同一个画面上 显示器帧率高而显卡输出帧率低时，同视频帧率高显示器帧率低的情况。 ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:7:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["knowledge"],"content":"视频码率 码率的概念出现在视频编码之后，因为压缩之后的视频已经成为二进制数据，所以使用码率的称呼。 码率的单位是 bps(bit per second)，即每秒多少比特。 与视频质量的关系： 分辨率不变的情况下，码率越大，压缩比越好，画面质量越清晰。 码率越高，精度越高，处理出的文件就越接近压缩前的原始状态，每一帧的图像质量越高，画质越清晰，当然对播放设备的解码能力要求也越高。 压缩比越小，视频体积越大，越接近源文件。 ","date":"2021-12-13","objectID":"/posts/knowledge/mm-base/:8:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/knowledge/mm-base/"},{"categories":["paper"],"content":"论文概况 Level：IEEE Transaction on multimedia 21 Keyword：Rainbow-DQN, Multi-type tiles, Full streaming system ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:1:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"问题形式化 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:2:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"模型 原始视频用网格划分成$N$块 tile，每个 tile 都被转码成$M$个不同的质量等级$q_i$。 基于传输控制模块得出的结果，播放器请求$t_i$个 tile 的$q_i$质量的版本并将其存储在缓冲区中，对应的缓冲区大小为$l_i$。 用户 Viewport 的信息用$V$表示，可以确定 FOV 的中心。 根据$V$可以将 tile 划分成 3 种类型：FOV、OOS、Base。 FOV 中的 tile 被分配更高的码率； OOS 按照与$V$的距离逐步降低质量等级$q_i$； Base 总是使用低质量等级$q_{Base}$但使用完整的分辨率。 传输的 tile 在同步完成之后交给渲染器渲染。 播放器根据各项指标计算可以评估播放性能： $\u003cV, B, Q, F, E\u003e$：viewport 信息$V$，网络带宽$B$，FOV 质量$Q$，重缓冲频率$F$，传输效率$E$。 传输控制模块用于确定每个 tile 的质量等级$q_i$和缓冲区大小$l_i$。 传输控制模块优化的最终目标是获取最大的性能： $$ performance = E_{max},\\ QoE \\in accept\\ range $$ ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:2:1","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"带宽评估 收集每个 tile 的下载日志来评估带宽。 使用指数加权移动平均算法 EWMA使评估结果光滑，来应对网络波动。 第$t$次评估结果使用$B_t$表示，用下式计算： $$ B_t = \\beta B_{t-1} + (1-\\beta)b_t $$ $b_t$是 B 的第$t$次测量值；$\\beta$是 EWMA 的加权系数。 $t=0$时，$B_0$被初始化为 0；所以在初始的$t$比较小的时候，$B_t$与理想值相比就很小。 这种影响会随着$t$增大而减少。 为了优化启动过程，对公式做出修改： $$ B_t = \\frac{\\beta B_{t-1} + (1-\\beta)b_t}{1 - \\beta^t} $$ $t$较小的时候，分母会放大$B_t$；$t$较大时，分母趋近于 1，影响随之消失。 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:2:2","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"FOV 表示和预测 3D 虚拟相机用于渲染视频，处于全景视频球面上的某条轨道，其坐标可以表示为$(\\theta, \\phi)$，可以直接从系统中获取。 相机始终朝向球的中心，所以用户的 FOV 中心坐标$(\\theta^{’}, \\phi^{’})$可以用$(\\theta, \\phi)$表示： $$ \\begin{cases} \\theta^{’} = (\\theta + \\pi)\\ mod\\ 2\\pi,\\ 0 \\le \\theta \\le 2\\pi \\ \\phi^{’} = \\pi - \\phi,\\ 0 \\le \\phi \\le \\pi \\end{cases} $$ 2D 网格中 tile 坐标$(u, v)$可以通过球面坐标使用 ERP 投影获得 $$ \\begin{cases} u = \\frac{\\theta^{’}}{2\\pi} \\cdot W, 0 \\le u \\le W. \\ v = \\frac{\\phi^{’}}{\\pi} \\cdot H, 0 \\le v \\le H. \\end{cases} $$ $W$和$H$分别表示使用 ERP 投影得到的矩形宽度和高度 短期的 FOV 预测基于目前和历史的 FOV 信息。 使用$(U_t, V_t)$表示$t$时刻的 FOV 中心位置；$U_{t1:t2}$和$V_{t1:t2}$分别表示从$t1$到$t2$过程中$U$和$V$的序列； $$ \\begin{cases} \\hat{U}{t+T_f} = f_U (U{t-T_p:t}). \\ \\hat{V}{t+T_f} = f_V (V{t-T_p:t}). \\end{cases} $$ $T_p$是过去记录的滑动窗口；$T_f$是短期的预测窗口；$f_U$和$f_V$分别对应$U$和$V$方向上的映射函数； 因为是时间序列回归模型，所以映射函数使用 LSTM。 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:2:3","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"QoE 评估 QoE 由 3 个部分组成：平均 FOV 质量$Q$、重缓冲频率$F$与 FOV 内 tile 的质量变化（因为平均分配所以不考虑）。 FOV 质量$Q$ 第$t$次的 FOV 质量评估表示为$Q_t$： $$ Q_t = \\frac{\\beta Q_{t-1} + (1-\\beta) \\frac{1}{k} \\cdot \\sum_{j=1}^{k} max{q_j, q_b}}{1 - \\beta^t} $$ $q_j$表示第$j$条 FOV tile 流的质量；$k$表示 FOV 内 tile 的数量； 为了避免评估结果的大幅波动，使用了 EWMA 来光滑结果。 当第$j$条 tile 流因为缓冲区不足不能成功播放时，$q_j = q_{Base}$（这表明了 Base tile 在提高 QoE 中的作用）。 重缓冲频率$F$ 在基于 tile 的传输中，每条流都属于一个缓冲区。所以当 FOV 中 tile 的缓冲区处于饥饿状态时，重缓冲就会发生。 重缓冲频率描述了 FOV 内的 tile 流在一段时间内的重新缓冲频率。 第$t$次重缓冲频率的评估表示为$F_t$： $$ F_t = \\frac{\\beta F_{t-\\tau} + (1-\\beta) \\frac{f_t}{\\tau}}{1 - \\beta^{\\tau}} $$ $f_t$表示播放失败的次数；$\\tau$表示一段时间； ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:2:4","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"传输效率评估 第$t$次传输效率评估表示为$E_t$，$E_t$通过传输的 FOV 内 tile 占总 tile 的比率来计算： $$ E_t = \\frac{\\beta E_{t-1} + (1-\\beta) \\frac{total^{FOV}}{total^{ALL}}}{1 - \\beta^t} $$ $total^{FOV}$表示 FOV 内 tile 的数据量；$total^{ALL}$表示 tile 的总共数据量； 效率计算并不在传输过程中完成，因为需要获取哪些 tile 在 FOV 中的信息，效率评估滞后于播放过程。 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:2:5","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"问题形式化 传输控制的任务：确定所有 tile 流的质量等级$\\chi$和缓冲区大小$\\psi$。 $$ \\chi = \u003cq_1, q_2, …, q_N\u003e \\ \\psi = \u003cl_1, l_2, …, l_N\u003e \\ \u003cQ, F, E\u003e = \\xi (B, V, \\chi, \\psi) $$ $\\chi$和$\\psi$与带宽$B$和 Viewport 轨迹$V$一起作用于系统$\\xi$，最终影响 FOV 质量$Q$，重缓冲频率$F$和传输效率$E$。 进一步，将目标形式化为获得每条 tile 流的$q_i$和$l_i$通过限制 QoE 满足可接受的范围、在此基础上最大化传输效率： $$ \\underset{\\chi, \\psi}{argmax} \\sum_{t=0}^{+\\infty} E_t, $$ $$ s.t.:\\ 0 \\le q_i \\le M, $$ $$ 0 \\le l_i \\le L, $$ $$ Q^{min} \\le Q_t \\le M, $$ $$ 0 \\le F_t \\le F^{max}. $$ $q_i$和$l_i$分别受限于质量版本数$M$和最大缓冲区大小$L$； $Q_t$受限于最低 QoE 标准$Q^{min}$； $F_t$受限于最大能忍受的重缓冲频率$F^{max}$。 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:2:6","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"系统架构 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:3:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"服务端 将原始视频转码为有不同比特率的多个版本。 转码后的视频被划分成多个 tile。 传输协议使用 MPEG-DASH。 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:3:1","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"客户端 评估器 任务：获取 QoE、FOV 预测、传输效率、网络带宽 组成： QoE 评估器：评估当前 FOV 质量=\u003eQ 和重缓冲频率=\u003eF（近似为 Q+F=QoE） FOV 预测器：基于历史 FOV 信息预测短期未来的 FOV=\u003eP 根据下载和播放日志：计算传输效率=\u003eE 并估计带宽=\u003eB 控制器 任务：控制传输过程中的推流 目标：保证 QoE 在可接受的范围之内、最大化传输效率 详细：基于 FOV 预测将 tile 划分成 3 种类型：FOV、OOS、Base 输入：Q、F、E、B（QoE+传输效率和带宽） 过程：Rainbow-DQN 输出：决定每个 tile 流的码率和缓冲区大小（作为下载器的输入） 下载器 输入：tile 码率和缓冲区大小 过程：基于 HTTP/2 进行并行下载 输出：下载好的 tile 视频缓冲区 任务：解码、同步、存储下载好的 tile 等待渲染器消耗，大小供控制器调节 随着 FOV 的切换缓冲区内容可能被循环利用 全景渲染器 任务：将同步好的 tile 拼接，tile 质量：FOV\u003eOOS\u003eBase 投影方式：ERP ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:3:2","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"控制器 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:4:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"控制过程 设定 QoE 的可接受范围。 将网络带宽和用户 FOV 设定为外部因素而非环境 为什么：因为这两个因素变化太快，在面对不同传输条件时，直接作为环境会导致决策过程的不稳定性并且难以收敛。 最优化的对象只是最大化累积的传输效率。 为什么：简单 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:4:1","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"tile 聚合和决策 tile 分类原则： 控制器无需为每个 tile 独立决定码率 Q 和缓冲区大小 L FOV 内的 tile 应该被分配相近的码率，FOV 内的 tile 应该聚集成一组，OSS 和 Base 同理 为什么：避免相邻 tile 的锐利边界，只考虑 3 组而非所有 tile 降低了计算复杂性和决策延迟 （能否实现独立的 tile 码率计算或更细粒度的划分值得调研？与内容感知的方案结合？） 基于距离的 tile 分类实现方式： 使用评估器预测出的 FOV 坐标来分类 FOV 和 OOS 的 tile tile 出现在未来 FOV 的可能性由距离计算 tile 中心点坐标$(\\omega_i, \\mu_i)$、FOV 坐标$(\\hat{U}, \\hat{V})$ 距离的变化区间内存在一个临界点，临界点之内的划分为 FOV，之外的划分为 OOS 度量距离的方式： $$ \\Delta Dis_U = min{|\\omega_i - \\hat{U}|, |1+\\omega_i - \\hat{U}|} $$ （这里为何不直接使用$|\\omega_i - \\hat{U}|$？） $$ Dis_i = \\begin{cases} {\\sqrt{({\\Delta Dis_{U}})^2 + {(\\mu_i - \\hat{V})}^2},\\ \\frac{R}{H} \\le \\hat{V} \\le 1 - \\frac{R}{H}} \\ {\\Delta Dis_U + |\\mu_i - \\hat{V}|,\\ Others} \\end{cases} $$ 因为 ERP 的投影方式会在两级需要更多的 tile，因此使用一个矩形来代表两极的 FOV （可以深入调研 ERP 在两极处的处理方式） $Dis_i$使用曼哈顿距离来测量。临界点初始化为$2\\cdot R$，并随着 FOV 中心和两极的垂直距离增长。 FOV 看作是半径为 R 的圆，使用欧式距离测量。临界点初始化为$R$ 聚合 tile 的决策 使用 2 个变量：$K$作为 FOV 和非 FOV 的 tile 的带宽分配比率；$Len$作为 tile 缓冲区的大小。 $K$确定之后，分配给 FOV 内 tile 的带宽被均匀分配（可否非均匀分配） $K$不直接与网络状况相关因此可以保持控制的稳定性 $Len$：所有传输的 tile 的缓冲区长度$l_i$都被设为$Len$ （文中并没有这样做的原因解释） ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:4:2","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"基于 DRL 的传输控制算法 相关术语解释：Rainbow DQN、RL Dictionary、PER、TD-Error 控制过程 首先调整 buffer 长度 Len，并划分 FOV 与非 FOV 的带宽分配。 等 viewport 预测完成之后，tile 被分类为属于 FOV 和 OOS 的 tile。 FOV 的带宽被平均分给其中每一个 tile 并决定 FOV 内 tile 的质量等级$q_i$。 非 FOV 的带宽按照与 FOV 的距离分配，每超过一个距离单位$Dis_i$就降低一级质量$q_i$。 最终的输出是请求序列，每个请求序列中包括质量等级$q_i$和预期的缓冲区大小$l_i$。 根据输出做出调整之后，接收奖励反馈并不断完成自身更新。 状态设计 状态设计为 5 元组：$\u003cK, Len, Q, F, E\u003e$（传输控制参数$K$，$Len$、QoE 指标：FOV 质量 Q 和重缓冲频率$F$、传输效率$E$） 没有直接使用带宽$B$和 viewport 轨迹$V$，因为： 随机性强与变化幅度较大带来的不稳定性（如何定义随机性强弱和变化幅度大小？） 希望设计的模型有一定的通用性，可以与不同的网络情况和用户轨迹相兼容 动作设计 两种动作：调整$K$和$Len$（两者的连续变化区间被离散化，调整的每一步分别用$\\Delta k$和$\\Delta l$表示） 调整的方式被形式化为二元组：$\u003cn_1, n_2\u003e$，$n_1$和$n_2$分别用于表示$K$和$Len$的调整 -n 0 n K 减少 n$\\Delta k$ 不变 增加 n$\\Delta k$ Len 减少 n$\\Delta l$ 不变 增加 n$\\Delta l$ 奖励函数 因为 QoE 的各项指标权重难以确定，没有使用传统的基于加权的方式。 设定了能接受的 QoE 范围和在此基础上最大化的传输效率作为最后的性能指标，形式化之后如下： $$ Reward = \\begin{cases} -INF,\\ F \\ge F^{max} \\ -INF,\\ E \\le E^{min} \\ E,\\ Others \\end{cases} $$ $-INF$意味着终止当前 episode；动作越能使系统满足高 QoE 的同时高效运行，得分越高； 为了最大化传输效率，使用$E$作为奖励回报。 FOV 质量$Q$并没有参与到奖励函数中，因为：高 Q 意味着高性能，但是低 Q 不一定意味着低性能，详细解释如下： 在带宽不足的情况下，低 Q 可能已经是这种条件下的满足性能的最好选择。 高传输效率意味着传输了更多的 FOV 数据，也能满足高 FOV 质量的目标。 模型设计 基于 Rainbow-DQN 模型： 输入是 5 元组$\u003cK, Len, Q, F, E\u003e$。 神经网络使用 64 维的 3 隐层模型。 为了提高鲁棒性，神经网络的第 3 层使用 Dueling DQN 的方式，将 Q 值$Q(s, a)$分解为状态价值$V(s)$和优势函数$A(s,a)$： $$ Q(s, a) = V(s) + A(s, a) $$ $V(s)$表示系统处于状态$s$时的性能；$A(s,a)$表示系统处于状态$s$时动作$a$带来的性能； 为了避免价值过高估计，使用 Double DQN 的方式，设计了两个独立的神经网络：评估网络和目标网络。 评估网络用于动作选择；目标网络是评估网络从最后一个 episode 的拷贝用于动作评估。 为了缓解神经网络的不稳定性（更快收敛），使用大小为$v$的回放池来按照时间序列保存客户端的经验。 因为网络带宽和 FOV 轨迹在短期内存在特定的规律性，回放池中有相似状态和相似采样时间的样本更加重要，出现了优先级 所以使用优先经验回放 PER，而优先级使用时间查分误差 TD-error 定义 $$ \\delta_i = r_{i+1} + \\gamma Q(s_{i+1}, arg\\underset{a}{max}Q(s_{i+1}, a; \\theta_i); \\theta_i^{’}) - Q(s_i, a_i; \\theta_i) $$ $r_i$是奖励；$\\gamma$是折扣因子 损失函数使用均方误差定义 $$ J = \\frac{1}{v} \\sum_{i=1}^{v} \\omega_i(\\delta_i)^2 $$ $\\omega_i$是回放缓冲中第 i 个样本的重要性采样权重 ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:4:3","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"实验验证 环境设定 传输控制模块：基于TensorForce（配置教程：用 TensorForce 快速搭建深度强化学习模型）； 开发工具集：OpenAI Gym 数据来源：使用全景视频播放设备收集，加入高斯噪声来产生更多数据。 结果分析 与其他 DQN 算法的对比——DQN、Double DQN、Dueling DQN 对比训练过程中每个 episode 中的最大累计奖励：$MAX_{reward}$ 对比模型收敛所需要的最少 episode：$MIN_{episode}$ 相同的带宽和 FOV 轨迹 与其他策略对比性能——高 QoE 和高传输效率 随机控制策略：随机确定 K 和 Len 固定分配策略：固定 K 和 Len 的值 只预测 Viewport 策略：使用 LSTM 做预测，不存在 OSS 与 Base，所有带宽都用于 FOV 带宽和 FOV 轨迹的均值和方差相等 与其他全景视频推流系统的对比 DashEntire360：使用 Dash 直接传送完整的 360 度视频，使用线性回归来估计带宽并动态调整视频比特率 360ProbDash：在 DashEntire360 的基础上划分 tile 基于 Dash 传输，使用可能性模型为 tile 分配比特率 DRL360：使用 DRL 来优化多项 QoE 指标 实现三种系统、使用随机网络带宽和 FOV 轨迹。 使用 DRL360 中提出的方式测量 QoE： $$ V_{QoE} = \\eta_1 Q - \\eta_2 F - \\eta_3 A $$ $A$是 viewport 随时间的平均变化，反映 FOV 质量 Q 的变化； $\\eta_1, \\eta_2, \\eta_3$分别是 3 种 QoE 指标的非负加权，使用 4 种加权方式来训练模型并对比： $\u003c1, 1, 1\u003e$，$\u003c1, 0.25, 0.25\u003e$，$\u003c1, 4, 1\u003e$，$\u003c1,1,4\u003e$ 在不同环境下的性能评估——带宽是否充足、FOV 轨迹是否活跃（4 种环境） ","date":"2021-12-11","objectID":"/posts/papers/note-for-rainbowDQN+tiles/:5:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/papers/note-for-rainbowDQN+tiles/"},{"categories":["paper"],"content":"论文概况 Link: 360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming Level: ACM MM 17 Keyword: Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation ","date":"2021-12-09","objectID":"/posts/papers/note-for-360ProbDASH/:1:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/papers/note-for-360ProbDASH/"},{"categories":["paper"],"content":"工作范围与目标 应用层-\u003e基于 tile-\u003eviewport 预测的可能性模型+预期质量的最大化 针对小 buffer 提出了target-buffer-based rate control算法来避免重缓冲事件（避免卡顿） 提出 viewport 预测的可能性模型计算 tile 被看到的可能性（避免边缘效应） 形式化 QoE-driven 优化问题： 在传输率受限的情况下最小化 viewport 内的质量失真和空间质量变化（获取受限状态下最好的视频质量） ","date":"2021-12-09","objectID":"/posts/papers/note-for-360ProbDASH/:2:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/papers/note-for-360ProbDASH/"},{"categories":["paper"],"content":"问题建模 形式化参数 $M*N$个 tile，M 指 tile 序列的序号，N 指不同的码率等级 $r_{i, j}$指比特率，$d_{i, j}$指失真，$p_{i}$指被看到的可能性（$\\sum_{i=1}^{N}p_{i} = 1$） $\\Phi(X)$指质量失真，$\\Psi(X)$指质量变化 目标 找到推流段的集合：$X = {x_{i, j}}$，其中${x_{i, j}} = 1$指被第$\u003ci, j\u003e$个 tile 被选中；$x_{i, j} = 0$则是未选中。 $$ \\underset{X}{min}\\ \\Phi(X) + \\eta \\cdot \\Psi(X) \\ s.t. \\sum_{i=1}^{N}\\sum_{j=1}^{M}x_{i, j}\\cdot r_{i, j} \\le R, \\ \\sum_{j=1}^{M}x_{i, j} \\le 1, x_{i, j} \\in {0, 1}, \\forall i. $$ 整个公式即为前所述的问题的形式化表达的公式化结果。 模型细节 $\\Phi(X)$和$\\Psi(X)$的计算=\u003e通过考虑球面到平面的映射 通过计算球面上点的 Mean Squared Error 来得到 S-PSNR 进而评估质量：$d_{i, j}$来表示第${\u003ci, j\u003e}$个段的 MSE $$ \\phi_i = \\frac{\\pi}{2} - h_i \\cdot \\frac{\\pi}{H}, \\Delta\\phi = \\Delta h \\cdot \\frac{\\pi}{H}, \\ \\theta_i = w_i \\cdot \\frac{2\\pi}{W}, \\ \\Delta\\theta = \\Delta w \\cdot \\frac{2\\pi}{W}, $$ $H$和$W$分别指按照 ERP 格式投影之后的视频高度和宽度 第$i$个 tile 的空间面积用$s_i$表示： $$ s_i\\ =\\ \\iint_{\\Omega_i}Rd\\phi Rcos\\phi d\\theta \\ =\\Delta\\theta R^2[sin(\\phi_i + \\Delta\\phi) - sin\\phi_i], $$ $R$指球的半径（$R = W/2\\pi$），所以整体的球面质量失真$D_{i, j}$可以计算出来： $$ D_{i, j} = d_{i, j} \\cdot s_i, $$ 结合每个 tile 被看到的概率$p_i$可以得出$\\Phi(X)$和$\\Psi(X)$ $$ \\Phi(X)=\\frac{\\sum_{i=1}^N\\sum_{j=1}^MD_{i, j}\\cdot x_{i,j}\\cdot p_i}{\\sum_{i=1}^N\\sum_{j=1}^Mx_{i,j}\\cdot s_i},\\ \\Psi(X) = \\frac{\\sum_{i=1}^N\\sum_{j=1}^Mx_{i, j}\\cdot p_i \\cdot\\ (D_{i,j}-s_{i} \\cdot \\Phi(X))^2}{\\sum_{i=1}^N\\sum_{j=1}^Mx_{i,j}\\cdot s_i}. $$ Viewport 的可能性模型 方向预测=\u003e线性回归模型 将用户的欧拉角看作是$yaw(\\alpha)$，$pitch(\\beta)$和$rool(\\gamma)$，应用线性回归做预测 $$ \\begin{cases} \\hat{\\alpha}(t_0 + \\delta) = m_{\\alpha}\\delta+\\alpha(t_0),\\ \\hat{\\beta}(t_0 + \\delta) = m_{\\beta}\\delta+\\beta(t_0),\\ \\hat{\\gamma}(t_0 + \\delta) = m_{\\gamma}\\delta+\\gamma(t_0). \\end{cases} $$ 预测错误的分布=\u003e高斯分布，根据公式均值和标准差都能从统计信息中计算出来 收集 5 名志愿者的头部移动轨迹并投影到 3 个方向上绘制成图，实验结果为预测错误呈现高斯分布（样本数可能不够？） $$ \\begin{cases} P_{yaw}(\\alpha) = \\frac{1}{\\sigma_{\\alpha}\\sqrt{2\\pi}}exp{-\\frac{[\\alpha-(\\hat{\\alpha}+\\mu_{\\alpha})]^2}{2\\sigma_{\\alpha}^2}},\\ P_{pitch}(\\beta) = \\frac{1}{\\sigma_{\\beta}\\sqrt{2\\pi}}exp{-\\frac{[\\beta-(\\hat{\\beta}+\\mu_{\\beta})]^2}{2\\sigma_{\\beta}^2}},\\ P_{roll}(\\gamma) = \\frac{1}{\\sigma_{\\gamma}\\sqrt{2\\pi}}exp{-\\frac{[\\gamma-(\\hat{\\gamma}+\\mu_{\\gamma})]^2}{2\\sigma_{\\gamma}^2}}. \\end{cases} $$ 3 个方向各自独立，因此最终的预测错误$P_E(\\alpha,\\beta,\\gamma)$可以表示为： $$ P_E(\\alpha, \\beta, \\gamma) = P_{yaw}(\\alpha)P_{pitch}(\\beta)P_{roll}(\\gamma). $$ 球面上点被看到的可能性 球面坐标为$(\\phi, \\theta)$点的可能性表示为$P_s(\\phi, \\theta)$ 因为一个点可能在多个不同的 viewport 里面，所以定义按照用户方向从点$(\\phi, \\theta)$出发能看到的点集$L(\\phi, theta)$ 因此空间点$s$被看到的可能性可以表示为： $$ P_s(\\phi, \\theta) = \\frac{1}{|L(\\phi, \\theta)|}\\sum_{(\\alpha, \\beta, \\gamma) \\in L(\\phi, \\theta)}P_E(\\alpha, \\beta, \\gamma), $$ 球面上 tile 被看到的可能性 tile 内各个点被看到的可能性的均值即为 tile 被看到的可能性（可否使用其他方式？） $$ p_i = \\frac{1}{|U_i|} \\sum_{(\\phi, \\theta) \\in U_i} P_s(\\phi, \\theta). $$ $U_i$表示 tile 内的空间点集 Target-Buffer-based Rate Control 因为长期的头部移动预测会产生较高的预测错误，所以不能采用大缓冲区（没有 cite 来证明这一点） 将处于相同时刻的段集合成一个块存储在缓冲区中。 在自适应的第 k 步，定义$d_k$作为此时的 buffer 占用情况（等到第 k 个块被下载完毕） $$ b_k = b_{k-1} - \\frac{R_k \\cdot T}{C_k} + T $$ $C_k$表示平均带宽，$R_k$表示总计的码率 为了避免重新缓冲设定目标 buffer 占用$B_{target}$，并使 buffer 占用保持在$B_{target}$（$b_k = B_{target}$） 因此总计的码率需要满足： $$ R_k = \\frac{C_k}{T} \\cdot (b_{k-1} - B_{target} + T), $$ 这里的$C_k$表示可以从历史的段下载信息中估计出来的带宽 设定$R$的下界$R_{min}$之后（没有说明为何需要设定下界），公式 12 可以修正为如下： $$ R_k = max{\\frac{C_k}{T} \\cdot (b_{k-1} - B_{target} + T), R_{min}}. $$ ","date":"2021-12-09","objectID":"/posts/papers/note-for-360ProbDASH/:3:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/papers/note-for-360ProbDASH/"},{"categories":["paper"],"content":"实现 ","date":"2021-12-09","objectID":"/posts/papers/note-for-360ProbDASH/:4:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/papers/note-for-360ProbDASH/"},{"categories":["paper"],"content":"服务端 视频裁剪器 将视频帧切割成 tile 编码器 对 tile 进行划分并将其编码成多种码率的段 MPD 产生器 添加SRD 特性来表示段之间的空间关系 添加经度和纬度属性来表示 添加质量失真和尺寸属性 Apache HTTP 服务器 存储视频段和 mpd 文件，向客户端推流 ","date":"2021-12-09","objectID":"/posts/papers/note-for-360ProbDASH/:4:1","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/papers/note-for-360ProbDASH/"},{"categories":["paper"],"content":"客户端 基础：dash.js 额外的模块 QoE-driver Optimizer $$ Output = HTTP\\ GET请求中的最优段 $$ $$ Input = Output\\ of\\ \\begin{cases} Target\\ buffer\\ based\\ Rate\\ Controller\\ Viewport\\ Probabilistic\\ Model\\ QR\\ Map \\end{cases} $$ Target-buffer-based Rate Controller $$ Output = 总计的传输码率，按照公式13计算而来 $$ $$ Input = Output\\ of\\ {Bandwidth\\ Estimation\\ module $$ Viewport Probabilistic Model $$ Output = 每个tile被看到的可能性，按照公式10计算而来 $$ $$ Input = Output\\ of\\ \\begin{cases} Orientation\\ Prediction\\ module\\ SRD\\ information \\end{cases} $$ QR MapQR=\u003eQuality-Rate $$ Output = 所有段的QR映射 $$ $$ Input = MPD中的属性 $$ Bandwidth Estimation（没有展开研究，因为不是关键？） $$ Output = 前3秒带宽估计的平均值 $$ $$ Input = 下载段过程中的吞吐量变化 $$ 可以通过onProgess()的回调函数XMLHttpRequest API获取 Orientation Prediction $$ Output = 用户方向信息的预测结果（yaw, pitch, roll） $$ $$ Input = Web\\ API中获取的DeviceOrientation信息，使用线性回归做预测 $$ ","date":"2021-12-09","objectID":"/posts/papers/note-for-360ProbDASH/:4:2","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/papers/note-for-360ProbDASH/"},{"categories":["paper"],"content":"评估 整体设定 将用户头部移动轨迹编码进播放器来模拟用户头部移动 积极操控网络状况来观察不同方案对网络波动的反应 详细设定 服务端 视频选择 2880x1440 分辨率、时长 3 分钟、投影格式 ERP 切分设置 每个块长 1s（$T=1$）、每个块被分成 6x12 个 tile（$N=72$） 每个段的码率设置为${20, 50, 100, 200, 300}$，单位 kpbs 视频编码 开源编码器 x264 视频分包 MP4Box 注意事项 每个段的确切尺寸可能与其码率不同，尤其对于长度较短的块。 为了避免这影响到码率自适应，将段的确切尺寸也写入 MPD 文件中 客户端 缓冲区设定（经过实验得出的参数） $B_{max}=3s$，$B_{target}=2.5s$，$R_{min}=200kbps$，$权重\\eta=0.0015$ 高斯分布设定 Yaw Pitch Roll $\\mu_{\\alpha}=-0.54,\\ \\sigma_{\\alpha}=7.03$ $\\mu_{\\beta}=0.18,\\ \\sigma_{\\beta}=2.55$ $\\mu_{\\gamma}=2.16,\\ \\sigma_{\\gamma}=0.15$ 比较对象 ERP：原始视频格式 Tile：只请求用户当前 viewport 的 tile，不使用 viewport 预测，作为 baseline Tile-LR：使用线性回归做预测，每个 tile 的码率被平均分配 性能指标 卡顿率：卡顿时间占播放总时长的比例 Viewport PSNR：直接反应 Viewport 内的视频质量 空间质量差异：Viewport 内质量的协方差 Viewport 偏差：空白区域在 Viewport 中的比例 ","date":"2021-12-09","objectID":"/posts/papers/note-for-360ProbDASH/:5:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/papers/note-for-360ProbDASH/"},{"categories":["paper"],"content":"论文概况 Link: https://dl.acm.org/doi/10.1145/3232565.3234686 Level: SIGCOMM 18 Keyword: UDP+FOV-aware+FEC ","date":"2021-12-08","objectID":"/posts/papers/note-for-dante/:1:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/papers/note-for-dante/"},{"categories":["paper"],"content":"工作范围 ","date":"2021-12-08","objectID":"/posts/papers/note-for-dante/:2:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/papers/note-for-dante/"},{"categories":["paper"],"content":"目标 在给定序列的帧中，为每个 tile设定 FEC 冗余，根据其被看到的可能性的加权最小化平均质量降低。 ","date":"2021-12-08","objectID":"/posts/papers/note-for-dante/:3:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/papers/note-for-dante/"},{"categories":["paper"],"content":"问题建模 输入 估计的丢包率$p$、发送速率$f$、有$n$个 tile 的$m$个帧($\u003ci, j\u003e$来表示第$i$个帧的第$j$个 tile 第$\u003ci, j\u003e$个 tile 的大小$v_{i, j}$、第$\u003ci, j\u003e$个 tile 被看到的可能性$\\gamma_{i, j}$、 如果第$\u003ci, j\u003e$ 个 tile 没有被恢复的质量降低率、最大延迟$T$ 输出 第$\u003ci, j\u003e$个 tile 的 FEC 冗余率$r_{i, j} = \\frac{冗余包数量}{原始包数量}$ 最优化问题的形式化 $$ minimize\\ \\sum_{0\u003ci\\le m}\\sum_{0\u003cj\\le n} \\gamma_{i, j}d_{i, j}(p, r_{i, j}) $$ $$ subject\\ \\ to\\ \\ \\frac{1}{f}\\sum_{0\u003ci\\le m}\\sum_{0\u003cj\\le n}v_{i, j}(1+r_{i, j}) \\le T $$ $$ r_{i, j} \\le 0 $$ （1）：最小化最终被看到的 tile 的质量衰减的加权和，权重按照被看到的可能性分配。 （2）：经过重新编码的包和原始的包需要在 T 时刻之前发出。 ​ Dante 将 1 个 GOP(Group of Pictures)中的所有帧当作一批处理，$T$作为 GOP 的持续时间 ​ $f$：使用 TCP Friendly Rate Control algorithm，基于估计的丢包率和网络延迟来计算得出 （3）：确保冗余率总是非负的。 关键变量是$d_{i, j}(p, r)$：丢包率是 p 情况下，采用 r 作为冗余率的第$\u003ci, j\u003e$个 tile 的质量衰减 $$ d_{i, j}(p, r) = \\delta_{i, j},\\ if\\ r \u003c \\frac{1}{1-p}; 0, otherwise. $$ 假设帧中有 k 个原始包，质量衰减发生在丢失的包不能被恢复的情况下。 FEC 可以容忍 $r \\cdot k$ 个丢包=\u003e即当 $p(rk+k)$ 大于 $rk$ 时会发生质量衰减。 过多的丢包会导致依赖链上所有帧的质量衰减，因此考虑帧之间的依赖关系之后，可以重新计算质量衰减： $$ d^{*}{i, j}(p, r) = \\sum{0\u003cc\\le i}w_{c, i}d_{c, j}(p, r) $$ $w_{c, i}$ 编码帧 i 对帧 c 的依赖作为单独的第 c 个帧的质量衰减的权重； 最终第 i 个帧的第 j 个 tile 的最终质量衰减就是所有依赖的质量衰减的和。 ","date":"2021-12-08","objectID":"/posts/papers/note-for-dante/:4:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/papers/note-for-dante/"},{"categories":["paper"],"content":"FEC 冗余的自适应逻辑 关于$d_{i, j}(p, r)$ ：因为是分段函数，所以其值会因为 r 和 p 的大小关系而急剧改变。 利用背包问题的思想可以将其规约成 NP 完全问题： 将每个 tile 看作是一个物品，共有 m*n 个。 如果$r_{i, j} \u003c \\frac{1}{1-p}$ ，则表示不把第\u003ci,j\u003e和物品放入背包；否则就是将其放入背包。 公式 1 可以转化为：最大化所有物品二元变量的线性组合； 公式 2 可以转化为：二元变量的另一个线性组合必须低于阈值约束。 因此整个问题就能被完全转化为0-1 背包问题 算法 整体上是背包问题的标准解法，能以线性复杂度（因为变量只是 B)解决问题。 ","date":"2021-12-08","objectID":"/posts/papers/note-for-dante/:5:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/papers/note-for-dante/"},{"categories":["paper"],"content":"原型设计 使用基于 TCP 和 UDP 的两条连接来分别传输控制信息（双向：到客户端的播放会话的起至点和到服务端的网络信息反馈）和视频数据包 服务端根据反馈的网络信息，在每个 GOP 的边界时刻运行算法 1 来确定下一个 GOP 的帧和 tile 的 FEC 冗余。 确定之后服务端使用 RS 码来插入冗余包，和原始视频数据包一起重新编码，并使用基于 TFRC 的发送率发送数据。 Dante 的实现是对应用程序级比特率适配策略的补充，并且可以通过对视频播放器进行最小更改来替换现有的底层传输协议来部署。 ","date":"2021-12-08","objectID":"/posts/papers/note-for-dante/:6:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/papers/note-for-dante/"},{"categories":["paper"],"content":"实验评估 环境：使用 Gilbert 模型来模拟实现丢包事件（而非使用统一随机丢包） 创造了两种网络条件 good（丢包率 0.5%）和 bad（丢包率 2%） ","date":"2021-12-08","objectID":"/posts/papers/note-for-dante/:7:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/papers/note-for-dante/"},{"categories":["paper"],"content":"局限性 效果主要依赖于 Viewport 预测的结果是否准确 ","date":"2021-12-08","objectID":"/posts/papers/note-for-dante/:8:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/papers/note-for-dante/"},{"categories":["paper"],"content":"度量指标 viewport 预测精度。 使用预测的 viewport 坐标和实际用户的 viewport 坐标的大圈距离来量化。 视频质量。 viewport 内部的 tile 质量（1～5）。 tile 在最高质量层之上花费的时间。 根据用户视线的分布而提出的加权质量度量。 ","date":"2021-11-22","objectID":"/posts/papers/note11/:1:0","tags":["Immersive Video"],"title":"沉浸式流媒体传输的实际度量","uri":"/posts/papers/note11/"},{"categories":["paper"],"content":"度量参数 分块策略 带宽 延迟 viewport 预测 HTTP 版本 持久化的连接数量 ","date":"2021-11-22","objectID":"/posts/papers/note11/:2:0","tags":["Immersive Video"],"title":"沉浸式流媒体传输的实际度量","uri":"/posts/papers/note11/"},{"categories":["paper"],"content":"背景 大多数的 HAS 方案使用 HTTP/1.1 协议进行请求-回应的事务来取得需要的资源、缓冲取到的视频段并以线性的顺序播放。传统的 HAS 中，只需要 1 个 GET 请求来取得下一个视频的暂时的部分。只要视频段的持续时间比网络内的时延高，这种方法就可行。 在基于 VR 的 HAS 方案中，播放 1 条视频片段就需要取得多种资源：1 次 GET 请求需要同时请求基础的 tile 层和每个空间视频 tile。使用 4x4 的 tile 方案时，客户端需要发起不少于 17 次 GET 请求。使用 1 s 数量级的分段持续时间，即使是 20 ms 的微小网络延迟也会显着阻碍客户端和服务器之间的整体吞吐量，因此会导致较低的视频质量。 ","date":"2021-11-15","objectID":"/posts/papers/note10/:1:0","tags":["Immersive Video"],"title":"沉浸式推流中应用层的优化","uri":"/posts/papers/note10/"},{"categories":["paper"],"content":"解决方案 ","date":"2021-11-15","objectID":"/posts/papers/note10/:2:0","tags":["Immersive Video"],"title":"沉浸式推流中应用层的优化","uri":"/posts/papers/note10/"},{"categories":["paper"],"content":"使用多条持久的 TCP 连接 大多数的现代浏览器都支持同时建立并维持多达 6 条 TCP 连接来减少页面加载时间，并行地获取请求的资源。这允许增加整体吞吐量，并部分消除网络延迟引入的空闲 RTT 周期。 类似地，基于 VR 的 HAS 客户端可以使用多个 TCP 连接并行下载不同的 tile。 ","date":"2021-11-15","objectID":"/posts/papers/note10/:2:1","tags":["Immersive Video"],"title":"沉浸式推流中应用层的优化","uri":"/posts/papers/note10/"},{"categories":["paper"],"content":"使用 HTTP/2 协议的服务端 push 特性 HTTP/2 协议引入了请求和相应的多路复用、头部压缩和请求优先级的特性，这可以减少页面加载时间。 服务端直接 push 短视频片段可以减少视频的启动时间和端到端延迟。 并且，服务端 push 特性可以应用在基于 tile 的 VR 视频推流中，客户端可以向服务器同时请求一条视频片段的所有 tile。 服务端可以使用特制的请求处理器，允许客户端为每个 tile 定义一系列质量等级。 因此可以将应用的启发式自适应的速率的决定传达给服务器，这允许客户端以期望的质量级别取得所有图块。 ","date":"2021-11-15","objectID":"/posts/papers/note10/:2:2","tags":["Immersive Video"],"title":"沉浸式推流中应用层的优化","uri":"/posts/papers/note10/"},{"categories":["paper"],"content":"最终的目标 主要的挑战是用户的临场感，这可以通过避免虚拟的线索来创造出接近真实的世界。 ","date":"2021-11-14","objectID":"/posts/papers/note9/:1:0","tags":["Immersive Video"],"title":"沉浸式流媒体面临的挑战和启示","uri":"/posts/papers/note9/"},{"categories":["paper"],"content":"具体的任务 从 360 度视频的采集到显示的过程中，引入了好几种失真。 应该重点增加新的拼接、投影和分包方式以减少噪音。 除了捕获和使用 360 度视频来表示真实世界和实际交互内容之外，环境中还包括 3D 对象。 3D 对象的合并对于真实的视图而言是一个挑战。 因为在推流会话中，用户的头部移动高度可变，所以固定的 tiling 方案可能会导致非最优的 viewport 质量。 推流框架中的 tile 数量应该被动态选择，进而提高推流质量。 自适应的机制应该足够智能来根据环境因素精确地做出适应。 应该制定基于深度强化学习的策略，来给 360 度视频帧中不同区域的 tile 分配合适的比特率。 用户在 360 度视频中的自由导航很容易让其感觉忧虑自己错过了什么重要的东西。 在 360 度视频中导航的时候，需要支持自然的可见角度方向。 丰富的环境应配备新颖的定向机制，以支持 360 度视频，同时降低认知负荷，以克服此问题。 真实的导航依赖 viewport 预测机制。 现代的预测方式应该使用时空图像特性以及用户的位置信息，采用合适的编解码器卷积 LSTM 结构来减少长期预测误差。 沉浸式的场景随着用户的交互应该发生变化。 由于用户与场景的交互而产生的新挑战是通过编码和传输透视图创建的。 因此预测用户的行为来实现对交互内容的高效编码和推流非常关键。 对 360 度视频的质量获取方法和度量手段需要进一步研究。 360 度视频中特殊的音效需要引起注意。 ","date":"2021-11-14","objectID":"/posts/papers/note9/:2:0","tags":["Immersive Video"],"title":"沉浸式流媒体面临的挑战和启示","uri":"/posts/papers/note9/"},{"categories":["paper"],"content":"背景 空间音频是一种全球状空间环绕的声音方式，采用多个声音通道来模拟现实世界中听到的声音。 360 度视频由于空间音频而变得更加可靠，因为声音的通道特性使其能够穿越时间和空间。 360 度视频显示系统在制作空间音频音轨方面的重要性无论怎样强调都不为过 ","date":"2021-11-14","objectID":"/posts/papers/note8/:1:0","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"空间音频的再现技术 ","date":"2021-11-14","objectID":"/posts/papers/note8/:2:0","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"物理重建 物理重建技术用于合成尽可能接近所需信号的整个声场。 立体声配置在最流行的声音再现方法中使用两个扬声器，以促进更多的空间信息（包括距离、方向感、环境和舞台合奏）。而多信道再现方法在声学环境中使用，并在消费类设备中流行。 多信道再现技术 同样的声压场也通过其他物理重建技术产生，如环境中存在的环境声学和波场合成（WFS）。 需要麦克风阵列来捕获更多的空间声场。 因为不能直接用于声场特性分析，麦克风记录的内容需要后期处理。 麦克风阵列用于语音增强、声源分离、回声消除和声音再现。 ","date":"2021-11-14","objectID":"/posts/papers/note8/:2:1","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"感知重建 心理声学技术用于感知重建，以产生对空间声音特征的感知。 感知重建技术复制空间音频的自然听觉感受来表示物理音频。 双耳录制技术 双耳录制技术是立体声录制的一种扩展形式，提供 3D 的听觉体验。 双耳录制技术通过使用两个 360 度麦克风尽可能的复制人耳，这与使用定向麦克风捕捉声音的常规立体声录音相同。 假人头部的 360 度麦克风用作人耳的代理，因为它提供了耳朵的精确几何坐标。 假人头部还产生与人头轮廓相互作用的声波。借助 360 度麦克风，与任何其他记录方法相比，空间立体图像的捕获更精确。 头部相关传递函数（HRTF） 用于双耳音频的实时技术中，以再现复杂的线索，帮助我们通过过滤音频信号来定位声音。 多个因素（如耳朵、头部和听力环境）会影响线索，因为在现实中，我们会重新定位自己以定位声音。 选择合适的录音/重放技术对于使听到的声音与真实场景中的体验相同至关重要。 ","date":"2021-11-14","objectID":"/posts/papers/note8/:2:2","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"环境声学 ","date":"2021-11-14","objectID":"/posts/papers/note8/:3:0","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"概述 环境声学也被称为 3D 音频，被用于记录、混成和播放一个中心点周围的 360 度音频。 ","date":"2021-11-14","objectID":"/posts/papers/note8/:3:1","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"区别 环境音频和传统的环绕声技术不同。 双声道和传统环绕声技术背后的原理是相同的，都是通过将声音信号送到特定的扬声器来创建音频。 环境音频不受任何特定扬声器的预先限制，因为它在即使音域旋转的情况下，也能创造出平滑的音频。 传统环绕声的格式只有在声音场景保持静态的情况下才能提供出色的成像效果。 环境音频提供一个完整的球体，将声音均匀地传播到整个球体。 ","date":"2021-11-14","objectID":"/posts/papers/note8/:3:2","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"格式 环境音频有 6 种格式，分别为：A、B、C、D、E、G。 ","date":"2021-11-14","objectID":"/posts/papers/note8/:3:3","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"用途 一阶环境音频的用途 第一阶的环境音频或 B 格式的环境音频，其麦克风用于使用四面体阵列表示线性 VR。 此外，这些在四个通道中进行处理，例如提供非定向压力水平的“W”。同时，“X、Y 和 Z”分别促进了从前到后、从侧到侧以及从上到下的方向信息。 一阶环境音频仅适用于相对较小的场景，因为其有限的空间保真度会影响声音定位。 高阶环境音频的用途 高阶环境音频通过增加更多的麦克风来增强一阶环境音频的性能效率。 ","date":"2021-11-14","objectID":"/posts/papers/note8/:3:4","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"总结 ","date":"2021-11-14","objectID":"/posts/papers/note8/:4:0","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/papers/note8/"},{"categories":["paper"],"content":"概述 在 360 度视频的推流过程中，根据用户头部的运动自适应地动态选择推流的区域，调整其比特率，以达到节省带宽的目的。 ","date":"2021-11-14","objectID":"/posts/papers/note7/:1:0","tags":["Immersive Video"],"title":"自适应策略之viewport依赖型","uri":"/posts/papers/note7/"},{"categories":["paper"],"content":"通常的实现方式 在服务端提供几个自适应集，来在遇到用户头部的突然运动的情况时，能保证 viewport 的平滑转换。 提出 QER(Quality-focused Regios)的概念使 viewport 内部的视频分辨率高于 viewport 之外的视频分辨率。 非对称的方式以不同的空间分辨率推流来节省带宽。 在播放过程中，客户端根据用户的方向来请求不同分辨率版本的视频。 优点是即使客户端对用户的方面做了错误预测，低质量的内容仍然可以在 viewport 中生成。 缺点是在大多数场景下，这种方案需要巨大的存储开销和处理负载。 ","date":"2021-11-14","objectID":"/posts/papers/note7/:2:0","tags":["Immersive Video"],"title":"自适应策略之viewport依赖型","uri":"/posts/papers/note7/"},{"categories":["paper"],"content":"自适应推流参数 可用带宽和网络吞吐量 Viewport 预测的位置 客户端播放器的可用缓冲 ","date":"2021-11-14","objectID":"/posts/papers/note7/:3:0","tags":["Immersive Video"],"title":"自适应策略之viewport依赖型","uri":"/posts/papers/note7/"},{"categories":["paper"],"content":"参数计算公式 第 n 个估计的 Viewport：$V^e(n)$ $V^e(n) = V_{fb}$ $V_{fb}$是最新报告的 viewport 位置 第 n 个估计的吞吐量：$T^e(n)$ $T^e(n) = T_{fb}$ $T_{fb}$是最新报告的吞吐量 比特率：$R_{bits}$ $R_{bits} = (1-\\beta)T^e(n)$ $\\beta$是安全边缘 第 n 个帧的客观度量质量：$VQ(k)$和最终客观度量质量$VQ$ $VQ=\\frac{1}{L}\\sum^L_{k=1}VQ(k)$ $VQ(k) = \\sum_{t=1}^{T^n}w_k(k) * D^n_t(V_t, k)$ $w_k = \\frac{A(t,k)}{A_{vp}}$ $L=总帧数$ $w_k$表示在第 k 个帧中与 viewport 所重叠的 tile 程度 $A(t,k)$表示第 k 个帧中 tile $t$ 重叠的区域 $A_{vp}$表示 viewport 中总共的区域 ","date":"2021-11-14","objectID":"/posts/papers/note7/:4:0","tags":["Immersive Video"],"title":"自适应策略之viewport依赖型","uri":"/posts/papers/note7/"},{"categories":["paper"],"content":"OMAF(Omnidirectional Media Format) OMAF是第 1 个国际化的沉浸式媒体格式，描述了对 360 度视频进行编码、演示、消费的方法。 OMAF与与现有格式兼容，包括编码（例如HEVC），文件格式（例如ISOBMFF），交付信号（例如DASH，MMT）。 OMAF中还包括编码、投影、分包和 viewport 方向的元数据。 ","date":"2021-11-11","objectID":"/posts/papers/note6/:1:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/papers/note6/"},{"categories":["paper"],"content":"OMAF+DASH-\u003eMPD OMAF 与 DASH 相结合，再加上一些额外的描述构成了 MPD 文件格式，用于向客户端通知 360 度媒体的属性。 OMAF 规定了 9 中媒体配置文件，包括 3 种视频配置文件：基于 HEVC 的 viewport 独立型、基于 HEVC 的 viewport 依赖型、基于 AVC 的 viewport 依赖型。 OMAF 为视角独立型的推流提供了无视 viewport 位置的连续的视频帧质量。 常规的 HEVC 编码方式和 DASH 推流格式可以用于 viewport 独立型的推流工作。 但是使用 HEVC/AVC 编码方式的基于 viewport 的自适应操作是 OMAF 的一项技术开发，允许无限制地使用矩形 RWP 来增强 viewport 区域的质量。 ","date":"2021-11-11","objectID":"/posts/papers/note6/:2:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/papers/note6/"},{"categories":["paper"],"content":"CMAF(Common Media Application Format) 致力于提供跨多个应用和设备之间的统一的编码格式和媒体配置文件。 CMAF 使请求低延迟的 segment 成为可能。 ","date":"2021-11-11","objectID":"/posts/papers/note6/:3:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/papers/note6/"},{"categories":["paper"],"content":"ISOBMFF(ISO Base Media File Format) ISOBMFF 是用于定时数据交换、管理和显示的最流行的文件格式。 文件由一系列兼容并且可扩展的文件级别的 box 组成。 每个 box 表示 1 个由 4 个指针字符代码组成的数据结构。 ISOBMFF 的媒体数据流和元数据流被分别分发。 媒体数据流中包括编码过的音频和视频数据。 元数据流中包括媒体类型、编码属性、时间戳、大小等元数据，也包括全向内容的额外信息如投影格式、旋转、帧分包、编码和分发等元数据。 ISOBMFF 为了访问方便，保证有价值信息能灵活聚合。 ","date":"2021-11-11","objectID":"/posts/papers/note6/:4:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/papers/note6/"},{"categories":["paper"],"content":"3DoF(3 Degree of Freedom) 在 3DoF 场景中，用户可以自由的移动头部以三个方向：摆动、俯仰、旋转。 ","date":"2021-11-11","objectID":"/posts/papers/note6/:5:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/papers/note6/"},{"categories":["paper"],"content":"3DoF+ 用户的头部可以以任意方向移动：上下、左右、前后 ","date":"2021-11-11","objectID":"/posts/papers/note6/:6:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/papers/note6/"},{"categories":["paper"],"content":"6DoF 不只用户的头部，用户的身体也是自由的。同时支持方向与位置的自由。 ","date":"2021-11-11","objectID":"/posts/papers/note6/:7:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/papers/note6/"},{"categories":["paper"],"content":"背景 用户使用头戴设备比使用传统显示器观看 360 度视频内容时的满意度对于扰乱更加敏感。 沉浸式的体验受到不完美的视角预测和高度动态化的网络状况的消极影响。 目前主要面临的挑战有以下 4 个： ","date":"2021-11-04","objectID":"/posts/papers/note5/:0:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"Viewport 预测 ","date":"2021-11-04","objectID":"/posts/papers/note5/:1:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"背景 HMD 的本质特征是快速响应用户头部的移动。当用户改变 viewport 时 HMD 处理交互并检测相关的 viewport 来精确播放器的信息，这样视野就能以正常的可视角度被提供给用户。Viewport 预测在优化的 360 度视频推流中非常必要。配备有位置传感器的可穿戴 HMD 允许客户端更新其视角方向相应的视角场景。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:1:1","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"分类 内容不可知的方式基于历史信息对 viewport 进行预测。 内容感知的方式需要视频内容信息来预测未来的 viewport。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:1:2","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"内容不可知方式 分类 平均线性回归 LR 航位推算 DR 聚类 机器学习 ML 编解码器体系结构 现有成果 Qian’s work——LR 使用平均线性回归和加权线性回归模型来做 viewport 预测，之后对与预测区域重叠的 tile 进行整体推流。 当预测后 0.5s、1s、2s 加权线性回归表现更好 Petrangeli’s work——LR 将被划分成 tile 的等矩形的帧分成 3 个区域：viewport 区、相邻区、其他区。 结合观察者头部的移动，将可变比特率分配给可见和不可见区域。 作者利用最近（100 毫秒）用户观看历史的线性外推来预测未来的注视点。 Mavlankar and Girod’s work——运动向量 使用运动向量比如观察者的平移、倾斜、缩放等方向上的速度和加速度，来执行视角区域预测。 La Fuente’s work——运动向量 考虑了两种预测变体：角速度和角加速度，从用户以前的方向数据来估计未来的头部方向。按照预测结果分配不同的量化参数到每个 tile 上。 当进行进一步的预测时（超过 2s），这种方式限制了预测的精度。 如果视频 tile 被基于错误的预测而被请求，用户的实际 viewport 可能会被没有请求因而没有内容的黑色 tile 所覆盖。 Ban’s work——KNN+LR 使用 KNN 算法利用跨用户观看历史，使用 LR 模型利用户个体化的行为。 就视角预测的准确率而言，分别取得了 20%和 48%的绝对和相对改进。 Liu’s work——cluster 提出了使用数据融合方法，通过考虑几个特征来估计未来视角位置。特征例如：用户的参与度、用户观看同一视频的行为、单个用户观看多个视频的行为、最终用户设备、移动性水平。 Petrangeli’s work——cluster 基于车辆轨迹预测的概念，考虑了类似的轨迹形成一个簇来预测未来的 viewport。 结果表明这种方法为更长的视野提高了精确度。 检查了来自三个欧拉角的不同轨迹，这样做可能导致性能不足。 Rossi’s work——cluster 提出了一种聚类的方法，基于球形空间中有意义的 viewport 重叠来确认用户的簇。 基于 Bron-Kerbosch（BK）算法的聚类算法能够识别大量用户，这些用户观看的是相同的 60%的 3s 长球形视频块。 与基准相比，该方法为簇提供了可兼容且重要的几何 viewport 重叠。 Jiang’s work 背景： LR 方法对于长期的预测视野会导致较差的预测精度。长短时记忆（LSTM）是一种递归神经网络（RNN）架构，适用于序列建模和模式开发。 方法： 为了在 FoV 预测中获取比 LR 方法更高的精确度，开发了一种使用带有 128 个神经元的 LSTM 模型的 viewport 预测方法。 分析了 360 度数据集，观察到用户在水平方向头部有快速转向，但是在垂直方向几乎是稳定的。 实验表明，这种方法同时考虑水平和垂直方向的头部移动时，比 LR 等方法产生了更少的预测错误。 Bao’s work 背景： 对 150 个用户进行了 16 个视频剪辑的主观实验，并对其行为进行了分析。 使用 3 个方向的欧拉角$\\theta$, $\\phi$, $\\psi$来表示用户在 3D 空间中头部的移动，结果表明不同方向的动作有强自相关性和消极的互相关性。因此多个角度的预测可以分开进行。 方法： 开发两个独立的 LSTM 模型来分别预测$\\theta$和$\\phi$，之后将预测结果应用于目标区域流来有效利用可用网络资源。 Hou’s work 提出一种基于深度学习的视角产生方法来只对提前预测的 360 度视频和 3 自由度的 VR 应用的 viewport tile 进行抽取和推流。（使用了大规模的数据集来训练模型） 使用包含多层感知器和 LSTM 模型来预测 6 自由度的 VR 环境中头部乃至身体的移动，预测的视野被预渲染来做到低延迟的 VR 体验。 Heyse’s work 背景： 在某些例子中，用户的移动在视频的不同部分中非常不稳定。这增加了机器学习方式的训练压力。 方法： 提出了一个基于 RL 模型的上下文代理，这个模型首先检测用户的显著移动，然后预测移动的方向。这种分层自学习执行器优于球形轨迹外推法（这种方法将用户运动建模为轨迹的一部分，而不是单位球体上的完整轨迹） Qian’s work 提出了一种叫做 Flare 的算法来最小化实际 viewport 和预测 viewport 之间的不匹配。 应用了一种 ML 方法来执行频繁的 viewport 预测，包括从 130 名用户收集的 1300 条头部运动轨迹的 4 个间隔。 使用 viewport 轨迹预测，Flare 可以将错误预测替换成最新预测。 Yu and Liu’s work 背景： LSTM 网络本身具有耗时的线性训练特性。编解码器的 LSTM 模型把训练过程并行化，相比于 LR 和 LSTM 本身而言，改善了预测精度。 方法： 使用基于注意力的 LSTM 编解码器网络体系结构来避免昂贵的递归并能更好地捕获 viewport 变化。 提出的体系结构相比于传统的 RNN，获得了更高的预测精度，更低的训练复杂度和更快的收敛。 Jamali’s work 提出使用 LSTM 编解码器网络来做长期的 viewport 预测（例如 3.5s）。 收集了低延迟异质网络上跨用户的方向反馈来调整高延迟网络上目标用户的预测性能。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:1:3","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"内容感知方式 背景 内容感知方式可以提高预测效率。 具体方法 Aladagli’s work 提出了一个显著性驱动的模型来提高预测精度。 没有考虑用户在 360 度视频中的视角行为。 viewport 预测错误可以通过理解用户对 360 度视频独特的可见注意力最小化。 Nguyen’s work 背景： 大多数现存的方法把显著性图看作是 360 度显示中的位置信息来获得更好的预测结果。 通用的显著性和位置信息体系结构基于固定预测模型。 方法： 提出了PanoSalNet来捕获用户在 360 度帧中独特的可见注意力来改善显著性检测的性能。 同时使用 HMD 特性和显著性图的固定预测模型获得了可测量的结果。 Xu’s work 提出了两个 DRL(Deep Reinforcement Learning)模型用于同时考虑运动轨迹和可见注意力特性的 viewport 预测网络。 离线模型基于内容流行度检测每个帧里的显著性。 在线模型基于从离线模型获得的显著性图和之前的 viewport 预测信息预测 viewport 方向和大小。 这个网络只能预测 30ms 的下一个 viewport 位置。 Xu’s work 收集了大规模的被使用带有眼部轨迹跟踪的 HMD 的 45 个观测者观察的动态 360 度视频数据集，提出了基于历史扫描路径和图像特征预测注视位移的方法。 在与当前注视点、viewport 和整个图像相关的三个空间尺度上执行了显著性计算。 可能的图像特性被通过向 CNN 喂图像和相应的显著性图，同时 LSTM 模型捕获历史信息来抽取出来。 之后将 LSTM 和 CNN 特性耦合起来，用于下一次的用户注视信息预测。 Fan’s work 用户更容易被运动的物体吸引，因此除了显著性图之外，Fan 等人也考虑了使用预训练 的 CNN 来估计用户未来注视点的内容运动图。 由于可能存在多个运动，这让预测变得不可靠，因此运动贴图的开发还需要进一步的研究。 Yang’s work 使用 CNN 模型基于历史观测角度信息预测了单 viewport。 接着考虑了一种使用内容不可知和内容感知方法如 RNN 和 CFVT 模型的融合层的 viewport 轨迹预测策略。 融合模型使其同时支持更好地预测并且提高了大概 40%的精度。 Ozcinar’s work 将 viewport 轨迹转换为基于 viewport 的视觉注意图，然后对不同大小的 tile 进行推流以保证更高的编码效率。 Li’s work 现有的预测模型对未来的预测能力有限，Li 等人提出了两种模型，分别用于 viewport 相关和基于 tile 的推流系统。 第一个模型应用了基于用户轨迹的 LSTM 编解码网络体系结构。 第二个模型应用了卷积 LSTM 编解码体系结构，使用序列的热图来预测用户的未来方向。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:1:4","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"总结 精确的方向预测使 360 度视频的客户端可以以高分辨率下载最相关的 tile。 当前采用显著性和位置信息的神经网络模型的性能比直接利用当前观察位置进行未来 viewport 位置估计的简单无运动的基线方法表现差。估计的显著性中的噪音等级限制了这些模型的预测精度。并且这些模型也引入了额外的计算复杂度。 对于 360 度视频注意点的可靠预测和用户观看可能性与显著性图之间关系的理解，显著性模型必须被改善并通过训练大规模的数据集来适应，尤其是被配备了不同摄像机旋转的镜头所捕获的数据。 另一方面，卷积 LSTM 编解码器和基于轨迹的预测方法适合长期预测，并能带来相当大的 QoE 改进，特别是在协作流媒体环境中。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:1:5","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"QoE 评估 ","date":"2021-11-04","objectID":"/posts/papers/note5/:2:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"背景 由于全方位视频非常普遍，因此，通过这种类型的视频分发来确定用户的特定质量方面是至关重要的。QoE 在视频推流应用中扮演着重要角色。在传统视频推流中，QoE 很大程度上被网络负载和分发性能所影响。现有的次优目标度量方法并不适用于全向视频，因为全向视频受网络状况和用户视角行为的影响很大。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:2:1","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"主观质量评估 主观质量评估是估计 360 度视频推流质量的现实并且可靠的方法。 Upenik’s work 用一台 MergeVR HMD 执行了主观测试来体验 360 度图像。 实验数据包括主观分数、视角轨迹、在每个图像上花费的时间由软件上获得。 视角方向信息被用于计算显著性图。 但是这项研究没有考虑对 360 度视频的评估。 Zhang’s work 为了弥补 360 度视频和常规视频度量方式之间的性能差距，为全景视频提出了一种主观质量评估方法，称为SAMPVIQ。 23 位参与者被允许观看 4 个受损视频，整体视频质量体验的评分在 0～5 分之间。 参与者之间存在较大的评分差异。 Xu’s work 提出两种主观测量方式：总体区分平均意见分数(O-DMOS)和矢量区分平均意见分数(V-DMOS)来获得 360 度视频的质量损失。 类似于传统食品的 DMOS 度量方式，O-DMOS 度量方式计算主观测试序列的总计区分分数。 Schatz’s work 研究了使用 HMD 观看 360 度内容时停顿事件的影响。 沉浸式内容的主观质量评估并非不重要，可能导致比实际推荐更多的开放性问题。 通常来讲人们的期望于传统的 HAS 相似，即如果可能的话，根本没有停顿。 可用的开源工具 AVTrack360，OpenTrack 和 360player 能捕获用户观看 360 度视频的头部轨迹。 VRate 是一个在 VR 环境中提供主观问卷调查的基于 Unity 的工具。 安卓应用*MIRO360*，支持未来 VR 主观测试的指南开发。 Cybersickness Cybersickness是一种获得高 QoE 的潜在障碍，它能引起疲劳、恶心、不适和呕吐。 Singla’s work 使用受限的带宽和分辨率，在不同的延迟情况下进行了两个主观实验。 开发了主观测试平台、测试方法和指标来评估 viewport 自适应 360 度视频推流中的视频感知等级和Cybersickness。 基于 tile 的推流在带宽受限的情况下表现很好。 47ms 的延迟实际上不影响感知质量。 Tran’s work 考虑了几个影响因子例如内容的空间复杂性，数量参数，分辨率特性和渲染模型来评估 cybersickness，质量，可用性和用户的存在。 VR 环境中快速移动的内容很容易引发 cybersickness。 由于高可用性和存在性，用户的 cybersickness 也可能加剧。 Singla’s work 评估了 28 名受试者在 Oculus Rift 和 HTC Vive 头戴式电脑上观看 6 个全高清和超高清分辨率 YouTube 视频时的观看不适感。 HMD 的类型轻微地影响感知质量。 分辨率和内容类型强烈影响个人体验。 女性用户感到cybersickness的人数更多。 空间存在感 空间存在感能增强沉浸感。 Zou’s work 方法： 提出了一个主观框架来测量 25 名受试者的空间存在感。 提出的框架包括三层，从上到下分别为：空间存在层、感知层、科技影响层。 心理上的空间存在感形成了空间存在层。 感知层以视频真实感、音频真实感和交互元素为特征。 科技影响层由几个模块组成，这些模块与感知层相连，以反映传感器的真实性。 Hupont’s work 应用通用感知的原则来研究在 Oculus HMD 和传统 2D 显示器上玩游戏的用户的空间存在感。 与 2D 显示器相比，3D 虚拟现实主义显示出更高的惊奇、沉浸感、存在感、可用性和兴奋感。 生理特征度量 Salgado’s work 方法： 捕获多种多样的生理度量，例如心率 HR，皮肤电活性 EDA、皮肤温度、心电图信号 ECG、呼吸速率、血压 BVP、脑电图信号 EEG 来评价沉浸式模拟器的质量。 Egan’s work 基于 HR 和 EDA 信号评估 VR 和非 VR 渲染模式质量分数。 相比于 HR，EDA 对质量分数有强烈的影响。 技术因素感知 不同的技术和感知特征，如失真、清晰度、色彩、对比度、闪烁等，用于评估感知视频质量。 Fremerey’s work 确定了可视质量强烈地依赖于应用的运动插值（MI）算法和视频特征，例如相机旋转和物体的运动。 在一项主观实验中，12 位视频专家回顾了使用 FFmpeg 混合、FFmpeg MCI（运动补偿插值）和 butterflow 插值到 90 fps 的四个视频序列。作者发现，与其他算法相比，MCI 在 QoE 方面提供了极好的改进。 总结 主观测试与人眼直接相关，并揭示了 360 度视频质量评估的不同方面的影响。 在这些方面中，空间存在感和由佩戴 VR 头戴设备观看 360 度视频导致的cybersickness极为重要，因为这些效果并不在传统的 2D 视频观看中出现。 主观评估需要综合的手工努力并因此昂贵耗时并易于出错，相对而言，客观评估更易于管理和可行。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:2:2","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"客观质量评估 由于类似的编码结构和 2D 平面投影格式，对 360 度内容应用客观质量评估很自然。 计算 PSNR 现有投影方式中的采样密度在每个像素位置并不均匀。 Yu’s work 为基于球形的 PSNR 计算引入 S-PSNR 和 L-PSNR。 S-PSNR 通过对球面上所有位置的像素点做同等加权来计算 PSNR。 利用插值算法，S-PSNR 可以完成对支持多种投影模式的 360 度视频的客观质量评估。 L-PSNR 通过基于纬度和访问频率的像素点加权测量 PSNR。 L-PSNR 可以测量 viewport 的平均 PSNR 而无需特定的头部运动轨迹。 Zakharchenko’s work 提出了一种 Craster Parabolic Projection-PSNR (CPP-PSNR) 度量方式来比较多种投影方案，通过不改变空间分辨率和不计算实际像素位置的 PSNR，将像素重新映射成 CPP 投影。 CPP 投影方式可能使视频分辨率大幅下降。 Sun’s work 提出了一种叫做 weighted-to-spherically-uniform PSNR (WS-PSNR)的质量度量方式，以此来测量原始和受损内容之间的质量变化。 根据像素在球面上的位置考虑权重。 计算 SSIM SSIM 是另一种质量评估指标，它通过三个因素反映图像失真，包括亮度、对比度和结构。 Chen’s work 为 2D 和 360 度视频分析了 SSIM 结果，引入了球型结构的相似性度量（S-SSIM）来计算原始和受损的 360 度视频之间的相似性。 在 S-SSIM 中，使用重投影来计算两个提取的 viewport 之间的相似性。 Zhou’s work 考虑相似性的权重提出了 WS-SSIM 来测量投影区域中窗口的相似性。 性能评估表明，与其他质量评估指标相比，WS-SSIM 更接近人类感知。 Van der Hooft’s work 提出了ProbGaze度量方式，基于 tile 的空间尺寸和 viewport 中的注视点。 考虑外围 tile 的权重来提供合适的质量测量。 相比于基于中心和基于平均的 PSNR 和 SSIM 度量方式，ProbGaze能估计当用户突然改变 viewport 位置时的视频质量变化。 Xu’s work 引入了两种客观质量评估度量手段：基于内容感知的 PSNR 和非内容感知的 PSNR，用于编码 360 度视频。 第一种方式基于空间全景内容对像素失真进行加权。 第二种方式考虑人类偏好的统计数据来估计质量损失。 基于 PSNR 和 SSIM 方式的改进 尽管各种基于 PSNR 和 SSIM 的方式被广阔地应用到了 360 度视频的质量评估中，但这些方式都没有真正地捕获到感知质量，特别是当 HMD 被用于观看视频时。因此需要为 360 度内容特别设计一种优化的质量度量方式。 Upenik’s work 考虑了一场使用 4 张高质量 360 度全景图像来让 45 名受试者在不同的编码设定下评估和比较客观质量度量方式性能的主观实验。 现有的客观度量方式和主观感知到的质量相关性较低。 Tran’s work 论证主观度量和客观度量之间相关性较高，但是使用的数据集较小。 基于 ML 的方式 基于 ML 的方式可以弥补客观评估和主观评估之间的差距。 Da Costa Filho’s work 提出了一个有两个阶段的模型。 首先自适应 VR 视频的播放性能由机器学习算法所确定。 之后模型利用估计的度量手段如视频质量、质量变化、卡顿时间和启动延迟来确定用户的 QoE。 Li’s work 引入了基于 DRL 的质量获取模型，在一次推流会话中同时考虑头部和眼部的移动。 360 度视频被分割成几个补丁。 低观看概率的补丁被消除。 参考和受损视频序列都被输入到深度学习可执行文件中，以计算补丁的质量分数。 之后分数被加权并加到一起得到最终的分数。 Yang’s work 考虑了多质量等级的特性和融合模型。 质量特性用region of interest(ROI)图来计算，其中包括像素点等级、区域等级、对象等级和赤道偏差。 混合模型由后向传播的神经网络构造而成，这个神经网络组合了多种质量特性来获取整体的质量评分。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:2:3","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"总结 精确的 QoE 获取是优化 360 度视频推流服务中重要的因素，也是自适应分发方案中基础的一环。 单独考虑 VR 中的可视质量对完整的 QoE 框架而言并不足够。 为能获得学界的认可，找到其他因素的影响也很必要，例如cybersickness，生理症状，用户的不适感，HMD 的重量和可用性，VR 音频，viewport 降级率，网络特性（延迟，抖动，带宽等），内容特性（相机动作，帧率，编码，投影等），推流特性（viewport 偏差，播放缓冲区，时空质量变化等）。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:2:4","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"低延迟推流 ","date":"2021-11-04","objectID":"/posts/papers/note5/:3:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"背景 360 度全景视频推流过程中的延迟由几部分组成：传感器延迟、云/边处理延迟、网络延迟、请求开销、缓冲延迟、渲染延迟和反馈延迟。 低延迟的要求对于云 VR 游戏、沉浸式临场感和视频会议等更为严格。 要求极低的终端处理延迟、快速的云/边计算和极低的网络延迟来确保对用户头部移动做出反馈。 现代 HMD 可以做到使传感器延迟降低到用户无法感知的程度。 传输延迟已经由 5G 移动和无线通信技术大幅减少。 但是，对于减少处理、缓冲和渲染延迟的工作也是必要的。 许多沉浸式应用的目标是 MTP 的延迟少于 20ms，理想情况是小于 15ms。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:3:1","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"减少启动时间 减少初始化请求的数据量 通常来讲，较小的视频 segment 能减少启动和下载时间。 Van der Hooft’s work 考虑了新闻相关内容的推流，使用的技术有： 服务端编码 服务端的用户分析 服务器推送策略 客户端积极存储视频数据 取得的效果： 降低了启动时间 允许不同网络设定下的快速内容切换 较长的响应时间降低了性能 Nguyen’s work 基于 viewport 依赖的自适应策略分析了自适应间隔延迟和缓冲延迟的影响。 使用服务端比特率计算策略来最小化响应延迟的影响。 根据客户端的响应估计可用的网络吞吐量和未来的 viewport 位置。 服务端的决策引擎推流合适的 tile 来满足延迟限制。 取得的效果： 对于 viewport 依赖型推流方案而言，较少的自适应和缓冲延迟不可避免。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:3:2","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"降低由 tile 分块带来的网络负载 在 HTTP/1.1 中，在空间上将视频帧分成矩形 tile 会增加网络负载，因为每个 tile 会产生独立的网络请求。 请求爆炸的问题导致了较长的响应延迟，但是可以通过使用 HTTP/2 的服务器推送特性解决。这个特型使服务器能使用一条 HTTP 请求复用多条消息。 Wei’s work 利用 HTTP/2 协议来促进低延迟的 HTTP 自适应推流。 提出的服务端推送的策略使用一条请求同时发送几个 segment 避免多个 GET 请求。 Petrangeli’s work 结合特定请求参数与 HTTP/2 的服务端推送特性来促进 360 度视频推流。 客户端为一个 segment 发送一条 call，服务器使用 FCFS 策略传送 k 个 tile。 利用 HTTP/2 的优先级特性可以使高优先级的 tile 以紧急的优先级被获取，进而改善网络环境中的高往返时间的性能。 Xu’s work 为 360 度内容采用了k-push策略：将 k 个 tile 推送到客户端，组成一个单独的时间段。 提出的方法与 QoE 感知的比特率自适应算法一起，在不同的 RTT 设定下，提高了 20%的视频质量，减少了 30%的网络传输延迟。 Yahia’s work 使用 HTTP/2 的优先级和多路复用功能，在两个连续的 viewport 预测之间，即在交付相同片段之前和期间，组织紧急视频块的受控自适应传输。 Yen’s work 开发了一种支持 QUIC 的体系结构来利用流优先级和多路复用的特性来实现 360 度视频的安全和低优先级的传输。 当 viewport 变化发生时，QUIC 能让常规的 tile 以低优先级推流，viewport 内的 tile 以高优先级推流，都通过一条 QUIC 连接来降低 viewport tile 的缺失率。 作者说测试表明基于 QUIC 的自适应 360 度推流比 HTTP/1.1 和 HTTP/2 的方案表现更好。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:3:3","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"使用移动边缘计算降低延迟 Mangiante’s work 提出了利用基于边缘处理的 viewport 渲染方案来减少延迟，同时利用终端设备上的电源和计算负载。 但是作者没有给出有效的算法或是建立一个实践执行平台。 Liu’s work 采用远端渲染技术，通过为不受约束的 VR 系统获取高刷新率来隐藏网络延迟。 采用 60GHz 的无线链路支持的高端 GPU，来加快计算速度和 4K 渲染，减少显示延迟。 尽管提供了高质量和低延迟的推流，但是使用了昂贵的带宽连接，这通常并不能获得。 Viitanen’s work 引入了端到端的 VR 游戏系统。通过执行边缘渲染来降低延迟，能源和计算开销。 为 1080p 30fps 的视频格式实现了端到端的低延迟（30ms）的系统。 前提是有充足的带宽资源、终端设备需要性能强劲的游戏本。 Shi’s work 考虑了不重视 viewport 预测的高质量 360 度视频渲染。 提出的 MEC-VR 系统采用了一个远端服务器通过使用一个自适应裁剪过滤器来动态适应 viewport 覆盖率，这个过滤器按照观测到的系统延迟增加 viewport 之外的区域。 基于 viewport 覆盖率的延迟调整允许客户端容纳和补偿突然的头部移动。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:3:4","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"共享 VR 环境中的延迟处理 共享 VR 环境中用户的延迟取决于用户的位置和边缘资源的分发。 Park’s work 通过考虑多个用户和边缘服务器之间的双向通信，提出了一种使用线性蜂窝拓扑中的带宽分配策略，以最小化端到端系统延迟。确定了推流延迟强烈地依赖于： 边缘服务器的处理性能 多个交互用户之间的物理和虚拟空间 Perfecto’s work 集成了深度神经网络和毫米波多播传输技术来降低协同 VR 环境中的延迟。 神经网络模型估计了用户即将来临的 viewport。 用户被基于预测的相关性和位置分组，以此来优化正确的 viewport 许可。 执行积极的多播资源调度来最小化延迟和拥塞。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:3:5","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"总结 在单用户和多用户的环境中，边缘辅助的解决方式对于控制延迟而言占主要地位。 此外还有服务端的 viewport 计算、服务端 push 机制和远程渲染机制都能用于低延迟的控制。 现有的 4G 网络足以支持早期的自适应沉浸式多媒体，正在成长的 5G 网络更能满足沉浸式内容的需求。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:3:6","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"360 度直播推流 ","date":"2021-11-04","objectID":"/posts/papers/note5/:4:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"背景 传统的广播电视频道是直播推流的流行来源。现在私人的 360 度直播视频在各个社交媒体上也有大幅增长。 因为视频生产者和消费者之间在云端的转码操作，360 度视频推流是更为延迟敏感的应用。 现有的处理设备在诸如转码、渲染等实时处理任务上受到了限制。 内容分发 Hu’s work 提出了一套基于云端的直播推流系统，叫做MELiveOV，它使高分辨率的全向内容的处理任务以毛细管分布的方式分发到多个支持 5G 的云端服务器。 端到端的直播推流系统包括内容创作模块、传输模块和 viewport 预测模块。 移动边缘辅助的推流设计减少了 50%的带宽需求。 Griwodz’s work 为 360 度直播推流开发了优化 FoV 的原型，结合了 RTP 和基于 DASH 的pull-patching来传送两种质量等级的 360 度视频给华为 IPTV 机顶盒和 Gear VR 头戴设备。 作者通过在单个 H.265 硬件解码器上多路复用多个解码器来实现集体解码器的想法，以此减少切换时间。 视频转码 Liu’s work 研究表明只转码 viewport 区域有潜力大幅减少高性能转码的计算需求。 Baig’s work 开发了快速编码方案来分发直播的 4K 视频到消费端设备。 采用了分层视频编码的方式来在高度动态且不可预测的 WiGig 和 WiFi 链路上分发质量可变的块。 Le’s work 使用 RTSP 网络控制协议为 CCTV 的 360 度直播推流提出了实时转码和加密系统。 转码方式基于 ARIA 加密库，Intel 媒体 SDK 和 FFmpeg 库。 系统可以管理并行的转码操作，实现高速的转码性能。 内容拼接缝合 相比于其他因素如捕获、转码、解码、渲染，内容拼接在决定整体上的推流质量时扮演至关重要的角色。 Chen’s work 提出了一种内容驱动的拼接方式，这种方式将 360 度帧的语义信息的不同类型看作事件，以此来优化拼接时间预算。 基于 VR 帧中的语义信息，tile 执行器模块选择合适的 tile 设计。 拼接器模块然后执行基于 tile 的拼接，这样，基于可用资源，事件 tile 有更高的拼接质量。 评估表明系统通过实现 89.4%的时间预算，很好地适应了不同的事件和时间限制。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:4:1","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"总结 相比于点播式流媒体，360 度直播推流面临多个挑战，例如在事先不知情的情况下处理用户导航、视频的首次流式传输以及实时视频的转码。在多用户场景中，这些挑战更为棘手。 关于处理多个用户的观看模式，可伸缩的多播可以用于在低带宽和高带宽网络上以接近于按需推流的质量等级。 基于 ROI 的 tile 拼接和转码可以显著地减少延迟敏感的交互型应用的延迟需求。 ","date":"2021-11-04","objectID":"/posts/papers/note5/:4:2","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/papers/note5/"},{"categories":["paper"],"content":"概况 现有的沉浸式流媒体应用都对带宽、QoS 和计算需求有着高要求，这主要得益于 5G 网络。 传统的中心化云计算和云存储体系结构不适于实时的高码率内容分发。 边缘缓存和移动边缘计算成为了推动沉浸式流媒体发展的关键技术。 解决方案 ","date":"2021-10-30","objectID":"/posts/papers/note4/:0:0","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/papers/note4/"},{"categories":["paper"],"content":"360 度视频的边缘协助推流 ","date":"2021-10-30","objectID":"/posts/papers/note4/:1:0","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/papers/note4/"},{"categories":["paper"],"content":"背景 主要的视频内容可以被传送到边缘节点乃至下游客户端来满足高分辨率等级和严格的低延迟要求。 在边缘计算中，处理和存储的任务被从核心网转移到边缘节点例如基站、微型数据中心和机顶盒等。 Hou’s work 提出边缘/云服务器渲染可以使计算更加轻便，可以让无线 VR/AR 体验可行并且便携。 Zhang’s work 为 VR 多人游戏提出了一种混合边缘云基础架构，中心云负责更新全局游戏事件，边缘云负责管理视图更新和大规模的帧渲染任务，以此来支持大量的在线联机人数的低延迟游戏。 进一步陈述了一种服务器选择算法，它基于 QoS 和玩家移动的影响确保所有 VR 玩家之间的公平性。 Lo’s work 考虑了为 360 度视频渲染提供边缘协助的设备的异质性。 边缘服务器将 HEVC tile 流转码为 viewport 视频流并传输到多个客户端。 最优化算法根据视频质量、HMD 类型和带宽动态决定边缘节点服务哪个客户端。 边缘缓存策略 背景 传统视频的缓冲方案并不能直接应用到 360 度视频上。 为了在启用边缘缓存的网络中促进 360 度视频的传输，两个传输节点之间的代理缓存被部署来使用户侧的内容可用。 边缘缓存能从实质上减少重复的传输并且可以使内容服务器更加可扩展。 Mahzai’s work 基于其他用户的观看行为为 360 度视频的流行内容提出了一种缓存策略。 与最不常用 (LFU) 和最近最少使用 (LRU) 缓存策略相比，在缓存使用方面的性能分别提高了至少 40% 和 17%。 Papaioannou’s work 提出了基于 tile 分辨率和需求统计信息的缓存策略，用最少的错误，提高要求 tile 的和缓存 tile 这两种版本的 viewport 覆盖率。 不同缓存和传输延迟的实验评估表明提高了缓存命中率，特别是对于分层编码的 tile。 Liu’s work 背景： 边缘缓存可以被在 Evolved Packet Core 处执行，因为 packet 大小很小所以这样可能会产生次优的性能。 另一种替换的方式是在 Radio Access Network 处缓存数据。但这样由于数据隧道和分包会变得更加复杂。 研究内容： 为移动网络提出了一种同时使用 RAN 和 EPC 的基于 tile 的缓存方案，以此在视频流延迟的约束下节省传输带宽。 为 EPC 和 RAN 的缓存节点分别被部署在 Packet Data Network Gateway 和 eNodeBs。 EPC 中的内容控制实体负责为 tile 内容改善缓存利用率。 这种联合的 tile 缓存设计能以优秀的可伸缩性为回程网络显著地减少带宽压力。 Maniotis’s work 为了利用协作传输的机会，提出了一种在包含宏蜂窝基站(MBS)和多个小基站(SBS)的蜂窝网络中的 tile 级别的视频流行度感知缓存和传输方案。 应用了一种高级的编码方式来创建灵活的 tile 编码结构，使在每个 SBS 中能协同缓存。 这种协同允许在 SBS 只存储可能被观看的图块，而其他图块可以通过回程链路获取。 Chen’s work 为被捕获内容从Drone base station到小基站的联合缓存和分发提出了一种echo-liquid状态的 DRL 模型，使用高频毫米波通信技术。 为了满足即时延迟的目标，基站可以从数据中缓存流行内容。 但是，小基站的广泛部署实际上消耗了很多能源。 Yang’s work 在计算资源受限制的 MEC 架构中，利用缓存和计算资源来降低对通信资源的要求。 但是这种结构需要资源敏感的任务调度来平衡通信开销和延迟。 Chakareski’s work 为multi-cell网络环境中的 AR/VR 应用探索了最前沿的缓存、计算和通信机制。 提出的框架允许基站利用适当的计算和缓存资源来最大化总计的回报。 只关注了缓存和渲染，没有考虑用户视角的感受以及处理事件。 Sun’s work 在内容到达终端之前，同时利用 FoV(Field of View)缓存和必要的计算操作来节省通信带宽而不牺牲响应时间。 对于同质的 FoVs，联合缓存和计算框架执行关于缓存和后期处理的最优决策。 对于异质的 FoVs，应用凹凸表达式来得到有吸引力的结果。 Rigazzi’s work 基于一个开源项目 Fog05 提出了一个三层(3C)解决方案来分发密集的任务（例如编解码和帧重建），穿越中心云层，受约束的雾层和边缘节点层。 利用了系统可伸缩性、互操作性和 360 度视频推流服务的生命周期循环。 实验性的评估表明在带宽、能源消耗、部署开销和终端复杂性方面取得了显著的减少。 Elbamby’s work 通过在延迟和可靠性的约束下，应用积极的计算和毫米波传输，为交互式的 VR 游戏提出了一个联合框架。 对视频帧做预计算和存储来减少 VR 流量。 评估表明这种联合机制可以减少多达 30%的端到端延迟。 边缘计算的优势 减少延迟 传统的云端节点距离用户较远，边缘计算使用户可以共享多个服务器池的协同计算资源。 降低能耗 根据网络架构和资源供应将计算卸载到分布式计算集群，能显著提高移动设备的性能。 负载均衡 边缘节点例如基站、小蜂窝和终端设备可以在用户端存储内容，降低了核心网的负载。 现有利用边缘计算的解决方案 大多数任务卸载的 MEC 方案只致力于优化带宽、能源或延迟。 发展中的方案同时致力于许多其他重要的目标：可靠性、可移动性、QoS、部署成本、安全性。 利用带缓存的边缘计算的能力可以增强可移动性、位置感知能力、高效的数据分发、网络上下文理解和提供服务的安全性。 层级化的边缘-云体系结构对于适应 360 度视频快速动态传输是必要的。 相比于单静态层，多个动态缓存模型可以帮助管理唐突的 viewport 和网络变化来改善多用户的 viewport 命中率。 无论环境怎样，主动缓存都可以通过采用预测机制来预取和缓存部分视频来提高感知质量。 ","date":"2021-10-30","objectID":"/posts/papers/note4/:1:1","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/papers/note4/"},{"categories":["paper"],"content":"360 度视频的协同传输 ","date":"2021-10-30","objectID":"/posts/papers/note4/:2:0","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/papers/note4/"},{"categories":["paper"],"content":"背景 360 度视频推流有较大的用户需求并且在逐渐增长。 目前推流 viewport 之外的冗余信息会浪费重要的网络带宽。 相同的 360 度视频内容，在带宽受限的网络之上被推流给多个用户时，码率的需求变得更难满足。 几个方法应用了 360 度视频的协同传输，进而改善传输效率。 ","date":"2021-10-30","objectID":"/posts/papers/note4/:2:1","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/papers/note4/"},{"categories":["paper"],"content":"方案 Ahmadi’s work 引入了基于 DASH 的加权 tile 方法来优化子用户组请求的 tile 编码性能。 提出了多播流方案基于被用户看到的可能性对 tile 分配适当的权重。 接着基于可用带宽和 tile 权重为每个子用户组选择 tile 的码率。 实际上因为相邻 tile 的不同质量导致了空间质量变化，最终造成糟糕的推流体验。 不必要的离散优化问题巨大，不能保证有积极的表现。 Bao’s work 基于动作预测和并发观看用户的信道条件提出了一种多播框架，来只分发可能被看到的 360 度视频块。 没有在无线多播传输中考虑优化资源分配。 Guo’s work 为每个用户假设了一种随机动作模式和不稳定的信道条件，并且开发了多播机会来避免冗余数据传输。 作者考虑了两个非凸的问题： 在给定视频质量的约束下，最小化平均传输时间和能源消耗。 在给定的传输时间和能源预算下，最大化每个用户的视频质量。 Long’s work 考虑了传输时间、视频质量的平滑性和能源限制，在单服务器多用户无线网络环境中优化多个用户的聚合效用。 为了减少传输复杂性，作者准备了多种质量的 tile，并为每组用户将 tile 划分到不相邻的子集中。 Zhang’s work 引入了一种使用 SVC 质量自适应方法的协同推流策略，来改善移动自组网环境中，观看 360 度内容的多个用户间的带宽共享。 当遇到可用网络资源限制时，提出的启发性方式基于被看到的可能性和聚合的组级别偏好设置选择最优的 tile 子集。 Kan’s work 提出了一种服务端混合多播-单播协同推流方案来分发不同质量的 360 度视频到多个用户。 基于用户的观看行为对其进行分簇，以此来轻松共享相同的视频内容。 为每个 tile 联合选择传输模式和适当的码率来提高整体的 QoE。 Huang and Zhang’s work 设计了一种 MIMO 网络中的 MAC 调度方式。 资源分配策略基于三个主要的函数 基于延迟的Motion-To-Photon(MTP)VR 帧权重计算。 基于最大Aggregate Delay-Capacity Utility（ADCU）的用户选择。 用于平衡 VR 数据传输的极高需求的链路自适应方法。 Li and Gao’s work 提出了多用户 VR 框架，其中边缘云自适应地存储和重用冗余 VR 帧，以减少计算和传输负载。 两级 cache 的设计：用户端的小型本地 cache 和边缘的大型中央 cache。 通过为所有用户产生背景视图和无论何时都重用帧，使得减少了内存需求。 评估表明帧相关数据和计算负载分别减少了 95%和 90%。 总结 对推流到多个临近用户的流行内容共享例如 360 度视频是一种自然的选择。 然而非协作式的用户对带宽的竞争会快速使整个网络瘫痪。 为了为多个用户获得改善的 QoE，研究者从以下几个方面做了努力： 确定多个用户可能的需求来公平地分配可用的网络资源。 分析跨用户的行为来精确传输要求的子帧到终端用户。 由于侧信道攻击，保护 VR 帧传输到多个终端用户。 ","date":"2021-10-30","objectID":"/posts/papers/note4/:2:2","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/papers/note4/"},{"categories":["knowledge"],"content":"常见的坑 所有标准库容器都支持迭代器，而只有少数几种支持下标运算符。 string虽然不是容器，但是支持很多容器的操作。 容器不为空时：begin()返回的是容器中第一个元素的位置；end()返回的是容器中最后一个元素的后一个位置。 容器为空时：begin()和end()返回的都是最后一个元素的后一个位置。 任何可能改变容器大小的操作都会使容器的迭代器失效。 ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/iterator/:1:0","tags":["C++"],"title":"重学C++：容器和迭代器","uri":"/posts/knowledge/cpp/iterator/"},{"categories":["knowledge"],"content":"必须要理解的点 和指针类似的是，迭代器支持对对象的间接访问。 和指针不同的是，获取迭代器不使用取地址符，有迭代器的类型都拥有返回迭代器的成员函数，如begin(), end()。 所有迭代器都支持的运算： 运算符 例子 含义 * *iter 返回迭代器iter指向元素的引用 -\u003e iter-\u003emem 解引用iter并获取该元素名为mem的成员，即(*iter).mem ++ ++iter 令iter指向当前元素的后一个元素 – --iter 令iter指向当前元素的前一个元素 == iter1 == iter2 如果两个迭代器指向相同的元素返回true，否则返回false != iter1 != iter2 上面例子的反面 迭代器的类型有两种：iterator和const_iterator。 vector\u003cint\u003e::iterator itv; // 可用于读写vector\u003cint\u003e中的元素 string::iterator its; // 可用于读写string对象中的元素 vector\u003cint\u003e::const_iterator citv; // 只能读取元素 string::const_iterator cits; // 只能读取元素 begin()和end()返回哪一种取决于对象本身是否被const修饰。 C++11 中引入了cbegin()和cend()来专门返回const_iterator。 认定一种类型是迭代器当且仅当它支持一套操作，这套操作能使我们访问容器内的元素或从某一个元素移动到另一个元素。 vector和string的迭代器支持的额外的运算： 运算 含义 iter + n 运算得到一个新迭代器，指向当前元素的后 n 个元素的位置 iter - n 运算得到一个新迭代器，指向当前元素的前 n 个元素的位置 iter += n 运算得到的新迭代器赋值给iter iter -= n 同上 iter1 - iter2 两个迭代器之间的距离，可正可负 \u003e, \u003c, \u003c=, \u003e= 同两类型的下标运算符中的数字的关系，位置靠前的较小 ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/iterator/:2:0","tags":["C++"],"title":"重学C++：容器和迭代器","uri":"/posts/knowledge/cpp/iterator/"},{"categories":["knowledge"],"content":"建议 一般不在意迭代器的类型，因此使用auto来标注。 循环结束的判断条件习惯使用迭代器和!=，这样可以不用在意容器类型。 凡是使用了迭代器的循环体中都不能有改变容器大小的操作如push_back()。 ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/iterator/:3:0","tags":["C++"],"title":"重学C++：容器和迭代器","uri":"/posts/knowledge/cpp/iterator/"},{"categories":["knowledge"],"content":"常见的坑与用法 vector的默认初始化是否合法取决于vector内对象所属的类是否要求显式初始化。 使用()和{}对vector执行初始化含义不同。 using std::vector; vector\u003cint\u003e v1{10}; // 存储1个int对象，值为10 vector\u003cint\u003e v2(10); // 存储10个int对象，值为0 vector\u003cint\u003e v3(10, 1); // 存储10个int对象，值都是1 vector\u003cint\u003e v4{10, 1}; // 存储2个int对象，值分别是10和1 使用{}执行列表初始化时按照顺序遵守 2 个守则： 如果{}内容可以用于初始化，则采用{}默认的初始化含义。 如果{}中的内容无法用{}默认的初始化含义做出解释，则会按照()的初始化含义去解释{}。 using std::vector; using std::string; vector\u003cstring\u003e v1{\"hi\"}; // 存储1个值为hi的string对象 vector\u003cstring\u003e v2{10}; // 存储10个值为空的string对象 vector\u003cstring\u003e v3{10, \"hi\"}; // 存储10个值为hi的string对象 与string相同，vector也有size_type作为其size()的返回值类型。 但是使用时必须首先指定vector由哪个类型定义。 std::vector\u003cint\u003e::size_type a; // 正确 std::vector::size_type a; // 错误 只有vector内元素的类型可以被比较时才能做比较运算，对于自定义类型需要手动定义运算符重载。 增加vector中的元素只能使用push_back() or emplace_back()，而不能使用对下标赋值的方式。 push_back() 和 emplace_back() 的区别来自于两者的函数签名不同： emplace_back() 支持通过传入参数在 vector 内部原地构造对象，因而只会调用构造函数 1 次； push_back() 不支持，所以至少会调用 2 次构造函数和 1 次析构函数（临时对象的构造函数和析构函数、vector 内对象的拷贝或移动构造函数）； 两者都支持传入右值引用作为参数，因而可以使用 push_back(std::move(obj)) or emplace_back(std::move(obj)) 来避免对象拷贝操作，从而改善性能。 可以使用 vector 来模拟 stack 的行为： stack.pop() \u003c=\u003e vector.pop_back() stack.top() \u003c=\u003e vector.back() stack.push() \u003c=\u003e vector.push_back() or vector.emplace_back() vector 在达到容量上限时会触发扩容操作，GCC 的扩容倍数是 2 ，MSVC 的是 1.5. 为什么使用倍数扩容而不是等长扩容？ 因为倍数扩容的单次操作平均时间复杂度是 O(1) （等比数列求和后平均，与扩容倍数相关）。 等长扩容的是O(n) （等差数列求和后平均，与扩容次数相关）。 为什么使用 1.5 倍或 2 倍而不使用 3 倍、4 倍？ 因为扩容的本质其实就是申请新内存空间、拷贝元素、释放旧内存空间。 一个直观的想法是新申请内存空间时可以重复利用旧内存空间。 对于 2 倍扩容的情况：1 2 4 8 16 32 ...，1+2\u003c4, 1+2+4\u003c8, 1+2+4+8\u003c16，这种情况下之前释放的内存空间无法满足扩容的需求。 对于 1.5 倍扩容的情况：1 2 3 4 6 9 13 ...，1+2\u003e=3, 2+3\u003e=4, 4+6\u003e=9, 6+9\u003e=13，这种情况下旧的内存空间可以满足扩容需求，因而存在内存重复利用的可能性。 所以 1.5 倍扩容可以更好的实现对内存的重复利用。 理论最优扩容满足的条件是 f(n-1)+f(n-2)=f(n) 即斐波那契数列，最优扩容因子通过极限可以求出为黄金分割率：1.618. Linux 为什么使用 2 倍扩容？ Linux 下主要使用 glibc 的 ptmalloc 来进行用户空间申请的，如果 malloc 的空间小于 128KB，其内部通过 brk()来扩张，如果大于 128KB，通过 mmap 将内存映射到进程地址空间。 Linux 引入了伙伴系统为内核提供了一种用于分配连续的页而建立的一种高效的分配策略，对固定分区和动态分区方式的折中。固定分区存在内部碎片，动态分区存在外部碎片，而且动态分区回收时的合并以及分配时的切片是比较耗时的。伙伴系统是将整个内存区域构建成基本大小 basicSize 的 1 倍、2 倍、4 倍、8 倍、16 倍等，即要求内存空间分区链均对应 2 的整次幂倍大小的空间，整齐划一，有规律的而不是乱糟糟的。 在分配和释放空间时，可以通过 log2(request/basicSize)向上取整的哈希算法快速找到对应内存块。通过伙伴系统管理空闲分区的了解，可以看到在伙伴系统中的每条空闲分区链中挂的都是 2^i 的页面大小，通过哈希思想进行空间分配与合并，非常高效。估计可能是这个原因 SGI-STL 选择以 2 倍方式进行扩容。 ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/vector/:1:0","tags":["C++"],"title":"重学C++：标准库类模板Vector","uri":"/posts/knowledge/cpp/vector/"},{"categories":["knowledge"],"content":"必须理解的点 vector是类模板而非类型。 vector中只能容纳对象，不能容纳引用。 vector对象能高效增长，增加vector中的元素需要使用 push_back() 或 emplace_back() 成员函数。 vector的成员函数（empty(), size()）和各种运算符（赋值、关系、下标）的操作使用方法和规则基本同string。 ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/vector/:2:0","tags":["C++"],"title":"重学C++：标准库类模板Vector","uri":"/posts/knowledge/cpp/vector/"},{"categories":["knowledge"],"content":"NOTE 不需要在创建vector时确定其中的元素及其大小，但是如果在创建时就已经知道容器中需要容纳的元素个数就可以直接指定vector的大小。 在循环体内部包含向vector对象添加元素的操作时，不应该使用foreach循环。 ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/vector/:3:0","tags":["C++"],"title":"重学C++：标准库类模板Vector","uri":"/posts/knowledge/cpp/vector/"},{"categories":["knowledge"],"content":"常见的坑 string.size()和string.length()等价。 string.size()和其他STL容器的命名风格相一致（如vector, map）。 string.length()出现主要是因为这样的命名符合人的直觉，有更好的可读性。 string::size_type是无符号类型，和int不同，能存放下任何string对象的大小。 +两边至少有一端需要是string对象，不允许两个字符串字面量单独相加。 using std::string; string a = \"a\"; string b = a + \"b\" + \"c\"; // 正确，从左到右运算时能保证至少一段是string对象 string c = \"b\" + \"c\" + a; // 错误，从左到右运算时第一个+左右都是字符串字面量 ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/string/:1:0","tags":["C++"],"title":"重学C++：标准库类型string","uri":"/posts/knowledge/cpp/string/"},{"categories":["knowledge"],"content":"必须要理解的点 string的初始化方式有两种，一种是默认初始化，另一种是拷贝初始化。 string.size()返回值类型为string::size_type，出现这种类型是为了体现标准库类型和机器无关的特性。 string对象的比较运算完全实现了运算符重载（==, !=, \u003c,\u003c=, \u003e, \u003e=）。 ==表明两个对象的内容和长度完全一致，反之任一不同则!=。 不等关系运算符比较的法则： 如果两个对象长度不同，但是从前到后内容一致，则长度较短的对象较小。 如果两个对象从前到后有对应位置的字符不同，则这个位置的两个字符的大小关系就是两个对象的大小关系。 string对象赋值操作就是内容的替换。 string对象相加操作就是内容的拼接，+=操作同理。 string对象可以与字符串字面量相加。 形如cname的C++头文件兼容形如ctype.h的C头文件，C++头文件中定义的名字可以在std中找到。 ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/string/:2:0","tags":["C++"],"title":"重学C++：标准库类型string","uri":"/posts/knowledge/cpp/string/"},{"categories":["knowledge"],"content":"建议 表达式中出现string.size()函数时就不应该使用int类型，这样可以避免int和unsigned混用的问题。 C++和C兼容的头文件作选择时，选择C++的头文件。 处理string对象中每一个字符时，使用foreach语句。 #include \u003ciostream\u003e #include \u003ccctype\u003e using std::string; string str{\"Some String\"}; for (auto c : str) { std::cout \u003c\u003c c \u003c\u003c std::endl; } // 使用引用来改变原字符串内容 for (auto \u0026c : str) { c = std::toupper(c); } std::cout \u003c\u003c str \u003c\u003c std::endl; 处理string对象中特定字符时使用[]（下标运算符）或者迭代器。 使用[]访问字符之前检查string对象是否为空。 std::string s = \"a\"; if (!s.empty()) { std::cout \u003c\u003c s[0] \u003c\u003c std::endl; } string对象下标使用string::size_type作为类型而非int。 using std::string; string a = \"Hello, world!\"; string::size_type index_of_space = a.find(\" \"); ","date":"2021-10-28","objectID":"/posts/knowledge/cpp/string/:3:0","tags":["C++"],"title":"重学C++：标准库类型string","uri":"/posts/knowledge/cpp/string/"},{"categories":["knowledge"],"content":"常见的坑 auto可以在一条语句中声明多个变量，但是所有变量的类型必须一致。 decltype在分析表达式类型时并不执行表达式。 decltype处理解引用操作之后返回的是引用类型，而引用类型的变量必须初始化。 decltype((variable))的结果永远是引用。 decltype(variable)的结果只有当variable是引用时才是引用。 ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/auto/:1:0","tags":["C++"],"title":"重学C++：类型推导","uri":"/posts/knowledge/cpp/auto/"},{"categories":["knowledge"],"content":"必须要理解的点 auto用于变量初始化时的类型推导，decltype用于分析表达式的类型。 auto对引用类型推导时实际上用的是引用对象的值。 auto与const：详见重学 C++：Const 二三事。 decltype与const：详见重学 C++：Const 二三事。 ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/auto/:2:0","tags":["C++"],"title":"重学C++：类型推导","uri":"/posts/knowledge/cpp/auto/"},{"categories":["knowledge"],"content":"建议 auto尽量只在类型较长但比较清晰时使用。 decltype尽量不要使用。 ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/auto/:3:0","tags":["C++"],"title":"重学C++：类型推导","uri":"/posts/knowledge/cpp/auto/"},{"categories":["knowledge"],"content":"常见的坑 仅用const修饰的对象只在单个文件中有效，如果想在多个文件之间共享const对象，必须在对象定义的前面加extern。 允许为一个常量引用绑定非常量的对象、字面量和表达式。 int i = 42; const int \u0026r1 = i; // 正确 const int \u0026r2 = 42; // 正确 const int \u0026r3 = r1 * 2; // 正确 int \u0026r4 = r1 * 2; // 错误 int \u0026r5 = i; r5 = 0; // 正确 r1 = 42; // 错误 指向常量的指针和常量指针： int err_numb = 0; const double pi = 3.1415; int *const cur_err = \u0026err_numb; const double *mut_pi_pointer = \u0026pi; const double *const pi_pointer = \u0026pi; 从声明语句的变量符号开始，自右向左看： cur_err首先是一个不可变对象，其次是一个指向int类型可变对象的指针。 mut_pi_pointer首先是一个可变对象，其次是一个指向double类型不可变对象的指针。 pi_pointer首先是一个不可变对象，其次是一个指向double类型不可变对象的指针。 当typedef遇到const时容易出现错误理解： typedef char *pstring; const pstring cstr = 0; const pstring *ps = 0; pstring是char *的别名，即指向char的指针。 const修饰的是pstring，因此cstr是：初始化值为nullptr的不可变指针。 错误理解会用char *替换掉pstring，即： const char *cstr = 0; 这样从cstr开始自右向左读的话，cstr就会被理解成：指向字符常量的可变指针。 constexpr属于顶层const，因此constexpr修饰指针意味着指针本身不可变。 auto默认会去除顶层const，保留底层const，如果需要顶层const则需要显式加入。 int i = 0; const int ci = i, \u0026cr = ci; auto b = ci; // b是一个初始化值为0的可变int对象 auto c = cr; // c同b auto d = \u0026i; // d是一个初始化为指向可变int类对象i的可变指针对象 auto e = \u0026ci; // e是一个初始化为指向不可变int类对象ci的可变指针对象 const auto f = ci; // f是一个初始化值为0的不可变int对象 decltype不会去除顶层const。 const int ci = 0; decltype(ci) x = 0; // x的类型是const int ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/const/:1:0","tags":["C++"],"title":"重学C++：Const二三事","uri":"/posts/knowledge/cpp/const/"},{"categories":["knowledge"],"content":"必须要理解的点 const对象在创建时必须进行初始化。 常量引用即对const对象的引用。 常量引用绑定不可变对象和可变对象时含义不同。 可变对象 不可变对象 用常量引用绑定 可以 必须 常量引用的含义 不能通过此引用改变对象的值 不可以改变对象的值 常量引用绑定到可变对象上：对原有可操作性质的窄化，减少操作肯定不会引发错误，所以是允许的。 非常量引用绑定到不可变对象上：对原有可操作性质的拓宽，增加不允许的操作会出错、，所以不可变对象必须使用常量引用。 因为指针是对象，而引用不是对象，所以const和指针的组合有 2 种情况，const和引用的组合只有 1 种情况。 指针 指向常量的指针（pointer to const）：不能通过此指针修改对应的量。 常量指针（const pointer）：指针本身的值不可变，即不能用指针指向其他对象，这种不可重新绑定的特性类似于引用。 引用 常量引用：不能通过此引用修改对应的量。 顶层const表示指针本身是常量，推广之后可以指任意对象是常量； 底层const表示指针指向的对象是常量，推广之后主要于指针和引用等复合类型的基本类型部分有关。 常量表达式指：值不会改变，在编译过程中就能得到计算结果的表达式。 为什么需要constexpr？ 因为实际中很难判断一个初始值是否为常量表达式。 使用constexpr相当于把验证变量的值是否是一个常量表达式的工作交给了编译器。 用constexpr声明的变量一定是一个变量，并且必须用常量表达式来初始化。 ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/const/:2:0","tags":["C++"],"title":"重学C++：Const二三事","uri":"/posts/knowledge/cpp/const/"},{"categories":["knowledge"],"content":"建议 如果认定变量是一个常量表达式，那就将其声明成constexpr类型。 ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/const/:3:0","tags":["C++"],"title":"重学C++：Const二三事","uri":"/posts/knowledge/cpp/const/"},{"categories":["knowledge"],"content":"常见的坑 \u0026和*在不同的上下文里面其含义并不相同，因此完全可以当成不同的符号看待。 int i = 42; int \u0026r = i; // \u0026在类型名后出现，是声明的一部分，表明r是一个引用 int *p; // *在类型名后出现，是声明的一部分，表明p是一个指针 p = \u0026i; // \u0026在表达式中出现，是取地址符 *p = 43; // *在表达式中出现，是解引用符 int \u0026r2 = *p; // \u0026是声明的一部分，*是解引用符 指针可以用0进行初始化成空指针，但是不可以用0赋值。 指针之间使用==来比较时，如果结果是true，对应多种情况： 都是空指针 都是同一个地址 都指向同一个对象 一个指针指向某一个对象，另一个指针指向另一对象的下一地址 ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/reference-and-pointer/:1:0","tags":["C++"],"title":"重学C++：引用和指针","uri":"/posts/knowledge/cpp/reference-and-pointer/"},{"categories":["knowledge"],"content":"必须要理解的点 引用和指针——都可以用于间接访问对象 引用 指针 复合类型 ✅ ✅ 表示符号 \u0026 * 含义 变量的别名 变量在内存中的地址 初始化和赋值时是否需要类型匹配 必须匹配（除常量引用） 必须匹配（除 void*和指向常量的指针） 是否需要初始化 必须初始化 无需初始化 可否重新绑定其他变量 不可以 可以 可否嵌套定义 不可以 可以 引用： 引用只能绑定在对象上，不能绑定在字面量或者表达式上。 引用只是原有对象的别名，并非对象，因此不可以定义引用的引用。 定义引用时并不开辟新的内存空间，因此不可以定义引用的指针。 指针： 指针本身就是一个对象，能执行的操作自由度远超过引用。 可以实现嵌套定义，即指针的指针。 可以实现指针的引用。 int i = 42; int *p; // p是int型指针 int *\u0026r = p; // r是指针p的引用，从r开始自右向左读，\u0026表明r是一个引用，引用的是指针，指针指向的类型是int r = \u0026i; // r是p的别名，即给p赋值为i的地址，即令p指向i *r = 0; // r是p的别名，对r解引用即对p解引用，即将p所指向的地址处变量的值赋值为0 指针初始化和赋值时需要使用\u0026运算符取得对象的地址。 指针值的情况： 指向一个对象。 指向紧邻对象所占空间的下一个位置。 空指针，没有指向任何对象。 无效指针，除上述情况之外。 对第 4 种无效指针的操作是未定义的，后果无法预计。 2、3 两种值虽然有效，但是因为没有指向任何对象，所以对其操作的后果同样无法预计。 void*眼中内存空间仅仅是内存空间，并不能访问内存空间中的对象。 ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/reference-and-pointer/:2:0","tags":["C++"],"title":"重学C++：引用和指针","uri":"/posts/knowledge/cpp/reference-and-pointer/"},{"categories":["knowledge"],"content":"建议 初始化所有的指针，并且在对象定义完成之后再定义指向它的指针。 避免使用0和NULL初始化空指针，应该使用nullptr。 在使用指针之前检查其是否为nullptr。 记住赋值改变的永远是等号左侧的对象。 面对复杂的指针或引用的声明语句时，从变量名开始自右向左阅读来弄清楚其真实含义。 ","date":"2021-10-26","objectID":"/posts/knowledge/cpp/reference-and-pointer/:3:0","tags":["C++"],"title":"重学C++：引用和指针","uri":"/posts/knowledge/cpp/reference-and-pointer/"},{"categories":["paper"],"content":"概述 360 度视频的推流手段逐渐从视角独立型方案变成基于 tile 的视角依赖型方案。 相比于常规视频，360 度视频被编码成全向的场景。 自适应 360 度视频推流利用 DASH 框架来实现比特率的自适应。 ","date":"2021-10-25","objectID":"/posts/papers/note3/:1:0","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/papers/note3/"},{"categories":["paper"],"content":"分类 ","date":"2021-10-25","objectID":"/posts/papers/note3/:2:0","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/papers/note3/"},{"categories":["paper"],"content":"Viewport-Independent Streaming 服务端的任务 使用如 ERP、CMP 等视角独立型的投影方式，360 度视频被投影到一个球体上。 客户端的任务 投影之后的视频直接被传送到客户端，并不需要来自传感器的方向信息。 客户端需要支持对应的投影格式。 客户端像处理传统视频一样完成比特率自适应。 基于网络特征向将要到来的 segment 请求相同投影格式的表示 DASH 插件需要支持相同质量视频的推流。 应用 视角独立型推流主要用于体育、教育和旅游视频内容。 优点 简单 缺点 相比于视角依赖型方案视频编码效率低了 30%。 为不可见的区域要求大量带宽和解码资源。 ","date":"2021-10-25","objectID":"/posts/papers/note3/:2:1","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/papers/note3/"},{"categories":["paper"],"content":"Viewport-Dependent Streaming 终端设备的任务 只接受特定的视频帧内容，包括等于或大于视角角度的可见信息。 监测相关的视角作为用户头部移动的回应，并且向服务端发送信号来精确播放器信息。 为服务端准备和用户方向相关的几个自适应集。 客户端的任务 根据网络情况和估计的视角位置决定获取哪个自适应集。 难点 可视区域的确定 与用户头部移动的同步 质量调整 提供平滑的播放体验 现有的工作 各种投影方式在实际推流中表现如何？ 相比于金字塔格式，为视角依赖型投影方案提出的多分辨率变体有最好的研究和开发(RD)性能。 偏移 CMP 获得了 5.6%到 16.4%的平均可见质量。 提出的框架可以基于已知的网络资源和未来的视角位置适应视角的尺寸和质量。 相比于理想的下载过程，这种二维自适应策略可以花费 20%的额外网络带宽下载超过 57%的额外视频块。 如何在网络资源受限的情况下提供高质量的推流？ 为视角依赖型推流产生不同质量的 segment。 当流中只有有限的 representation 时，利用 Quality Emphasized Regions 策略来缩放特定区域的分辨率。 在拥塞网络条件下，执行了基于网络回应的视角大小和比特率的联合适应，结果显示，相比于传送全部的 360 度场景，动态的视角覆盖率提供了更好的画面质量。 这种基于网络回应的自适应也确保基于整体拥塞变化做调整时能改善视频质量。 为立体视频的背景和前景视图采用不对称质量。 可以分别为背景块和前景块分别节省 15%和 41%的比特率。 DASH 需要做什么？ manifest 中需要包含视角位置信息和投影元数据。 优化获取 random access point 的周期来优化视角分辨率自适应体验。 考虑低延迟和活跃的视角切换。 ","date":"2021-10-25","objectID":"/posts/papers/note3/:2:2","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/papers/note3/"},{"categories":["paper"],"content":"Tile-based Streaming 传统视频被分成多个块，360 度视频在块的基础上还被分成多个大小相等或者不等的 tile，以此更加精确地调整画面的细节质量。 分块策略 基本完全交付 高级完全交付 部分交付 分块模式 1x1，3x2，5x3，6x4，8x5 其中 6x4 的模式实现了较好的带宽消耗和编码效率的折中。 在不同的带宽条件下，基本完全交付策略获得了大约 65%的带宽节约。 具体方案 ClusTile 基于分簇的方式，推送满足最小带宽需求的 tile 来克服编码效率和计算开销。 相比于传统和高级的基于 tile 的推流方案，分别实现了 72%和 52%的带宽节约。 当实际看到的和下载的 tile 有差异时，基于分簇的 tile 选取可能会导致选择不当。 Ghosh’s work 提议以最低可获得的质量下载周围和远处的 tile。 相比于其他算法，视角及其周边区域的可变质量提高了 20%的 QoE 水平。 Ozcinar’s work 介绍了一种自适应 360° 视频流框架。 利用视觉注意力度量来计算每个帧的最佳平铺模式。 使用选中的模式，为不同区域的 tile 分配非统一的比特率。 比特率的选取取决于估计的视角和网络状况。 因为很大部分的带宽被用于传输非视角内的 tile，框架难以优化视角内的质量。 Xie’s work 提出了一套优化框架，以此来最小化预取 tile 的错误，改善与不同比特率相关联的 tile 边界的平滑程度。 定义了两个 QoE 函数，目标是最小化： 预期质量失真$\\Phi(X)$ 当考虑 tile 看到概率时视角的空间质量方差$\\Psi(X)$： $$ \\Phi(X) = \\frac{\\sum_{i=1}^{N}\\sum_{j=1}^{M}D_{i,j} * x_{i,j} * p_{i,j}}{\\sum_{i=1}^{N}\\sum_{j=1}^{M}x_{i,j} * s_{i}} $$ $$ \\Psi(X) = \\frac{\\sum_{i=1}^{N}\\sum_{j=1}^{M}x_{i,j}*p_i * (D_{i,j} - s_i * \\Phi(X))^{2}}{\\sum_{i=1}^{N}\\sum_{j=1}^{M}x_{i,j}*s_i} $$ 基于目标缓冲区的自适应方法用于在需要短期视口预测的小缓冲区下进行平滑播放 在自适应的第 k 步，当第 k 个 segment 集合下载完成时，缓冲区占用率$b_k$由下面的式子给出： $$ b_k = b_{k-1} - \\frac{R_k*T}{C_k} + T $$ 为了避免用尽所有块，缓冲区的占用率被通过设定一个目标缓冲区水平$B_{target}$所控制，即$b_k = B_{target}$。 平均空间质量方差是 0.97，比其他基于 tile 的策略小。 所提出的概率自适应框架在感知质量上实现了约 39% 的增益，平均降低了 46% 的空间质量方差。 Vander Hooft’s work 将 360 度帧划分成视角内区域和视角外区域。 首先为所有区域都选择最低质量，然后提高视角内 tile 的质量。 如果带宽依然可用，接着提高剩下的 tile 的质量。 启发式的方式在带宽可用的基础上积极提高视角内 tile 的质量。 没有考虑视角比特率调整时视角预测的错误。 Nguyen’s work 提出了一种新的自适应机制，它在每个 segment 中同时考虑头部移动和视角的预测错误，动态地决定视角内的比特率。 联合适应扩展块的覆盖范围和比特率。 在不同记录的用户头部运动下的实验评估表明，在不获取非视角内区域过多带宽利用率的情况下，视角内容质量有所提高。 DASH SRD 扩展 DASH 的 SRD 扩展提供了多种版本的 tile 的关联来节省更多的比特率。 Le Feuvre and Concolato’s work 他们应用了这个 SRD 特性，引入了同时为独立的和运动受限的 HEVC tile 的不同优先级设定，以此来高效地实现基于 tile 的方案。 使用开源的 GPAC 多媒体框架开发了一个 DASH 客户端，以此来执行带有可配置参数的基于 tile 的推流。 D’Acunto’s work 提出了一种 MPEG-DASH SRD 方法来促进可缩放和可平移视频的平滑推流。 总是下载低分辨率的 tile 来避免用户移动视角时的重新缓冲。 当前视野区域被上采样并展示给用户，以此来支持高质量的缩放功能。 用JavaScript实现了 SRD 视频播放器。 Hosseini’s work 基于 SRD 实现了视角内容、相邻 tile 和剩余 tile 的优先级推流。 用 6 个 3D 网格构建了一套 3D 座标系来在 3D 空间中平滑地表示 tile。 相比于基础的方式，这种区分质量的推流方案节省了 72%的带宽。 Kim and Yang’s work 使用改进的 MPEG-DASH SRD 来在质量可变的 tile 层中作选择。 基于他们之前的工作设计并实现了一个支持多层渲染的 360° VR 播放器，以支持高度不可预测的头部运动数据的高分辨率和低延迟流。 Motion-Constrained TileSet 在 HEVC 中，运动约束贴图集(MCTS)是将整个帧表示为子视频的相邻分割，并为自由选择的贴图集提供解码支持。 Zare’s work 将 MCTS 的概念应用到了全景视频推流中。 将两个质量版本的视频分割成 tile，以原始的分辨率推流视角内的 tile，以低分辨率推流剩余的 tile。 它已经表明，选定图块的可变比特率会降低 30% 到 40% 的比特率。 Skupin’s work 陈述了一种使用 HEVC 编码器的基于 tile 的可变分辨率的推流系统。 使用立方贴图投影的 360 度视频被分割成 24 个网格，每个代表了一个独立的比特流。 两种不同质量的版本被推流到客户端，例如 8 个 tile 以高质量推送，16 个 tile 以低质量推送。 Son’s work 在基于视角的移动 VR 推流中，为独立的 tile 提取和传输实现了基于 MCTS 的 HEVC 和可缩放的 HEVC 编解码器。 节省了超过 47%的带宽。 相比于原始的 HM 和 SHM 编码器表现不佳，因为 MCTS 限制了时间运动信息。 Lee’s work 用 MCTS 编码 360 度视频 tile，并使用显著性检测网络将混合质量的视频 tile 推流给终端用户。 通过显著性模型改进 MCTS 的使用，可以在不增加任何复杂性的情况下灵活地对感兴趣的 tile 区域进行解码支持。 Scalable Video Code 可伸缩视频编码 SVC 是实现 viewport 自适应的一种替代策略。 基础层总被需要并且能从客户端预取来避免重新缓冲事件。 提高层改善 viewport 质量并且可以在带宽充足的时候被请求。 SVC 促进了一种高效的网络内缓存支持来减少多个客户端请求相同内容时的分发开销。 Nasrabadi’s work 使用了一种可伸缩编码方案来解决 360 度视频推流的重新缓冲的问题。 存在质量波动的问题，因为没有使用任何机制来处理 viewport 的预测错误。 Nguyen’s work 建议使用 SVC 协同 viewport 预测来克服网络信道和头部运动的随机性。 实验表明，所提出的平铺层更新和后期平铺终止特征可使 viewport 质量提高 17%。 AI 方法的应用 背景：传统视频推流中使用强化学习来高效调整视频比特率和实现长期的 QoE 回报。 和传统视频内容不同，360 度视频包含几个新的方面比如 tile 大小、viewport 预测等。 直接将现有的强化学习自适应策略应用到 360 度视频上可能会降低推流性能。 Fu’s work 为 360 度视频提出了称为360SRL的一种序列化强化学习方法，它基于之前决策的 QoE 回报而非估计的带宽状况做出自适应决策。 360SRL 使用基于 tile 的推流模拟器来增强训练阶段。 跟踪驱动的评估表明，360SRL 比基线适应方法取得了 12%的 QoE 改善。 Jiang’s work 基于历史带宽、缓冲区空间、tile 大小和 viewport 预测错误等，利用强化学习来做 viewport 和非 viewport 内 tile 的比特率选择。 所提出系统的架构由状态缓冲区、视口预测 (VPP) 和 tile 比特率选择 (TBS) 代理组成。 状态缓冲区向 VPP 和 TBS 代理提供用户查看模式和网络状态。 VPP 代理然后使用 LSTM 模型估计下一个 viewport 位置。 TBS 代理由 Asynchronous Advantage Actor-Critic (A3C)算法训练以执行合适的比特率决策。 Quan’s work 通过卷积神经网络(CNN)提取像素运动来分析用户 QoE，并使用它对 tile 动态分组，从而在视频质量和编码效率之间提供重要的平衡。 使用了基于强化学习的自适应代理，它可以智能地使每个图块的质量适应动态环境。 使用真实 LTE 带宽跟踪验证该方案，在感知质量方面表现出了卓越的性能，同时也节省了带宽资源。 背景：深度学习使强化学习能够使用多方面的状态和动作空间进一步优化聚合回报。 Kan and Xiao’s work 设计了一套深度强化学习的框架，基于对环境因素的探索和开发，自适应地调整推流策略。 这两种方案都采用 DRL 的 A3C 算法来进行比特率","date":"2021-10-25","objectID":"/posts/papers/note3/:2:3","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/papers/note3/"},{"categories":["paper"],"content":"概述 自适应方案可以在处理不同目标对象时帮助改善推流体验。 目标主要包括视频质量、功耗、负载均衡等在移动无线网和有线网接入的情形。 适应性的视频比特率需要同时匹配网络条件和质量目标的需求。 ","date":"2021-10-21","objectID":"/posts/papers/note2/:1:0","tags":["Immersive Video"],"title":"自适应视频推流方案","uri":"/posts/papers/note2/"},{"categories":["paper"],"content":"分类 ","date":"2021-10-21","objectID":"/posts/papers/note2/:2:0","tags":["Immersive Video"],"title":"自适应视频推流方案","uri":"/posts/papers/note2/"},{"categories":["paper"],"content":"服务端适应 大多数服务端适应的方案要求客户端发送系统或网络相关信息。 质量导向的适应方案（Quality-Oriented Adaptive Scheme/QOAS） 向终端用户提供了高知觉质量的媒体内容。 QOAS 是 C-S 架构，决策在服务器端产生。 QOAS 基于客户知觉质量的反馈，提供对推流质量等级的调整。 智能优先级适应方案（intelligent Prioritized Adaptive Scheme/iPAS） 专用于 802.11 网络。 iPAS 服务器上的基于固有印象的带宽分配模块被用于组合 QoS 相关的参数和视频内容特征来进行内容的优先级分类和带宽份额分配。 通过区分多媒体流，iPAS 提供可用无线信道的优先级分配。 设备导向的适应方案（Device-Oriented Adaptive multimedia Scheme/DOAS） 专用于 LTE 网络，建立在 LTE 下行链路调度机制之上。 DOAS 专门根据设备特性实现适配，尤其为多屏终端用户提供了卓越的 QoE。 ","date":"2021-10-21","objectID":"/posts/papers/note2/:2:1","tags":["Immersive Video"],"title":"自适应视频推流方案","uri":"/posts/papers/note2/"},{"categories":["paper"],"content":"客户端适应 基于吞吐量的自适应方案 这类方案基于估计的网络吞吐量从服务端选择视频的比特率。 HTTP 客户端通过之前的观察记录来估计网络的吞吐量。 通过测量端获取时间（segment fetch time/SFT）来代表发起和收到回复的瞬时 HTTP GET 请求之间的时间段，以此来确定一个推流会话中吞吐量的变化，进而独立地做出适应决策。 在分布式网络中，同时考虑并发和顺序的 SFT。通过比较实际的和理想的 SFT 来选择未来的 segment 的质量等级。 FESTIVE 算法 适用于多个 HAS 客户端共享一个常见的拥塞带宽链路的情形。 以效率、稳定性、公平性为度量因素的适应性算法。 探索了一种为分段调度、吞吐量估计和比特率选择而生的健壮的机制。 包含一个随机调度器来调度下一个视频块的下载。 多个客户端共享容量为$W$的满带宽链路，每个客户端$x$在$t$时刻播放的视频比特率为$b_x,_t$ ，需要避免以下 3 种问题： Inefficiency：多个 HAS 客户端必须能选择最可能的表示来提高 QoE。 $$ Inefficiency = \\frac{|\\sum_{x}b_x,_t - W|}{W} $$ 低Inefficiency值表明多个客户端对带宽实现了最有效的利用。 Unfairness：可用带宽应该被均等地分配。 $$ Unfairness = \\sqrt{1-JainFair} $$ 低Unfairness值表明多个客户端有相近的比特率。 Instability：不必要的比特率切换会损害推流体验 $$Instability = \\frac{\\sum_{d=0}^{k-1}|b_{x,t-d} - b_{x,t-d-1}|*w(d)}{\\sum_{d=1}^{k}b_{x,t-d} * w(d)}$$ Probe AND Adapt(PANDA)算法 用于检测网络状况，考虑未来比特率选择的平均目标数据比特率。 目标是当多个 HAS 客户端共享一个拥塞带宽信道时，通过正确探测网络，进而最小化比特率震荡。 PANDA 算法在性能上击败了 FESTIVE 算法，并且 PANDA 算法在这些解决方案中表现出了最好的适应性，在不同带宽情况和播放器设置下实现了最优的效率、公平性和稳定性。 整体上的推流质量不只依赖于本地的吞吐量测量，还依赖服务端的网络容量。 利用服务器发起的推送机制来降低 DASH 内容推流到移动客户端的端到端延迟。 利用HTTP/2的流终止特性来实现中间质量调整。 基于估计的用户 QoE，功耗和可用资源来改善用户端的推流体验。 虽然有证据表明性能得到了提高，但是评估工作只是在受控的 LAN 环境下有效。 Cross Session Stateful Predictor(CS2P)方案 一种数据驱动的吞吐量估计方案，以克服不准确的 HAS 流量预测问题。 将共享相似特性的推流会话分簇，然后对每个簇使用隐马尔科夫模型预测相应的吞吐量样本。 在一个大规模数据集上实验性的评估表明：CS2P 高效地估计了可用的网络吞吐量，进而改善了整体上的视频比特率的适应性。 CFA 和 Pytheas 等方案和 CS2P 类似，也使用数据驱动的控制器来估计可用的吞吐量。 但是这些工作不支持异构系统并且需要额外的训练复杂性，使其不够具有吸引力。 基于吞吐量的适应性方案主要的挑战在于对吞吐量的精确估计。 为 360 度视频采用一个没有经过精巧设计的吞吐量估计机制可能会导致不稳定性和较差的 QoE，在高度动态化的无线和蜂窝网络中尤甚。 基于缓冲区的自适应方案 客户端会在播放视频时根据当前缓冲区的占用情况请求将要到来的 segment。 如何克服不完整的网络信息的限制 在多客户端启用缓存的环境中，结合客户端测量工具集和补偿算法构造模型。 这个模型可以高效探测比特率切换时间并通过选择切换适当的比特率来进行补偿，最终实现了可达 20%的比特率改善。 Buffer Based Adaptation(BBA)方法 应用于 Netfix 客户端时可以减少可达 20%的重新缓冲事件。 BBA 方法考虑的缓冲区较大，因此对于比较短的视频不一定有这样的性能。 Buffer Occupancy-based Lyapunov Algorithm(BOLA) 把比特率适应性问题看作是与播放质量和重新缓冲时间相关的最优化问题。 BOLA 旨在通过把缓冲区大小保持在设定的目标水平来避免重新缓冲。 对于缓冲区级别的突然下降，BOLA 通过请求最低可用视频比特率来避免停顿事件的频率。 如何优化缓冲区利用率 Adaptation and Buffer Management Algorithm(ABMA+) 基于重新缓冲事件的可能性确定未来 representation 的下载时间。 通过基于预先计算的缓冲区大小和 segment 下载时间选择最大比特率来确保流畅的播放。 这样可以实现低计算开销的良好部署。 Scalable Video Coding(SVC)/Bandwidth Independent Efficient Buffering(BIEB) 基于层分发获取视频块，进而维持稳定的缓冲区大小来避免频繁的中断。 没有考虑 QoE 模型中的卡顿和质量切换。 涉及额外的编码和处理开销。 使用 PID 控制器的控制论方法 强制执行缓冲区设置点来使缓冲区保持在最佳水平。 略微降低视频比特率，以防止不必要的视频比特率调整。 在多个客户端竞争的情况下，不能保证公平性。 如何降低 DASH 流的排队延迟 DASH 流会经历最长可达 1s 的排队延迟和严重拥塞，导致缓冲区膨胀问题，而这会严重损害实时多媒体服务的 QoE。 旨在减少网络拥塞的主动队列管理 (AQM) 策略并没有充分减少这种不必要的延迟。 DASH 客户端根据网络设备的队列大小动态接收窗口大小可以显著减轻缓冲区膨胀效应。 由于长期的 viewport 预测的高度不确定性，充足的缓冲区空间对于 360 度视频的流畅播放来说并不可行。 通常小于 3s 的缓冲区大小对于短期的 viewport 预测来讲比较适合。 由于小缓冲区很有可能造成播放卡顿，因此较短持续时间的 segment 可以被用于基于 tile 的流中，但是相比于长持续时间的 segment，这样也会降低编码效率。 混合自适应方案 客户端同时考虑吞吐量和播放缓冲信号来确定即将到来的 segments 的视频比特率。 Model Predictive Control(MPC) 利用良好定义的参数集合来估计可用的网络和缓冲区资源，进而为高 QoE 的比特率做出最优调整的控制论方法。 提出的 QoE 模型采用视频的平均质量$R_k$，平均比特率切换，重新缓冲事件，和初始延迟$T_s$作计算： $$ QoE_1^K = \\sum_{k=1}^{K}q(R_k) - \\lambda\\sum_{k=1}^{K-1}|q(R_{k+1}) - q(R_k)| - \\mu\\sum_{k=1}^{K}(d_k(R_k)/C_k - B_k)_+ - \\mu_sT_s $$ $C_k$：第 k 个块的可用带宽，$B_k$：第 k 个块的可用缓冲区大小 $\\lambda, \\mu, \\mu_s$：可以根据用户兴趣进行调整的权重 MPC 用调和平均的方法来估计吞吐量，并且能够明确管理复杂的控制对象。 只研究了单播放器的情况，因此没有公平性的考量。 Throughput and Buffer Occupancy-based Adaptation(TBOA) 选择合适的视频比特率来获得单个或多个客户端环境中改进的推流体验。 激进地提高了比特率来最高效地利用可用的带宽。 等待缓冲区超过某个级别，然后降低比特率以获得稳定的性能。 为缓冲区等级设置三个阈值，例如： $0 \u003c B_{min} \u003c B_{low} \u003c B_{high}$ 目标区间在$B_{low}$和$B_{high}$之间。 算法努力使最优区间$B_{opt}满足$ $B_{opt} = B_{low} + B_{high} \\over 2$。 通过控制$B_{low}$和$B_{high}$的阈值，使缓冲区和比特率的变化稳定来应对未知的 TCP 吞吐量。 算法表现的流畅而公平，但是没有把用户满意度的度量考虑在内。 fuzzy logic-based DASH 控制重新缓冲事件和视频推流的质量。 考虑了平均吞吐量的估计方法，获得了更高的视频比特率和更少的质量波动。 没有考虑 QoE 度量。 为了更好地调整比特率做出的改进： 用 Kaufman’s Adaptive Moving Average/KAMA 测量法估计吞吐量。 用 Grey Prediction Model/GPM 来估计缓冲区等级。 竞争流模拟环境中，改进所取得的效果： 平均情况下达到 50%的公平性。 最好情况下达到 17%的更好的接收质量。 Spectrum-based Quality Adaptation(SQUAD)算法 解决吞吐量预测和缓冲区等级估计的不连续性。 吞吐量和缓冲区等级反馈信号都被用于选择恰当的质量。 在一开始获取最低质量的 segment 来减少启动时间。 在视频质量切换频率和幅度方面性能显著提高。 尚未有方案讨论如何在视频质量和带宽利用率之间做出很好的平衡。 Throughput Friendly DASH/TFDASH 获得多个竞争客户端情形下的公平性、稳定性和效率。 通过避免 OFF 端获得了最大并且公平的带宽利用率。 双阈值的缓冲区保证播放时的稳定性","date":"2021-10-21","objectID":"/posts/papers/note2/:2:2","tags":["Immersive Video"],"title":"自适应视频推流方案","uri":"/posts/papers/note2/"},{"categories":["paper"],"content":"360 度流媒体视频框架 ","date":"2021-10-20","objectID":"/posts/papers/note1/:1:0","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/papers/note1/"},{"categories":["paper"],"content":"视频采集和拼接 使用不同的 360 度视频采集相机可以将视频内容存储为 3D 的球形内容 ","date":"2021-10-20","objectID":"/posts/papers/note1/:1:1","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/papers/note1/"},{"categories":["paper"],"content":"使用不同的投影策略实现降维 策略主要分为 2 种：视角独立型和视角依赖型 视角独立型 整个 3D 的视频内容被按照统一的质量投影到 2D 平面上 主要包括等距长方形投影和立方贴图投影 等距长方形投影(ERP) 使用左右偏向和俯仰值将观察者周围的球体展平到二维表面上 视角范围：左 180 度～右 180 度、上 90 度～下 90 度 缺点： 极点处会使用比赤道处更多的像素进行表示，会消耗有限的带宽 由于图像失真导致压缩效率不足 立方贴图投影(CMP) 六面立方体组合用于将球体的像素映射到立方体上的相关像素 在游戏中被广泛应用 优点： 节省空间，相比于等距长方形投影视频体积能减少 25% 缺点： 只能渲染有限的用户视野 视角依赖型 视角内的内容比之外的内容有更高保真度的表示 主要包括金字塔投影、截断方形金字塔投影(TSP)和偏移立方贴图投影 金字塔投影 球体被投影到一个金字塔上，基础部分有最高的质量，大多数的投影区域属于用户的视角方向 优点： 节省空间，降低 80%的视频体积 缺点： 用户以 120 度旋转视角时，视频的质量会像旋转 180 度一样急速下降 截断方形金字塔投影 大体情况和金字塔投影相同，区别在与使用了被截断的方形金字塔 优点： 减少了边缘数据，提高了高码率视频的推流性能 缺点： 使边缘更加锐利 偏移立方贴图投影 与原始的立方贴图投影类似，球体的像素点被投影到立方体的 6 个面上 优点： 视角方向的内容会有更高的质量，提供平滑的视频质量变化 缺点： 存储开销很大 ","date":"2021-10-20","objectID":"/posts/papers/note1/:1:2","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/papers/note1/"},{"categories":["paper"],"content":"编码视频内容 目前主要的编码方式有 AVC/H.264 和 HEVC/H.265。 H.264 使用 16x16 的宏块结构对帧编码。 因为使用了编码器的动作预测的特性，编码的数据大小得到减少。 H.265 相比于同质量的 H.264 编码方式，H.265 编码减少了 50%的比特率。 H.265 支持 tiling 特性来实现高效视频推流。 每个 tile 在物理上被分割然后在普通的流中拼接，并且使用一个解码器来解码。 VVC 相比于 H.265，下一代标准 VVC 有望提高 30%的压缩效率。 ","date":"2021-10-20","objectID":"/posts/papers/note1/:1:3","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/papers/note1/"},{"categories":["paper"],"content":"分包和传输 分包 使用 DASH 协议分包。 传输 依赖于雾计算和边缘计算等技术可以缩短分发中心和客户端之间的距离进而实现快速响应和低缓冲时间。 ","date":"2021-10-20","objectID":"/posts/papers/note1/:1:4","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/papers/note1/"},{"categories":["paper"],"content":"渲染和展示 客户端处理 主流方案是使用客户端处理，但是由于会处理不属于用户视角范围内的视频内容，所以会造成计算资源的浪费。 云端处理 另一种方案是使用云端处理，只有用户视角内的视频内容会被传输到客户端，没有更多的带宽和客户端硬件资源要求。 ","date":"2021-10-20","objectID":"/posts/papers/note1/:1:5","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/papers/note1/"},{"categories":["knowledge"],"content":"常见的坑 int, short, long, long long都是带符号的，在前面添加unsigned就能得到无符号类型。 字符型被分为 3 种：char, signed char, unsigned char，前两种并不等价。 虽然有三种类型，但是实际上只有两种表现形式：有符号的和无符号的。 有符号类型在与无符号类型运算时会隐式转换为无符号类型。 虽然变量初始化时候使用了=号，但是初始化和变量赋值并不相同。 变量默认初始化： 变量类型 位置在函数内部 位置在函数外部 内置类型 undefined 0 自定义类型 由类决定 由类决定 #include \u003ciostream\u003e int default_initialize(int a) { // 输出必定是0 std::cout \u003c\u003c a \u003c\u003c std::endl; int b; return b; } int main() { int a; // 输出是随机值 std::cout \u003c\u003c default_initialize(a) \u003c\u003c std::endl; } 如果在函数体内部试图初始化一个extern标记的变量会引发错误。 在嵌套作用域中，内层作用域中的定义可以覆盖外层作用域中声明的变量。 可以显式使用域操作符::来指明使用哪层的变量。 ","date":"2021-10-18","objectID":"/posts/knowledge/cpp/Cpp-Types/:1:0","tags":["C++"],"title":"重学C++：类型系统基础","uri":"/posts/knowledge/cpp/Cpp-Types/"},{"categories":["knowledge"],"content":"必须要理解的点 字面量的意思就是从这个表示形式就能推断其对应类型的量，不同表示形式的字面量和不同类型是多对一的关系。 变量的组成部分：类型和值。说白了就是一个定性一个定量。 类型决定变量在内存里面的存储方式，包括大小和布局方式，以及能参与的运算。 值在实际代码运行过程中则被各种函数使用参与运算。 变量声明和定义： 声明的意思就是：我要用这个变量。 定义的意思就是：我要对这个操作的变量做出定义，规定其具体的细节。 声明 定义 规定变量的类型和名字 ✅ ✅ 申请空间 ✅ 初始化 ✅ 执行多次 ✅ 用extern标记未初始化的变量来表明只对变量作声明： extern int i; //只声明不定义 int i; //声明并且定义 extern int i = 10; //声明并且定义 Q：为什么会有声明和定义这两个概念？ A：因为 C++支持分离式编译机制，这允许程序被分割成若干个文件，每个文件可以被独立编译。如果要在多个文件中使用同一个变量，就必须要将声明和定义分离。变量的定义必须且只能出现在一个文件中，其他用到这个变量的文件必须对其进行声明，且绝对不能进行重复定义。 名字的作用域： 同一个名字在不同的作用域中可以指向不同的实体。 名字的有效区域始于声明语句，以声明语句所在的作用域末端结束。 ","date":"2021-10-18","objectID":"/posts/knowledge/cpp/Cpp-Types/:2:0","tags":["C++"],"title":"重学C++：类型系统基础","uri":"/posts/knowledge/cpp/Cpp-Types/"},{"categories":["knowledge"],"content":"建议 明确数值不可能为负时使用unsigned类型。 使用int执行整数运算，范围不够时使用long long。 使用double执行浮点数运算。 算术表达式中不要使用bool和char。 避免写出依赖实现环境的代码，否则代码不可移植。 避免有符号类型和无符号类型之间的隐式类型转换。 C++11 中引入了列表初始化，例如： // 传统的初始化方式 int units_sold = 0; int units_sold(0); // 现代的初始化方式 int units_sold{0}; int units_sold = {0}; 列表初始化在用于内置类型变量时，如果初始值存在丢失信息的风险，编译器会报错。 long double pi = 3.1415926536; int a{pi}, b = {pi}; // 错误：没有执行类型转换，因为可能丢失信息 int a(pi), b = pi; // 正确：执行了隐式类型转化，丢失了信息 对每个内置类型的变量都执行显式默认初始化以防止 undefined 行为。 在变量第一次使用的地方进行定义操作。 ","date":"2021-10-18","objectID":"/posts/knowledge/cpp/Cpp-Types/:3:0","tags":["C++"],"title":"重学C++：类型系统基础","uri":"/posts/knowledge/cpp/Cpp-Types/"},{"categories":["development"],"content":"原仓库地址：Immersive-Video-Sample 修改之后的仓库：Immersive-Video-Sample ","date":"2021-10-09","objectID":"/posts/development/Immersive-Video-Deploy/:0:0","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/development/Immersive-Video-Deploy/"},{"categories":["development"],"content":"Server 端搭建 ","date":"2021-10-09","objectID":"/posts/development/Immersive-Video-Deploy/:1:0","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/development/Immersive-Video-Deploy/"},{"categories":["development"],"content":"修改 Dockerfile 手动设置 wget 和 git 的 http_proxy 旧 package 目录 not found，修改为新 package 目录 因为找不到 glog 库因此加入软链接操作 ln -s /usr/local/lib64/libglog.so.0.6.0 /usr/local/lib64/libglog.so.0 ","date":"2021-10-09","objectID":"/posts/development/Immersive-Video-Deploy/:1:1","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/development/Immersive-Video-Deploy/"},{"categories":["development"],"content":"重新编译内核 运行脚本时显示 libnuma 错误因此推断与 numa 设置有关 执行numactl -H显示只有一个 node，报错输出显示需要至少两个 numa 节点 查询资料之后获知可以使用 fakenuma 技术创造新节点，但是 Ubuntu 默认的内核没有开启对应的内核参数 手动下载 Linux 内核源代码到/usr/src/目录 wget https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.11.1.tar.gz 解压 tar xpvf linux-5.11.1.tar.gz 复制现有内核配置 cd linux-5.11.1 \u0026\u0026 cp -v /boot/config-$(uname -r) .config 安装必要的包 sudo apt install build-essential libncurses-dev bison flex libssl-dev libelf-dev 进入内核配置界面 sudo make menuconfig 按下/键分别查询CONFIG_NUMA和CONFIG_NUMA_EMU位置 手动勾选对应选项之后保存退出 重新编译并等待安装结束 sudo make -j $(nproc) \u0026\u0026 sudo make modules_install \u0026\u0026 sudo make install 修改grub启动参数加入 fake numa 配置 sudo vim /etc/default/grub 找到对应行并修改为 GRUB_CMDLINE_LINUX=\"numa=fake=2\" 更新grub并重启 sudo update-grub \u0026\u0026 sudo reboot 执行numactl -H检查 numa 节点数目为 2 重新执行脚本如图说明一切正常 ","date":"2021-10-09","objectID":"/posts/development/Immersive-Video-Deploy/:1:2","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/development/Immersive-Video-Deploy/"},{"categories":["development"],"content":"Client 端搭建 需要 Ubuntu18.04 环境，虚拟机中安装之后按照 README 命令，执行脚本一切正常 ","date":"2021-10-09","objectID":"/posts/development/Immersive-Video-Deploy/:2:0","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/development/Immersive-Video-Deploy/"},{"categories":["knowledge"],"content":"Delete old sync files sudo rm /var/lib/pacman/sync/* ","date":"2021-06-11","objectID":"/posts/knowledge/linux/how-to-fix-GPGME-error/:1:0","tags":["linux"],"title":"修复 Archlinux 上出现的 GPGME Error","uri":"/posts/knowledge/linux/how-to-fix-GPGME-error/"},{"categories":["knowledge"],"content":"Re init pacman-key sudo pacman-key --init ","date":"2021-06-11","objectID":"/posts/knowledge/linux/how-to-fix-GPGME-error/:2:0","tags":["linux"],"title":"修复 Archlinux 上出现的 GPGME Error","uri":"/posts/knowledge/linux/how-to-fix-GPGME-error/"},{"categories":["knowledge"],"content":"Populate key sudo pacman-key --populate ","date":"2021-06-11","objectID":"/posts/knowledge/linux/how-to-fix-GPGME-error/:3:0","tags":["linux"],"title":"修复 Archlinux 上出现的 GPGME Error","uri":"/posts/knowledge/linux/how-to-fix-GPGME-error/"},{"categories":["knowledge"],"content":"Re sync sudo pacman -Syyy Now you can update successfully! ","date":"2021-06-11","objectID":"/posts/knowledge/linux/how-to-fix-GPGME-error/:4:0","tags":["linux"],"title":"修复 Archlinux 上出现的 GPGME Error","uri":"/posts/knowledge/linux/how-to-fix-GPGME-error/"},{"categories":["development"],"content":"Get Correct Version microsoft-edge-dev --version The output is Microsoft Edge 91.0.831.1 dev in my case. ","date":"2021-03-26","objectID":"/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/:1:0","tags":["Python"],"title":"在 microsoft-edge-dev 上设置 Python selenium","uri":"/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/"},{"categories":["development"],"content":"Get Corresponding WebDriver Find the corresponding version at msedgewebdriverstorage and download the zip. Extract it to you path like /usr/local/bin or $HOME/.local/bin. ","date":"2021-03-26","objectID":"/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/:2:0","tags":["Python"],"title":"在 microsoft-edge-dev 上设置 Python selenium","uri":"/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/"},{"categories":["development"],"content":"Write Code Following is a example. from msedge.selenium_tools import EdgeOptions, Edge options = EdgeOptions() options.use_chromium = True options.binary_location = r\"/usr/bin/microsoft-edge-dev\" options.set_capability(\"platform\", \"LINUX\") webdriver_path = r\"/home/ayamir/.local/bin/msedgewebdriver\" browser = Edge(options=options, executable_path=webdriver_path) browser.get(\"http://localhost:8000\") assert \"Django\" in browser.title ","date":"2021-03-26","objectID":"/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/:3:0","tags":["Python"],"title":"在 microsoft-edge-dev 上设置 Python selenium","uri":"/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/"},{"categories":["development"],"content":"Launch it ","date":"2021-03-26","objectID":"/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/:4:0","tags":["Python"],"title":"在 microsoft-edge-dev 上设置 Python selenium","uri":"/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/"},{"categories":["knowledge"],"content":"文件和目录的权限 下图为使用exa命令的部分截图 上图中的 Permission 字段下面的字母表示权限 第一个字母表示 文件类型 ： 属性 文件类型 - 普通文件 d 目录文件 l 符号链接 符号链接文件剩余的属性都是 rwxrwxrwx，是伪属性值，符号链接指向的文件属性才是真正的文件属性 c 字符设备文件 表示以字节流形式处理数据的设备，如 modem b 块设备文件 表示以数据块方式处理数据的设备，如硬盘驱动或光盘驱动 剩下的 9 个位置上的字符称为 文件模式 ，每 3 个为一组，分别表示文件所有者、文件所属群组以及其他所有用户对该文件的读取、写入和执行权限 属性 文件 目录 r 允许打开和读取文件 如果设置了执行权限，允许列出目录下的内容 w 允许写入或截断文件，但是不允许重命名或删除文件 如果设置了执行权限，那么允许目录中的文件被创建、被删除和被重命名 x 允许把文件当作程序一样来执行 允许进入目录 ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:1:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"id：显示用户身份标识 一个用户可以拥有文件和目录，同时对其拥有的文件和目录有控制权 用户之上是群组，一个群组可以由多个用户组成 文件和目录的访问权限由其所有者授予群组或者用户 下图为 Gentoo Linux 下以普通用户身份执行 id 命令的结果 uid 和 gid 分别说明了当前用户的用户编号与用户名、所属用户组的编号与组名 groups 后的内容说明了用户还属于哪些组，说明了其对应的编号和名称 许多类 UNIX 系统会将普通用户分配到一个公共的群组中如：users 现代 Linux 操作是创建一个独一无二的只有一个用户的同名群组 ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:2:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"chmod：更改文件模式 chmod 支持两种标识方法 八进制表示法 八进制 二进制 文件模式 0 000 --- 1 001 --x 2 010 -w- 3 011 -wx 4 100 r-- 5 101 r-x 6 110 rw- 7 111 rwx 常用的模式有 7,6,5,4,0 符号表示法 符号 含义 u user：表示文件或目录的所有者 g group：文件所属群组 o others：表示其他用户 a all：u+g+o 如果没有指定字符默认使用all + 表示添加一种权限 - 表示删除一种权限 例如： 符号 含义 u+x 所有者+可执行 u-x 所有者-可执行 +x 所有用户+可执行 o-rw 其他用户-读写 go=rw 群组用户和其他用户权限更改为读，写 u+x,go=rx 所有者+可执行，群组用户和其他用户权限更改为读，可执行 -R 表示递归设置 ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:3:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"umask：设置文件默认权限 使用八进制表示法表示从文件模式属性中删除一个位掩码 掩码的意思：用掩码来取消不同的文件模式 plain umask 可以看到输出为： plain 0022 不同 linux 发行版默认的文件权限不同，这里的输出是 Gentoo Linux 上普通用户对应的的输出 0022：先不看第一个 0,后面的 0|2|2 用二进制展开结果是：000|010|010 原始文件模式 --- rw- rw- rw- 掩码 000 000 000 010 结果 --- rw- rw- r-- 掩码中 1 对应位处的权限会被取消，0 则不受影响 所以会有这样的结果： 再来谈最前面的 0:因为除了 rwx 之外还有较少用到的权限设置 setuid 位:4000(8 进制) 设置此位到一个可执行文件时，有效用户 ID 将从实际运行此程序的用户 ID 变成该程序拥有者的 ID 设置场景：应用于由 root 用户拥有的程序，当普通用户运行一个具有 setuid 位的程序时，这个程序会以超级用户的权限执行，因此可以访问普通用户无法访问到的文件和目录 设置程序 setuid： plain chmod u+s program_name 结果： plain -rwsr-xr-x 可以看到第二组权限中第一个符号是 s setgid 位:2000(8 进制) 有效组 ID 从该用户的实际组 ID 更改为该文件所有者的组 ID 设置场景：当一个公共组下的成员需要访问共享目录下的所有文件时可以设置此位 对一个目录设置 setgid 位，则该目录下新创建的文件将由该目录所在组所有 plain chmod g+s dir_name 结果： plain drwxrwsr-x 可以看到第二组权限中最后一个符号是 s(替换了 x) sticky 位:1000(8 进制) 标记一个可执行文件是“不可交换的”，linux 中默认会忽略文件的 sticky 位，但是对目录设置 sticky 位，能阻止用户删除或者重命名文件，除非用户是这个目录的所有者，文件所有者或者 root 用来控制对共享目录的访问 plain chmod +t dir_name 结果： plain drwxrwxrwt 可以看到第三组权限中最后一个符号是 t(替换了 x) ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:4:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"su：以另一个用户身份运行 shell ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:5:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"使用 su 命令登录 plain su [-[l]] [user] 如果包含“-l”选项，得到的 shell session 会是 user 所指定的的用户的登录 shell 即 user 所指定的用户的运行环境将会被加载，工作目录会更改为此用户的主目录 ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:5:1","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"使用 su 命令执行单个命令 plain su -c ‘comand’ 命令内容必须用 ’’ 引用起来（也可以是双引号） ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:5:2","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"sudo：以另一个用户身份执行命令 ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:6:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"sudo 和 su 的区别 sudo 比 su 有更丰富的功能，而且可以配置 通过修改配置文件来配置 sudo plain EDITOR=vim visudo 执行上面的命令可以用 vim 来编辑 sudo 的配置文件 常用的场景是在将用户加入到 wheel 组之后使 wheel 组的用户能够访问 root 权限 使用 sudo 命令输入的不是 root 的密码，而是自己的密码 可以使用 `sudo -l`来查看通过 sudo 命令能获得的权限 ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:6:1","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"chown：更改文件所有者 ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:7:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"用法 plain chown [owner][:[group]] file … 第一个参数决定 chown 命令更改的是文件所有者还是文件所属群组，或者对两者都更改 参数 结果 bob 文件所有者=\u003ebob bob:users 文件所有者=\u003ebob 文件所属群组=\u003eusers :admins 文件所属群组=\u003eadmins bob: 文件所有者=\u003ebob 文件所属群组=\u003ebob 登录系统时的组 图中使用 root 用户在/home/ayamir 目录下创建了一个 foo.txt 文件，最后将此文件的所有者和所属组都改为了 ayamir（rg 是ripgrep） ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:7:1","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"chgrp：更改文件所属群组 这个命令是历史遗留问题，在早期的 UNIX 版本中，chown 只能更改文件的所有者，而不能改变文件的所属群组，因此出现了这个命令，事实上现在的 chown 已经能实现 chgrp 的功能，因此没必要再使用这个命令（其使用方式几乎与 chown 命令相同） ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:8:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"passwd：更改用户密码 ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:9:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"一般用法 plain passwd [user] 用来更改 user 用户的密码，如果想修改当前用户的密码则不需要指定 user 执行之后会提示输入旧密码和新密码，新密码需要再确认输入一次 拥有 root 用户权限的用户可以设置所有用户的密码 上图为 Gentoo Linux 下使用 passwd 命令修改 ayamir 用户密码的过程，这里可以看到 passwd 会强迫用户使用强密码，会拒绝短密码或容易猜到的密码（其他发行版可能输出会不一样） ","date":"2021-03-15","objectID":"/posts/knowledge/linux/linux-authority/:9:1","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/knowledge/linux/linux-authority/"},{"categories":["knowledge"],"content":"Arch Linux DNS 设置 安装dnsmasq sudo pacman -S dnsmasq 配置/etc/resolv.conf中的域名代理服务器 # Tencent nameserver 119.29.29.29 nameserver 182.254.118.118 # Ali nameserver 223.5.5.5 nameserver 223.6.6.6 # OpenDNS IPv4 nameservers nameserver 208.67.222.222 nameserver 208.67.220.220 # OpenDNS IPv6 nameservers nameserver 2620:0:ccc::2 nameserver 2620:0:ccd::2 # Google IPv4 nameservers nameserver 8.8.8.8 nameserver 8.8.4.4 # Google IPv6 nameservers nameserver 2001:4860:4860::8888 nameserver 2001:4860:4860::8844 # Comodo nameservers nameserver 8.26.56.26 nameserver 8.20.247.20 # Generated by NetworkManager nameserver 192.168.1.1 防止/etc/resolv.conf被修改 sudo chattr +i /etc/resolv.conf 减少主机名查找时间 sudo echo \"options timeout:1\" \u003e /etc/resolv.conf.tail 启动dnsmasq sudo systemctl enable dnsmasq.service --now ","date":"2021-01-26","objectID":"/posts/knowledge/linux/dns-settings-on-archlinux/:1:0","tags":["linux"],"title":"在 Linux 上手动设置 DNS","uri":"/posts/knowledge/linux/dns-settings-on-archlinux/"},{"categories":null,"content":"个人概况 23 岁，北京邮电大学 2022 级计算机科学与技术专业研究生，2018 级软件工程工学学士。 本科期间做的和发起的项目： 一个结合了 B 站和知乎部分功能的的前后端未分离的网站（php/css/html/js/mysql） 一个实现了拼单、拼车、群聊等功能的 Android App （java） 一套 Neovim 的配置方案，与其他小伙伴维护至今已获得 2.5K+ star 和 400+ fork （lua） 研究生期间的方向很乱，包括： 全景视频的分块传输和 Web 端播放系统（基于 Dash、WebXR、WebGL） 结合游戏引擎的物体级 ROI 编码增强（基于 Unity、WebRTC、OpenH264） 基于 Unity 的 VR 开发，以及结合 VR 的 HCI 研究。 尽管如此，本人不变的特性是喜欢 coding ，有代码审美，喜欢折腾并且经验丰富（比如自己修改 Linux 内核参数编译），愿意花时间在提高效率上。 目前的状态是在找实习，希望是能写一些系统级的、 Solid 的代码。 ","date":"0001-01-01","objectID":"/about/:1:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"友链 cl 的博客：https://clslaid.icu/ 黄鸡的博客：https://blog.hyiker.com/ Maxlinn 的博客：https://www.maxlinn.site/ z217 的博客：https://z217blog.cn/ Tackoil 的博客：https://tackoil.github.io/ Leeshy 的博客: https://leeshy-tech.github.io/ ","date":"0001-01-01","objectID":"/about/:2:0","tags":null,"title":"","uri":"/about/"}]
[{"categories":["development"],"content":"本文主要记录笔者在 Gentoo Linux 下面搭建 WebRTC 开发环境的过程。 ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:0:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["development"],"content":"准备工作 网络：可以科学上网的梯子 IDE：VSCode 或者 CLion ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:1:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["development"],"content":"安装depot_tools Google 有自己的一套用于管理 Chromium 项目的工具，名叫depot_tools，其中有包括git在内的一系列工具和脚本。 # 创建google目录用于存储google相关的代码 mkdir ~/google cd ~/google # clone depot_tools git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git 克隆完成之后需要将depot_tools的路径加到PATH中，Linux 上添加环境变量最简单的方式是修改~/.profile，这种方式与你的登录 shell 是什么没有关系，不管是fish还是bash还是zsh都会吃这种方式： # ~/.profile export GOOGLE_BIN=$HOME/google/depot_tools export PATH=$GOOGLE_BIN:$PATH 但是这种方式需要你注销重新登录。 ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:2:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["development"],"content":"克隆代码 mkdir webrtc-checkout cd webrtc-checkout fetch --nohooks webrtc gclient sync 整个 WebRTC 的项目代码大小约 20G，克隆过程中需要保证网络畅通顺畅，如果你的梯子有大流量专用节点最好，否则可能克隆完你的流量就用光了。 克隆期间可能会因为网络问题中断，重新执行gclient sync即可，直到所有的模块都克隆完毕。 按照官方的建议，克隆完成之后创建自己的本地分支，因为官方分支更新很快，不 checkout 的话，可能你的 commit 还没写完，就被 Remote 的 change 给覆盖了，还要手动处理冲突。 cd src git checkout master git new-branch \u003cbranch-name\u003e ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:3:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["development"],"content":"编译 WebRTC 关于 WebRTC 的版本可以在Chromium Dash查到： 如上图所示，113 分支是当前的稳定分支，对应的 tag 是branch-heads/5672： cd ~/google/webrtc-checkout/src git checkout branch-heads/5672 git switch -c m113 创建本地分支之后就可以用gn生成ninja文件了： gn gen out/Default --root=\".\" --args='is_debug=true target_os=\"linux\" target_cpu=\"x64\" rtc_include_tests=false rtc_use_h264=true rtc_enable_protobuf=false is_clang=true symbol_level=0 enable_iterator_debugging=false is_component_build=false use_rtti=true rtc_use_x11=true use_custom_libcxx=false treat_warnings_as_errors=false use_ozone=true' 这里使用了clang并且启用了h264，详细的gn参数可以参考gn-build-configuration和项目根目录下的webrtc.gni文件。 之后使用autoninja进行编译，编译时会吃满你 PC 的所有核心，编译时间取决于你 PC 的配置： autoninja -C out/Default 可以看到默认生成了几个样例的可执行文件。 cd 到obj目录下可以看到libwebrtc.a文件，就是编译链接之后最终生成的可以引用的库文件。 ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:4:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["development"],"content":"搭建开发环境 Google 官方给出了 Chromium 项目的CLion 配置指南，所以只需要照猫画虎给 WebRTC 配置一下。 ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:5:0","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["development"],"content":"配置 CLion 属性 因为整个项目比较大，所以需要调大 CLion 的 VM 内存和 intellisence 支持的文件大小： Help-\u003e Edit Custom VM Options，在文件的末尾添加： -Xmx8g 表示给 VM 设定8G的可用内存，这样基本上不用担心使用过程因为内存不足而 CLion 性能不够了。 Help-\u003eEdit Custom Properties，在文件的末尾添加： idea.max.intellisense.filesize=12500 表示为大小为12500KB也就是12M以下的文件提供 intellisense 支持。 ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:5:1","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["development"],"content":"配置 gdb vim ~/.gdbinit # 添加下面一行 source ~/google/webrtc-checkout/src/tools/gdb/gdbinit 之后在 CLion 中的Settings-\u003eToolchain-\u003eDebugger选择系统自带的 gdb：/usr/bin/gdb即可。 ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:5:2","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["development"],"content":"配置 intellisense 因为 WebRTC 用的是gn+ninja作为构建工具，而CLion目前只支持cmake，所以当要求配置CMakeLists.txt时直接无视即可。网络上有说用gn_to_cmake.py这个脚本的，但是我没看懂这东西的功能，反正是不能生成CMakeLists.txt，只是生成一个json文件，并不能用于 CLion 的索引。 我这边成功开启 IDE 语法高亮和索引的姿势是这样的： cd webrtc-checkout/src python3 ./tools/clang/scripts/generate_compdb.py -p ./out/Default -o ./compile_commands.json --target_os=linux 这一步会生成 CLion 可以自动识别的compile_commands.json文件，从而可以正确索引项目的代码并提供代码补全功能。 之后每次启动项目 CLion 就会自动索引项目文件，就可以愉快地看代码和写代码啦！ ","date":"2023-04-23","objectID":"/posts/webrtc-development-prepare/:5:3","tags":["WebRTC"],"title":"在Linux下如何搭建WebRTC的开发环境","uri":"/posts/webrtc-development-prepare/"},{"categories":["knowledge"],"content":"概况 WebRTC提供了视频自适应机制，其目的主要是通过降低编码的视频的质量来减少带宽和 CPU 消耗。 视频自适应发生的情形：带宽或 CPU 资源发出信号表明自己未被充分使用或被过度使用时，进行视频自适应。过度使用则降低质量，否则提高质量。 视频自适应调整的对象：帧率与分辨率。 资源 Resources监测指标来自于系统或视频流。例如，一个资源可以监测系统温度或者视频流的带宽使用率。 资源实现了Resource接口： 当资源检测到被过度使用则调用SetUsageState(kOveruse)； 当资源不再被过度使用则调用SetUsageState(kUnderuse)。 对所有的视频而言，默认有两种类型的资源： 质量标量资源 编码过度使用资源 ","date":"2022-09-15","objectID":"/posts/note-for-webrtc-1/:0:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/note-for-webrtc-1/"},{"categories":["knowledge"],"content":"QP 标量资源 质量标量资源监测发送视频流中编码之后的帧的量化参数（QP），确保视频流的对于当前的分辨率而言可以接受。 每一帧被编码之后，QualityScaler就能获得相应的 QP。 过度使用或者未被充分使用的信号在平均 QP 脱离 QP 阈值之后发出。 QP 阈值在EncoderInfo中的scaling_settings属性中设置。 需要注意的是 QP 标量只在降级偏好设置为MAINTAIN_FRAMERATE或BALANCED时启用。 ","date":"2022-09-15","objectID":"/posts/note-for-webrtc-1/:1:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/note-for-webrtc-1/"},{"categories":["knowledge"],"content":"编码使用资源 编码使用资源监测编码器需要花多长时间来编码一个视频帧，实际上这是 CPU 使用率的代理度量指标。 当平均编码使用超过了设定的阈值，就会触发过度使用的信号。 ","date":"2022-09-15","objectID":"/posts/note-for-webrtc-1/:2:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/note-for-webrtc-1/"},{"categories":["knowledge"],"content":"插入其他资源 自定义的资源可以通过Call::AddAdaptationResource方法插入。 自适应 资源发出过度使用或未充分使用的信号之后，会发送给ResourceAdaptationProcessor，其从VideoStreamAdapter中请求Adaptation提案。这个提案基于视频的降级偏好设置。 ResourceAdaptationProcessor基于获得的提案来确定是否需要执行当前的Adaptation。 ","date":"2022-09-15","objectID":"/posts/note-for-webrtc-1/:3:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/note-for-webrtc-1/"},{"categories":["knowledge"],"content":"降级偏好设置 有 3 种设置，在RtpParameters的头文件中定义： MAINTAIN_FRAMERATE: 自适应分辨率 MAINTAIN_RESOLUTION: 自适应帧率 BALANCED: 自适应帧率或分辨率 降级偏好设置在RtpParameters中的degradation_perference属性中设置。 VideoSinkWants和视频流自适应 自适应完成之后就会通知视频流，视频流就会转换自适应为VideoSinkWants。 这些接收器需求向视频流表明：在其被送去编码之前需要施加一些限制。 对于自适应而言需要被设置的属性为： target_pixel_count: 对于每个视频帧要求的像素点总数，为了保持原始的长宽比，实际的像素数应该接近这个值，而不一定要精确相等， max_pixel_count: 每个视频帧中像素点的最大数量，不能被超过。 max_framerate_fps: 视频的最大帧率，超过这个阈值的帧将会被丢弃。 VideoSinkWants可以被任何视频源应用，或者根据需要可以直接使用其基类AdaptationVideoTraceSource来执行自适应。 ","date":"2022-09-15","objectID":"/posts/note-for-webrtc-1/:4:0","tags":["WebRTC"],"title":"WebRTC 中关于视频自适应的相关设置","uri":"/posts/note-for-webrtc-1/"},{"categories":["paper"],"content":"整体概况 Link：Modeling the Perceptual Quality for Viewport-Adaptive Omnidirectional Video Streaming Considering Dynamic Quality Boundary Artifact Level：IEEE TCSVT 2021 DQB: Dynamic Quality Boundary，指在基于分块的 FoV 自适应全景视频推流过程中低质量分块区域的暴露和质量切换现象。 DQB 现象实际上就是 FoV 内分块间的质量差异和随时间变化的分块质量变化。 这篇论文主要的贡献在于深入研究了这种现象，并且针对此提出了可以利用现存的 QoE 评估指标的模型，并且可以实际应用。 Model 的建立 执行一系列主观评估，由低质量分块的比例和质量导致的感知质量的降低可以基于主观实验结果完成建模。 结合剩下分块的感知质量可以完成单帧质量模型的建模。 最后将一段时间内的所有帧的感知质量池化，就完成了整个的模型。 ","date":"2022-03-20","objectID":"/posts/note-for-dqb/:0:0","tags":["QoE"],"title":"Note for DQB","uri":"/posts/note-for-dqb/"},{"categories":["paper"],"content":"主观实验的设定 获得 FoV 内帧的感知质量（低质量分块和高质量分块同时存在） 获取整个视频的感知质量（与上面的实验过程相近，只是过程中没有暂停） 获取整个视频的感知质量（没有引入 DQB，所有分块质量相同） 实验结果 ","date":"2022-03-20","objectID":"/posts/note-for-dqb/:1:0","tags":["QoE"],"title":"Note for DQB","uri":"/posts/note-for-dqb/"},{"categories":["paper"],"content":"帧质量感知模型 从上面的实验结果可以看出来高质量区域与低质量区域的质量差距 $d_n$ 越大，DQB 效应越显著（符合直觉）。将这部分影响因素看作是感知质量的主要影响因素： $$ d_n = Q_{H, n} - Q_{L, n} $$ $Q_{H, n}$ 和 $Q_{L, n}$ 分别表示第 $n$个 帧高质量分块和低质量分块的感知质量。 这两个质量从主观实验 3 的主观质量获得，在之后的训练过程中可以被客观质量评估的结果所替换。 为了调查质量差异 $d_n$ 和感知质量降低 $D_n$ 之间的关系，通过使用实验 1 的帧质量分数计算得出第$n$个帧的感知质量降低： $$ D_n = Q_{H, n} - Q_{HL, n} $$ $Q_{HL, n}$是实验 1 中评分得到的第$n$个帧的 FoV 内感知质量。 在 6 个视频上的实验结果如下图： 可以看到二者的关系可以近似为线性相关，即： $$ D_n = k_1 d_n $$ $k_1$ 作为线性回归的参数，可以计算出来。 但是对于不同取值的 $p_n$ ， $k_1$ 的取值也相当不同，两者之间的关系可以见下图： 数学表示可以建模为： $$ k_1 = a_1 \\cdot ln(a_2 \\cdot p_n + a_3) \\cdot sgn(p_n - P) $$ $sgn$ 是符号函数，$a_1, a_2, a_3$ 可以从回归中计算出来， $P$ 表示低质量分块的比例。按照图中的回归结果，$P = 0.118$ 时，用户几乎没办法注意到低质量区域的存在。 最终，由低质量区域暴露引起的感知质量降低 $D_n$ 可以计算为： $$ D_n = a_1 \\cdot ln(a_2 \\cdot p_n + a_3) \\cdot (Q_{H, n} - Q_{L, n}) \\cdot sgn(p_n - P) $$ 那么实际的感知质量 $Q_n$ 可以计算为： $$ Q_n = Q_{H, n} - D_n $$ ","date":"2022-03-20","objectID":"/posts/note-for-dqb/:2:0","tags":["QoE"],"title":"Note for DQB","uri":"/posts/note-for-dqb/"},{"categories":["paper"],"content":"时间池化 可以采用下面两种方式之一完成 ","date":"2022-03-20","objectID":"/posts/note-for-dqb/:3:0","tags":["QoE"],"title":"Note for DQB","uri":"/posts/note-for-dqb/"},{"categories":["paper"],"content":"Exp Minkowski-Based 单个帧的感知质量由衰减指数加权，衰减指数表示在主观评估中观察到的近因效应。 最终整个视频的感知质量 $PQ$ 可以计算为： $$ PQ = \\Big[\\frac{1}{N} \\sum_{n=1}^{N} exp\\big( \\frac{n-N}{\\delta} \\big) \\cdot {Q_n}^p \\Big]^{1/p} $$ $N$ 是整个视频的帧数。 $p$ 是 Minkowski指数，高 $p$ 值强调了最高质量帧的影响。 $\\delta$ 是控制近因效应强度的指数时间常数，以帧的数量的形式给出，高 $\\delta$ 值对应较弱的近因效应。 $p$ 和 $\\delta$ 的值可以通过对主观帧质量和视频序列的整体质量进行回归得到。 ","date":"2022-03-20","objectID":"/posts/note-for-dqb/:3:1","tags":["QoE"],"title":"Note for DQB","uri":"/posts/note-for-dqb/"},{"categories":["paper"],"content":"Quality Contribution-Based 之前的研究表明，传统视频在时间维度上的感知质量降低主要与每帧的显示时长相关。 FoV 自适应的全景视频也与之类似，感知质量与降低质量帧和高质量帧的持续时间相关。因此采用Quality Contribution的概念来描述每帧对视频感知质量的影响（考虑每帧的空间感知质量和显示时长）。 时间池化是由相应的显示时长加权的每帧的质量贡献的函数，特别的，质量贡献是从 MOS 和显示持续时间之间初步找到的对数关系所导出的： $$ C_n = Q_n \\cdot (p_1 + p_2 \\cdot log(T)) $$ $C_n$ 是第 $n$ 帧的贡献， $T$ 是每帧的显示时长， $T = Max(T, 33.3ms)$，即当帧率不低于 30fps 时，时间不连续性可以忽略。 接着，二级时间池化法用于池化单帧的分布。这种方法将 FoV 内的帧以注视水平划分为短时帧组(GoFs)，并以 GoF 的质量作为长期时间池化的基本单位来评估感知质量。 给出每帧的质量贡献之后，每个 GoF 的质量可以计算为 $$ Q_{GoF} = \\frac{\\sum_{n \\in N} \\big( C(n) \\cdot T(n) \\big)}{\\sum_{n \\in N} T(n)} $$ 接下来组合 GoF 的质量得到长期时间池化，即可以获得感知质量。 质量严重受损的帧会影响相邻帧的感知质量，视频中质量最差的部分主要决定整个视频的感知质量。因此提出选择计算出的质量低于平均值 75%的 GoF，以此计算平均质量并作为整个视频的感知质量。 ","date":"2022-03-20","objectID":"/posts/note-for-dqb/:3:2","tags":["QoE"],"title":"Note for DQB","uri":"/posts/note-for-dqb/"},{"categories":["paper"],"content":"Overview Link: Toward Immersive Experience: Evaluation for Interactive Network Services Level: IEEE Network 2022 Keywords: QoE Metrics Background Compared with traditional QoE for regular video/audio services, the existing work on IE is still in its infancy. This work aims at providing systematic and comprehensive research on IE for interactive network services, mainly studying the following three fundamental and challenging issues. What is the essential difference between IE and traditional QoE? Which categories of factors mainly influence IE? How to evaluate IE in an efficient and intelligent manner? IE versus traditional QoE ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:0:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Theoretical definitions Existing concepts of IE can be classified into two categories. The subjective sense of being surrounded or experiencing multi-sensory stimulation when interacting with the virtual environment. The user’s psychological state of deep involvement, engagement, absorption, or engrossment. Traditional QoE: A subjective measure from the user perspective of the overall value of the provided service and application. We can summary two significant points as follows to distinguish IE and traditional QoE: Both IE and traditional QoE are devoted to characterizing user’s subjective experience for network services. In terms of application scenarios, IE concentrates on the evaluation of network services equipped with interactive characteristics while traditional QoE is generally appropriate for regular audio/video services. IE is much more complex, fine-grained and multi-dimensional perception, which is produced through the interplay between multi-sensory data and diverse cognitive processes. ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:1:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Technical challenges Growing data volume Stricter delay constraint Increasing data dimension ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:2:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"IFs on IE ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:3:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Network-aware IFs Actually, when heterogeneous streams are delivered to the network, their transmission quality is dependent on the outside network conditions(e.g., delay, jitter, throughput, and so on), as well as the streaming strategy (e.g., encoding, transmission protocol, and so on) inside streams, which ultimately impact end users’ IE. To this end, we can further subdivide this category into two classes including network QoS and stream-related IFs. QoS: low latency high throughput high reliability temporal synchronization among heterogeneous streams stream-related IFs the form of data compression strategy resource scheduling scheme ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:4:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"User-aware IFs IE may be influenced by human users while human users can perceive IE, for which we can subdivide this category into three classes based on such correlations. User profile Physiological IFs Psychological IFs It is obvious that users with diverse user profiles have distinctive influences on IE. The psychology and physiology of users can highly reflect the IE for the application. For psychological IFs, they are able to directly demonstrate a user’s positive or negative feedback for interactive network services. However, this can hardly be simply measured. For physiological IFs, some of them(e.g., heart rate, blood pressure) can be objectively measured by affordable medical sensors. ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:5:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Device-aware IFs With regard to device-aware IFs, two broad classes can be gotten according to internal systems(e.g., CPU) and external specifications(e.g., screen size, FOV) of the device. IE management in the device level mainly lies in two aspects. The selection of terminal type(e.g., mobile phone, laptop, VR/AR glasses) The corresponding possession of hardware(e.g., CPU, GPU, battery). ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:6:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Context-aware IFs Typically, IE for interactive network services is generated by interacting with the virtual environment. To this end, we can derive two primary classes. Virtual context: focuses on the specific virtual application scenario. Physical context: focuses on its surrounding physical environment. We can provide constructive suggestions for different contexts. For example, online virtual games are appropriate to play outside for the broad horizon, but watching a 3D film is more proper inside the home. We can suggest appropriate application types with different technical requirement to guarantee users’ IE according to existing network resources and the surrounding environment. Light-weight IE evaluation We proposed two light-weight IE evaluation approaches by respectively exploiting the AI technology and exploring the mathematical relationship among IFs and IE, which are appropriate for different cases according to the data amount. ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:7:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"AI-based Existing popular studies focusing on DL-based models(e.g., DNNs, LSTMs) can hardly satisfy the stringent delay requirement. We employ a multi-view learning combining with lightweight ML methods(e.g., SVM, decision tree) for fast and accurate IE evaluation. The raw data through multi-view learning is first represented by multiple feature extractors according to their heterogeneous properties. Each modality is regraded as a particular view for multi-modal applications. Motivations are: It can provide efficient dimension reduction via subspace mapping. Subspace learning-based approaches can map the high-dimensional raw data to a latent subspace, in which its dimensionality is lower than that of raw data. Multi-view learning is more applicable to the IE context with abundant infomation, which can overcome the weakness of ML-based methods regarding evaluation accuracy for interactive network services. Multi-view learning can take full advantage of the associated and complementary features from redundant views for evaluation performance improvement. ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:8:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Statistical function-based AI-based approach may achieve better evaluation performances under large amounts of data, they lack strong interpretability and cannot explicitly explain the inherent relations among IFs and IE. We introduced statistical function-based approach to analyze the mathematical relationship among IFs and IF under limited data. Existing statistical function-based approaches for user experience evaluation are broadly divided into three categories: Exponential model Logarithmic model Linear regression model Notably, in order to further improve evaluation performance for interactive network services via statistical function-based approaches, two fundamental and significant issues need to be concerned as follows: How to comprehensively explore diverse and various IFs for accurate IE evaluation? How to conduct an efficient dimension reduction method for fast IE evaluation? Case study ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:9:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"Multi-view generation We can construct multiple views from expert prior knowledge or via the random subspace method, which is a random sampling algorithm for automatic feature set partitioning. Here we partition multi-modal data into three specific views according to different modalites.(e.g., audio, video, and haptic signals). ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:10:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"View combination Then we adopt subspace learning-based approaches to obtain an appropriate subspace from the above-mentioned multiple views. Importantly, canonical correlation analysis in subspace learning plays a significant role in dimension reduction, and outputs the optimal projection for each view. ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:11:0","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["paper"],"content":"IE evaluation Finally, based on the optimal and combined projection subspace, decision tree is deployed here to evaluation IE. The key point is find a general and robust evaluation approach: $$ f: X \\rarr Y $$ Result is: $$ Y = X^{\\top} {\\beta} + {\\epsilon} $$ ${\\epsilon}$ is the noise, ${\\beta}$ can be considered as influencing degree of various IFs to the IE. IE evaluation for multi-modal applications must satisfy more stringent delay requirements in the context of higher-dimensional data. So we apply the LASSO estimation, which is equipped with sparse solutions for the linear regression model, is incorporated to alleviate the issue of high-dimensional data for fast IE evaluation. Dataset: VisTouch Compare obejcts: Ridge regression Exponential model Performance metric: MAE Test result: ","date":"2022-03-09","objectID":"/posts/note-for-toward-immersive-experience/:11:1","tags":["QoE Metrics"],"title":"Note for Toward Immersive Experience","uri":"/posts/note-for-toward-immersive-experience/"},{"categories":["development"],"content":"Overview MLflow是一个用于管理机器学习全生命周期的框架。 其主要的作用是： 完成训练和测试过程中不同超参数的结果的记录、对比和可视化——MLflow Tracking 以一种可复现重用的方式包装 ML 代码——MLflow Projects 简化模型部署的难度——MLflow Models 提供中心化的模型存储来管理全生命周期——MLflow Model Registry 现在主要用到的是第三个，所以先记录Models的用法 MLflow Models MLflow Models本质上是一种格式，用来将机器学习模型包装好之后为下游的工具所用。 这种格式定义了一种惯例来让我们以不同的flavor保存模型进而可以被下游工具所理解。 ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:0:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"存储格式 每个MLflow Model是一个包含任意文件的目录，根目录之下有一个MLmodel文件，用于定义多个flavor。 flavor是MLflow Model的关键概念，抽象上是部署工具可以用来理解模型的一种约定。 MLflow定义了其所有内置部署工具都支持的几种标准flavor，比如描述如何将模型作为Python函数运行的python_function flavor。 目录结构示例如下： MLmode文件内容示例如下： 这个模型可以用于任何支持pytorch或python_function flavor的工具，例如可以使用如下的命令用python_function来 serve 一个有python_function flavor的模型： mlflow models serve -m my_model ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:1:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"Model Signature 模型的输入输出要么是column-based，要么是tensor-based。 column-based inputs and outputs can be described as a sequence of (optionally) named columns with type specified as one of the MLflow data type. tensor-based inputs and outputs can be described as a sequence of (optionally) named tensors with type specified as one of the numpy data type. ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:2:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"Signature Enforcement Schema enforcement checks the provided input against the model’s signature and raises an exception if the input is not compatible. It only works when using MLflow model deployment tools or loading models as python_function. It has no impact on native model. Name Ordering Enforcement The input names are checked against the model signature. If there are any missing inputs, MLflow will raise an exception. Extra inputs will be ignored. Prioritized method is matching by name if provided in input schema, then according to position. Input Type Enforcement For column-based signatures, MLflow will perform safe type conversions if necessary. Only lossless conversions are allowed. For tensor-based signatures, type checking is strict(any dismatch will throw an exception). Handling Integers With Missing Values Integer data with missing values is typically represented as floats in Python. Best way is to declare integer columns as doubles whenever there can be missing values. Handling Data and Timestamp Python has precision built into the type for datatime values. Datetime precision is ignored for column-based model signature but is enforced for tensor-based signatures. ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:2:1","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"Log Models with Signatures Pass signature object as an argument to the appropriate log_model call to include a signature with model. The model signature object can be created by hand or inferred from datasets with valid model inputs and valid model outputs. Column-based example The following example demonstrates how to store a model signature for a simple classifier trained on the Iris dataset: import pandas as pd from sklearn import datasets from sklearn.ensemble import RandomForestClassifier import mlflow import mlflow.sklearn from mlflow.models.signature import infer_signature iris = datasets.load_iris() iris_train = pd.DataFrame(iris.data, columns=iris.feature_names) clf = RandomForestClassifier(max_depth=7, random_state=0) clf.fit(iris_train, iris.target) signature = infer_signature(iris_train, clf.predict(iris_train)) mlflow.sklearn.log_model(clf, \"iris_rf\", signature=signature) The same signature can be created explicitly as follows: from mlflow.models.signature import ModelSignature from mlflow.types.schema import Schema, ColSpec input_schema = Schema([ ColSpec(\"double\", \"sepal length (cm)\"), ColSpec(\"double\", \"sepal width (cm)\"), ColSpec(\"double\", \"petal length (cm)\"), ColSpec(\"double\", \"petal width (cm)\"), ]) output_schema = Schema([ColSpec(\"long\")]) signature = ModelSignature(inputs=input_schema, outputs=output_schema) Tensor-based example from keras.datasets import mnist from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten from keras.optimizers import SGD import mlflow import mlflow.keras from mlflow.models.signature import infer_signature (train_X, train_Y), (test_X, test_Y) = mnist.load_data() trainX = train_X.reshape((train_X.shape[0], 28, 28, 1)) testX = test_X.reshape((test_X.shape[0], 28, 28, 1)) trainY = to_categorical(train_Y) testY = to_categorical(test_Y) model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1))) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(100, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(10, activation='softmax')) opt = SGD(lr=0.01, momentum=0.9) model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY)) signature = infer_signature(testX, model.predict(testX)) mlflow.keras.log_model(model, \"mnist_cnn\", signature=signature) The same signature can be created explicitly as follows: import numpy as np from mlflow.models.signature import ModelSignature from mlflow.types.schema import Schema, TensorSpec input_schema = Schema([ TensorSpec(np.dtype(np.uint8), (-1, 28, 28, 1)), ]) output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 10))]) signature = ModelSignature(inputs=input_schema, outputs=output_schema) ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:2:2","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"Model Input Example Model inputs can be column-based (i.e DataFrame) or tensor-based (i.e numpy.ndarrays). A model input example provides an instance of a valid model input which can be stored as separate artifact and is referenced in the MLmodel file. ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:3:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"Log Model with column-based example An example can be a single record or a batch of records. The sample input can be passed in as a Pandas DataFrame, list or dict. The given example will be converted to a Pandas DataFrame and then serialized to json using the Pandas split-oriented format. input_example = { \"sepal length (cm)\": 5.1, \"sepal width (cm)\": 3.5, \"petal length (cm)\": 1.4, \"petal width (cm)\": 0.2 } mlflow.sklearn.log_model(..., input_example=input_example) ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:3:1","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"Log Model with Tensor-based example An example must be a batch of inputs. The axis 0 is the batch axis by default unless specified otherwise in the model signature. The sample input can be passed in as a numpy ndarray or a dict mapping a string to a numpy array. # each input has shape (4, 4) input_example = np.array([ [[ 0, 0, 0, 0], [ 0, 134, 25, 56], [253, 242, 195, 6], [ 0, 93, 82, 82]], [[ 0, 23, 46, 0], [ 33, 13, 36, 166], [ 76, 75, 0, 255], [ 33, 44, 11, 82]] ], dtype=np.uint8) mlflow.keras.log_model(..., input_example=input_example) ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:3:2","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"Model API MLflow includes integrations with several common libraries. For example, mlflow.sklearn contains save_model, log_model, and load_model functions for scikit-learn models. Additionally, we can use mlflow.models.Model class to create and write models which has 4 key functions: add_flavor to add a flavor to the model. Each flavor has a string name and a dict of key-value attributes, where the values can be any object that can be serialized to YAML. save to save the model to a local directory. log to log the model as an artifact in the current run using MLflow tracking. load to load a model from a local directory or from an artifact in a previous run. ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:4:0","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["development"],"content":"Pytorch mlflow.pytorch module defines utilities for saving and loading MLflow Models with the pytorch flavor. We can use mlflow.pytorch.save_model() and mlflow.pytorch.log_model() methods to save pytorch models in MLflow format. We can use mlflow.pytorch.load_mode() to load MLflow Models with pytorch flavor as pytorch model objects. This loaded PyFunc model can be scored with both DataFrame input and numpy array input. ","date":"2022-03-07","objectID":"/posts/note-for-mlflow/:4:1","tags":["ML"],"title":"MLflow 的用法","uri":"/posts/note-for-mlflow/"},{"categories":["knowledge"],"content":"Context Create an HTML5 canvas Get the canvas id Obtain WebGL Context The parameter WebGLContextAttributes is not mandatory. Attributes Description Default value alpha true: provide an alpha buffer to the canvas; true depth true: drawing buffer contains a depth buffer of at least 16 bits; true stencil true: drawing buffer contains a stencil buffer of at least 8 bits; false antialias true: drawing buffer performs anti-aliasing true premultipliedAlpha true: drawing buffer contains colors with pre-multiplied alpha true preserveDrawingBuffer true: buffers will not be cleared and will preserve their values until cleared or overwritten by the author false let canvas = document.getElementById(\"my_canvas\"); let context = canvas.getContext(\"webgl\", { antialias: false, stencil: true }); Geometry ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:0:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Definition A 2D or 3D model drawn using vertices is call a mesh. Each facet in a mesh is called a polygon and a polygon is made of 3 or more vertices. // create a 2D triangle which lies on the coordinates {(-5, -5), (5, -5), (5, 5)} let vertices = [ -0.5, -0.5 // Vertex 0 0.5, -0.5, // Vertex 1 0.5, 0.5, // Vertex 2 ]; Similarly, we can create an array for the indices follow the sequence. let indices = [0, 1, 2]; drawArrays(): pass the vertices of the primitive using JavaScript arrays. drawElements(): pass both vertices and indices of the primitive using JavaScript arrays. ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:1:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Buffer Objects A buffer object indicates a memory area allocated in GPU. We can store data of the models corresponding to vertices, indices, color and etc. There are 2 types of buffer objects: Vertex Buffer Object(VBO): It holds the per-vertex data of the graphical model that is going to be rendered. Index Buffer Object(IBO): It holds the indices of the graphical model that is going to be rendered. After defining the required geometry and storing them in JavaScript arrays, we need to pass these arrays to the buffer objects, from where the data will be passed to the shader programs. Create an empty buffer. let vertex_buffer = gl.createBuffer(); let index_buffer = gl.createBuffer(); Bind an appropriate array object to the empty buffer. void bindBuffer(enum target, Object buffer); // ARRAY_BUFFER represents vertex data gl.bindBuffer(gl.ARRAY_BUFFER, vertex_buffer); // ELEMENT_ARRAY_BUFFER represent index data gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, index_buffer); Pass the data (vertices/indices) to the buffer using one of the typed arrays. void bufferData(enum target, Object data, enum usage); // Usage specifies how to use the buffer object data to draw shapes // gl.STATIC_DRAW -- Data will be specified once and used many times. // gl.STREAM_DRAW -- Data will be specified once and used a few times. // gl.DYNAMIC_DRAW -- Data will be specified repeatedly and used many times. // vertex buffer gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW); // index buffer gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(indices), gl.STATIC_DRAW); Unbind the buffer (Optional/Recommended). gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, null); Shader Shaders are written in ES SL which has variables of its own data types, qualifiers, built-in inputs and outputs. ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:2:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Data Types Type Description void empty value bool true or false int signed integer float floating scalar vec2, vec3, vec4 n-component floating point vector bvec2, bvec3, bvec4 boolean vector ivec2, ivec3, ivec4 signed integer vector mat2, mat3, mat4 2x2, 3x3, 4x4 float matrix sampler2D access a 2D texture samplerCube access cube mapped texture ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:3:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Qualifiers Qualifier Description attribute acts as a link between a vertex shader and OpenGL ES for per-vertex data. Its value changes for every execution of the vertex shader. uniform links shader programs and the WebGL application. Its value is read-only. It can be used for to declare a variable with any basic data types: uniform vec4 lightPosition;. varying forms a link between a vertex shader and fragment shader for interpolated data. It can be used with the following data types: float, vec2, vec3, vec4, mat2, mat3, mat4, arrays like: varying vec3 normal; ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:4:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Vertex Shader Vertex shader is a program code, which is called on every vertex. Programmer have to define attribute in code of vertex shader to handle data. The attribute point to a VBO written in JavaScript. ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:5:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Predefined Variables OpenGL ES SL provides the following predefined variables for every vertex shader Variables Description highp vec4 gl_Position Holds the position of the vertex mediump float gl_PointSize Holds the transformed point size attribute vec2 coordinates; void main(void) { gl_Position = vec4(coordinates, 0.0, 1.0); } gl_Position is the predefined variable which is available only in the vertex shader. It contains the vertex position. As vertex shader is a per-vertex operation, the gl_Position value is calculated for each vertex. ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:5:1","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Fragment Shader A mesh is formed by multiple triangles, and the surface of each triangle is known as a fragment. Fragment shader is the code that runs on every pixel on each fragment. This is written to calculate and fill the color on individual pixels. ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:6:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Predefined Variables Variables Description mediump vec4 gl_FragCoord; Holds the fragment position within the frame buffer bool gl_FrontFacing; Holds the fragment that belongs to a front-facing primitive mediump vec2 gl_PointCoord; Holds the fragment position within a point mediump vec4 gp_FragColor; Holds the output fragment color value of the shader mediump vec4 gl_FragData[n]; Holds the fragment color for color attachment n void main(void) { gl_FragColor = vec4(0, 0.8, 0, 1); } ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:6:1","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Store and Compiling let vertCode = \"attribute vec2 coordinates;\" + \"void main(void) {\" + \"gl_Postion = vec4(coordinates, 0.0, 1.0);\" + \"}\"; let fragCode = \"void main(void) {\" + \"gl_FragColor = vec4(0, 0.8, 0, 1);\" + \"}\"; Compilation involves following 3 steps Creating the shader object Attaching the source code to the created shader object Compiling the program let vertShader = gl.createShader(gl.VERTEX_SHADER); gl.shaderSource(vertShader, vertCode); gl.compileShader(vertShader); Same process for fragment shader let fragShader = gl.createShader(gl.FRAGMENT_SHADER); gl.shaderSource(fragShader, fragCode); gl.compileShader(fragShader); ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:7:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Combined Program Create a program object Attach both the shaders Link both the shaders Use the program let shaderProgram = gl.createProgram(); gl.attachShader(shaderProgram, vertShader); gl.attachShader(shaderProgram, fragShader); gl.linkProgram(shaderProgram); gl.useProgram(shaderProgram); Associating Attributes \u0026 Buffer Objects Get the attribute location Point the attributes to a vertex buffer object Enable the attribute // ulong getAttribLocation(Object program, string name) let coordinatesVar = gl.getAttribLocation(shaderProgram, \"coordinates\"); // void vertexAttribPointer(location, int size, enum type, bool normalized, long stride, long offset) gl.vertexAttribPointer(coordinatesVar, 3, gl.FLOAT, false, 0, 0); gl.enableVertexAttribArray(coordinatesVar); Drawing a Model ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:8:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"drawArrays() void drawArrays(enum mode, int first, long count); mode: gl.POINTS, gl.LINE_STRIP, gl.LINE_LOOP, gl.LINES, gl.TRIANGLE_STRIP, gl.TRANGLE_FAN, gl.TRIANGLES. first: specified the starting element in the enabled arrays. (Non-negative) count: specifies the number of elements to be rendered. WebGL will create the geometry in the order in which the vertex coordinates while rendering the shapes. draw a triangle: let vertices = [-0.5, -0.5, -0.25, 0.5, 0.0, -0.5]; gl.drawArrays(gl.TRIANGLES, 0, 3); draw two contiguous triangles: let vertices = [ -0.5, -0.5, -0.25, 0.5, 0.0, -0.5, 0.0, -0.5, 0.25, 0.5, 0.5, -0.5, ]; gl.drawArrays(gl.TRIANGLES, 0, 6); ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:9:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"drawElements() void drawElements(enum mode, long count, enum type, long offset); mode: same as drawArrays(); count: same as drawArrays(); type: specifies the data type of the indices which must be UNSIGNED_BYTE or UNSIGNED_SHORT; offset: specifies the starting point for rendering, usually the first element (0); If use drawElements() to draw a model, then index buffer object should also be created along with the vertex buffer object. The vertex data will be processed once and used as many time as mentioned in the indices. draw a triangle: let vertices = [-0.5, -0.5, 0.0, -0.25, 0.5, 0.0, 0.0, -0.5, 0.0]; let indices = [0, 1, 2]; gl.drawElements(gl.TRIANGLES, indices.length, gl.UNSIGNED_SHORT, 0); draw two contagious triangles: let vertices = [ -0.5, -0.5, 0.0, -0.25, 0.5, 0.0, 0.0, -0.5, 0.0, 0.25, 0.5, 0.0, 0.5, -0.5, 0.0, ]; let indices = [0, 1, 2, 2, 3, 4]; gl.drawElements(gl.TRIANGLES, indices.length, gl.UNSIGNED_SHORT, 0); ","date":"2022-03-03","objectID":"/posts/webgl-samples-explanation/:10:0","tags":["WebGL"],"title":"WebGL 样例的解释","uri":"/posts/webgl-samples-explanation/"},{"categories":["knowledge"],"content":"Structure of WebGL Application WebGL application code is a combination of JavaScript and OpenGL Shader Language. JavaScript is required to communicate with the CPU. OpenGL Shader Language is required to communicate with the GPU. Samples ","date":"2022-03-03","objectID":"/posts/webgl-samples/:0:0","tags":["WebGL"],"title":"WebGL 样例","uri":"/posts/webgl-samples/"},{"categories":["knowledge"],"content":"2D coordinates \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003cbody\u003e \u003ccanvas width=\"300\" height=\"300\" id=\"my_canvas\"\u003e\u003c/canvas\u003e \u003cscript\u003e // 1. Prepare the canvas and get context let canvas = document.getElementById(\"my_canvas\"); let gl = canvas.getContext(\"experimental-webgl\"); // 2. Define the geometry and store it in buffer objects let vertices = [ -0.5, 0.5, // vertex 1 -0.5, -0.5, // 0.0, -0.5, ]; // Create buffer object let vertex_buffer = gl.createBuffer(); // Bind an empty array buffer to it gl.bindBuffer(gl.ARRAY_BUFFER, vertex_buffer); // Pass the vertices data to the buffer gl.bufferData( gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW ); // Unbind the buffer gl.bindBuffer(gl.ARRAY_BUFFER, null); // 3. Create and compile Shader programs // Vertex shader source code let vertCode = \"attribute vec2 coordinates;\" + \"void main(void) {\" + \" gl_Position = vec4(coordinates, 0.0, 1.0);\" + \"}\"; // Create a vertex shader object let vertShader = gl.createShader(gl.VERTEX_SHADER); // Attach vertex shader source code gl.shaderSource(vertShader, vertCode); // Compile the vertex shader gl.compileShader(vertShader); // Fragment shader source code let fragCode = \"void main(void) {\" + \"gl_FragColor = vec4(0.0, 0.0, 0.0, 0.1);\" + \"}\"; let fragShader = gl.createShader(gl.FRAGMENT_SHADER); gl.shaderSource(fragShader, fragCode); gl.compileShader(fragShader); // Create a shader program object to store combined shader program let shaderProgram = gl.createProgram(); // Attact vertex and fragment shader gl.attachShader(shaderProgram, vertShader); gl.attachShader(shaderProgram, fragShader); // Link both programs gl.linkProgram(shaderProgram); // Use the combined shader program object gl.useProgram(shaderProgram); // 4. Associate the shader programs to buffer objects // Bind vertex buffer object gl.bindBuffer(gl.ARRAY_BUFFER, vertex_buffer); // Get the attribute location let coord = gl.getAttribLocation(shaderProgram, \"coordinates\"); // Point an attribute to the currently bound VBO gl.vertexAttribPointer(coord, 2, gl.FLOAT, false, 0, 0); // Enable the attribute gl.enableVertexAttribArray(coord); // 5. Drawing the required object (triangle) // Clear the canvas gl.clearColor(0.5, 0.5, 0.5, 0.9); // Enable the depth test gl.enable(gl.DEPTH_TEST); // Clear the color buffer bit gl.clear(gl.COLOR_BUFFER_BIT); // Set the view port gl.viewport(0, 0, canvas.width, canvas.height); // Draw the triangle gl.drawArrays(gl.TRIANGLES, 0, 3); \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2022-03-03","objectID":"/posts/webgl-samples/:1:0","tags":["WebGL"],"title":"WebGL 样例","uri":"/posts/webgl-samples/"},{"categories":["knowledge"],"content":"Overview JavaScript JavaScript is used to write the control code of the program, which includes the following actions: Initialization: initialize WebGL context. Arrays: create arrays to hold the data of the geometry. Buffer objects: create buffer objects by passing the arrays as parameters. Shaders: create, compile and link the shaders. Attributes: create attributes, enable them and associate them with buffer objects. Uniforms: associate the uniforms. Transformation matrix: create transformation matrix. Vertex Shader The vertex shader is executed for each vertex provided in the vertex buffer object when start the rendering process by invoking the methods drawElements() and drawArrays(). It calculates the position of each vertex of a primitive polygon and stores it in the varying gl_position It calculates the other attributes such as color, texture coordinates and vertices that are normally associated with a vertex. Primitive Assembly Here the triangles are assembled and passed to the rasterizer. Resterization The pixels in the final image of the primitive are determined. Culling: Initially the orientation of the polygons is determined. All those triangles with improper orientation that are not visible in view area are discarded. Clipping: If a triangle is partly outside the view area, then the part outside the view area is removed. Fragment Shader The fragment shader gets: data from the vertex shader in varying variables primitives from the rasterization stage then: calculates the color value for each pixel between the vertices stores the color values of every pixel in each fragment Fragment Operations The fragment operations may include: Depth Color buffer blend Dithering Once all the fragments are processed, a 2D image is formed and displayed on the screen. Frame Buffer Frame buffer is the final destination of the rendering pipeline. Frame buffer is a portion of graphics memory that hold the scene data. This buffer contains details such as width and height of the surface (in pixels), color of each pixel and depth and stencil buffers. ","date":"2022-03-03","objectID":"/posts/webgl-pipeline/:0:0","tags":["WebGL"],"title":"WebGL 中的管线","uri":"/posts/webgl-pipeline/"},{"categories":["knowledge"],"content":"Coordinate System There are x, y, z axes in WebGL, where the z axis signifies depth. The coordinates in WebGL are restricted to (1, 1, 1) and (-1, -1, -1). Positive value meaning: z: near viewer. x: near right. y: near top. Graphics System ","date":"2022-03-03","objectID":"/posts/webgl-basics/:0:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"Vertices To draw a polygon, we need to mark the points on the plane and join them to form a desired polygon. A vertex is a point which defines the conjunction of the edges of a 3D object. Use javascript arrays to stores points’ coordinates like [0.5, 0.5, 0.5]. ","date":"2022-03-03","objectID":"/posts/webgl-basics/:1:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"Indices The numerical values which are used to identify the vertices is call Indices. ","date":"2022-03-03","objectID":"/posts/webgl-basics/:2:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"Arrays There are no predefined methods in WebGL to render the vertices directly. let vertices = [0.5, 0.5, 0.1, -0.5, 0.5, -0.5]; ","date":"2022-03-03","objectID":"/posts/webgl-basics/:3:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"Buffers Buffers are the memory areas of WebGL that hold the data. There are various buffers: drawing buffer frame buffer vertex buffer index buffer The vertex buffer and index buffer are used to describe and process the geometry of the model, stores data about vertices and indices respectively. The frame buffer is a portion of graphics memory that hold the scene data. This buffer contains details such as width and height of the surface (in pixels), color of each pixel, depth and stencil buffers. ","date":"2022-03-03","objectID":"/posts/webgl-basics/:4:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"Mesh The WebGL API provides two methods to draw 2D or 3D objects: drawArrays() drawElements() They accept a parameter called mode using which you can select the object you want to draw. mode: points or lines or triangles We can construct primitive polygons using points, lines and triangles. Thereafter, we can form a mesh using these polygons. A 3D object drawn using primitive polygons is called a mesh. ","date":"2022-03-03","objectID":"/posts/webgl-basics/:5:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"Shader Programs Since WebGL uses GPU accelerated computing, the information about these triangles should be transferred from CPU to GPU which takes a lot of communication overhead. WebGL provides a solution to reduce the communication overhead. Since it uses ES SL(Embedded System Shader Language) that runs on GPU, we write all the required programs to draw graphical elements on the client system using shader programs(OpenGL ES Shader Language). Shader is a snippet that implements algorithms to get pixels for a mesh. There are two types of shaders: Vertex Shader and Fragment Shader. ","date":"2022-03-03","objectID":"/posts/webgl-basics/:6:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"Vertex Shader called on every vertex. used to transform the geometry from one place to another. handle the data of each vertex such as vertex coordinates, normals, colors, and texture coordinates. vertex transformation normal transformation and normalization texture coordinate generation texture coordinate transformation lighting color material application ","date":"2022-03-03","objectID":"/posts/webgl-basics/:6:1","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"Fragment Shader(Pixel Shader) A mesh is formed by multiple triangles. The surface of each of the triangles is known as a fragment. Fragment shader is the code that runs on all pixels of every fragment. It is written to calculate and fill the color on individual pixels. operations on interpolated values texture access texture application fog color sum ","date":"2022-03-03","objectID":"/posts/webgl-basics/:6:2","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["knowledge"],"content":"OpenGL ES SL Variables To handle the data in the shader programs, ES SL provides three types of variables. Attributes: hold the input values of the vertex shader program. Attributes point to the vertex buffer objects that contains per-vertex data. Uniforms: hold the input data that is common for both vertex and fragment shaders, such as light position, texture coordinates and color. Varyings: used to pass the data from the vertex shader to the fragment shader. ","date":"2022-03-03","objectID":"/posts/webgl-basics/:7:0","tags":["WebGL"],"title":"WebGL 基础知识","uri":"/posts/webgl-basics/"},{"categories":["paper"],"content":"Bitrate Adaptation Schemes ","date":"2022-02-27","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:0:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Client-based Recently, most of the proposed bitrate adaptation schemes reside at the client side, according to the specifications in the DASH standard. Purposes: Minimal rebuffering events when the playback buffer depletes. Minimal startup delay especially in case of live video streaming. A high overall playback bitrate level with respect to network resources. Minimal video quality oscillations, which occur due to frequent switching. ","date":"2022-02-27","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Available bandwidth-based The client makes its representation decisions based on the measured available network bandwidth, which is usually calculated as the size of the fetched segment(s) divided by the transfer time. This scheme suffers from poor QoE due to a lack of a reliable bandwidth estimation methods, which results in frequent buffer underruns. General context Based on segment fetch time(SFT) measures the time starting from sending the HTTP GET request to receiving the last byte of the segment. Sequential and parallel segment fetching method in CDNs, by using metric that compares the expected segment fetch time(ESFT) with the measured SFT to determine if the selected segment bitrate matches the network capacity. Based on the bitrate observed for the last segment downloaded and the estimated throughput that was calculated during the previous estimation. Probe AND Adapt tries to eliminate the ON-OFF steady state issue as well as reduce bitrate oscillations when multiple clients share the same bottleneck link. piStream enables clients to estimate bandwidth based on a resource monitor module that act as a physical-layer daemon. SVC with DASH prefetches base layers of future segments or downloads enhancement layers for existing segments using a bandwidth-sloping-based heuristic. Mobile context Static DASH2M uses HTTP/2 server push and stream terminate properties to reduce the battery consumption of the mobile device. Adaptive k-push scheme propose to increase/decrease k according to a bandwidth increase/decrease while keeping in mind the overall power consumption in a push cycle. LOw-LatenceY Prediction-based adaPtation(LOLYPOP) leverages TCP throughput predictions on multiple times scales to achieve low latency and improve QoE. Motive GeoStream: introduce the use of geostatistics to estimate future bandwidth in unknown locations. ","date":"2022-02-27","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:1","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Playback Buffer-Based The client uses the playout buffer occupancy as a criterion to select the next segment bitrate during video playback. This scheme suffers from many limitations including low overall QoE and instability issues, especially in the case of long-term bandwidth fluctuations. SVC-based approaches have limitations related to the complexity of SVC. Base combines the buffer size with a tool-set of client metrics for accurate rate selection and smooth switching. BBA aims to maximize the average video quality and avoid unnecessary rebuffering events, but suffers from QoE degradation during long-term bandwidth fluctuations. BOLA uses online control algorithm that treats bitrate adaptation as a utility maximization problem. Provide strong theorectical proof that it is near optimal, design a QoE model that incorporates both the average playback quality and the rebuffering time. It is implemented and available in the dash.js player. BIEB maximizes video quality based on SVC priority while reducing the number of quality oscillations and avoiding stalls and frequent bitrate switching. it maintains a stable buffer occupancy before increasing the quality. QUEuing Theory approach to DASH Rate Adaptation(QUETRA) allows to calculate the expected buffer occupancy given a bitrate choice, network throughput, and buffer capacity. ","date":"2022-02-27","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:2","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Mixed Adaptation The client makes its bitrate selection based on a combination of metrics including available bandwidth, buffer occupancy, segment size and/or duration. Simple client Control-theoretic based: FastMPC, Another Optimization problem: Streaming video over HTTP with Consistent Quality Towards agile and smooth video adaptation in dynamic HTTP streaming aims to balance bandwidth utilization and smoothness in DASH in both single and multiple CDN(s) scenarois. SQUAD is a lightweight bitrate adaptation algorithm that uses the available bandwidth and buffer information to increase the average bitrate while minimizing the number of quality switches. Multi-path solution for abr in wireless networks avoids the problems of TCP congestion control by using parallel TCP streams. SARA is Segment-Aware Rate Adaptation algorithm based on the segment size variation, the available bandwidth estimate and the buffer occupancy. It extends MPD file to include the size of every segment. ABMA+ selects the highest segment representation based on the estimated probability of video rebuffering. It makes use of buffer maps, which define the playout buffer capacity that is required under certain conditions to satisfy a rebuffering threshold and to avoid heavy online calculations. GTA uses a cooperative game in coalition formation then formulates the bitrate selection problem as a bargaining process and consensus mechanism. GTA improves QoE and video stability without increasing the stall rate or startup delay. Multiple clients ELASTIC generates a long-lived TCP flow and avoids the ON-OFF steady state behavior which leads to bandwidth overestimations. Ensure bandwidth fairness between competing clients based on network feedback assistance, but without taking the QoE into consideration. In addition, it ignores quality oscillations in its bitrate decisions. Adaptation algorithm for HAS uses current buffer occupancy level to estimate available bandwidth and average bitrate of the different bitarte levels from MPD as metrics in its bitrate selection. FESTIVE contains: a bandwidth estimator module a bitrate selection and update method with stateful player a randomized scheduler which incorporates the buffer size to schedule the download of the next segment. TSDASH uses a logarithmic-increase-multiplicative-decrease (LIMD) based bandwidth probing algorithm to estimate the available bandwidth and a dual-threshold buffer for the bitrate adaptation. ","date":"2022-02-27","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:3","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"MDP-Based The video streaming process is formulated as a finite MDP to be able to make adaptation decisions under fluctuating network conditions. This scheme may suffer from instability, unfairness and underutilization when the number of clients increases, probably because such factors are not taken into account in the MDP models and due to clients’ decentralized ON-OFF patterns. Real-time best-action search algorithm over multiple access networks uses both Bluetooth and WiFi links to simultaneously download video segments. However, this scheme shows limitations during user mobility which negatively affect QoE. Optimizing in Vehicular environment introduces a three-variant of RL-based algorithms which take advantage of the historical bandwidth samples to build an accurate bandwidth estimation model. Multi-agent Q-Learning-based for fairness uses a central manager in charge of collecting QoE statistics and coordination between the competing clients. The algorithm ensures a fair QoE distribution and improves QoE while avoiding suboptimal decisions.(without considering stalls and quality switches) Online learning adaptation aims to select the optimal representation and maximize the long-term expected QoE. The reward function is calculated from a combination of quality oscillations, segment quality and stalls experienced by the client. It exploits a parallel learning technique to avoid slow convergence and suboptimal solutions. mDASH aims to improve QoE during long-term bandwidth variations. It takes buffer size, bandwidth conditions and bitrate stability as Markov state variables. Pensive does not rely on pre-programmed models or assumptions about the environment, but gradually learns the best policy for bitrate decisions through observation and experience. D-DASH combines DL and RL to improve QoE, achieves a good trade-off between policy optimality and convergence speed during the decision process. ","date":"2022-02-27","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:1:4","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Server-Based Server-based schemes use a bitrate shaping method at the server side and do not require any cooperation from the client. The switching between the bitrates is implicitly controlled by the bitrate shaper. The client still makes its own decisions, but the decisions are more or less determined by the shaping method on the server. Traffic shaping analyzes instability and unfairness issues in the presence of multiple HAS players competing for the available bandwidth. This method can be deployed at a home gateway to improve fairness, stability and convergence delay, and to eliminate the OFF periods during the steady states. Tracker-assisted adaptation uses a architecture which consists of clients communicating with a server through a shared proxy and a server having a tracker functionality that manages the clients’ statuses and helps them share knowledge about their statues. Quality Adaptation Controller aims to control the size of the server sending buffer in order to adjust and select the most appropriate bitrate level for each DASH player. It maintains the playback buffer occupancy of each player as stable as possible and to match bitrate level decisions with the available bandwidth. Multi-Source Stream system: the client fetches the segments from multi-source stream servers. Cons: Produce high overhead on the server side with a high complexity These schemes also need modifications to the MPD or a custom server software to implement the bitrate adaptation logic.(a violation of the DASH-standard design principles) ","date":"2022-02-27","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/:2:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/"},{"categories":["paper"],"content":"Paper Overview Link: https://ieeexplore.ieee.org/document/8424813 Level: IEEE Communications Surveys \u0026 Tutorials 2019 Background ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:0:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Traditional non-HAS IP-based streaming The client receives media that is typically pushed by a media server using connection-oriented protocol such as Real-time Messaging Protocol(RTMP/TCP) or connectionless protocol such as Real-time Transport Protocol(RTP/UDP). Real-time Streaming Protocol(RTSP) is a common protocol to control the media servers, which is responsible for setting up a streaming session and keeping the state information during this session, but is not responsible for actual media delivery(task for protocol like RTP). The media server performs rate adaption and data delivery scheduling based on the RTP Control Protocol(RTCP) reports sent by the client. When it comes to NAT and firewall, additional protocols or configurations are needed during the session establishment. The characteristics result in complex and expensive servers. These scalability and vendor dependency issues as well as high maintenance costs have resulted in deployment challenges for protocols like RTSP. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:1:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"HAS Around 2005, HTTP adaptive streaming(HAS) became popular and dominant, which treated the media content like regular Web content and delivered it in small pieces over HTTP protocol. HTTP as application and TCP as the transport-layer protocol. Client pull the data from a standard HTTP server, which simply hosts the media content. HAS solutions employ dynamic adaptation with respect to varying network conditions to provide a seamless streaming experience. The original file/stream is partitioned into segments (also called chunks) of equi-length playback time. Multiple versions(also called representations) of each segment are generated that vary in bitrate/resolution/quality using an encoder or a transcoder. The server generates an index file, which is a manifest that lists the available representations including HTTP urls to identify the segments along with their availability times. The client first receives the manifest that contains the metadata for video, audio, subtitles and other features, then constantly measures certain parameters: available network bandwidth, buffer status, battery and CPU levels, etc. According to these parameters, the HAS client repeatedly fetches the most suitable next segment among the available representations from the server. Advantages: It use HTTP to deliver video segments, which simplifies the traversal through NATs and firewalls. At the server side, it use conventional Web servers or caches available within the networks of ISPs and CDNs. At the client side, it requests and fetches each segment independently from others and maintains the playback session state, whereas the server is not required to maintain any state. It doesn’t require a persistent connection between the client and server, which improves system scalability and reduces implementation and deployment costs. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:2:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Comparison Summary Challenges ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:3:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Multi-Client Competition/Stability Issues A centralized management controller can enhance the overall video quality, while improve QoE. A robust HAS scheme should achieve 3 main objectives: Stability: HAS clients should avoid frequent bitrate switching. Fairness: Multiple HAS clients competing for available bandwidth should equally share network resources based on viewer, content and device characteristics. High Utilization: While the clients attempt to be stable and fair, network resources should be used as efficiently as possible. A streaming session consists of 2 states: buffer-filling state and steady state. The buffer-filling state aims to fill the playback buffer and reach a certain threshold where the playback can be initiated or resumed. The steady state is to keep the buffer level above a minimum threshold despite bandwidth fluctuation or interruptions. The steady state consists of 2 activity periods referred to as ON and OFF. The client requests a segment every $T_s$ time units, where $T_s$ represents the content time duration of each segment, and sum of ON and OFF period durations equals $T_s$. ON period: client downloads the current segment and notes the achieved throughput value that will be later used in selecting the appropriate bitrate for future segments. OFF period: client becomes idle temporarily. There are different cases during competition process. The ON periods of clients don’t overlap during the current segment download, each client will overestimate the available bandwidth. So longer download time will cause the initially non-overlapping ON periods to eventually start overlapping. As the amount of overlap increases, the clients will have lower bandwidth estimations and start selecting segments that have lower bitrate. These segment will take less time to download, causing the amount of overlap among the ON periods to precedurally shorten, until the process reverts to its initial situation. The cycle repeats itself, causing periodic up and down shift in the selected bitrates, leading to unstable video quality, unfairness, and underutilization. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:4:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Consistent-Quality Streaming The correlation between video bitrate and its perceptual quality is non-linear. Different video content types have unique characteristics. Differences of inter-stream and intra-stream video scene complexity across content. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:5:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"QoE Optimization and Measurement HAS scheme uses application control loop, which also interacts with a lower-layer control loop(such as TCP congestion control). It plays a key role in determining the viewer QoE. Factors influencing QoE are categorized as: Perceptual, directly perceived by the viewer. Technical, indirectly affecting the QoE. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:6:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Perceptual Perceptual factors include the video image quality, initial delay, stalling duration and frequency. The impact of these factors differs depending on the users subjectivity. Most users consider initial delays less critical than stalling. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:6:1","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Technical Technical factors include the algorithms, parameters, and hardware/software used in streaming system. Specifically, factors are: Server side: encoding parameters, video qualities and segment size. Client side: adaptation parameters and environment that clients reside in. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:6:2","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"QoE measurement Objective matrics: Peak Signal-to-Noise Ratio(PSNR), Structural SIMilarity(SSIM and SSIMplus), Perceived Video Quality(PVQ) and Statistically Indifferent Quality Variation(SIQV). Subjective matrics: Mean Opinion Score(MOS). Quality-of-Service (QoS)-derived matrics: startup delay, average video bitrate, quality switches and rebuffering events. Try to optimize each metric is difficult because it may result in conflicts. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:6:3","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["paper"],"content":"Inter-Destination Multimedia Synchronization Online communities are drifting towards watching online videos together in a synchronized manner. Having Multiple streaming clients distributed in different geographical locations poses challenges in delivering video content simultaneously, while keeping the playback state of each client the same. Typically, IDMS solutions involve a master node to which clients synchronize their playout to. Rainer et proposed an IDMS architecture for DASH by using a distribute control scheme where peers can communicate and negotiate a reference placback timestamp in each session. In another work, Rainer et provided a crowdsourced subjective evaluation to find a asynchronism threshold at which QoE was not significantly affected. ","date":"2022-02-26","objectID":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/:7:0","tags":["ABR","Survey"],"title":"Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)","uri":"/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/"},{"categories":["development"],"content":"最近几天一直在用WebXR的技术重构目前的基于分块的全景视频自适应码率播放客户端，下面简述一下过程。 首先结论是：分块播放+自适应码率+完全的沉浸式场景体验=Impossible（直接使用 WebXR 提供的 API） ","date":"2022-02-25","objectID":"/posts/webxr-for-panoramic-video/:0:0","tags":["WebXR"],"title":"使用 WebXR 完成基于分块的全景视频自适应码率播放器","uri":"/posts/webxr-for-panoramic-video/"},{"categories":["development"],"content":"分块播放 分块播放的本质是将一整块的全景视频从空间上划分成多个小块，各个小块在时间上与原视频的长度是相同的。 在实际播放的时候需要将各个小块按照原有的空间顺序排列好之后播放，为了避免各个分块播放进度不同的问题，播放时还需要经过统一的时间同步。 对应到 web 端的技术实现就是： 一个分块的视频\u003c-\u003e一个\u003cvideo\u003eh5 元素\u003c-\u003e一个\u003ccanvas\u003eh5 元素 视频的播放过程就是各个分块对应的\u003ccanvas\u003e元素不断重新渲染的过程 各个分块时间同步的实现需要一个基准视频进行对齐，大体上的原理如下： let baseVideo = null; let videos = []; initBaseVideo(); initVideos(); for (video in videos) { video.currentTime = baseVideo.currentTime; } ","date":"2022-02-25","objectID":"/posts/webxr-for-panoramic-video/:1:0","tags":["WebXR"],"title":"使用 WebXR 完成基于分块的全景视频自适应码率播放器","uri":"/posts/webxr-for-panoramic-video/"},{"categories":["development"],"content":"自适应码率 自适应码率的方案使用dashjs库实现，即对每个分块\u003cvideo\u003e元素的播放都用dashjs的方案控制： import { MediaPlayer } from \"dashjs\"; let videos = []; let dashs = []; let mpdUrls = []; initVideos(); initMpdUrls(); for (let i = 0; i \u003c tileNum; i++) { let video = videos[i]; let dash = MediaPlayer().create(); dash.initialize(video, mpdUrls[i], true); dash.updateSettings(dashSettings); dashs.push(dash); } 通过对dashSettings的调整的可以设置各种可用的 dash 参数如不同质量版本下的缓冲区长度，播放暂停时是否终止后台下载等。 ","date":"2022-02-25","objectID":"/posts/webxr-for-panoramic-video/:2:0","tags":["WebXR"],"title":"使用 WebXR 完成基于分块的全景视频自适应码率播放器","uri":"/posts/webxr-for-panoramic-video/"},{"categories":["development"],"content":"沉浸式场景体验 全景视频的完全的沉浸式体验目前在Oculus Browser上有两种实现方式： 直接使用浏览器默认的全屏功能之后选择视频为：普通视频或 180 度视频或 360 度视频。 使用最新的WebXR session的layers特性，手动代码实现。 第 1 种方式因为并没有给出实际的API，所以不可能与分块传输的视频相结合，所以只能使用第 2 种方式手动实现。 其对应的草案标准地址：https://www.w3.org/TR/webxrlayers-1/ 可以看到目前最新的开发标准刚在 1 个月前完成。 WebXR中的开发流程如下： 判断浏览器是否支持immersive-vr，如果支持就请求xrSession，所需的特性为layers： import { WebXRButton } from \"webxr-button.js\"; let xrButton = new WebXRButton({ onRequestSession: onRequestSession, onEndSession: onEndSession, }); let xrSession = null; function onRequestSession() { if (!xrSession) { navigator.xr .requestSession(\"immersive-vr\", { requiredFeatures: [\"layers\"], }) .then(onSessionStarted); } else { onEndSession(); } } function onEndSession() { xrSession.end(); } if (navigator.xr) { navigator.xr.isSessionSupported(\"immersive-vr\").then((supported) =\u003e { if (supported) { xrButton.enabled = supported; } }); } 获取到需要的xrSession之后请求ReferenceSpace，并创建会话中需要的对象，之后用创建的图层更新会话的渲染器状态，并设置requestAnimationFrame需要的回调函数： let xrRefSpace = null; let xrMediaFactory = null; function onSessionStarted(session) { xrSession = session; xrButton.textContent = \"Exit XR\"; xrMediaFactory = new XRMediaBinding(session); session.requestReferenceSpace('local').then((refSpace) = { xrRefSpace = refSpace; let baseLayer = xrMediaFactory.createEquirectLayer(baseVideo, { space: refSpace, centralHorizontalAngle: Math.PI * 2 }); session.updateRenderState({layers: [baseLayer]}); session.requestAnimationFrame(onXRFrame); }); } 最后设置每次xrSession要求渲染新帧的函数，并设定渲染循环： let xrViewerPose = null; function onXRFrame(time, frame) { let session = frame.session; session.requestAnimationFrame(onXRFrame); xrViewerPose = frame.getViewerPose(xrRefSpace); console.log(xrViewerPose); } onXRFrame函数在每次渲染新帧时调用，其中每帧对应的观看者的相对位置以及头戴设备的线速度和角速度等变量可以从xrViewerPose中取得。 这么看WebXR的完全沉浸式体验是可行的，但是问题出在需要与分块结合。 xrMediaFactory作为XRMediaBinding绑定到当前xrSession的实例对象，可以用来创建采用等距长方形投影的方式的图层XREquirectLayer： 虽然这里出现了可以创建采用Equirectangular方式投影的图层，并可以通过指定其初始化参数完成不同大小的偏移创建，但是这里的处理方式还是将一个完整视频从映射到球面上的方式，即不管怎么改变参数，创建出来的总是有 4 条曲边的球面块： 并不能实现每个分块以特定的映射逻辑将其不重不漏的铺到球面上的功能。 不过就算可以实现这样的功能，因为 1 个图层与 1 个视频块相绑定，在实际创建中发现： 在一个xrSession中最多只能创建 16 个图层，并不能与MxN的分块逻辑相对应； 创建 16 个图层之后整个xrSession会变得异常卡顿，视频已无法正常播放； 那么是否可以先将多个分块的视频从空间上拼接好，将最终拼接好的视频进行等距长方投影？ 首先从实际的实现上没法完成，因为每个视频在 h5 中本质是\u003cvideo\u003e元素，多个\u003cvideo\u003e元素并不能在DOM的基础上实现空间的复原，就算有办法做到，最后在与layer绑定时也必须是 1 个\u003cvideo\u003e元素而这 1 个\u003cvideo\u003e元素还需要实现各个部分的自适应码率变化，这完全是不可行的。 测试的代码地址：media-layer-sample 进一步的解决办法是存在的： 因为目前的WebXR不能够满足需求，所以需要深入WebGL的层面，手动设计一套将各个分块以等距长方投影的方式映射到球面上的逻辑，同时还要与WebXR上层的处理 API 相对应，任务工作量和难度还需要进一步评估。 ","date":"2022-02-25","objectID":"/posts/webxr-for-panoramic-video/:3:0","tags":["WebXR"],"title":"使用 WebXR 完成基于分块的全景视频自适应码率播放器","uri":"/posts/webxr-for-panoramic-video/"},{"categories":["development"],"content":" 远程启动jupyter notebool： jupyter notebook --no-browser --ip=\"\u003cserver-ip\u003e\" --port=\"\u003cserver-port\u003e\" 激活预先配置好的conda环境，这里假设环境名为keras-tf-2.1.0： conda activate keras-tf-2.1.0 安装ipykernel： pip3 install ipykernel --user 为ipykernel安装环境： python3 -m ipykernel install --user --name=keras-tf-2.1.0 打开notebook更改服务之后刷新即可： ","date":"2022-02-15","objectID":"/posts/use-jupyter-notebook-in-conda-env/:0:0","tags":["Conda","Jupyter"],"title":"在 Jupyter Notebook 中设置 Conda 环境","uri":"/posts/use-jupyter-notebook-in-conda-env/"},{"categories":["paper"],"content":"LiveObj LiveDeep方法利用卷积层从视频内容中提取深层特征，不受动态背景的影响。然而在整个推流会话中需要更新一个带有大量权重的巨大的神经网络模型。同时因为没有历史视频和用户的轨迹的数据，模型需要在运行时从随机权重开始训练。而这会导致两个问题： 模型需要花很长时间从一次预测错误中恢复； 在初始化的阶段预测率成功率很低； 为了解决这两个问题，提出预训练的模型来分析视频内容，对视频的语义进行层次化。 基于对内容的分析，进一步设计了一个轻量级用户模型，将用户偏好映射到不同的视频内容。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:0:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"用户观看行为分析 在直播推流中，不能通过分析其他用户的行为模式来得到特定用户的ROI，因此只能直接从视频内容本身入手。 通过对视频内容从空间和时间两个维度的分析得出结论：用户的ROI与物体的大小无关，而是很大程度上依赖于物体在视频中的语义，即用户倾向于观看有意义的事物。 这一结论可以给出推断FoV的直觉：基于检测视频中有意义的物体。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:1:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Methods 首先提出两种直观的通过分析视频内容的视点预测方法，进一步总结这些方法的局限性，并逐步切换到对LiveObj的讨论。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:2:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Basic method Basic方法检测视频中所有的对象并使用其中心作为预测的中心。 给出每个帧中的 $k$ 个物体， $\\vec{O} = [o_1, o_2, o_3, …, o_k]$ ，其中每个 $o_i(i = 1, …, k)$ 表示物体的中心坐标： $o_i = \u003co^{(x)}_i, o^{(y)}_i\u003e$ 。 最终的预测中心点坐标可以计算出来： $$ C_x = \\frac{1}{k} \\sum^{k}_{i=1} o^{(x)}_i;\\ C_y = \\frac{1}{k} \\sum^{k}_{i=1} o^{(y)}_i $$ ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:2:1","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Over-Cover method 受LiveMotion方法的启发，其创建了不规则的预测FoV来覆盖更多的潜在的区域，Over-Cover的方式预测的FoV会覆盖所有包含物体的区域。 采用YOLOv3来处理帧并检测物体，接着每个检测到的对象生成与该对象共享相同中心的预测子视图，所有子视图的聚合形成最终的预测视口。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:2:2","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Summary for intuitive methods Basic方式可能会在多个物体的场景中无法正确选择目标； Over-Cover方式覆盖所有可能的目标来满足较高的精度，但会导致更高的带宽使用量； Velocity方式能很快的适应用户偏好的变化，但是预测精度在长期预测的情况下会显著下降； ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:2:3","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"LiveObj Method Over-Cover方法将所有检测到的目标合并到预测的FoV中而导致冗余问题，而用户一次只能观看其中的几个。 为了解决这个问题，提出基于用户的反馈选择最吸引人的目标，例如用户当前的FoV来形成预测的FoV。 基于这种想法而提出LiveObj，一种基于轨迹的VP方式，通过从Over-Cover方法的结果中过滤掉用户更小可能性看到的目标来缩小最终的FoV。 Object Detection：处理视频帧并检测目标； User View Estimation：分析用户反馈并用Velocity的方式估计FoV； Object tracking：追踪用户观看的目标； RL-based modeling：接受估计出的FoV和被追踪的目标，最终更新每个分块的状态（选中或未选中） ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:3:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Object Detection and Tracking Detection：YOLOv3； Tracking：追踪的基本假设是用户会在接下来的一段时间内接着观看当前看着的目标。追踪任务在直播推流的运行时完成。因此每隔几秒收集用户反馈，并进一步推断用户之前正在观看的目标，然后据此更新追踪目标。 追踪算法： ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:3:1","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"User View Estimation 分析用户的反馈处于两个目的： 估计未来的用户的FoV； 校准当前用户FoV以及要跟踪的对象； 给出用户反馈（即过去片段中实际的FoV），首先更新用户FoV并分析用户的行为模式，并根据此模式计算出下一帧中的预期用户速度。然后识别更新后的FoV中的对象，这些对象确定为ROI，对象追踪步骤将这些更新用于未来的片段来提高预测精度。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:3:2","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"RL-based Modeling 因为预测的误差和用户实际FoV的变化，可能会导致追踪的目标从FoV中消失，而这会使整个预测算法完全失效。所以提出一个基于RL的模型来为每个分块建立用户行为模型，旨在最小化预测误差。 出发点是不同的分块有不同的概率包含有意义的目标，并且更可能包含有意义目标的分块通常对目标检测错误更敏感。 将上面的观察形式化为一个策略学习过程 $M$： $$ M = \u003cS, A, P_{s, a, s’}, R\u003e $$ 其中 $S$ 和 $A$ 表示状态和动作， $P_{s, a, s’}$ 是给定状态 $s$ 的情况下选择动作 $a$ 的概率，转移之后的状态为 $s’$ ，$R$ 表示奖励函数。 系统的目标是通过设定不同的 $P_{s, a, s’}$ 的值，来学习每个分块对目标检测误差的不同的敏感性。 状态-价值函数用于估计在为所有可能的状态 $s \\in S$ 选择动作 $a$ 时的价值，形式化为： $$ v = E[Q_{s, a} | S_t = s] $$ $$ Q_{s, a} = R^a_s + \\gamma \\sum_{s’ \\in S} P_{s, a, s’} v $$ 其中：$\\gamma$ 是奖励参数。 最终的目标是通过计算每个 $P_{s, a, s’}$ 找到最大的 $max(Q_{s, a})$。 而这一过程很耗费时间，因此使用修改之后的Q-learning过程，用贪心的方式来解决最优化问题。 Q-learning过程在直播推流中有别于传统点播中的应用： 预测同时基于当前的输入（目标追踪和FoV估计的结果）和历史状态（分块是否被选择）； 奖励基于用户的反馈在线生成，并且会在整个推流会话中变化，而不是预先设定好的奖励矩阵 $R$ ； 由于直播推流中内容的不可提前获取性， $Q$ 表必须在每次预测中更新； 特别的，为每个分块都创建一个 $Q$ 表，对于每个 $Q$ 表有4种类型： object only; object and viewport; viewport only; no objects or viewport; 将这4种类型和2种中历史状态（选中或未选中）组合之后，得到每个表中状态 $s$ 的8个选项组合； 对每个状态而言，有2种动作（选中或不选中），因此每个表有8个状态和2个动作。 对每个表的奖励基于用户是否看到了分块而更新。 基于状态 $s$ 的对动作 $a$ 的选择转化成了：在相同输入的情况下找到 $max(Q(s, s’))$； LiveROI LiveObj的基础是对象检测算法，用于分析视频内容的敏感性。但是其检测性能可能会受到算法、对象的缩放程度和全景视频导致的扭曲失真的影响，进而引起预测误差。类似于LiveObj的出发点，LiveROI的目标是通过使用动作识别来对视频内容进行分析，这会降低预测性能与前面所提因素的敏感性。 使用3D-CNN等预先训练的模型来分析每个分块上的视频内容，以完成动作识别。同时基于NLP技术，使用轻量级用户模型将用户偏好映射到不同的视频内容。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:3:3","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"用户对视频内容的偏好 最基本的研究问题是：找到直播视频内容中的有效特征和信号或用户的行为，这些与用户的未来的FoV有强相关关系，因此可以将其作为预测因子。 通过对两个固定主题的视频的实验可以得出： 用户花绝大多数的时间在视频中有意义的部分； ROI在空间上只占整个帧很小的部分； ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:4:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"LiveROI Method 融合视频内容感知和用户偏好反馈（即以用户头部运动轨迹的形式）来预测实时VR视频流中的FoV。 主要想法是使用CV算法去理解每个分块的内容，除此之外，采用实时的用户反馈方便分块的选择。 需要满足的条件是：所有分块上的视频处理开销应该保持较小，以避免视频冻结和累计的实时延迟。 使用3D-CNN进行视频理解，重点是识别视频中隐含的有意义的动作，动作识别结果用于以自然语言的格式描述视频内容。这种3D-CNN模型可以在公共数据集上进行训练，因此具有通用性，以适应各种类型的动作和视频，这使得它可以用于实时VR流传输，因为在流传输会话之前没有关于视频内容的先验知识。 但是具有有意义动作的区域可能不是用户最后会确定的FoV，尤其是在目标视频中存在多个有意义动作的情况下。 为了解决这一问题，通过收集用户关于偏好视频内容的实时描述，进一步设计了基于“词/短语”的用户偏好模型。 采用词语嵌入的方法，通过比较两个来源短语的语义相似度，确定最佳匹配区域作为预测FoV，以此来桥接动作识别结果和用户偏好模型。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:5:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"Workflow 3D-CNN的输入数据包含一批 $T$ 张图像，因此统一在一个视频片段中子采样 $T$ 帧。 每个子采样的帧都划分成 $M \\times N$ 个分块，VP问题定义为确定要包含在FoV中的分块。 为了避免由于分块带来的潜在的信息损失（有意义的动作被划分成多个分块），每个用于动作识别的输入图像是从比原始分块边界更大的区域中所提取出来的，但是将共享与原始分块相同的中心。 3D-CNN模块的输出是动作识别结果，即结果矩阵。 面对 $M \\times N$ 个分块，为了满足性能要求，将每个分块的动作识别过程视为相互独立的过程，创建 $m \\times n$ 个线程来实现并行识别，每个线程向结果矩阵输出对应分块的结果向量。 在预测的最后一步，生成包含所有分块的预测分数的得分向量。进一步对所有的分数向量进行排序，并定位第 $M$ 个值，该值设定为选择分块进入预测FoV中的阈值。通过控制 $M$ 的大小可以控制预测的FoV的大小，分数向量中的分数表示用户对分块内容的感兴趣程度。 为了计算分数向量，进一步设计用户向量，其中包含描述用户偏好的词或短语。考虑到推流过程中用户可能会改变兴趣，用户向量会基于用户实时轨迹更新。 在给定用户向量和结果矩阵中的词或短语的情况下，考虑到非自然语言中的两个不同的词可能具有相近的含义，不直接进行词比较，而是使用词分析来计算其相关性。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:5:1","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"CNN Model 采用ECO lite模型完成VR直播推流中的动作识别。所有来自同一视频片段的图像都被储存在一个缓冲帧集合中。 ECO lite模型为2D CNN提取特征图的任务收集工作帧集合（分别由前一视频片段和当前视频片段的缓冲帧集合的后半部分和前半部分组成），在下一个阶段，从每个片段获得的特征图被堆叠到更高的表示中，之后被送到之后的3D CNN中用于最终的动作预测。具体的识别过程中同样使用多线程并行处理，处理1帧图像是每次创建和分块数相同的线程，为每个分块都初始化一个ECO lite模型。 显然预训练的模型不能为直播推流提供正确的推理结果，但是它可以看作是对视频内容的验证，即：给定一种类型的视频内容，其实其本身被误分类了，但在同一个模型之下它总是会被分类进在整个推流过程中都有相近分数的簇中。 利用这个特性，基于动作识别模型提供的对视频内容的描述，进一步设计动态的用户模型来映射用户偏好到不同的视频内容上。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:5:2","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"NLP Model 为了桥接动作识别和用户偏好向量，必须分析词/短语之间的相似性。 然而现有的ML算法不能直接处理生数据，因为输入必须是数值。为了解决这个问题，采用单词嵌入技术，使用多种语言模型以数值向量的形式来表示单词，以此来确保有相近意义的词有相近密度的表示。 具体处理时使用Phrase2Vec作为NLP模块的模型（作为Word2Vec的扩展，能更好的分析两个短语之间的相似性）。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:5:3","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"用户模型与预测 图5.3阐明了基于结果向量和用户向量的预测过程。由动作识别得出的结果向量，包括一个动作向量 $A$ 和一个权重向量 $W$ 。用户向量包括偏好向量 $P$ 和可能性向量 $L$ 。$A$ 和 $P$ 包含词和短语，描述了视频内容和用户偏好。 $W$ 和 $L$ 分别由表示神经网络对动作结果的置信度和用户对视频内容的参考可能性的值组成。 假设每帧25个分块，CNN模块的输出结果是25个 $A$ 向量和25个 $W$ 向量；对与用户偏好，只使用1个 $P$ 向量和1个 $L$ 向量。 最终的分数向量 $S$ 计算为每个 $A$ 和 唯一的 $P$ 之间的相关性。结果也受相应的 $W$ 和 $L$ 的影响而调整。 假设余弦相似性函数为 $\\rho$ ，那么 $A$ 和 $P$ 中的每个 $a_i$ 和 $p_i$ 的计算可以表示为： $$ {\\rho}_i (a_i, p_i) = Phrase2Vec(a_i, p_i) $$ 设定每个向量中包含5个元素，分数向量 $S$ 计算为： $$ S = L \\cdot W \\cdot \\sum {\\rho} (A, P) $$ 对应于25个分块，最终的分数向量中包含25个元素。 $s_k$ 表示 $k_{th}$ 分块的分数值，详细算法： 分数向量更新完毕之后就可以获得每个分块内容和用户偏好之间的相关性，用帧上每个分块的亮度来做可视化： 将分数向量中的元素从高到低排序，选定 $\\frac{1}{3}$ 作为阈值，将前 $\\frac{1}{3}$ 的分块看作相同的分数等级作为最后的预测区域。 为了应对推流过程中用户偏好的变化，为分数向量的计算设计动态加权的用户偏好向量。 设定用户偏好向量 $P$ 的大小与动作向量 $A$ 的大小相同，一旦系统获取到用户实际的FoV位置，就计算其视野中心并定位到相应的分块，使用前一视频片段中该选中分块的动作向量 $A’$ 来更新用户的偏好向量。 ","date":"2022-01-25","objectID":"/posts/note-for-content-based-vp-for-live-streaming-2/:6:0","tags":["Content-based predict"],"title":"Note for Content Based Vp for Live Streaming (2)","uri":"/posts/note-for-content-based-vp-for-live-streaming-2/"},{"categories":["paper"],"content":"LiveMotion ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:0:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Motivation 基于视频中物体的运动模式来做对应的FoV预测。 将用户的FoV轨迹与视频内容中运动物体的轨迹结合到一起考虑： 细节可以参见：note-for-content-motion-viewport-prediction. LiveDeep 受限于Motion识别算法，前面提出的LiveMotion只能作用于有清晰并且容易分别的前景背景边界的视频，其健壮性并不能满足全景直播推流的场景。 ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:1:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Method LiveDeep处理问题的场景为： 视频内容在线生成； 没有历史用户数据； 预测需要满足实时性的要求； LiveDeep的设计原则： online：在线训练在线预测； lifelong：模型在整个视频播放会话中更新； real-time：预测带来的处理延迟不能影响推流延迟； CNN的设计： 在推流会话的运行时收集并标注训练数据； 以交替迭代的方式进行基于当前视频片段的推理和基于之前视频片段的训练； 子采样少部分的代表帧来运行VP以满足实时性的要求； ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:2:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Framework ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:3:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Setup 分包器将视频按照DASH标准将视频分段，每个段作为训练模型和预测的单元； 考虑到不同的视频可能具有不同的帧速率，在每个单元中统一采样 $k$ 帧而非以固定的采样率采样； 将每帧图像划分成 $x \\times y$ 个分块，最终每个单元中要处理的分块数为 $k \\times x \\times y$ ； 训练集来自于用户的实时反馈，根据实际FoV和预测FoV之间的差距来标注数据； 用户的轨迹数据来自于用户的实时头部轨迹，采样的帧与CNN模块采样的帧同步； ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:3:1","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"Details 在用于训练的图像还没有被标注之前并不能直接预测，所以CNN模块只能以随机的权重给出预测结果。用预测结果与实际结果计算出损失值之后以此来更新CNN模型； LSTM模型只能以用户观看到视频之后的实际轨迹作为训练的输入输入数据； 对下一个片段而言，首先使用两个模块独立做出预测。每个模块的预测都基于子采样之后的 $k$ 个帧； 为了产生对整个片段的预测结果，假设相邻的帧之间共享相同的视野中心（时空局部性）； 取两个模块预测输出的共同的部分作为最终的预测结果； ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:3:2","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"CNN Module 使用经典的CNN：VGG作为骨干网络，修改最后一层，只输出两类：感兴趣的和不感兴趣的。 ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:4:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"推理和视口生成 直观上的想法是选择被分类为感兴趣的部分，并且这些所选部分在原始帧中的位置将指示其他帧中可能感兴趣的FoV。 实际上存在的问题是：几乎所有的部分都被分类为感兴趣的一类，最终结果是整个帧被选择作为预测的结果。 所以不直接使用CNN网络的输出，而是在被分类为感兴趣的部分中进一步细分。通过对输出的分数排序并选择前 $M$ 份比例的输出作为最终的结果，这样通过控制 $M$ 的大小可以调整精度和消耗的带宽。 ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:4:1","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"训练过程 在传统的监督训练中，训练时间取决于可接受的最低损失值和epoch的值。为了满足实时性，LiveDeep采用较高的最低损失值和较低的最大epoch值。 High acceptable loss value：因为直接对从被分类为感兴趣的部分中去获取最终结果，所以通过实验证明，损失值应该要比常规的CNN更高：设定为0.2。 The number of epochs：因为直播推流的特殊性，重复的训练并不能持续降低损失，所以采用较小的值：10。 The batch size：受限于训练的图像，将其设定为训练图像的个数即： $k \\times x \\times y$。 Dynamic learning rate： ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:4:2","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"LSTM Module 单纯的CNN模型可能会导致对视频内容有强记忆性，而这会使模型在面对新视频内容时需要花较长的时间去接受用户偏好，即对于用户偏好的快速切换不能做出即时响应。而LSTM的模块用于弥补这一缺陷； 采用与原始的LSTM模型相同的训练过程：先用收集的训练数据训练模型然后推断未来的数据。 收集用户在过去的视频片段中的用户轨迹，包括从 $k$ 个子采样帧中的 $k$ 个采样点，因此作为训练数据，同时将每个采样点中每个帧的索引指定为时间戳。最终模型的输出是预测出的分块的索引。 ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:5:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"混合模型 将CNN模块得到的输出作为主要的结果，接着结合LSTM模块的输出结果作为最终的预测结果。 ","date":"2022-01-22","objectID":"/posts/note-for-content-based-vp-for-live-streaming-1/:6:0","tags":["Content-based predict"],"title":"Content Based VP for Live Streaming (1)","uri":"/posts/note-for-content-based-vp-for-live-streaming-1/"},{"categories":["paper"],"content":"论文概况 Link：Popularity-Aware 360-Degree Video Streaming Level：IEEE INFOCOM 2021 Keywords：Dynamic tiling, Cross-user division, Heuristic QoE optimization ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:1:0","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"Motivation 将视频划分成分块进行编码之后，会降低编码效率，并增大服务端的存储压力。（细节可以参见Optile） 而分块时根据用户的ROI来确定不同的大小，并在客户端预取，这可以节省带宽。 用户的ROI推断利用跨用户的偏好来确定，即所谓的Popularity-Aware。 ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:2:0","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"Model and Formulation ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:3:0","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"Video Model 视频从时间上被分成固定长度的片段，接着每个片段被从空间上划分成 $C$ 个分块。 除了常规的分块之外， $M$ 个宏块也被建构出来。 每个常规分块和宏块都被编码成 $V$ 个不同的码率质量等级并存储在服务端。 整个推流过程可以看作是一系列连续的下载任务。 客户端在每次下载任务中的目标是：选择恰当分块（宏块或者常规分块的集合）的恰当质量。 用 $L$ 表示客户端请求分块时，缓冲区中已经下载但还没有查看的视频的视频长度，为了避免缓冲事件，分块需要在缓冲区被清空即 $L = 0$ 之前被下载完毕。 ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:3:1","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"QoE Model $$ Q(V_k) = Q_{0}(V_k) - {\\omega}_v I_v (V_k) - {\\omega}_r I_r (V_k) $$ $V_k$ 表示下载的第 $k$ 段视频质量； $Q_0$ 表示平均质量； $I_v$ 表示由质量变化导致的质量损害； $I_r$ 表示由缓冲事件导致的质量损害； ${\\omega}_v$ 和 ${\\omega}_r$ 分别表示质量变化和缓冲的加权因子； 平均质量： $$ Q_0(V_k) = q(\\overline{V_k}) $$ $\\overline{V_k}$ 表示FoV内的平均视频质量； $q(\\cdot)$ 表示视频质量和用户实际感知质量之间的映射函数； 质量变化：两个连续段之间的质量差异和FoV内不同空间位置tile的质量差异会导致用户不适。 $$ I_v(V_k) = |Q_0(V_k) - Q_0(V_{k-1})| + \\widehat{V_k} $$ $|Q_0(V_k) - Q_0(V_{k-1})|$ 表示连续段间的FoV内时间质量差异； $\\widehat{V_k}$ 表示一个视频段的FoV内空间质量差异； 缓冲： $$ L_r(V_k) = {(\\frac{S(V_k)}{R} - L, 0)}_+ $$ $S(V_k)$ 表示段数据量大小； $R$ 表示下载吞吐量； ${(x)}_+ = max \\lbrace x, 0 \\rbrace$ ； ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:3:2","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"Formulation 用 ${\\beta}^v_m ({\\beta}^v_c)$ 表示对应的宏块或常规块是否被下载： ${\\beta}^v_m = 1$ 表示下载编码的质量等级为 $v$ 的宏块，消耗的带宽为 $B^v_m$ ，反之 $ {\\beta}^v_m = 0$ 表示不下载； ${\\beta}^v_c = 1$ 表示下载编码的质量等级为 $v$ 的常规块，消耗的带宽为 $B^v_c$，反之 ${\\beta}^v_m = 0$ 表示不下载； 客户端应该优先下载覆盖用户FoV 的宏块，如果没有这样的宏块则去下载对应的常规块的集合。 优化目标： $$ max\\ Q(\\lbrace v | {\\forall}_{m, v} {\\beta}^v_m = 1 \\rbrace) + Q(\\lbrace v | {\\forall}_{c, v} {\\beta}^v_c = 1 \\rbrace) $$ 同时需要满足以下3个约束： $$ \\sum^{M}_{m=1} \\sum^{V}_{v=1} {\\beta}^v_m + 1(\\sum^{C}_{c=1} \\sum^{V}_{v=1} {\\beta}^v_c) = 1 $$ $$ \\sum^{V}_{v=1} {\\beta}^v_c \\le 1,\\ for\\ c = 1, …, C $$ $$ \\sum^{M}_{m=1} \\sum^{V}_{v=1} {\\beta}^v_m B^v_m + \\sum^{C}_{c=1} \\sum^{V}_{v=1} {\\beta}^v_c B^v_c \\le R \\cdot L $$ $Q(\\cdot)$ 是公式1中定义的质量； $R$ 是网络带宽； $1(x) = 1 \\iff x \u003e 0$ ；$1(x) = 0 \\iff x \\le 0$ ； 约束1强制为观看区域下载宏块或常规块的集合，只下载宏块的一个质量版本； 约束2规定只下载常规块的一个质量版本； 约束3保证视频数据可以在开始播放之前被完全下载； 给出用户的观看区域之后，候选的宏块或对应的常规块集合也可以求出。 将QoE最大化的问题分解成两个子问题： 确定宏块的质量等级； 确定常规块的质量等级； 最后的解取这两种方案能取得更大QoE的那种。 如果QoE模型不考虑常规块之间的质量差异，则整体的QoE等价于下载的常规块的平均质量等级。 确定常规块质量等级的问题则可以简化为： $$ max\\ \\sum_{c \\in C} \\sum^{V}_{v=1} Q({\\beta}^v_c v) $$ 需要满足以下2个约束： $$ \\sum^{V}_{v=1} {\\beta}^v_c = 1,\\ for\\ c \\in C $$ $$ \\sum_{c \\in C} \\sum^{V}_{v=1} {\\beta}^v_c B^v_c \\le R \\cdot L $$ $C$ 表示覆盖观看区域的常规块集合。 简化之后的子问题可以通过对多项选择背包问题的简化，证明为是NP-hard问题，基于此提出启发式算法。 ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:3:3","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"基于宏块的流行性感知推流 ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:4:0","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"基于观看区域确定宏块 不同用户对相同视频的观看有着相似的ROI，其视野中心是相近的，因此首先确定其视野中心并聚类到一起。 不能直接应用的知名聚类算法： 需要事先确定簇（即宏块）数量的算法（事先并不能确定需要多少宏块）：K-means 簇会越聚越大的算法（这样会失去节约带宽的优点）：DBSCAN 提出的算法用2个参数 $\\lambda$ 和 $\\gamma$ 来保证彼此相近的两个视野中心被归入同一簇，同时基于簇的宏块不至于太大。 被归入同一簇的视野中心之间的距离应该小于等于 $\\lambda$； 同一个簇的任意两个视野中心之间的距离应该小于等于 $\\gamma$； 为了确定这两个参数，还需要考虑常规块的大小带来的影响。 算法描述： 给出用 $P$ 表示的点集，其中每个点表示一个用户的视野中心位置； 用 $N_p = \\lbrace q | q \\in P \\land q \\neq p \\land dist(p, q) \\le \\lambda \\rbrace$ 来表示与点 $p$ 之间欧式距离小于 $\\lambda$ 的点集（即为临近点集）； 初始化拥有最多临近点的点所在的簇，例如： $p = {argmax}_{p \\in P} |N_p|$； 添加临近簇内任何点的点到簇中，扩张过程直到找不到符合条件的点位置； 检查簇中任意两个点之间的距离是否大于 $\\gamma$ ，如果存在这种情况就使用K-means算法将这个簇分成两个子簇； 从 $P$ 中移除簇中的点； 重复1-4的过程直到 $P = \\empty$； ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:4:1","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"宏块优化 通过简单地覆盖簇中用户的所有观看区域来为每个簇建构宏块可能会导致建构出不必要的大宏块，因此需要确定恰当的宏块大小。 首先需要确定哪些用户的观看区域应该被用于构建宏块，这样用户下载宏块时的带宽使用率小于下载一组常规块时的带宽使用率：$B_m$ 和 $B_c$ 分别表示覆盖相同观看区域的宏块和常规块的数据量大小。 为了解决用户头部运动的随机性，宏块应该在覆盖用户观看区域之外加上一些边界区域。边界区域可以基于用户观看中心的变化来确定，变化通过在推流观看过程中以固定采样率记录。 一个视频片段中 $x(y)$ 坐标的变化定义为 $x(y)$ 坐标的标准差。 实验发现：在一个视频片段中，用户的 $x(y)$ 坐标的变化很小。 分别用 $A_x$ 和 $A_y$ 表示 $x$ 和 $y$ 方向上的变化，构建的宏块应该覆盖用户的观看区域，并为 $x(y)$ 方向加上 $\\frac{A_x}{2}(\\frac{A_y}{2})$ 的边缘区域。 宏块构造问题的形式化： 为每个用户 $i$ 引入二元变量 ${\\alpha}_i$ ，${\\alpha}_i = 1$ 表示此用户的观看区域用于构建宏块，反之则没有； 实际应用中即为：如果 ${\\alpha}_i = 1$ ，则用户 $i$ 可以下载宏块；否则用户只能下载对应的常规块集合。 问题的目标是：在下载宏块或相同质量等级的常规块集合时，最小化所有用户的总带宽使用量。 $$ \\underset{\\lbrace {\\alpha}_i \\rbrace}{min}\\ \\sum^{N_j}_{i=1} {\\alpha}_i B_m + (1-{\\alpha}_i) B_c $$ $N_j$ 表示在 $j^{th}$ 簇中的用户数量；解决问题之后，可以用所有 ${\\alpha}_i = 1$ 的用户观看区域构建宏块； 尽管暴力枚举法可以完成最优求解，但是其时间复杂度为 $O(2^{N_j})$ ，为了减少实际建构宏块的时间，提出一种类似于随机采样一致性算法的迭代算法，每次迭代中，所做工作如下： 随机选取用户观察区域的子集。 编码宏块，用 $B_m$ 表示构建的宏块的带宽使用量。 检查建构的宏块是否覆盖用户 $i \\in \\lbrace 1, …N_j \\rbrace$ ，是则${\\alpha}_i = 1$；否则 ${\\alpha}_i = 0$。 检查总共的带宽使用量是否比之前迭代的更小，是则用当前迭代建构的宏块更新最终的宏块；否则继续迭代。 为了避免预测失败时用户看到空白区域，在下载观看区域的高质量宏块或常规块集合之外，也以最低质量下载其余的常规块。 ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:4:2","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["paper"],"content":"流行性感知推流 服务端基于多个用户的历史观看信息建构宏块，同时也使用常规块的划分方案编码视频。 客户端在推流过程中选择恰当块（宏块或常规块集）的恰当的质量等级来最大化用户的QoE。 流行性感知的推流算法首先为每个视频段预测用户的观看区域，之后预取相应的宏块或常规块集。 使用岭回归做VP，输入用户在一系列历史帧中的观看区域中心坐标，输出未来帧中用户的观看区域位置。 基于预测的观看区域，算法确定是否存在覆盖预测区域及其边缘区域的宏块，是则搜索并下载满足条件的最高质量的宏块；否则下载相应区域的常规块集。 选择常规块集时首先为所有要选择的块确定满足贷款限制的最高质量等级，分配完之后如果还有剩余的带宽，算法会根据常规块与视野中心距离的远近程度提高一个质量等级，越近越优先提高。同时考虑到空间质量差异会降低QoE，所以提高质量的行为只有在超过半数的常规块满足条件时才会执行。 ","date":"2022-01-18","objectID":"/posts/note-for-popularity-aware-360-degree-video-streaming/:4:3","tags":["Dynamic tiling","Heuristic"],"title":"Note for Popularity Aware 360-Degree Video Streaming","uri":"/posts/note-for-popularity-aware-360-degree-video-streaming/"},{"categories":["knowledge"],"content":"VR 和 360 度全景视频都是获得沉浸式体验的重要途径，除此之外，AR（Argmented Reality）和 MR（Mixed Reality）也是比较火的概念，可以用来对比学习。 ","date":"2022-01-17","objectID":"/posts/summary-for-vr-and-panoramic-video/:0:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/summary-for-vr-and-panoramic-video/"},{"categories":["knowledge"],"content":"全景视频 全景视频实际上事先通过特殊的全景摄像机录制好视频，之后可以在HMD中观看。虽然看到的图像相对于用户当前环境而言是虚拟的，但是终归是从实际环境中录制而来的，本质上更贴近普通视频的全景推广。 在全景视频的观看过程中，用户只有 3DoF 的自由度，即只能完成头部的 3 个角度的运动，同时手柄实际上并不能和视频中的内容进行交互。 全景视频的主要应用在于实景导览，通过事先由拍摄者带着全景录像设备行走拍摄，用户观看时实际是将自己带入到全景设备的位置上，同时移动头部来观察不同角度的视频。 ","date":"2022-01-17","objectID":"/posts/summary-for-vr-and-panoramic-video/:1:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/summary-for-vr-and-panoramic-video/"},{"categories":["knowledge"],"content":"VR VR 主要做的工作是创造出一个完全虚拟的环境，用户戴上HMD之后可以通过其看到虚拟环境中的事物，同时也可以使用HMD配套的手柄等设备进行操作，完成与虚拟环境之间的交互； VR 支持的是 6DoF 的自由度，即除了头部的运动之外也支持身体的前后、左右、上下的移动，手柄； VR 的主要应用在于游戏，比如广受好评的Beat Saber（又称节奏光剑），用户根据音乐节奏通过挥动手柄（在虚拟环境中被建模成光剑）来准确地按照提示的方向去砍击方块； ","date":"2022-01-17","objectID":"/posts/summary-for-vr-and-panoramic-video/:2:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/summary-for-vr-and-panoramic-video/"},{"categories":["knowledge"],"content":"AR 和 MR AR 主要做的工作是将虚拟世界中的事物投影到现实世界中，主体是现实世界，虚拟事物用于增强现实世界。 MR 主要做的工作是将现实世界中的事物虚拟化进入虚拟世界中，主体是虚拟世界，现实事物混合进虚拟世界中。 AR 实现起来比较简单，只需要将计算机产生的图像投影显示在现实中即可，目前的应用比如游戏Pokémon GO里面的AR-mode，启用之后游戏中遇到的Pokémon就可以投影在现实中。 MR 实现起来比较复杂，首先需要用摄像头扫描物体，得到的 2D 图像再交给计算机采用算法进行 3D 重建，最后将虚拟化建模好的物体展示到虚拟世界中，目前的应用比如Meta推出的Workrooms，线上的远距离视频会议在虚拟世界中可以变成虚拟人物之间面对面的交流。 ","date":"2022-01-17","objectID":"/posts/summary-for-vr-and-panoramic-video/:3:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/summary-for-vr-and-panoramic-video/"},{"categories":["knowledge"],"content":"总结 全景视频侧重于对虚拟环境的观察，而 VR 侧重于对虚拟环境的交互。 全景视频实际上是将用户带入到全景摄像机的位置上，让用户产生自己身临拍摄的环境中的感觉，本质上是对传统视频的推广； VR 实际上是将用户完全带入到虚拟的环境中，用户可以和虚拟环境中的事物进行交互，而虚拟环境中发生的一切都和现实无关，本质上是对传统游戏的推广； 全景视频实际上和 VR、AR、MR 这种概念距离比较远，实际上只是因为全景摄像机相较于普通摄像机的 360 度视角的特殊性，这能让用户产生沉浸感。 VR 相比于 AR、MR 而言，是纯粹的虚拟环境，并不涉及到现实事物（除了HMD配套的手柄等设备），而纯粹的虚拟环境将人带入到了一个完全不同的世界，也是 VR 沉浸式体验的来源。 AR 和 MR 是虚拟和现实交融的技术，前者主体是现实，后者主体是虚拟环境。 ","date":"2022-01-17","objectID":"/posts/summary-for-vr-and-panoramic-video/:4:0","tags":["VR","Panoramic Video"],"title":"VR 和 全景视频的区别总结","uri":"/posts/summary-for-vr-and-panoramic-video/"},{"categories":["paper"],"content":"论文概况 Link：Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network Level：IEEE Transactions on Broadcasting 2021 Keywords：Cross-user vp, Sequetial RL ABR ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:1:0","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"主要工作 使用跨用户注意力网络CUAN来做VP； 使用360SRL来做ABR 将上面两者集成到了推流框架中； ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:2:0","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"VP ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:3:0","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"Motivation 形式化VP问题如下： 给出 $p^{th}$ 用户的 $1-t$ 时间内的历史视点坐标 $L^{p}_{1:t} = \\lbrace l^p_1, l^p_2, …, l^p_t \\rbrace$ ，其中 $l^p_t = (x_t, y_t), x_t \\in [-180, 180]; y_t \\in [-90, 90]$ ； 同一视频的不同用户视点表示为 $L^{1:M}_{1:t+T}$ ， $M$ 表示其他用户的数量； 目标是预测未来的 $T$ 个时刻的视点位置 $L^p_i, i = t+1, …, t+T$ ； 最终可以用数学公式表达为： $$ \\underset{F}{min} \\sum^{t+T}_{k = t+1} {\\parallel l^p_k - \\hat{l}^p_k \\parallel}_1 $$ 现有的用KNN做的跨用户预测基于LR的模型，而LR的模型很容易产生偏差，所以为了增强KNN的性能，同时考虑单用户的历史视点轨迹和跨用户的视点轨迹。 提出一种注意力机制来自动提取来自其他用户视口的有用信息； 对于与当前用户有相似偏好的用户轨迹信息给与更多的注意； 相似性通过基于过去时间段内其他用户的轨迹计算出来； ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:3:1","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"Design 轨迹编码器模块从用户的历史视点位置提取时间特征； 使用LSTM来编码用户的观看路径； 为了预测 ${(t+1)}^{th}$ 帧的视点位置，首先向LSTM输入 $p^{th}$ 用户的历史视点坐标： $$ f^{p}_{t+1} = h(l^p_1, l^p_2, …, l^p_t) $$ $h(\\cdot)$ 是LSTM的输入输出函数； 接着使用相同的LTSM编码其他用户的观看轨迹： $$ f^{i}_{t+1} = h(l^i_1, l^i_2, …, l^i_{t+1}), i \\in \\lbrace 1, …, M \\rbrace $$ 注意力模块从其他用户的视点轨迹中提取与 $p^{th}$ 用户相关的信息 首先推导出 $p^{th}$ 用户和其他用户之间的相关系数： $$ s^{pi}_{t+1} = z(f^{i}_{t+1}, l^{p}_{t+1}), i \\in \\lbrace 1, …, M \\rbrace \\cup \\lbrace p \\rbrace; $$ $s^{th}_{t+1}$ 表示 $p^{th}$ 用户和 $i^{th}$ 用户之间的相似性；$z()$ 由内积运算建模（还可用其他方式建模比如多个FC层）； 接着将相关系数规范化： $$ {\\alpha}^{pi}_{t+1} = \\frac{e^{s^{pi}_{t+1}}}{\\sum_{i \\in \\lbrace 1,… M \\rbrace \\cup {\\lbrace p \\rbrace}^{e^{s^{pi}_{t+1}}}}} $$ 最后得到融合特征： $$ g^{p}_{t+1} = \\sum_{i \\in {\\lbrace 1,…M \\rbrace \\cup \\lbrace p \\rbrace}} {\\alpha}^{pi}_{t+1} \\cdot f^{i}_{t+1} $$ 融合特征被最后用于VP。 VP模块预测 ${(t+1)}^{th}$ 帧的视点位置 $$ \\hat{l}^{p}_{t+1} = r(g^{p}_{t+1}) $$ 函数 $r(\\cdot)$ 由一层FC建模。值得注意的是，对应于未来 T 帧的视点是以滚动方式预测的。 ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:3:2","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"Loss 损失函数定义为预测的视点位置和实际视点位置之间的所有绝对差异的总和： $$ L = \\sum^{t+T}_{i=t} {|\\hat{l}^p_i - l^p_i|}_1 $$ ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:3:3","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"Details 使用PyTorch实现； 函数 $h(\\cdot)$ 由两个堆叠的LSTM层组成，两者都有32个神经元； 函数 $r(\\cdot)$ 包含一个带有32个神经元的FC层，接着是Tanh函数； 历史视点和未来视点的长度设定为1秒和5秒； 每次迭代从数据集中随机产生2048个样本； 所有训练变量的优化函数采用Adam； $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$； $learning\\ rate = 10^{-3}, training\\ epoch = 50$； ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:3:4","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"ABR ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:4:0","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"Formulation 全景视频被切分成 $m$ 个长度为 $T$ 秒的视频片段，每个视频片段空间上划分成 $N$ 个分块，分别以 $M$ 个不同的码率等级编码。因此对于每段有 $N \\times M$ 个可选的编码块。 ABR的目标是为每个片段找到最优的码率集 $X = \\lbrace x_{i, j} \\rbrace \\in Z^{N \\times M}$ （ $x_{i, j} = 1$ 意味着为 $i^{th}$ 块选择 $j^{th}$ 的码率等级）： $$ \\underset{X}{max} \\sum^{m}_{t=1} Q_t $$ $Q_t$ 表示 $t^{th}$ 段的QoE分数，与以下几个方面有关： VIewport Quality： $$ Q^1_t = \\sum^{N}_{i=1} \\sum^{M}_{j=1} x_{i,j} \\cdot p_i \\cdot r_{i,j} $$ $p_i$ 表示 $i^{th}$ 分块的规范化观看概率； $r_{i,j}$ 记录块 $(i, j)$ 的码率； Viewport Temporal Variation： $$ Q^2_t = |Q^1_t - Q^{1}_{t-1}| $$ Viewport Spatial Variation： $$ Q^3_t = \\frac{1}{2} \\sum^{N}_{i=1} \\sum_{u \\in U_i} p_i \\cdot p_u \\sum^{M}_{j=1} |x_{i,j} \\cdot r_{i,j} - x_{u,j} \\cdot r_{u,j}| $$ $U_i$ 表示 $i^{th}$ 个分块的1跳邻居中的tile索引[1]； Rebuffering： $$ Q^4_t = max(\\frac{\\sum^{N}_{i=1} \\sum^{M}_{j=1} x_{i,j} \\cdot r_{i,j} \\cdot T}{\\xi_t} - b_{t-1}, 0) $$ $\\xi_t$ 表示网络吞吐量； $b_{t-1}$ 表示播放器的缓冲区占用率； 最终的QoE可以由上面的指标定义： $$ Q_t = Q^1_t - \\eta_1 \\cdot Q^2_t - \\eta_2 \\cdot Q^3_t - \\eta_3 \\cdot Q^4_t $$ $\\eta_*$ 是可调节的参数，与不同的用户偏好对应。 ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:4:1","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"Sequential RL-Based ABR 假设基于tile的全景推流ABR过程也是MDP。 细节在360SRL中已经说明清楚。 ","date":"2022-01-15","objectID":"/posts/note-for-srlabr-cross-user/:4:2","tags":["Cross-user vp","ABR","Immersive Video"],"title":"Note for srlABR Cross User","uri":"/posts/note-for-srlabr-cross-user/"},{"categories":["paper"],"content":"论文概况 Link：360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming Level：ICME 2019 Keywords：ABR、RL、Sequential decision ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:1:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"创新点 在MDP中，将N维决策空间内的一次决策转换为1维空间内的N次级联顺序决策处理来降低复杂度。 ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:2:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"问题定义 原始的全景视频被划分成每段固定长度为 $T$ 的片段， 每个片段包含 $N$ 个分块，并以 $M$ 的码率等级独立编码， 因此对每个片段，有 $N \\times M$ 种可选的编码块。 为了保证播放时的流畅性，需要确定最优的预取集合： ${a_0, …, a_i, …, a_{N-1}}, i \\in \\lbrace 0, …, N-1 \\rbrace, a_i \\in \\lbrace 0, …, M-1 \\rbrace $ 分别用 $q_{i, a_i}$ 和 $w_{i, a_i}$ 表示码率选择为 $a^{th}_i$ 的 $i^{th}$ 分块的质量和相应的分块片段大小。 用 $p_i \\in [0, 1]$ 表示 $i^{th}$ 块的被看到的可能性。 ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:3:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"顺序ABR决策 ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:4:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"代理设计 ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:5:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"状态 对于 $i^{th}$ 维，输入状态包括原始的环境状态 $s_t$ ； 与之前维度的动作集合相关的信号： $u^{i}_{s_t} = \\lbrace Th, C_i, p_{0:i-1}, q_{0:i-1}, b_t, p_i, S_i, Q_{t-1} \\rbrace$ $Th$ ：表示过去 m 次下载一个段的平均吞吐量； $C_i \\in R^M$ ：表示 $i^{th}$ 个分块的可用块大小向量； $p_{0:i-1}$ 和 $q_{0:i-1, a^{0:i-1}_{t}}$ 分别表示选中的码率集合和看到之前 $i-1$ 个分块的概率集； $b_t$ 是缓冲区大小； $p_i$ 是 $i^{th}$ 个分块被看到的可能性； $S_i$ 是之前选择的 $i-1$ 个分块的块大小之和： $S_i = \\sum^{i-1}_{h=0} C_{h, a^h_t}$ ； $Q_{t-1}$ 记录了最后一个段中 $N$ 个分块的平均视频质量； ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:5:1","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"动作 动作空间离散，代理输出定义为价值函数：$f(u^i_{s_t}, a^i_t)$ 表示所选状态的价值 $a^i_t \\in \\lbrace 0, …, M-1 \\rbrace$ 处于状态 $u_{s_t}^i$ . ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:5:2","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"回报 回报定义为下列因素的加权和： 平均视频质量 $q^{avg}_t$，空间视频质量方差 $q^{s_v}_t$，时间视频质量方差 $q^{t_v}_t$ ，重缓冲时间 $T^r_t$ $$ q^{avg}_t = \\frac{1}{\\sum^{N-1}_{i=0} p_i} \\cdot \\sum^{N-1}_{i=0} p_i \\cdot q_{i, a_i} $$ $$ q^{s_v}_t = \\frac{1}{\\sum^{N-1}_{i=0} p_i} \\cdot \\sum^{N-1}_{i=0} p_i \\cdot |q_{i, a_i} - q^{avg}_t| $$ $$ q^{t_v}_t = |q^{avg}_{t-1} - q^{avg}_t| $$ $$ T^r_t = max \\lbrace T_t - b_{t-1}, 0 \\rbrace $$ $$ R_t = w_1 \\cdot q^{avg}_t - w_2 \\cdot q^{s_v}_t - w_3 \\cdot q^{t_v}_t - w_4 \\cdot T^r_t $$ ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:5:3","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"训练方法 使用DQN作为基本的算法来学习动作-价值函数 $Q(s_t, a_t; \\theta)$ ，其中 $\\theta$ 作为参数，对应的贪心策略为 $\\pi(s_t; \\theta) = \\underset{\\theta}{argmax} Q(s_t, a_t; \\theta)$ 。 DQN网络的关键想法是更新最小化损失函数的方向上的参数： $$ L(\\theta) = E[y_t - Q(s_t, a_t; \\theta)] $$ $$ y_t = r(s_t, a_t) + \\gamma Q(s_{t+1}, \\pi(s_{t+1}; {\\theta}’); {\\theta}’) $$ ${\\theta}’$ 表示固定且分离的目标网络的参数； $r(\\cdot)$ 是即时奖励函数，即上面公式5中的 $R_t$ ； $\\gamma \\in [0, 1]$ 是折扣因子； 为了缓解过拟合，引入 double-DQN 的结构，所以公式7被重写为： $$ y_t = r(s_t, a_t) + \\gamma Q(s_{t+1}, {\\pi}(s_{t+1}; \\theta); {\\theta}’) $$ 利用公式6和公式8可以得出 $i^{th}$ 维的暂时损失函数： $$ l^i_t = Q_{target} - Q(u^i_{s_t}, a^i_t; \\theta), \\forall i \\in [0, …N-1] $$ 其中 $Q_{target}$ 满足： $$ Q_{target} = r_t + {\\gamma}_u \\cdot Q(u^0_{s_{t+1}}, \\pi(u^0_{s_{t+1}}; 0); {\\theta}’) $$ ${\\gamma}_u$ 和 ${\\gamma}_b$ 分别代表”Top MDP“和”Bottom MDP“的折扣因子，训练中设定 ${\\gamma}_b = 1$ 。 观察公式9和公式10可以看出每维都有相同的目标函数，意味着无法区别每个独立维度的动作 $a^i_t$ 对 $r_t$ 的贡献。 为了克服限制，根据某个分块的动作 $a^i_t$ 与其观看概率成正比的先验知识，向 $l^i_t$ 添加一个额外的 $r^i_{extra}$ ： $$ l^i_t = r^i_{extra} + Q_{target} - Q(u^i_{s_t}, a^i_t; \\theta), \\forall i \\in [0, …N-1] $$ $$ r^i_{extra} = \\begin{cases} 0, p_i \u003e P ; \\ -a^i_t, p_i \\le P \\end{cases} $$ 通过设定一个观看概率的阈值 $P$ ，对观看概率低于 $P$ 但选择了高码率的分块施加 $-a^i_t$ 的奖励。 因此最终的平均损失可以形式化为： $$ l^{avg}_t = \\frac{1}{N} \\sum^{N-1}_{i=0} l^i_t $$ 接着使用梯度下降法来更新模型，学习率设定为 $\\alpha$： $$ \\theta \\larr \\theta + \\alpha \\triangledown l^{avg}_t $$ 同时，在训练阶段利用经验回放法来提高360SRL的泛化性。 ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:6:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["paper"],"content":"实现细节 特征从输入状态中通过特征提取网络提取出来。 初始的4个输入通过带有128个过滤器的1维卷积层被传递，4个输入核心大小分别为 $1 \\times m$ 、 $1 \\times M$ 、 $1 \\times N$ 、 $1 \\times M$ ，后续这4个输入被喂给有128个神经元的全连接层； 随后特征映射被连接成一个张量，接着是具有1024个神经元和256个神经元的前向网络； 整个动作-价值网络的输出是M维的向量。 特征提取层和前向网络层都使用 Leaky-ReLU作为激活函数，最后是层归一化层。 ","date":"2022-01-13","objectID":"/posts/note-for-360srl/:7:0","tags":["Immersive Video","ABR"],"title":"Note for 360SRL","uri":"/posts/note-for-360srl/"},{"categories":["knowledge"],"content":"视口预测是什么？ 视口预测 (Viewport Predict) 是全景视频中特有的一种用于进一步优化码率自适应的方式。 相较于全景视频 360 度无死角的特性，用户实际上能看到的内容其实只是全景视频中的一个小窗口，这个小窗口就是视口 (Viewport) 。 因为用户在观看全景视频时会在 3DoF 的自由度下转动头部去观看全景视频在空间上的不同部分，所以视口预测做的事情就是在用户的观看过程中预测相较于预测执行时刻的下一时刻的视口位置。 ","date":"2022-01-07","objectID":"/posts/summary-for-vp/:1:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/summary-for-vp/"},{"categories":["knowledge"],"content":"VP 在传输中所处的作用 基于 tile 的全景视频传输方式之所以热门，就是因其可以通过只传输用户 FoV 内的分块而大幅减少观看过程中消耗的带宽。 所以对用户 FoV 的预测是首先要处理的因素，如果 VP 精度很高，那么所有的带宽都可以用很高的码率去传输 FoV 内的分块。 ","date":"2022-01-07","objectID":"/posts/summary-for-vp/:2:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/summary-for-vp/"},{"categories":["knowledge"],"content":"两种方式的基本假设 基于轨迹的方法的基本假设 相对于当前时刻，前 $hw$ (history window)内用户的 FoV 位置对未来可预测的 $pw$ (predict window)内用户的 FoV 位置有影响，比如用户只有很小可能性会在很短的一段单位时间内做 180 度的转弯，而更小角度的调整则更可能发生。 基于内容的方法的基本假设 用户的 FoV 变化是因为对视频内容感兴趣，即 ROI 与 FoV 之间有相关关系，比如在观看篮球比赛这样的全景视频时，用户的 FoV 更可能专注于篮球。 按照提取 ROI 的来源不同可以分为两种类型： 从视频内容本身出发，使用 CV 方法去猜测 ROI； 从用户观看视频的热图出发，相当于得到了经过统计之后的平均 FoV 分布，以此推测其他用户的 ROI； 基于轨迹的方式是要在最表层的历史和预测的轨迹之间学习，即假设两者之间只有时空关系。 跨用户的方式则假设由用户群体所得出的热图可以用来预测单个用户的 FoV，即利用共性来推断个性。 基于内容的方式直接提取视频显著图来推断 FoV，即进一步假设共性与视频内容本身有关系。 ","date":"2022-01-07","objectID":"/posts/summary-for-vp/:3:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/summary-for-vp/"},{"categories":["knowledge"],"content":"跨用户预测的概念 基本假设 就单个用户而言，在观看视频过程中其 FoV 的变化看似随机，但是其行为可能从用户群体的角度去看是跨用户相通的，即多个用户在观看视频时可能会表现出相似的，可以学习的行为模式，这种行为模式可以帮助提高 VP 的精度。 实际应用 基于轨迹的跨用户：如果训练的模型是基于轨迹的离线模型如 LSTM，那么实际上训练好的模型已经学习到了这种跨用户的行为模式；而如果采用的是边训练边预测的模型如 LR（输入历史窗口的经纬度数据，输出预测窗口的经纬度数据），那么这样的模型就是纯粹的单用户模型。 基于内容的跨用户：将用户在观看视频帧时的注意点作为研究对象，找到用户群体在面对同一帧视频时共同关注的空间区域，而这就是用户间相似的行为模式。这种与内容相结合的跨用户方式即为实际研究中所指的跨用户的研究方式。（实际上就是基于内容的研究方法，只不过出发点不是视频本身，而是用户在观看视频时的 FoV） ","date":"2022-01-07","objectID":"/posts/summary-for-vp/:4:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/summary-for-vp/"},{"categories":["knowledge"],"content":"实际应用 图中 3 个黄色矩形表示 3 种方法： ROI extract：基于内容的预测 Multiple watchers’ FoV：跨用户的预测 Multiple watchers’ trajectories：基于轨迹的预测 绿色渐变矩形表示直接使用用户当前的历史轨迹数据去训练模型，接着做出预测。 ","date":"2022-01-07","objectID":"/posts/summary-for-vp/:5:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/summary-for-vp/"},{"categories":["knowledge"],"content":"研究方法 基于轨迹的方法 在线训练：输入历史窗口的位置信息，不断迭代修正模型，输出预测窗口的位置信息。 离线训练：输入任何采样条件下的多对 hw 和 pw 信息来拟合模型。 跨用户的方法 求出多个用户在同一帧上的热图，以此作为 FoV 预测的依据。 基于内容的方法 提取视频帧中的显著图，以此作为 FoV 预测的依据。 ","date":"2022-01-07","objectID":"/posts/summary-for-vp/:6:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/summary-for-vp/"},{"categories":["knowledge"],"content":"优点 使用回归实现的在线训练模型实现简单，反应迅速，有优秀的短期预测精度。 因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入，跨用户的热图可以帮助长期的预测，可以提供合理的离线全视频 FOV 预测，并具有一致的性能。 显著图对于 ROI 集中突出的预测效果较好。 ","date":"2022-01-07","objectID":"/posts/summary-for-vp/:7:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/summary-for-vp/"},{"categories":["knowledge"],"content":"缺点 使用回归实现的在线训练模型在预测窗口增大时，性能会显著下降。 提取显著图的方式一方面训练开销比较大，另一方面对于 ROI 不够集中突出的视频效果并不好。 ","date":"2022-01-07","objectID":"/posts/summary-for-vp/:8:0","tags":["Viewport predict"],"title":"全景视频中视口预测相关方法总结","uri":"/posts/summary-for-vp/"},{"categories":["paper"],"content":"论文概况 Link：Content Assisted Viewport Prediction for Panoramic Video Streaming Level：IEEE CVPR 2019 CV4ARVR Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:1:0","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"主要工作 ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:2:0","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"基于轨迹预测 输入：历史窗口轨迹 模型：64个神经元的单层LSTM，在输入层后面加上一个额外的减法层进行点归一化，以及一个加法层来恢复输出之前的值；用ADAM进行优化，MAE作为损失函数。 ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:2:1","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"跨用户热图 除了观看者自己的历史FOV轨迹之外，其他观看者对同一视频帧的观看方向也有启发性。 对视频的每一帧，首先收集用户的观看方向（坐标使用原始的来自三个方向的欧拉角表示，而非经纬度）。 接着将坐标投影到用经纬度表示的180x360像素的平面图上，对于图中的每个像素点，可以数出其被看到的次数；并对周围像素应用二维高斯光滑。 上面的过程可以为视频生成热图： ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:2:2","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"视频帧的显著图 鉴于观看相同的全景视频时跨用户行为的共性，进一步假设是内容促使多个观众观看公共区域，因此提取出每个帧的显著图可能会表明用户的RoI。 对特定的视频帧，应用经典的特征密集型方法——Ittykoch，它首先根据强度、边缘、颜色和方向将图像分解为多个特征通道，然后将它们组合成识别显著区域。 除了在静态视频帧上检测显著性之外，进一步进行背景减法来减少不太可能感兴趣的区域：应用基于高斯混合的背景/前景分割算法，高级思想是在连续帧之间临时过滤变化的像素点。 结合上面这两个过程可以为视频帧提取时间显著图。 ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:2:3","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"多模态融合 使用包含3个LSTM分支的深度学习模型来融合上述的几种预测方式的结果。 基于轨迹的LSTM（图中绿色分支）从历史窗口 $hw$ 中接受 $n$ 个坐标的输入，接着预测未来窗口 $pw$ 中的 $m$ 个坐标，用 $trj_y_{i}$ 表示； 基于热图的LSTM（图中蓝色分支）将每个预测步骤对应的视频帧的热图作为输入，并在 $pw$ 中输出第2组 $m$ 个坐标的预测，用 $ht_y_{i}$ 表示： 对于每个热图，让其通过3个卷积层，每个卷积层后面都有一个最大池化层。然后，在此图像特征提取之后，应用展平步骤和1个密集层来回归坐标（经纬度表示）。 基于显著图的LSTM采用与热图相似的架构，将显著图作为输入，在 $pw$ 中输出第3组 $m$ 个坐标的预测，用 $sal_y_{i}$ 表示。 对热图和显著图的分支，应用 TimeDistributed层，以便其参数在预测步骤中保持一致。 最终在每个预测步骤连接 $trj_y$ ， $ht_y$，和 $sal_y$ ，并产生一个最终输出 $y$ 。 每个模型的损失函数采用MAE，优化函数采用ADAM。 为每个分支的输出以及最终的输出都检查损失，单独和联合地去调整其参数。 ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:2:4","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"评估 使用2折的交叉验证。 ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:3:0","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"超参数 $pw$ 的大小：0.1s，1.0s，2.0s； $hw$ 的大小：0.05s，0.6s，1.0s；（分别与上面的 $pw$ 对应） 用于训练的用户数：[3, 10, 30] ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:3:1","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"结果与分析 所有模型的预测精度随着 $pw$ 的增长而下降，表明长期预测问题更难解决； 所有模型的精度预测误差几乎是纬度预测误差的二倍，可能由于运动区域在水平方向的翻倍； 线性回归模型只有在 $pw$ 很短的时候预测精确，随着 $pw$ 的增长，其预测精度会迅速下降； 基于 LSTM 的轨迹模型始终优于所有 $pw$ 的基线模型，但更多的训练观众无助于显着提高准确性。 跨用户的热图和显著图可以帮助长期的预测，可以提供合理的离线全视频FOV预测，并具有一致的性能（因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入），当 $pw$ 增长时，其预测精度超过了基于历史轨迹的模型； 结合3种模型之后，可以平衡来自历史轨迹、跨用户兴趣和内容显著性的输入，不论 $pw$ 长或短都能产生优化的预测结果； ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:3:2","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"例外情况 M3在经度上的表现并不适用于上面图中标示的两个视频（Mega.Coaster和GTR.Drives.First.Ever） 原因分析： 这两个视频的共同特点是在驾驶路径的一侧具有高运动内容的驾驶内容，因此用户在观看这些视频时，大多数FOV始终以行驶轨迹为中心。因此用户不太可能改变其观看方向，这导致即使 $pw = 2.0s$ 时，单一基于轨迹的模型的预测精度也更高。相比之下，从对内容角度出发的分析无济于事，但可能会引入观众可能会忽略的变道，进而造成预测误差。 ","date":"2022-01-06","objectID":"/posts/note-for-content-assisted-prediction/:3:3","tags":["Trajectory-based predict","Content-based predict"],"title":"Note for Content Assisted Prediction","uri":"/posts/note-for-content-assisted-prediction/"},{"categories":["paper"],"content":"Dash客户端自适应逻辑 tile priority setup：根据定义的规则对tile进行优先级排名。 rate allocation：收集网络吞吐量信息和tile码率信息，使用确定的tile优先级排名为其分配码率，努力最大化视频质量。 rate adaption：在播放过程中，执行码率自适应算法，基于播放速度、质量切换的次数、缓冲区占用情况等。 ","date":"2021-12-30","objectID":"/posts/note-for-gpac/:1:0","tags":["Immersive Video","DASH"],"title":"Note for GPAC","uri":"/posts/note-for-gpac/"},{"categories":["paper"],"content":"tile priority setup Dash客户端加载带有SRD信息的MPD文件时，首先确定使用SRD描述的tile集合。 确定tile之间的编码依赖（尤其是使用HEVC编码的tile时） 为每个独立的tile向媒体渲染器请求一个视频对象，并向其通知tile的SRD信息。 渲染器根据需要的显示大小调整SRD信息之后，执行视频对象的最终布局。 一旦tile集合被确定，客户端向每个tile分配优先级。（每次码率自适应执行的时候都需要分配tile优先级） ","date":"2021-12-30","objectID":"/posts/note-for-gpac/:1:1","tags":["Immersive Video","DASH"],"title":"Note for GPAC","uri":"/posts/note-for-gpac/"},{"categories":["paper"],"content":"Rate allocation 首先需要估计可用带宽（tile场景和非tile场景的估计不同） 在一个视频段播放过程中，客户端需要去下载多个段（并行-HTTP/2） 带宽可以在下载单个段或多个段的平均指标中估计出来。 一旦带宽估计完成，码率分配将tile根据其优先级进行分类。 一开始所有的tile都分配成最低的优先级对应的码率，然后从高到低依次增长优先级高的tile的码率。 一旦每个tile的码率分配完成，将为目标带宽等于所选比特率的每个tile调用常规速率自适应算法 ","date":"2021-12-30","objectID":"/posts/note-for-gpac/:1:2","tags":["Immersive Video","DASH"],"title":"Note for GPAC","uri":"/posts/note-for-gpac/"},{"categories":["paper"],"content":"论文概况 Link：A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP Level：ACM SIGCOMM 15 Keywords：Model Predictive Control，ABR，DASH ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:1:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"Motivation 关于码率自适应的逻辑，现有的解决方案还没有形成清晰的、一致的意见。不同类型的方案之间优化的出发点并不相同，比如基于速率和基于缓冲区，而且没有广泛考虑各方面的因素并形成折中。 文章引入了控制论中的方法，将各方面的影响因素形式化为随机优化控制问题，利用模型预测控制MPC将两种不同出发点的解决方案结合到一起，进而解决其最优化的问题。而仿真结果也证明，如果能运行一个最优化的MPC算法，并且预测误差很低，那么MPC方案可以优于传统的基于速率和基于缓冲区的策略。 ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:2:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"背景 播放器端为QoE需要考虑的问题： 最小化冲缓冲事件发生的次数； 在吞吐量限制下尽可能传输码率较高的视频； 最小化播放器开始播放花费的时间（启动时间）； 保持播放过程平滑，尽可能避免大幅度的码率变化； 这些目标相互冲突的原因： 最小化重缓冲次数和启动时间会导致只选择最低码率的视频； 尽可能选择高码率的视频会导致很多的重缓冲事件； 保持播放过程平滑可能会与最小的重缓冲次数与最大化的平均码率相冲突； ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:3:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"控制论模型 ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:4:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"视频推流模型 参数形式化 将视频建模成连续片段的集合，即：$V = \\lbrace 1, 2, …, K \\rbrace$，每个片段长为$L$秒； 每个片段以不同码率编码，$R$ 作为所有可用码率的集合； 播放器可以选择以码率$R_k \\in R$ 下载第$k$块片段，$d_k(R_k)$ 表示以码率$R_k$编码的视频大小； 对于恒定码率CBR的情况，$d_k(R_k) = L \\times R_k$； 对于变化码率VBR的情况，$d_k \\sim R_k$； 选择的码率越高，用户感知到的质量越高： $q(\\cdot):R \\rightarrow \\R_+$ 是一个不减函数，是选择的码率 $R_k$ 到用户感知到的视频质量 $q(R_k)$ 的映射； 片段被下载到回访缓冲中，其中包含下载了的但还没看过的片段。 $B(t) \\in [0, B_{max}]$ 表示 $t$ 时刻缓冲区的占用， $B_{max}$ 表示内容提供商的策略和播放器的存储限制； 播放过程形式化 在 $t_k$ 时刻，视频播放器开始下载第 $k$ 个块，这个块的下载时间可以计算为： $d_k(R_k) / C_k$； $C_k$ 表示下载过程中经历的平均下载速度； 一旦第 $k$ 个块下载完毕，播放器等待 $\\Delta t_k$ 时间并在 $t_{k+1}$ 时刻下载下一个块 $k+1$ ； 假设等待时间 $\\Delta t_k$ 很短并且不会导致重缓冲事件，用 $C_t$ 表示 $t$ 时刻的网络吞吐量： $$ t_{k+1} = t_k + \\frac{d_k(R_k)}{C_k} + \\Delta t_k $$ $$ C_k = \\frac{1}{t_{k+1} - t_k - \\Delta t_k} \\int_{t_k}^{t_{k+1} - \\Delta t_k} C_t dt $$ $B(t)$ 的变化取决于下载的块和播放的块的数量： 在第 $k$ 个块下载完毕之后缓冲区占用增长 $L$ 秒；用户观看一个块之后缓冲区占用减少 $L$ 秒； $B_k = B(t_k)$ 表示播放器开始下载第 $k$ 个块时的缓冲区占用； 缓冲区占用的动态变化可以表示为： $$ B_{k+1} = \\big( (B_k - \\frac{d_k(R_k)}{C_k})_+ + L - \\Delta t_k \\big)_+ $$ 其中 $(x)_+ = max\\lbrace x, 0 \\rbrace $ 确保其非负； 如果 $B_k \u003c d_k(R_k) / C_k$ ，表示缓冲区在播放器还在下载第 $k$ 个块时变空，而这会导致重缓冲事件； 等待时间 $\\Delta t_k$ 的确定也称为块调度问题，本文中假设播放器在第 $k$ 个块下载完毕之后尽可能快地去下载第 $k+1$ 个块（除了缓冲区满了的情况，播放器等待缓冲区中的块被消耗之后再下载新的块）： $$ \\Delta t_k = \\Big( \\big( B_k - \\frac{d_k(R_k)}{C_k} \\big)_+ + L - B_max \\Big)_+ $$ ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:4:1","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"QoE最大化问题 QoE的组成部分： 平均视频质量：在所有块中每个块平均的质量，计算为： $$ \\frac{1}{K} \\sum^K_{k=1} q(B_k) $$ 平均质量变化：相邻块之间质量变化的平均值，计算为： $$ \\frac{1}{K-1} \\sum^{K-1}_{k=1} | q(R_{k+1}) - q(R_k) | $$ 重缓冲总计时间：对每个块而言，当轮到其被消耗时但下载块的过程还没完成即出现了重缓冲，总时间计算为： $$ \\sum^K_{k=1} (\\frac{d_k(R_k)}{C_k} - B_k)_+ $$ 启动延迟 $T_s$ ，假设 $T_s \\ll B_{max}$ 。 对不同用户而言，上述4种因素的重要程度不同。使用上述分量的加权，定义视频块 $1$ 到 $K$ 的QoE： $$ QoE^K_1 = \\sum^K_{k=1} q(R_k) - \\lambda \\sum^K_{k=1} | q(R_{k+1}) - q(R_k) | - \\mu \\sum^K_{k=1} (\\frac{d_k(R_k)}{C_k} - B_k)_+ - \\mu_s T_s,\\ \\lambda, \\mu, \\mu_s \\nless 0 $$ 相对较小的 $\\lambda$ 表示用户不太关心视频质量变化； $\\lambda$ 越大表明越需要使视频质量变得光滑。 相对较大的 $\\mu$ 表示用户很在意重缓冲； 在这里文章倾向于启动延迟很低，所以采用大 $\\mu_s$ ； QoE的最大化： 输入：吞吐量迹 ${C_t, t \\in [t_1, t_{K+1}]}$ 输出：码率选择 $R_1, …, R_K$；启动时间 $T_s$ ； 需要注意：当最大化的决策发生在播放过程中时，启动时间便不再存在； ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:4:2","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"算法 上图中的QoE最大化问题是一种随机优化控制问题，随机性源自可获得的吞吐量 $C_t$ 。 $t_k$ 时刻播放器选择码率 $R_k$ ，只有过去的吞吐量 $\\lbrace C_t, t \\le t_k \\rbrace$ 可知，未来的值 ${C_t, t \u003e t_k}$ 未知。 但是，吞吐量预测器可以用于获取对吞吐量的预测，定义其为 $\\lbrace \\hat{C_t}, t \u003e t_k \\rbrace$ 。 基于这样的预测和缓冲区的信息（精确可知），码率选择器对下个块 $k$ 的码率选择可以表示为： $$ R_k = f \\big( B_k, \\lbrace \\hat{C_t}, t \u003e t_k \\rbrace, \\lbrace R_i, i \u003c k \\rbrace \\big) $$ 文章只关注码率自适应算法，假设已经得到了预测值，并根据预期预测误差对其进行了表征，即： 我们着重于 $f(\\cdot)$ 的设计以及预测误差对比较控制算法性能的影响。 现有的两类自适应算法：基于速率和基于缓冲区，分别可以表示为： $$ R_k = f \\big( \\lbrace \\hat{C_t}, t \u003e t_k \\rbrace, \\lbrace R_i, i \u003c k \\rbrace \\big) $$ $$ R_k = f(B_k, \\lbrace R_i, i \u003c k \\rbrace) $$ 前者只基于吞吐量的预测结果而不管缓冲区状况；后者只基于缓冲区而不管未来的吞吐量可能状况； 这两种方法在原则上都只是次优的，理想情况下我们想要同时考虑缓冲区占用和吞吐量预测结果。 ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:4:3","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"MPC for Optimal Bitrate Adaptation ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:5:0","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"Why MPC MPC天然适合码率自适应问题。 Strawman solutions 码率自适应问题本质是随机控制优化问题，就这一点而言，有两个知名控制算法： Proportional-integral-derivation(PID) control. Markov Decision Process(MDP) based control. PID相较MDP而言计算起来更加简单，只能用于使系统稳定，不能显式地优化QoE目标；此外PID被设计用于有连续的时间和连续的状态空间的问题中，用于当前这种高度离散化的问题中会导致性能亏损和不稳定。 应用MDP的话可以将吞吐量和缓冲区状态形式化为马氏过程，然后使用诸如值迭代和策略迭代等标准算法求出最优解。 （然而，这有一个很强的假设，即吞吐量动态遵循马尔可夫过程，不清楚这在实践中是否成立。我们将MDP的潜在用途和吞吐量动态分析作为未来的工作。） Case for MPC 理想情况下，如果给出未来吞吐量的完美数据，那么启动时间 $T_s$ 和最优码率选择 $R_1, … R_K$ 可以一下子就计算出来； 实际情况中，虽然不能得到未来吞吐量的完美预测，但是我们可以假设吞吐量在较短的时间段 $[t_k, t_{k+N}]$ 内不会剧烈变化。 基于此，可以使用当前视界中的预测来应用第1个码率 $R_k$ ，之后将视界向前移动到 $[t_{k+1}, t_{k+N+1}]$ 。 而这种方案就称为MPC。MPC的一般好处在于，MPC可以利用预测在约束条件下在线优化动态系统中的复杂控制目标。 ","date":"2021-12-23","objectID":"/posts/note-for-mpc/:5:1","tags":["DASH","MPC","ABR"],"title":"Note for MPC","uri":"/posts/note-for-mpc/"},{"categories":["paper"],"content":"论文概况 Link：TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming Level：ACM MM 21 Keywords：Adaptive tiling and bitrate，Mobile streaming ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:1:0","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"创新点 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:2:0","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"背景 现有的固定的tile划分方式严重依赖viewport预测的精度，然而viewport预测的准确率往往变化极大，这导致基于tile的策略实际效果并不一定能实现其设计初衷：保证QoE的同时减少带宽浪费。 考虑同样的viewport预测结果与不同的tile划分方式组合的结果： 从上图可以看到： 如果采用$6 \\times 6$的分块方式，就会浪费26，32两个tile的带宽，同时15，16，17作为本应在实际viewport中的tile并没有分配最高的优先级去请求。 如果采用$5 \\times 5$的分块方式，即使预测的结果与实际的viewport有所出入，但是得益于tile分块较大，所有应该被请求的tile都得到了最高的优先级，用户的QoE得到了保证。 另一方面，基于tile的方式带来了额外的编解码开销（可以看这一篇论文：note-for-optile），而这样的性能需求对于移动设备而言是不可忽略的。 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:2:1","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"创新 除了考虑常见的因素如带宽波动和缓冲区占用之外，提出同时自适应分块策略和码率分配以应对变化的viewport预测性能和受限的移动设备的解码能力。 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:2:2","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"论文组织 首先使用现实世界的轨迹分析了典型的viewport预测算法并确定了其性能的不确定性。 接着讨论了不同的分块策略在tile选择和解码效率上的影响。 自适应的分块策略可以适应viewport预测的错误，并能保证tile选择的质量。 为解码时间建构了分析模型，可以在给定受限的计算资源时用于选择恰当的分块策略和码率。 形式化了优化模型，讨论了自适应算法的细节。 评估证明了方案的优越性。 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:2:3","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"Motivation ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:3:0","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"分块策略对tile选择的影响 实现4种轻量的viewport预测算法：线性回归LR、岭回归RR、支持向量回归、长短期记忆LSTM。 设置历史窗口大小为2s，预测窗口大小为1s；viewport的宽度和高度分别为100°和90°。 默认的分块策略为$6 \\times 6$；头部移动数据集来自公开数据集。 viewport预测的不准确性 研究表明，用户的头部运动主要发生在水平方向而较少发生在垂直方向，所以只分析水平方向的预测。 实际的商业移动终端只有有限的传感和处理能力，并不能支持高频的viewport预测采样。 视频内容的不同类型会显著影响预测的精度，基于录像环境（室内或户外）和相机的运动状态分类。 改变采样频率会直接影响viewport预测的精度，频率越低，精度越低。 相机运动的viewport预测错误率比相机静止的明显更高。 通过分块容忍预测错误 因为不管tile的哪个部分被包含在预测的viewport中，只要包含一部分就会请求整个tile，所以增大每个tile的尺寸能吸收预测错误。 实验验证： 设定从$4 \\times 4$到$10 \\times 10$的分块方式，使用不同的预测误差来检查分块设定可以容纳的最大预测误差，同时保持tile选择结果的相同质量。 用$F_1$分数来表示tile选择的质量：$F_1 = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}$。 实验结果表明更大的tile尺寸更能容忍预测错误。 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:3:1","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"分块策略对解码复杂性的影响 虽然当前的移动设备硬件性能发展迅速，但是实时的高码率高分辨率全景视频的解码任务还是充满挑战。 分块对于编码的影响： tile越小，帧内和帧间内容的相关区域就越小，编码效率越低。 直接影响解码复杂性的因素： tile的数量。 视频的分辨率。 用于解码的资源。 固定其中1个因素改变另外2个因素来检查其对解码的影响： 根据对图的观察可以得出这3个因素在经验上是相互独立的，因为这三幅图之中的图像几乎相同。 分别用$F_n(x), F_r(x), F_c(x)$表示tile数量、分辨率、线程数量为$x$时，解码时间与基线时间的比值。 将这3个比值作为3个乘子建立分析模型： $$ D = D_0 \\cdot F_n(x_1) \\cdot F_r(x_2) \\cdot F_c(x_3) $$ 上式表示计算整体的解码时间，其中tile数量为$x_1$、分辨率为$x_2$、线程数量为$x_3$；$D_0$时解码的基线时间。 这个模型将用于帮助做出分块和码率适应的决策。 注意在实际情况中，可供使用的计算资源（线程数）是受限的，需要根据设备当前可用的计算资源来分配。 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:3:2","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"TBRA的设计 $S = \\lbrace s_1, s_2, … \\rbrace$ 表示360°视频分块方式的集合； 对于分块方式$s_i$，$|s_i|$ 表示这种方案中tile的数量； 当 $i \u003c j$ 时，假设 $|s_i| \u003c |s_j|$； 对于分块方式$s$， $b_{i, j}$ 表示第 $i$ 块的tile $j$，$i \\le 块的数量, j \\le |s|$； 目标是确定分块方式$s$，并为每个tile确定其码率$b_{i, j}$； ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:4:0","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"分块自适应 自适应的概念 分块尺寸大小会导致viewport容错率和传输效率的变化。 分块尺寸小，极端情况下每个像素点作为一个tile，viewport容错率最小，但是传输效率达到100%； 分块尺寸大，极端情况下整个视频帧作为一个tile，viewport容错率最大，但是传输效率最小； 优化的目标就是在这两种极端条件中找到折中的最优解。 分块选择 以$\\overline{r_d}, d \\in \\lbrace left, right, up, down \\rbrace$为半径扩大预测区域；$e_d$表示过去n秒中方向 $d$ 的预测错误平均值； $$ \\overline{r_d} = (1-\\alpha) \\cdot \\overline{r_d} + \\alpha \\cdot e_d $$ 预测区域的扩展被进一步用于tile选择，受过去预测精度的动态影响。 下一步检查不同分块方式，进而找到QoE和传输效率之间的折中。 对于每个分块方式，比较基于扩展的预测区域的tile选择的质量。使用2个比值作为QoE和传输效率的度量： $$ Miss\\ Ratio = \\frac{of\\ missed\\ pixels\\ in\\ expanded\\ prediction}{of\\ viewed\\ pixels} $$ $$ Waste\\ ratio = \\frac{of\\ unnecessary\\ pixels\\ in\\ expanded\\ prediction}{of\\ viewed\\ pixels} $$ 这2个比值的tradeoff可以在上图中清晰地看出。 使用分块方式对应的惩罚$Tiling\\ i_{penalty}$来评估其性能： $$ Tiling\\ i_{penalty} = \\beta \\cdot Miss\\ Ratio + |1/cos(\\phi_i)| \\cdot Waste\\ Ratio $$ $\\phi_i$ 是viewport $i$ 的中心纬度坐标，它表明随着viewport的垂直移动，浪费率的权重会发生变化。（因为投影方式是ERP） 检查完所有的方式之后，最终选择惩罚最小的分块方式。 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:4:1","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"码率自适应 视频质量 $w_{i, j}$表示在第 $i$ 个视频块播放时，tile $j$ 的权重；在当前方案中 $w_{i, j} = 0\\ or\\ 1$ 取决于tile是否在预测的viewport中。 $q(b_{i, j})$ 是tile比特率选择 $b_{i, j}$ 与用户实际感知到的质量之间的非递减映射函数。 第 $i$ 个视频块的质量等级可以定义为： $$ Q^{(1)}_i = \\sum^n_{j=1} w_{i, j} q(b_{i, j}) $$ 使用最新研究的主观视频质量模型： $$ subjective\\ PSNR:\\ q_i = PSNR_i \\cdot [M(v_i)]^{\\gamma} [R(v_i)]^{\\delta} $$ $M(v_i)$ 是检测阈值；$R(v_i)$ 是视网膜滑移率；$v_i$ 是第播放 $i$ 个视频块时viewport的移动速度；$\\gamma = 0.172, \\delta = -0.267$ 质量变化 连续视频块之间的强烈质量变化会损害QoE，定义质量变化作为响铃两个视频块之间质量的变化： $$ Q^{(2)}_i = |Q^{(1)}_1 - Q^{(1)}_{i-1}|,\\ i \\in [2, m] $$ 重缓冲时间 参数设置： $C_i$ 表示下载视频块 $i$ 的预计吞吐量； $B_i$ 表示客户端开始下载视频块 $i$ 时缓冲区的占用率； $B_{default}$ 表示在启动阶段默认的缓冲区填充等级，记 $B_{default} = B_1$； 下载第 $i$ 个视频块需要时间 $\\sum^n_{j=1} b_{i, j} / C_i$ ； 每个视频块的长度为 $L$ ； 缓冲区的状态应该在每次视频块被下载的时候都得到更新，则下一个视频块 $i+1$ 的缓冲区占用情况可以计算为： $$ B_{i+1} = max\\lbrace B_1 - \\sum^n_{j=1} b_{i, j} / C_i,\\ 0\\rbrace + L $$ 下载第 $i$ 个视频块时的重缓冲时间可以计算为： $$ Q^{(3)}_i = max \\lbrace \\sum^n_{j=1} b_{i, j} / C_i - B_i,\\ 0 \\rbrace + t_{miss} $$ 第一部分是下载时间过长且缓冲区耗尽，视频无法播放情况下的重新缓冲时间； 第二部分 $t_{miss}$ 表示下载缺失的tile所花费的时间（在视频块播放过程中被看到但是之前没有分配码率的tile）。 优化目标 第 $i$ 个视频块的整体优化目标可以定义为前述3个指标的加权和： $$ Q_i = pQ^{(1)}_i - qQ^{(2)}_i - rQ^{(3)}_i $$ 各个系数的符号分配表示：最大化视频质量、最小化块间质量变化、最小化重缓冲时间。 传统意义上使用所有视频块的平均QoE作为优化对象，但实际上很难获得从块 $1$ 到块 $m$ 的整个视界的完美的未来信息。 为了处理预测长期吞吐量和用户行为的难度，采用基于MPC的框架，在有限的范围内优化多个视频块的QoE，最终的目标函数可以形式化为： $$ \\underset{b_{i, j}, i \\in [t, t+k-1], j \\in [1, n]}{max} \\sum^{t+k-1}_{i=t} Q_i $$ 因为短期内的viewport预测性能和网络状况可以很容易得到，QoE优化可以通过使用窗口 $[t, t+k-1]$ 内的预测信息； 接着将视界向前移动到 $[t+1, t+k]$ ，更新新的优化窗口的信息，为下一个视频块执行QoE优化，直到最后一个窗口。 使用基于MPC的公式的优点：由于受限的问题规模，每个优化问题的实例都是实际可解的。 高效求解 提出的公式天然适合在线求解，得益于短窗口的实例问题规模很小，QoE优化可以通过详尽搜索定期解决。 但是因为优化过程需要高频调用，所以对于大的搜索空间还是充满挑战。 为了支持实时优化，需要对搜索空间进行高效剪枝，确定几点约束： 解码时间需要被约束； 解码时间应该短于回放长度。 给定移动设备上可用的计算资源，可以得到支持的最大解码线程数。 基于解码时间的分析模型，由于解码复杂度和分辨率的单调性，可以找到设备能够限定时间内解码的最大质量水平，这会将码率选择限制在有界搜索空间内。 码率选择应该考虑吞吐量的限制：$\\sum^n_{j=1} b_{i, j} \\le LC_i$ ； 不会主动耗尽缓冲区，无需让其处理吞吐量的波动。 码率选择应该考虑tile的分类； tile的码率不应该低于同一个视频块中更低权重tile的码率： $b_{i, j} \\ge b_{i, j’}, \\forall w_{i, j} \u003e w_{i, j’}$ 。 属于相同类别的tile比特率选择应该是同一个等级； 这使码率自适应在tile类的级别上执行而非单个tile的级别，大大减小了搜索空间的规模。 当优化窗口中的吞吐量和用户行为保持稳定时，同一个窗口中的tile应该有相同的结果。 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:4:2","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"TBRA workflow 这样的方式需要在服务端存储大量的按照不同分块方式划分的不同码率版本的视频块，这一点可以进一步研究。 但是对于移动终端设备而言，这样的解决方案只引入了可以忽略不计的开销。 观察到tile自适应问题具有全局最优通常就是局部最优的特点，因此可以大大减少计算量。 基于MPC的优化workflow还可以有效地解决码率自适应问题。 ","date":"2021-12-21","objectID":"/posts/note-for-tbra/:4:3","tags":["Immersive Video","Dynamic tiling","Mobile device","MPC"],"title":"Note for TBRA","uri":"/posts/note-for-tbra/"},{"categories":["paper"],"content":"论文概况 Link：Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019 Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:1:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"Workflow Tracking：VR motion追踪算法：应用了高斯混合模型来检测物体的运动。 Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户viewport来自动更正潜在的预测错误。 Update：viewport动态更新算法：动态调整预测的viewport大小去覆盖感兴趣的潜在viewport，同时尽可能保证最低的带宽消耗。 Evaluation：经验用户/视频评估：构建VR viewport预测方法原型，使用经验360°视频和代表性的头部移动数据集评估。 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:2:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"全景直播推流的预备知识 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:3:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"VR推流直播 相比于传统的2D视频推流的特别之处： VR系统是交互式的，viewport的选择权在客户端； 呈现给用户的最终视图是整个视频的一部分； ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:3:1","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"用户头部移动的模式 在大量的360°视频观看过程中，用户主要的头部移动模式有4种，使用$i-j\\ move$来表示； 其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。 $1-1\\ move$：单个物体以单一方向移动； $1-n\\ move$：单个物体以多个方向移动； $m-n\\ move$：多个物体以多个方向移动； $Arbitrary\\ move$：用户不跟随任何感兴趣的物体而移动，viewport切换随机； 现有的直播VR推流中的viewport预测方法是基于速度的方式，这种方式只对$1-1\\ move$这一种模式有效。 本方案的目标是提出对4种模式都有效的预测策略。 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:3:2","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"系统架构 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:4:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"理论创新 核心功能模块： motion detection：区分运动物体与静止的背景。 feature selection：选择代表性的特征并对运动物体做追踪。 这两个模块使系统能识别用户可能感兴趣的viewport。 使用贝叶斯方法分析用户观看行为并形式化用户的兴趣模型。 使用错误恢复机制来使当预测错误被检测到时的预测viewport去适应实际的viewport，尽管不能消除预测错误但是能避免在此基础上进一步的预测错误。 使用动态viewport更新算法来产生大小可变的viewport，通过同时考虑跟踪到的viewport轨迹和用户当前的速度（矢量）。 这样，即使用户的运动模式很复杂也能有更高的概率去覆盖潜在的视图。 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:4:1","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"具体实施 虽然提出的运动追踪和错误处理机制是计算密集型的任务，但是这些组件都部署在video packager中，运行在服务端。 将生成VR视图的工作负载移动到服务端，进一步减少了客户端的计算开销以及网络开销。 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:4:2","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"形式化 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:5:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"基于运动轨迹的viewport预测 使用GMM完成运动检测，使用Shi-Tomasi algorithm解决运动轨迹跟踪问题。 运动检测 GMM前景提取 特征选取与过滤 采用 Shi-Tomasi algorithm 从视频中检测代表性的特征，直接检测得到的代表性特征数量较多而难以追踪。 采用两种过滤的方法来减少要追踪的特征数量。 比较当前帧和前一帧的特征，只保留其共有的部分。 采用第1步中运动检测的方式，只保留运动的部分。 viewport生成 经过选择和过滤之后的特征通常分布在不能被单一用户视图所覆盖的广阔区域中。 在整个360°视频中可能存在多个运动的物体，即$m-n\\ move$。 提出一种系统的方式来产生用户最可能跟随观看的viewport。 直觉是用户更可能将大部分注意力放在两种类型的物体上： 离用户更近的物体。 就物理形状而言更“重要”的物体。 这两种类型的物体大多包含最密集和最大量的特征，因此通过所有特征的重心来计算预测用户视图的中心。 对于剩余的特征列表：$\\vec{F} = [f_1, f_2, f_3, …, f_k]$，其中$f_i(i = 1 … k)$表示特征$f_i = \u003cf^{(x)}_i, f^{(y)}_i\u003e$的像素点坐标，则预测出的viewport中心坐标可以计算出来： $$ l_x = \\frac{1}{k} \\sum^k_{i=1} f^{(x)}_i;\\ l_y = \\frac{1}{k} \\sum^k_{i=1} f^{(y)}_i. $$ 考虑到即使预测的viewport中包含用户观看的物体，预测得到的viewport也可能会与实际的viewport存在差异。 所以预测的viewport可能比实际的viewport要大，所以使用缩放因子$S_c$来产生预测的viewport。 给出用户viewport的大小$S_{user}$，预测的viewport可以通过$S_{pre} = S_c \\cdot S_{user}$计算出来。 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:5:1","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"基于用户反馈的错误恢复 video packager可以通过HMD和web服务器通过反向路径从用户处检索用户实际视图的反馈信息。 基于反馈的错误恢复机制在以下两种场景中表现良好： 没有运动的物体 如果没有检测到运动的物体，则用户很可能是在观看静止的物体，这会导致基于运动目标的viewport预测失败。 在这种场景中，可以认为视频内容已经不再是决定用户viewport的因素，而只取决于用户自身的行为。 因此采用基于速度的方式来预测viewport。（这样的决策可以在运动检测模块没有检测到运动物体时就做出） 一旦从反馈路径上得到用户信息，可以产生用户viewport位置向量：$\\vec{L} = [l_1, l_2, l_3, …, l_M]$，其中$l_i$表示第$i$个帧中用户viewport的位置，$M$表示视频播放缓冲区中的帧数。那么可以计算viewport速度： $$ \\vec{V} = \\frac{\\vec{(l_2 - l_1)} + \\vec{(l_3 - l_2)} ….(l_M - l_{M-1})}{M-1} = \\frac{(\\vec{l_M - l_1})}{M-1} $$ 下一帧的预测位置$L_{M=1}$也可以计算出来： $$ l_{M+1} = l_M + \\vec{V} $$ 预测视图与实际视图的不匹配 一旦运动追踪策略检测到用户实际的视图和预测的视图不同，就会触发恢复机制去追踪用户实际在看着的物体。 可以使用运动追踪方式确定用户实际观察的物体的速度。 给出前一帧匹配的特征$\\vec{FA} = [fA_1, fA_2, fA_3, …, fA_p]$和当前帧的特征$\\vec{FB} = [fB_1, fB_2, fB_3, …, fB_p]$，可以计算出速度： $$ V_x = \\frac{1}{p} (\\sum^p_{i=1} fB^{(x)}_i - \\sum^p_{i=1}fA^{(x)}_i),\\ V_y = \\frac{1}{p} (\\sum^p_{i=1} fB^{(y)}_i - \\sum^p_{i=1}fA^{(y)}_i), $$ 假设预测的viewpoint是$(l_x, l_y)$，修改之后的viewpoint是$(l_x + V_x,\\ l_y + V_y)$。 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:5:2","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"动态viewport更新 前述的错误恢复机制发生在viewport预测错误出现之后，任务是避免未来更多的错误。 动态的viewport更新则努力避免viewport预测错误。 关键思想是扩大预测的viewport大小，以高概率去覆盖$m-n\\ move$和$arbitrary\\ move$下所有潜在的运动目标；更重要的是动态调整视图的大小去获得更高效的带宽利用率。 对于一个360°全景视频，将360°的帧均分为$N = n \\times n$个网格，每个网格看作是一个tile，预测的viewport即为$N$个tile的子集。 使用贝叶斯方法分析用户的观看行为，每个tile分配一个独立的贝叶斯模型，所以每个tile可以独立更新。 设$X$表示用户viewport，$Y$表示静态内容，$Z$表示运动物体。 未来的用户viewport可以以条件概率计算为$P(X|Y,\\ Z)$，$Y$与$Z$相互独立。 用户的viewport可以通过反馈信息得出$P(X)$；用户观看静态特征可以表示为$P(X|Y)$；用户观看动态特征可以表示为$P(X|Z)$。 $P(X|Y, Z)$可以计算为： $$ P(X|Y, Z) = \\frac{P(Y|X) \\cdot P(Z|X) \\cdot P(X)}{P(Y, Z)} $$ 只要用户开始观看，对于tile $T_i$，就能得到其先验概率$P(Y_i|X_i)$和$P(Z_i|X_i)$，进而根据贝叶斯模型计算出$P(X|Y, Z)$。 为每个tile定义两种属性： 当前状态：表示此tile是否属于预测的viewport（属于标记为$PREDICTED$，不属于标记为$NONPREDICTED$）。 生存期：表示此tile会在view port中存在多长时间（例如定义3种等级：$ZERO$，$MEDIUM$，$HIGH$，实际的定义划分可以根据具体的用户和视频设定）。 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:5:3","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"预测步骤 按照形式化中提出的3步，分为系统初始化、帧级别的更新、缓冲区级别的更新。 系统初始化 初始化阶段中，view更新算法将所有的$N$个tile标注为$PREDICTED$，并将生存期设置为$MEDIUM$，即系统向用户发送完整的一帧作为自举。 这样设定的原因在于：当用户第一次启动视频会话时，允许“环视”类型的移动，这可能会覆盖360°帧的任意viewport。 帧级别的更新 给定一帧，应用修改后的motion追踪算法在运动区域中选择特征，而不使用特征的密度做进一步的过滤。 使用有多个tile的多个视图来覆盖一个放大的区域，该区域包含作为预测viewport的移动对象上的所有特征，这样就能适应$m-n\\ move$中的用户行为。 设计帧级别的算法标记选择的tile作为$PREDICTED$并设置其生存期为$HIGH$（直觉上讲运动中的物体或用户所感兴趣的静态特征会更以长时间保留在viewport之中）。 缓冲区级别的更新 以缓冲区长度为间隔检索用户的实际视图，基于此可以对tile的两种属性做出调整。 对于与用户实际视图重叠的tile，设置为$PREDICTED$和$HIGH$。 对于用户实际视图没有出现但出现在预测的视图中的tile，生存期减1，如果生存期减为$ZERO$，就重设其状态为$NONPREDICTED$，将其从预测的viewport中移除。 ","date":"2021-12-20","objectID":"/posts/note-for-content-motion-viewport-prediction/:6:0","tags":["Immersive Video","Content-based predict","Live video"],"title":"Note for Content Motion Viewport Prediction","uri":"/posts/note-for-content-motion-viewport-prediction/"},{"categories":["paper"],"content":"论文概况 Link：QoE-driven Mobile 360 Video Streaming: Predictive View Generation and Dynamic Tile Selection Level：ICCC 2021 Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles ","date":"2021-12-16","objectID":"/posts/note-for-rnnqoe/:1:0","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/note-for-rnnqoe/"},{"categories":["paper"],"content":"系统建模与形式化 ","date":"2021-12-16","objectID":"/posts/note-for-rnnqoe/:2:0","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/note-for-rnnqoe/"},{"categories":["paper"],"content":"视频划分 先将视频划分成片段：$\\Iota = {1, 2, …, I}$表示片段数为$I$的片段集合。 接着将片段在空间上均匀划分成$M \\times N$个tile，FOV由被用户看到的tile所确定。 使用ERP投影，$(\\phi_i, \\theta_i),\\ \\phi_i \\in (-180\\degree, 180\\degree], \\theta_i \\in (-90\\degree, 90\\degree]$来表示用户在第$i$个片段中的视点坐标。 播放过程中记录用户头部运动的轨迹，积累的数据可以用于FOV预测。 跨用户之间的FOV轨迹可以用于提高预测精度。 ","date":"2021-12-16","objectID":"/posts/note-for-rnnqoe/:2:1","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/note-for-rnnqoe/"},{"categories":["paper"],"content":"QoE模型 前提 视频编解码器预先确定，无法调整每个tile的码率。 实现 每个tile都以不同的码率编码成不同的版本。 每个tile都有两种分辨率的版本。 QoE内容 客户端收到的视频质量和观看时的卡顿时间。 质量形式化 对于每个片段$i \\in \\Iota$，$S_i = {\\tau_{i, j}}_{j=1}^{M \\times N}$是用来表示用户实际看到的tile的集合的向量。 $\\tau_{i, j} = 1$表示第$i$个段中的第$j$个tile被看到；$\\tau_{i, j} = 0$表示未被看到。 同样的， $\\tilde{S}_i = {\\tilde{\\tau}_{i, j}}_{j = 1}^{M \\times N}$ 表示经过FOV预测和tile选择之后成功被传送到用户头戴设备上的tile集合的向量。 $\\tilde{\\tau}_{i, j} = 1$表示第$i$个段中的第$j$个tile被用户接收；$\\tilde{\\tau}_{i, j} = 0$表示未被接收。 第$i$个段的可感知到的质量可以表示为： $$ Q_i = \\sum_{j = 1}^{M \\times N} p_{i, j}b_{i, j}\\tau_{i, j}\\tilde{\\tau}_{i, j} $$ $b_{i, j}$表示第$i$个片段的第$j$个tile的码率；$p_{i, j}$表示对不同位置tile所分配的权重； 关于权重$p_{i, j}$ 研究表明用户在全景视频FOV中的注意力分配并不是均等的，越靠近FOV中心的tile对用户的QoE贡献越大。 下面讨论单个片段的情况：用$(\\phi_j, \\theta_j)$表示tile中心点的坐标，并映射到笛卡尔坐标系上$(x_j, y_j, z_j)$： $$ x_j = cos\\theta_jcos\\phi_j,\\ y_j = sin\\theta_j,\\ z_j = -cos\\theta_jsin\\phi_j $$ 则两个tile之间的半径距离$d_{j, j’}$可以表示为： $$ d_{j, j’} = arccos(x_j x_{j’} + y_j y_{j’} + z_j z_{j’}) $$ 对于第$i$个片段，假设用户FOV中心的tile为$j^*$，那么第$j$个tile的权重可以计算出来： $$ p_{i, j} = (1 - d_{j, j^*} / \\pi) \\tau_{i, j} $$ 卡顿时间形式化 当$\\tilde{\\tau}_{i, j}$与$\\tau_{i, j}$出现分歧时，用户就不能成功收到请求的tile，头戴设备中显示的内容就会被冻结，由此导致卡顿。 对于任意的片段$i \\in \\Iota$，相应的卡顿时间$D_i$可以计算出来： $$ D_i = \\frac{\\sum_{j = 1}^{M \\times N} b_{i, j} \\cdot [\\tau_{i, j} - \\tilde{\\tau}_{i, j}]^+}{\\xi} $$ $[x]^+ = max \\lbrace x, 0 \\rbrace $；$\\xi$表示可用的网络资源（已知，并且在推流过程中保持为常数） 卡顿发生于在播放时，用户FOV内的tile还没有被传输到用户头戴设备中的时刻，终止于所有FOV内tile被成功传送的时刻。 质量与卡顿时间的结合 $$ max\\ QoE = \\sum_{i = 1}^I (Q_i - wD_i) $$ $w$表示卡顿事件的惩罚权重。例如，w＝1000意味着1秒视频暂停接收的QoE惩罚与将片段的比特率降低1000 bps相同。 ","date":"2021-12-16","objectID":"/posts/note-for-rnnqoe/:2:2","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/note-for-rnnqoe/"},{"categories":["paper"],"content":"联合viewport预测与tile选择 联合框架包括viewport预测和动态tile选择两个阶段。 viewport预测阶段集成带有注意力机制的RNN，接收用户的历史头部移动信息作为输入，输出每个tile出现在FOV中的可能性分布。 选择tile阶段为预测的输出建立的上下文空间，基于上下文赌博机学习算法来选择tile并确定所选tile的质量版本。 ","date":"2021-12-16","objectID":"/posts/note-for-rnnqoe/:3:0","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/note-for-rnnqoe/"},{"categories":["paper"],"content":"Viewport预测 FOV预测问题可以看作是序列预测问题。 不同用户观看相同视频时的头部移动轨迹有强相关性，所以跨用户的行为分析可以用于提高新用户的viewport预测精度。 被广泛使用的LSTM的变体——Bi-LSTM（Bi-directional LSTM）用于FOV预测。 输入参数构造 为了构造Bi-LSTM学习网络，需要对不同用户的viewpoint特性作表征。 在用户观看事先划分好的$I$个片段时，记录每个片段对应的viewpoint坐标： $\\Phi_{1:I} = {\\phi_i}^I_{i = 1},\\ \\Theta_{1:I} = {\\theta_i}^I_{i=1}$ 预测时使用的历史信息的窗口大小记为$k$； 对于第$i, (i \u003e k)$个片段，相应的viewpoint特性由$\\Phi_{i-1:i-k}$和$\\Theta_{i-1:i-k}$所给出； 列索引$m_i$和行索引$n_i$作为viewpoint tile $(\\phi_i, \\theta_i)$的标签，由独热编码表示； 通过滑动预测的窗口，所看到的视频片段的特性和标签可以被获取。 LSTM网络构造 整个网络包含3层： 遗忘门层决定丢弃哪些信息； 更新门层决定哪类信息需要存储； 输出门层过滤输出信息。 为了预测用户在第$i$个段的viewpoint，LSTM网络接受$\\Phi_{i-1:i-k}$和$\\Theta_{i-1:i-k}$作为输入；输出行列索引； $$ m_i = LSTM(\\theta_{i-k}, …, \\phi_{i-1}; \\alpha) $$ $$ n_i = LSTM(\\theta_{i-k}, …, \\theta_{i-1}; \\beta) $$ $\\alpha, \\beta$是学习网络的参数；分类交叉熵被用作网络训练的损失函数。 Bi-LSTM的特殊构造 将公共单向的LSTM划分成2个方向。 当前片段的输出利用前向和反向信息，这为网络提供了额外的上下文，加速了学习过程。 双向的LSTM不直接连接，不共享参数。 每个时间槽的输入会被分别传输到前向和反向的LSTM中，并分别根据其状态产生输出。 两个输出直接连接到Bi-LSTM的输出节点。 引入注意力机制为每步时间自动分配权重，从大量信息中选择性地筛选出重要信息。 将Softmax层堆叠在网络顶部，以获取不同tile的viewpoint概率。 ","date":"2021-12-16","objectID":"/posts/note-for-rnnqoe/:3:1","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/note-for-rnnqoe/"},{"categories":["paper"],"content":"动态tile选择 使用上下文赌博机学习算法来补偿viewport预测错误对QoE造成的影响。 上下文赌博机学习算法概况 上下文赌博机学习算法是一个基于特征的exploration-exploitation技术。 通过在多条手臂上重复执行选择过程，可以获得在不同上下文中的每条手臂的回报。 通过exploration-exploitation，目标是最大化累积的回报。 组成部分形式化 上下文 直觉上讲，当预测的viewpoint不够精确时，需要扩大FOV并选择更多的tile进行传输。 为了做出第$i$个片段上的预测viewpoint填充决策，定义串联的上下文向量： $c_i = [f^s_i, f^c_i]$，$f^s_i$表示自预测的上下文，$f^c_i$表示跨用户之间的预测上下文。 预测输出的用户$u$的viewpoint tile索引用$[\\tilde{m}^u_{i-1}, \\tilde{n}^u_{i-1}]$表示； 实际的用户$u$的viewpoint tile索引用$[m_{i-1}^u, n_{i-1}^u]$表示； 那么对第$i$个片段而言，自预测的上下文可以计算出来： $$ f_i^s = [|m_{i-1}^u - \\tilde{m}^u_{i-1}|, |n_{i-1}^u - \\tilde{n}^u_{i-1}|] $$ 跨用户的上下文信息获取：使用KNN准则选择一组用户，其在前$k$个片段中的轨迹最接近用户$u$的轨迹。 使用$\\zeta$表示获得的用户集合，使用 $$E_{\\zeta_u}(m_i) = \\frac{1}{|\\zeta_u|}\\sum_{u \\in \\zeta_u} |m_i^u - \\tilde{m}_i^u|$$ $$E_{\\zeta_u}(n_i) = \\frac{1}{|\\zeta_u|}\\sum_{u \\in \\zeta_u}|n_i^u - \\tilde{n}_i^u|$$ 表示预测误差，则： $$ f_i^u = [E_{\\zeta_u}(m_i), E_{\\zeta_u}(n_i)] $$ 手臂 根据从第一个阶段得到的每个tile的可能性分布，所有的tile可以用倒序的方式排列。 最高可能性的tile被看作FOV的中心，高码率以此tile为中心分配。 剩余的带宽用于扩展FOV，低可能性的tile被顺序选择来扩展FOV直至带宽耗尽。 手臂的状态$a \\in {0, 1}$表示tile选择的策略： $a = 0$表示viewpoint预测准确，填充tile分配了高质量； $a = 1$表示viewpoint预测不准确，填充tile分配的质量较低，为了传送尽可能多的tile而减少卡顿； 回报 给定上下文$c_i$，选择手臂$a$，预期的回报$r_{i, a}$建模为$c_i$和$a$组合的线性函数： $$ \\Epsilon[r_{i, a}|c_{i, a}] = c_{i, a}^T \\theta_a^* $$ 未知参数$\\theta_a$表示每个手臂的特性，目标是为第$i$个片段选择最优的手臂： $$ a_i^* = \\underset{a}{argmax}\\ c_{i, a}^T \\theta_a^* $$ 使用LinUCB算法做出特征向量的精确估计并获取$\\theta_a^*$。 ","date":"2021-12-16","objectID":"/posts/note-for-rnnqoe/:3:2","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/note-for-rnnqoe/"},{"categories":["paper"],"content":"实验评估 评估准备 使用现有的viewpoint轨迹数据集，所有视频被编码为至少每秒25帧，长度为20到60秒； 视频每个片段被划分为$6 \\times 12$的tile，每个的角度是$30\\degree \\times 30\\degree$； 初始FOV设定为$90\\degree \\times 90\\degree$，在viewpoint周围是$3 \\times 3$的tile； 每个片段的长度为500ms； 默认的预测滑动窗口大小$k = 5$； HD和LD版本分别以按照HEVC的$QP={32, 22}$的参数编码而得到； 训练集和测试集的比例为$7:3$； Bi-LSTM层配置有128个隐单元； batch大小为64； epoch次数为60； 性能参数 预测精度 视频质量 由传送给用户的有效码率决定：例如实际FOV中的tile码率总和 卡顿时间 规范化的QoE 实际取得的QoE与在viewpoint轨迹已知情况下的QoE的比值 对比目标 预测阶段——预测精度 LSTM LR KNN 取tile的阶段——规范化的QoE 两个阶段都使用纯LR 只预测而不做动态选择 ","date":"2021-12-16","objectID":"/posts/note-for-rnnqoe/:4:0","tags":["Immersive Video","RNN","Trajectory-based predict","Dynamic tile selection"],"title":"Note for RnnQoE","uri":"/posts/note-for-rnnqoe/"},{"categories":["paper"],"content":"论文概况 Link：OpTile: Toward Optimal Tiling in 360-degree Video Streaming Level：ACM MM 17 Keyword：Dynamic tile division, Optimize encoding efficiency, Optimize tile size ","date":"2021-12-13","objectID":"/posts/note-for-optile/:1:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"背景知识 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:2:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"编码过程概述 对一帧图像中的每一个 block，编码算法在当前帧的已解码部分或由解码器缓冲的临近的帧中搜索类似的 block。 当编码器在邻近的帧中找到一个 block 与当前 block 紧密匹配时，它会将这个类似的 block 编码进一个动作向量中。 编码器计算当前 block 和引用 block 之间像素点的差异，通过应用变换（如离散余弦变换），量化变换系数以及对剩余稀疏矩阵系数集应用无损熵编码（如 Huffman 编码）对计算出的差异进行编码。 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:2:1","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"对编码过程的影响 基于 tile 的方式会减少可用于拷贝的 block 数量，增大了可供匹配的 tile 之间的距离。 不同的投影方式会影响编码变换输出的系数稀疏性，而这会降低视频编码效率。 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:2:2","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"投影过程 因为直接对 360 度图像和视频的编码技术还没有成熟，所以 360 度推流系统目前还需要先将 3D 球面投影到 2D 平面上。 目前应用最广的投影技术主要是 ERP 和 CMP，分别被 YouTube 和 Meta 采用。 ERP 投影 基于球面上点的左右偏航角$\\theta$与上下俯仰角$\\phi$将其映射到宽高分别为$W$和$H$的矩形上。 对于平面坐标为$(x, y)$的点，其球面坐标分别为： $$ \\theta = (\\frac{x}{W} - 0.5) * 360 $$ $$ \\phi = (0.5 - \\frac{y}{H}) * 180 $$ CMP 投影 将球面置于一个立方体中，光线从球心向外发射，并分别与球面和立方体相交于两点，这两点之间便建立了映射关系。 之后将立方体 6 个平面拼接成矩形，就可以使用标准的视频编码方式进行压缩。 关于投影方式还可以参考这里的讲解：谈谈全景视频投影方式 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:2:3","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"tile 方式的缺点 降低编码效率 tile 划分越细，编码越低效 增加更大的整体存储需求 可能要求更多的带宽 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:2:4","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"OpTile 的设计 直觉上需要增大一些 tile 的大小来使与这些 tile 相关联的片段能捕获高效编码所需的类似块。 同时也需要 tile 来分割视频帧来减少传输过程中造成的带宽浪费。 为了明白哪些片段的空间部分可以被高效独立编码，需要关于 tile 的存储大小的不同维度的信息。 为了找到切分视频的最好位置，需要在片段播放过程中用户 viewport 运动轨迹的偏好。 将编码效率和浪费数据的竞争考虑到同一个问题之中，这个问题关注的是一个片段中所有可能的视图的分布。 片段的每个可能的视图可以被 tile 的不同组合所覆盖。 目标是为一个片段选择一个 tile 覆盖层，以最小化固定时间段内视图分布的总传输带宽。 目标分离的部分考虑整个固定时间段的表示（representation）的存储开销。 目标的存储部分与下载的带宽部分相竞争。例如，如果一个不受欢迎的视频一年只观看一次，那么我们更喜欢一个紧凑的表示，我们可以期望向用户发送更多未观看的像素。 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:3:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"问题形式化 segment/片段 推流过程中可以被下载的连续播放的视频单元 basic sub-rectangle/基本子矩形 推流过程中可以被下载的片段中最小的空间划分块 solution sub-rectangle/解子矩形 片段中由若干基本子矩形组成的任何矩形部分 $x$ 用于表示子矩形在解中的存在的二元向量 $c^{(stor)}$ 每个子矩形存储开销相关的向量 $c^{(view)}$ 给定一个 segment 中用户 viewport 的分布，$c^{(view)}$指相关子矩形的预期下载字节 $\\alpha$ 分配到$c^{(view)}$的权重，以此来控制相对于传输一个片段的存储开销 考虑将 1 个矩形片段划分成 4 个基本子矩形，其对应的坐标如下： 4 个基本子矩形可以有 9 种分配方式，成为解子矩形，如下（因为需要保持对应的空间关系，所以只有 9 种）： $x$的形式化 可以用向量$x$来分别表示上图中子矩形在解中的存在： $$ [1 \\times 1\\ at\\ (0, 0),\\ 1 \\times 1\\ at\\ (0, 1),\\ 1 \\times 1\\ at\\ (1, 0), \\ 1 \\times 1\\ at\\ (1, 1),\\ 1 \\times 2\\ at\\ (0, 0),\\ 1 \\times 2\\ at\\ (1, 0), \\ 2 \\times 1\\ at\\ (0, 0),\\ 2 \\times 1\\ at\\ (0,1),\\ 2 \\times 2\\ at\\ (0,0).] $$ （$x$中每个二元变量的的组成：$1 \\times 1$表示子矩形的形状，$(0,0)$表示所处的位置） 要使$x$有效，每个基本子矩形必须被$x$中编码的子矩形精确覆盖一次。例如： $[0, 0, 0, 0, 1, 1, 0, 0, 0]$=\u003e有效（第 5 和第 6 次序的位置分别对应$e$和$f$子矩形，恰好覆盖了所有基本子矩形 1 次） $[0,0,0,1,1,0,0,0,0]$=\u003e无效（第 4 和第 5 次序的位置分别对应$d$和$e$子矩形，没有覆盖到$(1,0)$基本子矩形） $[0,0,0,1,1,1,0,0,0]$=\u003e无效（第 4、第 5 和第 6 次序的位置分别对应$d$、$e$和$f$子矩形，$(1,1)$基本子矩形被覆盖了两次） $c^{(stor)}$的形式化 与每个$x$相对应的向量$c^{(stor)}$长度与$x$相等，其中每个元素是$x$中对应位置的子矩形的存储开销的估计值。 $c^{(view)}$的形式化 考虑分发子矩形的网络带宽开销时，需要考虑所有可能被分发的 360 度表面的视图。 为了简化问题，将片段所有可能的视图离散化到一个大小为$V$的集合中。 集合中每个元素表示一个事件，即向观看 360 度视频片段的用户显示基本子矩形的唯一子集。 注意到片段中被看到的视频区域可以包含来自多个视角的区域。 将之前离散化好的大小为$V$的集合中每个元素与可能性相关联：$[p_1, p_2, …, p_V]$。 考虑为给定的解下载视图$V$的开销，作为需要为该视图下载的数据量： $$ quantity = x^{\\top}diag(d_v)c^{(stor)} $$ $d_v$是一个二元向量，其内容是按照$x$所描述的表示方案，对所有覆盖视图的子矩形的选择。 例如对于 ERP 投影中位置坐标为$yaw = 0, pitch = 90$即处于等矩形顶部的图像，对应的$d_{view-(0, 90)} = [1, 1, 0, 0, 1, 0, 1, 1, 1]$ （即上面图中$a, b, e, g, h, i$位置的子矩形包含此视图所需的基本子矩形）。 给出一个片段中的用户 viewport 分布，$c^{(view)}$的元素是相关联的子矩形预期的下载字节。 $$ c^{(view)} = \\sum_v p_v diag(d_v) c^{(stor)} $$ 最后，将优化问题的基本子矩形覆盖约束编码为矩阵$A$。 $A$是一个列中包含给定子矩形解所覆盖的基本子矩形信息的二元矩阵。 对于$2 \\times 2$的矩形片段，其$A$有 4 行 9 列，例子如下： 因此最终的问题可以形式化为一个整数线性程序： $c^{(stor)}$ 可以理解为存储一段$\\Delta t$时间长的片段的子矩形的存储开销； $c^{(view)}$ 可以理解为传输一个视图所需要的所有的子矩形的传输开销。 $\\alpha$ 控制相比于传输一个片段的相对存储开销，同时应该考虑片段的流行度。 即$\\alpha$应该与所期望的片段在$\\Delta t$的时间间隔内的下载次数成比例，$\\alpha$应该可以通过经验测量以合理的精度进行估计。 可以通过将$x$的二元离散限制放松到$0 \\le x_i \\le 1\\ \\forall i$构成一个线性程序，其解为整数。 （对于有 33516 个变量的$x$，其解可以在单核 CPU 上用 7~10 秒求出） ","date":"2021-12-13","objectID":"/posts/note-for-optile/:4:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"开销向量建构 首先需要建构出存储开销向量$c^{(stor)}$，但是对于有$n$个基本子矩形的子矩形，其建构复杂度为$O(n^2)$。 因此对每个子矩形进行编码来获得存储开销并不可行，所以利用视频压缩与运动估计之间的强相关性来预测$c^{(stor)}$的值。 给定一个视频，首先暂时将其分成长度为 1 秒的片段，每个片段被限定为只拥有 1 个 GOP，片段的大小表示为$S_{orig}$。 接着抽取出每个片段中的动作序列用于之后的分析。 将片段从空间上划分成基本子矩形，每个基本子矩形包含$4 \\times 4 = 16$个宏块（例如：$64 \\times 64$个像素点）。 独立编码每个基本子矩形，其大小表示为$S_i$。 通过分析动作向量信息，可以推断出如果对基本子矩形$i$进行独立编码，指向基本子矩形$i$的原始运动向量应该重新定位多少。 将其表示为$r_i$。 每个运动向量的存储开销可以计算为： $$ o = \\frac{\\sum_i S_i - S_{orig}}{\\sum_i r_i} $$ 即：存储开销的整体增长除以被基本子矩形边界所分割的运动向量数。 如果基本子矩形被融合进更大的子矩形$t$，使用$m_t$来表示由于融合操作而无须再进行重定位的运动向量的数量： $$ m_t = \\sum_{i \\in t} r_i - r_t $$ $i \\in t$表示基本子矩形位于子矩形$t$中。 为了估计任意子矩形$t$的大小，使用下面 5 个参数： $$ \\sum_{i \\in t} S_i,\\ \\sum_{i \\in t} r_i,\\ m_t,\\ o,\\ n $$ $n$表示$t$中基本子矩形的数量。 实际操作： 创建了来自 4 个单视角 360 度视频的 6082 个 tile 数据集。4 个视频都以两种分辨率进行编码：$1920 \\times 960$和$3980 \\times 1920$。 为了产生 tile，从视频中随机选取片段，随机选取 tile 的位置，宽度和高度。 设置 tile 的 size 最大为$12 \\times 12$个基本子矩形。 对于每个选择的 tile，为其建构一个数据集元素： 计算上面提到的 5 参数的特性向量。 使用 FFmpeg 编码 tile 的视频段来得到存储该段需要的空间。 使用多层感知机 MLP 来估计 tile 的大小。 MLP 中包含 50 个节点的单隐层，激活函数为 ReLU 函数，300 次迭代的训练过程使用L-BFGS 算法。 为了评估 MLP 的预测效果，使用 4 折的交叉验证法。 每次折叠时先从 3 个视频训练 MLP，接着使用训练好的模型去预测第 4 个视频的 tile 大小。 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:5:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"实现 将视频划分成1秒长的片段，之后为每个片段解决整数线性问题来确定最优的tile划分策略。 使用MLP模型估计每个tile的存储开销。 根据视图的集合$d$及其对应的可能性分布$p$，来估计视图的下载开销$c^{(view)}$。 构造矩阵$A$时，限制最大的tile大小为$12 \\times 12$的基本子矩形（如果设置每个基本子矩形包含$64 \\times 64$的像素，tile的最大尺寸即为$768 \\times 768$的像素）。 使用GNU Linear Programming Kit来解决问题。 将所有可能的解子矩形编码进一个二元向量$x$中来表示解。 GLPK的解表明一个可能的解子矩形是否应该被放入解中。 基于最终得到的解，划分片段并使用ffmepg以同样参数的x264格式进行编码。 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:6:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"评估 度量指标 服务端存储需求。 客户端需要下载的字节数。 数据来源 数据集：dash.ipv6.enstb.fr 评估准备 下载5个使用ERP投影的视频，抽取出测试中用户看到的对应部分。 每个视频都有$1920 \\times 960$和$3840 \\times 1920$的两种分辨率的版本。 $1920 \\times 960$视频的基本子矩形尺寸为$64 \\times 64$的像素。 $3840 \\times 1920$视频的基本子矩形尺寸为$128 \\times 128$的像素。 将视频划分成1秒长的片段，对每个片段都产生出MLP所需的5元组特性。 之后使用训练好的MLP模型来预测所有可能的tile的大小。 数据选择 从数据集中随机选择出40个用户的集合。 假设100°的水平和垂直FOV，并使用40个用户的头部方向来为每个片段产生$p_v$和$d_v$。 即：分块的决策基于每个片段的内容特征信息与用户的经验视图模式。 参数设定：$\\alpha = 0,1,1000$. 对比实验： 一组使用由ILP得出的结构进行分块； 另外一组： $1920 \\times 960$的视频片段分别使用$64 \\times 64$，$128 \\times 128$，$256 \\times 256$，$512 \\times 512$的方案固定大小分块。 $3840 \\times 1920$的视频片段分别使用$128 \\times 128$，$256 \\times 256$，$512 \\times 512$，$1024 \\times 1024$的方案固定大小分块。 划分结果对比 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:7:0","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"服务端的存储大小 按照$\\alpha = 0$方案分块之后的视频大小几乎与未分块之前的视频大小持平，有时甚至略微小于未分块前的视频大小。 因为所有分块方案都使用相同的编码参数，所以重新编码带来的有损压缩并不会影响竞争的公平性。 如果将$\\alpha$的值调大，存储的大小会略微增大；固定分块大小的方案所得到的存储大小也会随tile变小而变大。 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:7:1","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["paper"],"content":"客户端的下载大小 预测完美的情况——下载的tile没有任何浪费 $\\alpha= 1000$的情况下，OpTile的表现总是最好的。 正常预测的情况 预测的方法：假设用户的头部方向不会改变，预测的位置即为按照当前方向几秒之后的位置。 相比于完美假设的预测，所有分块方案的下载大小都增大了。 $\\alpha = 1000$的方案在两个视频的情况下都取得了最小的下载大小。在剩下的3个视频中，OpTile方案的下载大小比起最优的固定分块大小方案不超过25%。 尽管固定分块大小的方案可能表现更好，但是这种表现随视频的改变而变化显著。 因为固定分块的方案没有考虑视频内容的特性与用户的观看行为。 ","date":"2021-12-13","objectID":"/posts/note-for-optile/:7:2","tags":["Immersive Video","Content-based division","Dynamic tiling"],"title":"Note for OpTile","uri":"/posts/note-for-optile/"},{"categories":["knowledge"],"content":"媒体处理过程 ","date":"2021-12-13","objectID":"/posts/mm-base/:0:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["knowledge"],"content":"解协议 将流媒体传输方案中要求的数据解析为标准的相应封装格式数据。 音视频在网络中传播时需要遵守对应的传输方案所要求的格式，如 DASH、HLS 将媒体内容分解成一系列小片段，每个片段有不同的备用码率版本。 同时应用层的协议会要求在媒体文件本身之外，传输信令数据（如对播放的控制或网络状态的描述） 解协议的过程会去除信令数据并保留音视频内容，需要的话还要对视频段进行拼接，最终将其还原成传输之前的媒体格式如 MP4，FLV 等。 ","date":"2021-12-13","objectID":"/posts/mm-base/:1:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["knowledge"],"content":"封装格式 封装格式如 AVI、MPEG、Real Video 将音频和视频组合打包成一个完整的文件. 封装格式不会影响视频的画质，影响画质的是视频的编码格式。 解封装过程就是将打包好的封装格式分离成某种编码的音频压缩文件和视频压缩文件，有时也包含字幕和脚本。 比如 FLV 或 TS 格式数据，解封装之后得到 H.264-AVC 编码的视频码流和 AAC 编码的音频码流。 ","date":"2021-12-13","objectID":"/posts/mm-base/:2:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["knowledge"],"content":"编码 视频的本质是一帧又一帧的图片。 所以对于一部每秒 30 帧，90 分钟，分辨率为 1920x1080，24 位的真彩色的视频，在压缩之前的大小$S$满足： $$ 一帧大小s = 1920 * 1080 * 24 = 49766400(bit) = 6220800(Byte) \\ 总帧数n = 90 * 60 * 30 = 162000 \\ 总大小S = s * n = 6220800 * 162000 = 1.0077696*10^{12}(Byte) \\approx 939(GB) $$ 因为未经压缩的视频体积过于庞大，所以需要对其进行压缩，而压缩就是通常所说的编码。 视频编码方式：H.264-AVC，H.265-HEVC，H.266-VVC 音频编码方式：MP3，AAC 压缩比越大，解压还原之后播放的视频越失真，因为压缩过程中不可避免地丢失了视频中原有图像的数据信息。 ","date":"2021-12-13","objectID":"/posts/mm-base/:3:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["knowledge"],"content":"解码 解码就是解压缩过程。 解码之后能够得到系统音频驱动和视频驱动能识别的音频采样数据（如 PCM 数据）和视频像素数据（如 YUV420，RGB 数据）。 ","date":"2021-12-13","objectID":"/posts/mm-base/:4:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["knowledge"],"content":"音视频同步 根据时间，帧率和采样率采用一定的算法，同步解码出来的音频和视频数据，将其分别送至声卡和显卡播放。 视频质量指标 ","date":"2021-12-13","objectID":"/posts/mm-base/:5:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["knowledge"],"content":"分辨率 分辨率指的是视频图像在一个单位尺寸内的精密度。 将视频放大足够大的倍数之后就能看到组成影像的基本单元：像素。 视频的分辨率从数值上描述了像素点的个数，如 1920x1080：视频在水平方向有 1920 个像素，垂直方向有 1080 个像素。 常见的描述方式： 1080P：指视频有1080 行像素，P=\u003eProgressive（逐行扫描） 2K：指视频有2000 列像素 MP：像素总数，指像素的行数 P 与列数 K 乘积的结果（百万像素） 1080P 的分辨率为 1920x1080=2073600，所以 1080P 通常也称为 200 万像素分辨率 通常视频在同样大小的情况下，分辨率越高，所包含的像素点越多，画面就越细腻清晰 参考链接： 科普：视频分辨率是什么？ 「1080p」和「2k、4k」的关系与差别在哪里？ ","date":"2021-12-13","objectID":"/posts/mm-base/:6:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["knowledge"],"content":"视频帧率 帧率的单位 FPS(Frame Per Second)或 Hz，即每秒多少帧，决定视频画面的流畅程度。 低帧率会导致播放卡顿，镜头移动不顺畅，并伴随画面模糊的主观体验； 帧率过高则会造成眩晕的感觉。 不同帧率的视频在支持不同帧率的设备上播放： 若设备最高支持 60fps，则播放 120fps 视频的时候，设备会每隔一帧删除一帧，被删除的帧即成为无效帧。 所以高帧率的视频在低帧率设备上播放时会导致播放卡顿。 若设备最高支持 120fps，则播放 60fps 视频的时候，设备会每隔一帧复制一帧，来填补空缺的帧位置。 但是效果和在 60fps 上的设备播放一样，不能提升播放流畅度。 关于显卡对帧率的影像： 显示器帧率低而显卡输出帧率高时，会导致画面撕裂：显示器同时将两帧或几帧显示在同一个画面上 显示器帧率高而显卡输出帧率低时，同视频帧率高显示器帧率低的情况。 ","date":"2021-12-13","objectID":"/posts/mm-base/:7:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["knowledge"],"content":"视频码率 码率的概念出现在视频编码之后，因为压缩之后的视频已经成为二进制数据，所以使用码率的称呼。 码率的单位是 bps(bit per second)，即每秒多少比特。 与视频质量的关系： 分辨率不变的情况下，码率越大，压缩比越好，画面质量越清晰。 码率越高，精度越高，处理出的文件就越接近压缩前的原始状态，每一帧的图像质量越高，画质越清晰，当然对播放设备的解码能力要求也越高。 压缩比越小，视频体积越大，越接近源文件。 ","date":"2021-12-13","objectID":"/posts/mm-base/:8:0","tags":["Multimedia"],"title":"多媒体基础知识","uri":"/posts/mm-base/"},{"categories":["paper"],"content":"论文概况 Level：IEEE Transaction on multimedia 21 Keyword：Rainbow-DQN, Multi-type tiles, Full streaming system ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:1:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"问题形式化 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:2:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"模型 原始视频用网格划分成$N$块tile，每个tile都被转码成$M$个不同的质量等级$q_i$。 基于传输控制模块得出的结果，播放器请求$t_i$个tile的$q_i$质量的版本并将其存储在缓冲区中，对应的缓冲区大小为$l_i$。 用户Viewport的信息用$V$表示，可以确定FOV的中心。 根据$V$可以将tile划分成3种类型：FOV、OOS、Base。 FOV中的tile被分配更高的码率； OOS按照与$V$的距离逐步降低质量等级$q_i$； Base总是使用低质量等级$q_{Base}$但使用完整的分辨率。 传输的tile在同步完成之后交给渲染器渲染。 播放器根据各项指标计算可以评估播放性能： $\u003cV, B, Q, F, E\u003e$：viewport信息$V$，网络带宽$B$，FOV质量$Q$，重缓冲频率$F$，传输效率$E$。 传输控制模块用于确定每个tile的质量等级$q_i$和缓冲区大小$l_i$。 传输控制模块优化的最终目标是获取最大的性能： $$ performance = E_{max},\\ QoE \\in accept\\ range $$ ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:2:1","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"带宽评估 收集每个tile的下载日志来评估带宽。 使用指数加权移动平均算法EWMA使评估结果光滑，来应对网络波动。 第$t$次评估结果使用$B_t$表示，用下式计算： $$ B_t = \\beta B_{t-1} + (1-\\beta)b_t $$ $b_t$是B的第$t$次测量值；$\\beta$是EWMA的加权系数。 $t=0$时，$B_0$被初始化为0；所以在初始的$t$比较小的时候，$B_t$与理想值相比就很小。 这种影响会随着$t$增大而减少。 为了优化启动过程，对公式做出修改： $$ B_t = \\frac{\\beta B_{t-1} + (1-\\beta)b_t}{1 - \\beta^t} $$ $t$较小的时候，分母会放大$B_t$；$t$较大时，分母趋近于1，影响随之消失。 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:2:2","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"FOV表示和预测 3D虚拟相机用于渲染视频，处于全景视频球面上的某条轨道，其坐标可以表示为$(\\theta, \\phi)$，可以直接从系统中获取。 相机始终朝向球的中心，所以用户的FOV中心坐标$(\\theta^{’}, \\phi^{’})$可以用$(\\theta, \\phi)$表示： $$ \\begin{cases} \\theta^{’} = (\\theta + \\pi)\\ mod\\ 2\\pi,\\ 0 \\le \\theta \\le 2\\pi \\ \\phi^{’} = \\pi - \\phi,\\ 0 \\le \\phi \\le \\pi \\end{cases} $$ 2D网格中tile坐标$(u, v)$可以通过球面坐标使用ERP投影获得 $$ \\begin{cases} u = \\frac{\\theta^{’}}{2\\pi} \\cdot W, 0 \\le u \\le W. \\ v = \\frac{\\phi^{’}}{\\pi} \\cdot H, 0 \\le v \\le H. \\end{cases} $$ $W$和$H$分别表示使用ERP投影得到的矩形宽度和高度 短期的FOV预测基于目前和历史的FOV信息。 使用$(U_t, V_t)$表示$t$时刻的FOV中心位置；$U_{t1:t2}$和$V_{t1:t2}$分别表示从$t1$到$t2$过程中$U$和$V$的序列； $$ \\begin{cases} \\hat{U}{t+T_f} = f_U (U{t-T_p:t}). \\ \\hat{V}{t+T_f} = f_V (V{t-T_p:t}). \\end{cases} $$ $T_p$是过去记录的滑动窗口；$T_f$是短期的预测窗口；$f_U$和$f_V$分别对应$U$和$V$方向上的映射函数； 因为是时间序列回归模型，所以映射函数使用LSTM。 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:2:3","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"QoE评估 QoE由3个部分组成：平均FOV质量$Q$、重缓冲频率$F$与FOV内tile的质量变化（因为平均分配所以不考虑）。 FOV质量$Q$ 第$t$次的FOV质量评估表示为$Q_t$： $$ Q_t = \\frac{\\beta Q_{t-1} + (1-\\beta) \\frac{1}{k} \\cdot \\sum_{j=1}^{k} max{q_j, q_b}}{1 - \\beta^t} $$ $q_j$表示第$j$条FOV tile流的质量；$k$表示FOV内tile的数量； 为了避免评估结果的大幅波动，使用了EWMA来光滑结果。 当第$j$条tile流因为缓冲区不足不能成功播放时，$q_j = q_{Base}$（这表明了Base tile在提高QoE中的作用）。 重缓冲频率$F$ 在基于tile的传输中，每条流都属于一个缓冲区。所以当FOV中tile的缓冲区处于饥饿状态时，重缓冲就会发生。 重缓冲频率描述了FOV内的tile流在一段时间内的重新缓冲频率。 第$t$次重缓冲频率的评估表示为$F_t$： $$ F_t = \\frac{\\beta F_{t-\\tau} + (1-\\beta) \\frac{f_t}{\\tau}}{1 - \\beta^{\\tau}} $$ $f_t$表示播放失败的次数；$\\tau$表示一段时间； ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:2:4","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"传输效率评估 第$t$次传输效率评估表示为$E_t$，$E_t$通过传输的FOV内tile占总tile的比率来计算： $$ E_t = \\frac{\\beta E_{t-1} + (1-\\beta) \\frac{total^{FOV}}{total^{ALL}}}{1 - \\beta^t} $$ $total^{FOV}$表示FOV内tile的数据量；$total^{ALL}$表示tile的总共数据量； 效率计算并不在传输过程中完成，因为需要获取哪些tile在FOV中的信息，效率评估滞后于播放过程。 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:2:5","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"问题形式化 传输控制的任务：确定所有tile流的质量等级$\\chi$和缓冲区大小$\\psi$。 $$ \\chi = \u003cq_1, q_2, …, q_N\u003e \\ \\psi = \u003cl_1, l_2, …, l_N\u003e \\ \u003cQ, F, E\u003e = \\xi (B, V, \\chi, \\psi) $$ $\\chi$和$\\psi$与带宽$B$和Viewport轨迹$V$一起作用于系统$\\xi$，最终影响FOV质量$Q$，重缓冲频率$F$和传输效率$E$。 进一步，将目标形式化为获得每条tile流的$q_i$和$l_i$通过限制QoE满足可接受的范围、在此基础上最大化传输效率： $$ \\underset{\\chi, \\psi}{argmax} \\sum_{t=0}^{+\\infty} E_t, $$ $$ s.t.:\\ 0 \\le q_i \\le M, $$ $$ 0 \\le l_i \\le L, $$ $$ Q^{min} \\le Q_t \\le M, $$ $$ 0 \\le F_t \\le F^{max}. $$ $q_i$和$l_i$分别受限于质量版本数$M$和最大缓冲区大小$L$； $Q_t$受限于最低QoE标准$Q^{min}$； $F_t$受限于最大能忍受的重缓冲频率$F^{max}$。 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:2:6","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"系统架构 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:3:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"服务端 将原始视频转码为有不同比特率的多个版本。 转码后的视频被划分成多个tile。 传输协议使用MPEG-DASH。 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:3:1","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"客户端 评估器 任务：获取 QoE、FOV预测、传输效率、网络带宽 组成： QoE评估器：评估当前FOV质量=\u003eQ和重缓冲频率=\u003eF（近似为Q+F=QoE） FOV预测器：基于历史FOV信息预测短期未来的FOV=\u003eP 根据下载和播放日志：计算传输效率=\u003eE并估计带宽=\u003eB 控制器 任务：控制传输过程中的推流 目标：保证QoE在可接受的范围之内、最大化传输效率 详细：基于FOV预测将tile划分成3种类型：FOV、OOS、Base 输入：Q、F、E、B（QoE+传输效率和带宽） 过程：Rainbow-DQN 输出：决定每个tile流的码率和缓冲区大小（作为下载器的输入） 下载器 输入：tile码率和缓冲区大小 过程：基于HTTP/2进行并行下载 输出：下载好的tile 视频缓冲区 任务：解码、同步、存储下载好的tile等待渲染器消耗，大小供控制器调节 随着FOV的切换缓冲区内容可能被循环利用 全景渲染器 任务：将同步好的tile拼接，tile质量：FOV\u003eOOS\u003eBase 投影方式：ERP ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:3:2","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"控制器 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:4:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"控制过程 设定QoE的可接受范围。 将网络带宽和用户FOV设定为外部因素而非环境 为什么：因为这两个因素变化太快，在面对不同传输条件时，直接作为环境会导致决策过程的不稳定性并且难以收敛。 最优化的对象只是最大化累积的传输效率。 为什么：简单 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:4:1","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"tile聚合和决策 tile分类原则： 控制器无需为每个tile独立决定码率Q和缓冲区大小L FOV内的tile应该被分配相近的码率，FOV内的tile应该聚集成一组，OSS和Base同理 为什么：避免相邻tile的锐利边界，只考虑3组而非所有tile降低了计算复杂性和决策延迟 （能否实现独立的tile码率计算或更细粒度的划分值得调研？与内容感知的方案结合？） 基于距离的tile分类实现方式： 使用评估器预测出的FOV坐标来分类FOV和OOS的tile tile出现在未来FOV的可能性由距离计算 tile中心点坐标$(\\omega_i, \\mu_i)$、FOV坐标$(\\hat{U}, \\hat{V})$ 距离的变化区间内存在一个临界点，临界点之内的划分为FOV，之外的划分为OOS 度量距离的方式： $$ \\Delta Dis_U = min{|\\omega_i - \\hat{U}|, |1+\\omega_i - \\hat{U}|} $$ （这里为何不直接使用$|\\omega_i - \\hat{U}|$？） $$ Dis_i = \\begin{cases} {\\sqrt{({\\Delta Dis_{U}})^2 + {(\\mu_i - \\hat{V})}^2},\\ \\frac{R}{H} \\le \\hat{V} \\le 1 - \\frac{R}{H}} \\ {\\Delta Dis_U + |\\mu_i - \\hat{V}|,\\ Others} \\end{cases} $$ 因为ERP的投影方式会在两级需要更多的tile，因此使用一个矩形来代表两极的FOV （可以深入调研ERP在两极处的处理方式） $Dis_i$使用曼哈顿距离来测量。临界点初始化为$2\\cdot R$，并随着FOV中心和两极的垂直距离增长。 FOV看作是半径为R的圆，使用欧式距离测量。临界点初始化为$R$ 聚合tile的决策 使用2个变量：$K$作为FOV和非FOV的tile的带宽分配比率；$Len$作为tile缓冲区的大小。 $K$确定之后，分配给FOV内tile的带宽被均匀分配（可否非均匀分配） $K$不直接与网络状况相关因此可以保持控制的稳定性 $Len$：所有传输的tile的缓冲区长度$l_i$都被设为$Len$ （文中并没有这样做的原因解释） ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:4:2","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"基于DRL的传输控制算法 相关术语解释：Rainbow DQN、RL Dictionary、PER、TD-Error 控制过程 首先调整buffer长度Len，并划分FOV与非FOV的带宽分配。 等viewport预测完成之后，tile被分类为属于FOV和OOS的tile。 FOV的带宽被平均分给其中每一个tile并决定FOV内tile的质量等级$q_i$。 非FOV的带宽按照与FOV的距离分配，每超过一个距离单位$Dis_i$就降低一级质量$q_i$。 最终的输出是请求序列，每个请求序列中包括质量等级$q_i$和预期的缓冲区大小$l_i$。 根据输出做出调整之后，接收奖励反馈并不断完成自身更新。 状态设计 状态设计为5元组：$\u003cK, Len, Q, F, E\u003e$（传输控制参数$K$，$Len$、QoE指标：FOV质量Q和重缓冲频率$F$、传输效率$E$） 没有直接使用带宽$B$和viewport轨迹$V$，因为： 随机性强与变化幅度较大带来的不稳定性（如何定义随机性强弱和变化幅度大小？） 希望设计的模型有一定的通用性，可以与不同的网络情况和用户轨迹相兼容 动作设计 两种动作：调整$K$和$Len$（两者的连续变化区间被离散化，调整的每一步分别用$\\Delta k$和$\\Delta l$表示） 调整的方式被形式化为二元组：$\u003cn_1, n_2\u003e$，$n_1$和$n_2$分别用于表示$K$和$Len$的调整 -n 0 n K 减少n$\\Delta k$ 不变 增加n$\\Delta k$ Len 减少n$\\Delta l$ 不变 增加n$\\Delta l$ 奖励函数 因为QoE的各项指标权重难以确定，没有使用传统的基于加权的方式。 设定了能接受的QoE范围和在此基础上最大化的传输效率作为最后的性能指标，形式化之后如下： $$ Reward = \\begin{cases} -INF,\\ F \\ge F^{max} \\ -INF,\\ E \\le E^{min} \\ E,\\ Others \\end{cases} $$ $-INF$意味着终止当前episode；动作越能使系统满足高QoE的同时高效运行，得分越高； 为了最大化传输效率，使用$E$作为奖励回报。 FOV质量$Q$并没有参与到奖励函数中，因为：高Q意味着高性能，但是低Q不一定意味着低性能，详细解释如下： 在带宽不足的情况下，低Q可能已经是这种条件下的满足性能的最好选择。 高传输效率意味着传输了更多的FOV数据，也能满足高FOV质量的目标。 模型设计 基于Rainbow-DQN模型： 输入是5元组$\u003cK, Len, Q, F, E\u003e$。 神经网络使用64维的3隐层模型。 为了提高鲁棒性，神经网络的第3层使用Dueling DQN的方式，将Q值$Q(s, a)$分解为状态价值$V(s)$和优势函数$A(s,a)$： $$ Q(s, a) = V(s) + A(s, a) $$ $V(s)$表示系统处于状态$s$时的性能；$A(s,a)$表示系统处于状态$s$时动作$a$带来的性能； 为了避免价值过高估计，使用Double DQN的方式，设计了两个独立的神经网络：评估网络和目标网络。 评估网络用于动作选择；目标网络是评估网络从最后一个episode的拷贝用于动作评估。 为了缓解神经网络的不稳定性（更快收敛），使用大小为$v$的回放池来按照时间序列保存客户端的经验。 因为网络带宽和FOV轨迹在短期内存在特定的规律性，回放池中有相似状态和相似采样时间的样本更加重要，出现了优先级 所以使用优先经验回放PER，而优先级使用时间查分误差TD-error定义 $$ \\delta_i = r_{i+1} + \\gamma Q(s_{i+1}, arg\\underset{a}{max}Q(s_{i+1}, a; \\theta_i); \\theta_i^{’}) - Q(s_i, a_i; \\theta_i) $$ $r_i$是奖励；$\\gamma$是折扣因子 损失函数使用均方误差定义 $$ J = \\frac{1}{v} \\sum_{i=1}^{v} \\omega_i(\\delta_i)^2 $$ $\\omega_i$是回放缓冲中第i个样本的重要性采样权重 ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:4:3","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"实验验证 环境设定 传输控制模块：基于TensorForce（配置教程：用TensorForce快速搭建深度强化学习模型）； 开发工具集：OpenAI Gym 数据来源：使用全景视频播放设备收集，加入高斯噪声来产生更多数据。 结果分析 与其他DQN算法的对比——DQN、Double DQN、Dueling DQN 对比训练过程中每个episode中的最大累计奖励：$MAX_{reward}$ 对比模型收敛所需要的最少episode：$MIN_{episode}$ 相同的带宽和FOV轨迹 与其他策略对比性能——高QoE和高传输效率 随机控制策略：随机确定K和Len 固定分配策略：固定K和Len的值 只预测Viewport策略：使用LSTM做预测，不存在OSS与Base，所有带宽都用于FOV 带宽和FOV轨迹的均值和方差相等 与其他全景视频推流系统的对比 DashEntire360：使用Dash直接传送完整的360度视频，使用线性回归来估计带宽并动态调整视频比特率 360ProbDash：在DashEntire360的基础上划分tile基于Dash传输，使用可能性模型为tile分配比特率 DRL360：使用DRL来优化多项QoE指标 实现三种系统、使用随机网络带宽和FOV轨迹。 使用DRL360中提出的方式测量QoE： $$ V_{QoE} = \\eta_1 Q - \\eta_2 F - \\eta_3 A $$ $A$是viewport随时间的平均变化，反映FOV质量Q的变化； $\\eta_1, \\eta_2, \\eta_3$分别是3种QoE指标的非负加权，使用4种加权方式来训练模型并对比： $\u003c1, 1, 1\u003e$，$\u003c1, 0.25, 0.25\u003e$，$\u003c1, 4, 1\u003e$，$\u003c1,1,4\u003e$ 在不同环境下的性能评估——带宽是否充足、FOV轨迹是否活跃（4种环境） ","date":"2021-12-11","objectID":"/posts/note-for-rainbowdqn-tiles/:5:0","tags":["Immersive Video","DRL","Rainbow-DQN"],"title":"Note for RainbowDQN and Multitype Tiles","uri":"/posts/note-for-rainbowdqn-tiles/"},{"categories":["paper"],"content":"论文概况 Link: 360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming Level: ACM MM 17 Keyword: Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation ","date":"2021-12-09","objectID":"/posts/note-for-360probdash/:1:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/note-for-360probdash/"},{"categories":["paper"],"content":"工作范围与目标 应用层-\u003e基于tile-\u003eviewport预测的可能性模型+预期质量的最大化 针对小buffer提出了target-buffer-based rate control算法来避免重缓冲事件（避免卡顿） 提出viewport预测的可能性模型计算tile被看到的可能性（避免边缘效应） 形式化QoE-driven优化问题： 在传输率受限的情况下最小化viewport内的质量失真和空间质量变化（获取受限状态下最好的视频质量） ","date":"2021-12-09","objectID":"/posts/note-for-360probdash/:2:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/note-for-360probdash/"},{"categories":["paper"],"content":"问题建模 形式化参数 $M*N$个tile，M指tile序列的序号，N指不同的码率等级 $r_{i, j}$指比特率，$d_{i, j}$指失真，$p_{i}$指被看到的可能性（$\\sum_{i=1}^{N}p_{i} = 1$） $\\Phi(X)$指质量失真，$\\Psi(X)$指质量变化 目标 找到推流段的集合：$X = {x_{i, j}}$，其中${x_{i, j}} = 1$指被第$\u003ci, j\u003e$个tile被选中；$x_{i, j} = 0$则是未选中。 $$ \\underset{X}{min}\\ \\Phi(X) + \\eta \\cdot \\Psi(X) \\ s.t. \\sum_{i=1}^{N}\\sum_{j=1}^{M}x_{i, j}\\cdot r_{i, j} \\le R, \\ \\sum_{j=1}^{M}x_{i, j} \\le 1, x_{i, j} \\in {0, 1}, \\forall i. $$ 整个公式即为前所述的问题的形式化表达的公式化结果。 模型细节 $\\Phi(X)$和$\\Psi(X)$的计算=\u003e通过考虑球面到平面的映射 通过计算球面上点的Mean Squared Error来得到S-PSNR进而评估质量：$d_{i, j}$来表示第${\u003ci, j\u003e}$个段的MSE $$ \\phi_i = \\frac{\\pi}{2} - h_i \\cdot \\frac{\\pi}{H}, \\Delta\\phi = \\Delta h \\cdot \\frac{\\pi}{H}, \\ \\theta_i = w_i \\cdot \\frac{2\\pi}{W}, \\ \\Delta\\theta = \\Delta w \\cdot \\frac{2\\pi}{W}, $$ $H$和$W$分别指按照ERP格式投影之后的视频高度和宽度 第$i$个tile的空间面积用$s_i$表示： $$ s_i\\ =\\ \\iint_{\\Omega_i}Rd\\phi Rcos\\phi d\\theta \\ =\\Delta\\theta R^2[sin(\\phi_i + \\Delta\\phi) - sin\\phi_i], $$ $R$指球的半径（$R = W/2\\pi$），所以整体的球面质量失真$D_{i, j}$可以计算出来： $$ D_{i, j} = d_{i, j} \\cdot s_i, $$ 结合每个tile被看到的概率$p_i$可以得出$\\Phi(X)$和$\\Psi(X)$ $$ \\Phi(X)=\\frac{\\sum_{i=1}^N\\sum_{j=1}^MD_{i, j}\\cdot x_{i,j}\\cdot p_i}{\\sum_{i=1}^N\\sum_{j=1}^Mx_{i,j}\\cdot s_i},\\ \\Psi(X) = \\frac{\\sum_{i=1}^N\\sum_{j=1}^Mx_{i, j}\\cdot p_i \\cdot\\ (D_{i,j}-s_{i} \\cdot \\Phi(X))^2}{\\sum_{i=1}^N\\sum_{j=1}^Mx_{i,j}\\cdot s_i}. $$ Viewport的可能性模型 方向预测=\u003e线性回归模型 将用户的欧拉角看作是$yaw(\\alpha)$，$pitch(\\beta)$和$rool(\\gamma)$，应用线性回归做预测 $$ \\begin{cases} \\hat{\\alpha}(t_0 + \\delta) = m_{\\alpha}\\delta+\\alpha(t_0),\\ \\hat{\\beta}(t_0 + \\delta) = m_{\\beta}\\delta+\\beta(t_0),\\ \\hat{\\gamma}(t_0 + \\delta) = m_{\\gamma}\\delta+\\gamma(t_0). \\end{cases} $$ 预测错误的分布=\u003e高斯分布，根据公式均值和标准差都能从统计信息中计算出来 收集5名志愿者的头部移动轨迹并投影到3个方向上绘制成图，实验结果为预测错误呈现高斯分布（样本数可能不够？） $$ \\begin{cases} P_{yaw}(\\alpha) = \\frac{1}{\\sigma_{\\alpha}\\sqrt{2\\pi}}exp{-\\frac{[\\alpha-(\\hat{\\alpha}+\\mu_{\\alpha})]^2}{2\\sigma_{\\alpha}^2}},\\ P_{pitch}(\\beta) = \\frac{1}{\\sigma_{\\beta}\\sqrt{2\\pi}}exp{-\\frac{[\\beta-(\\hat{\\beta}+\\mu_{\\beta})]^2}{2\\sigma_{\\beta}^2}},\\ P_{roll}(\\gamma) = \\frac{1}{\\sigma_{\\gamma}\\sqrt{2\\pi}}exp{-\\frac{[\\gamma-(\\hat{\\gamma}+\\mu_{\\gamma})]^2}{2\\sigma_{\\gamma}^2}}. \\end{cases} $$ 3个方向各自独立，因此最终的预测错误$P_E(\\alpha,\\beta,\\gamma)$可以表示为： $$ P_E(\\alpha, \\beta, \\gamma) = P_{yaw}(\\alpha)P_{pitch}(\\beta)P_{roll}(\\gamma). $$ 球面上点被看到的可能性 球面坐标为$(\\phi, \\theta)$点的可能性表示为$P_s(\\phi, \\theta)$ 因为一个点可能在多个不同的viewport里面，所以定义按照用户方向从点$(\\phi, \\theta)$出发能看到的点集$L(\\phi, theta)$ 因此空间点$s$被看到的可能性可以表示为： $$ P_s(\\phi, \\theta) = \\frac{1}{|L(\\phi, \\theta)|}\\sum_{(\\alpha, \\beta, \\gamma) \\in L(\\phi, \\theta)}P_E(\\alpha, \\beta, \\gamma), $$ 球面上tile被看到的可能性 tile内各个点被看到的可能性的均值即为tile被看到的可能性（可否使用其他方式？） $$ p_i = \\frac{1}{|U_i|} \\sum_{(\\phi, \\theta) \\in U_i} P_s(\\phi, \\theta). $$ $U_i$表示tile内的空间点集 Target-Buffer-based Rate Control 因为长期的头部移动预测会产生较高的预测错误，所以不能采用大缓冲区（没有cite来证明这一点） 将处于相同时刻的段集合成一个块存储在缓冲区中。 在自适应的第k步，定义$d_k$作为此时的buffer占用情况（等到第k个块被下载完毕） $$ b_k = b_{k-1} - \\frac{R_k \\cdot T}{C_k} + T $$ $C_k$表示平均带宽，$R_k$表示总计的码率 为了避免重新缓冲设定目标buffer占用$B_{target}$，并使buffer占用保持在$B_{target}$（$b_k = B_{target}$） 因此总计的码率需要满足： $$ R_k = \\frac{C_k}{T} \\cdot (b_{k-1} - B_{target} + T), $$ 这里的$C_k$表示可以从历史的段下载信息中估计出来的带宽 设定$R$的下界$R_{min}$之后（没有说明为何需要设定下界），公式12可以修正为如下： $$ R_k = max{\\frac{C_k}{T} \\cdot (b_{k-1} - B_{target} + T), R_{min}}. $$ ","date":"2021-12-09","objectID":"/posts/note-for-360probdash/:3:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/note-for-360probdash/"},{"categories":["paper"],"content":"实现 ","date":"2021-12-09","objectID":"/posts/note-for-360probdash/:4:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/note-for-360probdash/"},{"categories":["paper"],"content":"服务端 视频裁剪器 将视频帧切割成tile 编码器 对tile进行划分并将其编码成多种码率的段 MPD产生器 添加SRD特性来表示段之间的空间关系 添加经度和纬度属性来表示 添加质量失真和尺寸属性 Apache HTTP服务器 存储视频段和mpd文件，向客户端推流 ","date":"2021-12-09","objectID":"/posts/note-for-360probdash/:4:1","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/note-for-360probdash/"},{"categories":["paper"],"content":"客户端 基础：dash.js 额外的模块 QoE-driver Optimizer $$ Output = HTTP\\ GET请求中的最优段 $$ $$ Input = Output\\ of\\ \\begin{cases} Target\\ buffer\\ based\\ Rate\\ Controller\\ Viewport\\ Probabilistic\\ Model\\ QR\\ Map \\end{cases} $$ Target-buffer-based Rate Controller $$ Output = 总计的传输码率，按照公式13计算而来 $$ $$ Input = Output\\ of\\ {Bandwidth\\ Estimation\\ module $$ Viewport Probabilistic Model $$ Output = 每个tile被看到的可能性，按照公式10计算而来 $$ $$ Input = Output\\ of\\ \\begin{cases} Orientation\\ Prediction\\ module\\ SRD\\ information \\end{cases} $$ QR MapQR=\u003eQuality-Rate $$ Output = 所有段的QR映射 $$ $$ Input = MPD中的属性 $$ Bandwidth Estimation（没有展开研究，因为不是关键？） $$ Output = 前3秒带宽估计的平均值 $$ $$ Input = 下载段过程中的吞吐量变化 $$ 可以通过onProgess()的回调函数XMLHttpRequest API获取 Orientation Prediction $$ Output = 用户方向信息的预测结果（yaw, pitch, roll） $$ $$ Input = Web\\ API中获取的DeviceOrientation信息，使用线性回归做预测 $$ ","date":"2021-12-09","objectID":"/posts/note-for-360probdash/:4:2","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/note-for-360probdash/"},{"categories":["paper"],"content":"评估 整体设定 将用户头部移动轨迹编码进播放器来模拟用户头部移动 积极操控网络状况来观察不同方案对网络波动的反应 详细设定 服务端 视频选择 2880x1440分辨率、时长3分钟、投影格式ERP 切分设置 每个块长1s（$T=1$）、每个块被分成6x12个tile（$N=72$） 每个段的码率设置为${20, 50, 100, 200, 300}$，单位kpbs 视频编码 开源编码器x264 视频分包 MP4Box 注意事项 每个段的确切尺寸可能与其码率不同，尤其对于长度较短的块。 为了避免这影响到码率自适应，将段的确切尺寸也写入MPD文件中 客户端 缓冲区设定（经过实验得出的参数） $B_{max}=3s$，$B_{target}=2.5s$，$R_{min}=200kbps$，$权重\\eta=0.0015$ 高斯分布设定 Yaw Pitch Roll $\\mu_{\\alpha}=-0.54,\\ \\sigma_{\\alpha}=7.03$ $\\mu_{\\beta}=0.18,\\ \\sigma_{\\beta}=2.55$ $\\mu_{\\gamma}=2.16,\\ \\sigma_{\\gamma}=0.15$ 比较对象 ERP：原始视频格式 Tile：只请求用户当前viewport的tile，不使用viewport预测，作为baseline Tile-LR：使用线性回归做预测，每个tile的码率被平均分配 性能指标 卡顿率：卡顿时间占播放总时长的比例 Viewport PSNR：直接反应Viewport内的视频质量 空间质量差异：Viewport内质量的协方差 Viewport偏差：空白区域在Viewport中的比例 ","date":"2021-12-09","objectID":"/posts/note-for-360probdash/:5:0","tags":["Immersive Video","Trajectory-based predict","Probabilistic Model"],"title":"Note for 360ProbDASH","uri":"/posts/note-for-360probdash/"},{"categories":["paper"],"content":"论文概况 Link: https://dl.acm.org/doi/10.1145/3232565.3234686 Level: SIGCOMM 18 Keyword: UDP+FOV-aware+FEC ","date":"2021-12-08","objectID":"/posts/note-for-dante/:1:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/note-for-dante/"},{"categories":["paper"],"content":"工作范围 ","date":"2021-12-08","objectID":"/posts/note-for-dante/:2:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/note-for-dante/"},{"categories":["paper"],"content":"目标 在给定序列的帧中，为每个tile设定FEC冗余，根据其被看到的可能性的加权最小化平均质量降低。 ","date":"2021-12-08","objectID":"/posts/note-for-dante/:3:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/note-for-dante/"},{"categories":["paper"],"content":"问题建模 输入 估计的丢包率$p$、发送速率$f$、有$n$个tile的$m$个帧($\u003ci, j\u003e$来表示第$i$个帧的第$j$个tile 第$\u003ci, j\u003e$个tile的大小$v_{i, j}$、第$\u003ci, j\u003e$个tile被看到的可能性$\\gamma_{i, j}$、 如果第$\u003ci, j\u003e$ 个tile没有被恢复的质量降低率、最大延迟$T$ 输出 第$\u003ci, j\u003e$个tile的FEC冗余率$r_{i, j} = \\frac{冗余包数量}{原始包数量}$ 最优化问题的形式化 $$ minimize\\ \\sum_{0\u003ci\\le m}\\sum_{0\u003cj\\le n} \\gamma_{i, j}d_{i, j}(p, r_{i, j}) $$ $$ subject\\ \\ to\\ \\ \\frac{1}{f}\\sum_{0\u003ci\\le m}\\sum_{0\u003cj\\le n}v_{i, j}(1+r_{i, j}) \\le T $$ $$ r_{i, j} \\le 0 $$ （1）：最小化最终被看到的tile的质量衰减的加权和，权重按照被看到的可能性分配。 （2）：经过重新编码的包和原始的包需要在T时刻之前发出。 ​ Dante将1个GOP(Group of Pictures)中的所有帧当作一批处理，$T$作为GOP的持续时间 ​ $f$：使用TCP Friendly Rate Control algorithm，基于估计的丢包率和网络延迟来计算得出 （3）：确保冗余率总是非负的。 关键变量是$d_{i, j}(p, r)$：丢包率是p情况下，采用r作为冗余率的第$\u003ci, j\u003e$个tile的质量衰减 $$ d_{i, j}(p, r) = \\delta_{i, j},\\ if\\ r \u003c \\frac{1}{1-p}; 0, otherwise. $$ 假设帧中有k个原始包，质量衰减发生在丢失的包不能被恢复的情况下。 FEC可以容忍 $r \\cdot k$ 个丢包=\u003e即当 $p(rk+k)$ 大于 $rk$ 时会发生质量衰减。 过多的丢包会导致依赖链上所有帧的质量衰减，因此考虑帧之间的依赖关系之后，可以重新计算质量衰减： $$ d^{*}{i, j}(p, r) = \\sum{0\u003cc\\le i}w_{c, i}d_{c, j}(p, r) $$ $w_{c, i}$ 编码帧i对帧c的依赖作为单独的第c个帧的质量衰减的权重； 最终第i个帧的第j个tile的最终质量衰减就是所有依赖的质量衰减的和。 ","date":"2021-12-08","objectID":"/posts/note-for-dante/:4:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/note-for-dante/"},{"categories":["paper"],"content":"FEC冗余的自适应逻辑 关于$d_{i, j}(p, r)$ ：因为是分段函数，所以其值会因为r和p的大小关系而急剧改变。 利用背包问题的思想可以将其规约成NP完全问题： 将每个tile看作是一个物品，共有m*n个。 如果$r_{i, j} \u003c \\frac{1}{1-p}$ ，则表示不把第\u003ci,j\u003e和物品放入背包；否则就是将其放入背包。 公式1可以转化为：最大化所有物品二元变量的线性组合； 公式2可以转化为：二元变量的另一个线性组合必须低于阈值约束。 因此整个问题就能被完全转化为0-1背包问题 算法 整体上是背包问题的标准解法，能以线性复杂度（因为变量只是B)解决问题。 ","date":"2021-12-08","objectID":"/posts/note-for-dante/:5:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/note-for-dante/"},{"categories":["paper"],"content":"原型设计 使用基于TCP和UDP的两条连接来分别传输控制信息（双向：到客户端的播放会话的起至点和到服务端的网络信息反馈）和视频数据包 服务端根据反馈的网络信息，在每个GOP的边界时刻运行算法1来确定下一个GOP的帧和tile的FEC冗余。 确定之后服务端使用RS码来插入冗余包，和原始视频数据包一起重新编码，并使用基于TFRC的发送率发送数据。 Dante的实现是对应用程序级比特率适配策略的补充，并且可以通过对视频播放器进行最小更改来替换现有的底层传输协议来部署。 ","date":"2021-12-08","objectID":"/posts/note-for-dante/:6:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/note-for-dante/"},{"categories":["paper"],"content":"实验评估 环境：使用Gilbert模型来模拟实现丢包事件（而非使用统一随机丢包） 创造了两种网络条件good（丢包率0.5%）和bad（丢包率2%） ","date":"2021-12-08","objectID":"/posts/note-for-dante/:7:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/note-for-dante/"},{"categories":["paper"],"content":"局限性 效果主要依赖于Viewport预测的结果是否准确 ","date":"2021-12-08","objectID":"/posts/note-for-dante/:8:0","tags":["Immersive Video","UDP","Heuristic"],"title":"Note for Dante","uri":"/posts/note-for-dante/"},{"categories":["paper"],"content":"度量指标 viewport预测精度。 使用预测的viewport坐标和实际用户的viewport坐标的大圈距离来量化。 视频质量。 viewport内部的tile质量（1～5）。 tile在最高质量层之上花费的时间。 根据用户视线的分布而提出的加权质量度量。 ","date":"2021-11-22","objectID":"/posts/note11/:1:0","tags":["Immersive Video"],"title":"沉浸式流媒体传输的实际度量","uri":"/posts/note11/"},{"categories":["paper"],"content":"度量参数 分块策略 带宽 延迟 viewport预测 HTTP版本 持久化的连接数量 ","date":"2021-11-22","objectID":"/posts/note11/:2:0","tags":["Immersive Video"],"title":"沉浸式流媒体传输的实际度量","uri":"/posts/note11/"},{"categories":["paper"],"content":"背景 大多数的HAS方案使用HTTP/1.1协议进行请求-回应的事务来取得需要的资源、缓冲取到的视频段并以线性的顺序播放。传统的HAS中，只需要1个GET请求来取得下一个视频的暂时的部分。只要视频段的持续时间比网络内的时延高，这种方法就可行。 在基于VR的HAS方案中，播放1条视频片段就需要取得多种资源：1次GET请求需要同时请求基础的tile层和每个空间视频tile。使用4x4的tile方案时，客户端需要发起不少于17次GET请求。使用 1 s 数量级的分段持续时间，即使是 20 ms 的微小网络延迟也会显着阻碍客户端和服务器之间的整体吞吐量，因此会导致较低的视频质量。 ","date":"2021-11-15","objectID":"/posts/note10/:1:0","tags":["Immersive Video"],"title":"沉浸式推流中应用层的优化","uri":"/posts/note10/"},{"categories":["paper"],"content":"解决方案 ","date":"2021-11-15","objectID":"/posts/note10/:2:0","tags":["Immersive Video"],"title":"沉浸式推流中应用层的优化","uri":"/posts/note10/"},{"categories":["paper"],"content":"使用多条持久的TCP连接 大多数的现代浏览器都支持同时建立并维持多达6条TCP连接来减少页面加载时间，并行地获取请求的资源。这允许增加整体吞吐量，并部分消除网络延迟引入的空闲 RTT 周期。 类似地，基于 VR 的 HAS 客户端可以使用多个 TCP 连接并行下载不同的tile。 ","date":"2021-11-15","objectID":"/posts/note10/:2:1","tags":["Immersive Video"],"title":"沉浸式推流中应用层的优化","uri":"/posts/note10/"},{"categories":["paper"],"content":"使用HTTP/2协议的服务端push特性 HTTP/2协议引入了请求和相应的多路复用、头部压缩和请求优先级的特性，这可以减少页面加载时间。 服务端直接push短视频片段可以减少视频的启动时间和端到端延迟。 并且，服务端push特性可以应用在基于tile的VR视频推流中，客户端可以向服务器同时请求一条视频片段的所有tile。 服务端可以使用特制的请求处理器，允许客户端为每个tile定义一系列质量等级。 因此可以将应用的启发式自适应的速率的决定传达给服务器，这允许客户端以期望的质量级别取得所有图块。 ","date":"2021-11-15","objectID":"/posts/note10/:2:2","tags":["Immersive Video"],"title":"沉浸式推流中应用层的优化","uri":"/posts/note10/"},{"categories":["paper"],"content":"最终的目标 主要的挑战是用户的临场感，这可以通过避免虚拟的线索来创造出接近真实的世界。 ","date":"2021-11-14","objectID":"/posts/note9/:1:0","tags":["Immersive Video"],"title":"沉浸式流媒体面临的挑战和启示","uri":"/posts/note9/"},{"categories":["paper"],"content":"具体的任务 从360度视频的采集到显示的过程中，引入了好几种失真。 应该重点增加新的拼接、投影和分包方式以减少噪音。 除了捕获和使用360度视频来表示真实世界和实际交互内容之外，环境中还包括3D对象。 3D对象的合并对于真实的视图而言是一个挑战。 因为在推流会话中，用户的头部移动高度可变，所以固定的tiling方案可能会导致非最优的viewport质量。 推流框架中的tile数量应该被动态选择，进而提高推流质量。 自适应的机制应该足够智能来根据环境因素精确地做出适应。 应该制定基于深度强化学习的策略，来给360度视频帧中不同区域的tile分配合适的比特率。 用户在360度视频中的自由导航很容易让其感觉忧虑自己错过了什么重要的东西。 在360度视频中导航的时候，需要支持自然的可见角度方向。 丰富的环境应配备新颖的定向机制，以支持360度视频，同时降低认知负荷，以克服此问题。 真实的导航依赖viewport预测机制。 现代的预测方式应该使用时空图像特性以及用户的位置信息，采用合适的编解码器卷积LSTM结构来减少长期预测误差。 沉浸式的场景随着用户的交互应该发生变化。 由于用户与场景的交互而产生的新挑战是通过编码和传输透视图创建的。 因此预测用户的行为来实现对交互内容的高效编码和推流非常关键。 对360度视频的质量获取方法和度量手段需要进一步研究。 360度视频中特殊的音效需要引起注意。 ","date":"2021-11-14","objectID":"/posts/note9/:2:0","tags":["Immersive Video"],"title":"沉浸式流媒体面临的挑战和启示","uri":"/posts/note9/"},{"categories":["paper"],"content":"背景 空间音频是一种全球状空间环绕的声音方式，采用多个声音通道来模拟现实世界中听到的声音。 360度视频由于空间音频而变得更加可靠，因为声音的通道特性使其能够穿越时间和空间。 360度视频显示系统在制作空间音频音轨方面的重要性无论怎样强调都不为过 ","date":"2021-11-14","objectID":"/posts/note8/:1:0","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"空间音频的再现技术 ","date":"2021-11-14","objectID":"/posts/note8/:2:0","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"物理重建 物理重建技术用于合成尽可能接近所需信号的整个声场。 立体声配置在最流行的声音再现方法中使用两个扬声器，以促进更多的空间信息（包括距离、方向感、环境和舞台合奏）。而多信道再现方法在声学环境中使用，并在消费类设备中流行。 多信道再现技术 同样的声压场也通过其他物理重建技术产生，如环境中存在的环境声学和波场合成（WFS）。 需要麦克风阵列来捕获更多的空间声场。 因为不能直接用于声场特性分析，麦克风记录的内容需要后期处理。 麦克风阵列用于语音增强、声源分离、回声消除和声音再现。 ","date":"2021-11-14","objectID":"/posts/note8/:2:1","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"感知重建 心理声学技术用于感知重建，以产生对空间声音特征的感知。 感知重建技术复制空间音频的自然听觉感受来表示物理音频。 双耳录制技术 双耳录制技术是立体声录制的一种扩展形式，提供3D的听觉体验。 双耳录制技术通过使用两个360度麦克风尽可能的复制人耳，这与使用定向麦克风捕捉声音的常规立体声录音相同。 假人头部的360度麦克风用作人耳的代理，因为它提供了耳朵的精确几何坐标。 假人头部还产生与人头轮廓相互作用的声波。借助360度麦克风，与任何其他记录方法相比，空间立体图像的捕获更精确。 头部相关传递函数（HRTF） 用于双耳音频的实时技术中，以再现复杂的线索，帮助我们通过过滤音频信号来定位声音。 多个因素（如耳朵、头部和听力环境）会影响线索，因为在现实中，我们会重新定位自己以定位声音。 选择合适的录音/重放技术对于使听到的声音与真实场景中的体验相同至关重要。 ","date":"2021-11-14","objectID":"/posts/note8/:2:2","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"环境声学 ","date":"2021-11-14","objectID":"/posts/note8/:3:0","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"概述 环境声学也被称为3D音频，被用于记录、混成和播放一个中心点周围的360度音频。 ","date":"2021-11-14","objectID":"/posts/note8/:3:1","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"区别 环境音频和传统的环绕声技术不同。 双声道和传统环绕声技术背后的原理是相同的，都是通过将声音信号送到特定的扬声器来创建音频。 环境音频不受任何特定扬声器的预先限制，因为它在即使音域旋转的情况下，也能创造出平滑的音频。 传统环绕声的格式只有在声音场景保持静态的情况下才能提供出色的成像效果。 环境音频提供一个完整的球体，将声音均匀地传播到整个球体。 ","date":"2021-11-14","objectID":"/posts/note8/:3:2","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"格式 环境音频有6种格式，分别为：A、B、C、D、E、G。 ","date":"2021-11-14","objectID":"/posts/note8/:3:3","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"用途 一阶环境音频的用途 第一阶的环境音频或B格式的环境音频，其麦克风用于使用四面体阵列表示线性VR。 此外，这些在四个通道中进行处理，例如提供非定向压力水平的“W”。同时，“X、Y和Z”分别促进了从前到后、从侧到侧以及从上到下的方向信息。 一阶环境音频仅适用于相对较小的场景，因为其有限的空间保真度会影响声音定位。 高阶环境音频的用途 高阶环境音频通过增加更多的麦克风来增强一阶环境音频的性能效率。 ","date":"2021-11-14","objectID":"/posts/note8/:3:4","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"总结 ","date":"2021-11-14","objectID":"/posts/note8/:4:0","tags":["Immersive Video"],"title":"360度视频的音频处理","uri":"/posts/note8/"},{"categories":["paper"],"content":"概述 在360度视频的推流过程中，根据用户头部的运动自适应地动态选择推流的区域，调整其比特率，以达到节省带宽的目的。 ","date":"2021-11-14","objectID":"/posts/note7/:1:0","tags":["Immersive Video"],"title":"自适应策略之viewport依赖型","uri":"/posts/note7/"},{"categories":["paper"],"content":"通常的实现方式 在服务端提供几个自适应集，来在遇到用户头部的突然运动的情况时，能保证viewport的平滑转换。 提出QER(Quality-focused Regios)的概念使viewport内部的视频分辨率高于viewport之外的视频分辨率。 非对称的方式以不同的空间分辨率推流来节省带宽。 在播放过程中，客户端根据用户的方向来请求不同分辨率版本的视频。 优点是即使客户端对用户的方面做了错误预测，低质量的内容仍然可以在viewport中生成。 缺点是在大多数场景下，这种方案需要巨大的存储开销和处理负载。 ","date":"2021-11-14","objectID":"/posts/note7/:2:0","tags":["Immersive Video"],"title":"自适应策略之viewport依赖型","uri":"/posts/note7/"},{"categories":["paper"],"content":"自适应推流参数 可用带宽和网络吞吐量 Viewport预测的位置 客户端播放器的可用缓冲 ","date":"2021-11-14","objectID":"/posts/note7/:3:0","tags":["Immersive Video"],"title":"自适应策略之viewport依赖型","uri":"/posts/note7/"},{"categories":["paper"],"content":"参数计算公式 第n个估计的Viewport：$V^e(n)$ $V^e(n) = V_{fb}$ $V_{fb}$是最新报告的viewport位置 第n个估计的吞吐量：$T^e(n)$ $T^e(n) = T_{fb}$ $T_{fb}$是最新报告的吞吐量 比特率：$R_{bits}$ $R_{bits} = (1-\\beta)T^e(n)$ $\\beta$是安全边缘 第n个帧的客观度量质量：$VQ(k)$和最终客观度量质量$VQ$ $VQ=\\frac{1}{L}\\sum^L_{k=1}VQ(k)$ $VQ(k) = \\sum_{t=1}^{T^n}w_k(k) * D^n_t(V_t, k)$ $w_k = \\frac{A(t,k)}{A_{vp}}$ $L=总帧数$ $w_k$表示在第k个帧中与viewport所重叠的tile程度 $A(t,k)$表示第k个帧中tile $t$ 重叠的区域 $A_{vp}$表示viewport中总共的区域 ","date":"2021-11-14","objectID":"/posts/note7/:4:0","tags":["Immersive Video"],"title":"自适应策略之viewport依赖型","uri":"/posts/note7/"},{"categories":["paper"],"content":"OMAF(Omnidirectional Media Format) OMAF是第1个国际化的沉浸式媒体格式，描述了对360度视频进行编码、演示、消费的方法。 OMAF与与现有格式兼容，包括编码（例如HEVC），文件格式（例如ISOBMFF），交付信号（例如DASH，MMT）。 OMAF中还包括编码、投影、分包和viewport方向的元数据。 ","date":"2021-11-11","objectID":"/posts/note6/:1:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/note6/"},{"categories":["paper"],"content":"OMAF+DASH-\u003eMPD OMAF与DASH相结合，再加上一些额外的描述构成了MPD文件格式，用于向客户端通知360度媒体的属性。 OMAF规定了9中媒体配置文件，包括3种视频配置文件：基于HEVC的viewport独立型、基于HEVC的viewport依赖型、基于AVC的viewport依赖型。 OMAF为视角独立型的推流提供了无视viewport位置的连续的视频帧质量。 常规的HEVC编码方式和DASH推流格式可以用于viewport独立型的推流工作。 但是使用HEVC/AVC编码方式的基于viewport的自适应操作是OMAF的一项技术开发，允许无限制地使用矩形RWP来增强viewport区域的质量。 ","date":"2021-11-11","objectID":"/posts/note6/:2:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/note6/"},{"categories":["paper"],"content":"CMAF(Common Media Application Format) 致力于提供跨多个应用和设备之间的统一的编码格式和媒体配置文件。 CMAF使请求低延迟的segment成为可能。 ","date":"2021-11-11","objectID":"/posts/note6/:3:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/note6/"},{"categories":["paper"],"content":"ISOBMFF(ISO Base Media File Format) ISOBMFF是用于定时数据交换、管理和显示的最流行的文件格式。 文件由一系列兼容并且可扩展的文件级别的box组成。 每个box表示1个由4个指针字符代码组成的数据结构。 ISOBMFF的媒体数据流和元数据流被分别分发。 媒体数据流中包括编码过的音频和视频数据。 元数据流中包括媒体类型、编码属性、时间戳、大小等元数据，也包括全向内容的额外信息如投影格式、旋转、帧分包、编码和分发等元数据。 ISOBMFF为了访问方便，保证有价值信息能灵活聚合。 ","date":"2021-11-11","objectID":"/posts/note6/:4:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/note6/"},{"categories":["paper"],"content":"3DoF(3 Degree of Freedom) 在3DoF场景中，用户可以自由的移动头部以三个方向：摆动、俯仰、旋转。 ","date":"2021-11-11","objectID":"/posts/note6/:5:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/note6/"},{"categories":["paper"],"content":"3DoF+ 用户的头部可以以任意方向移动：上下、左右、前后 ","date":"2021-11-11","objectID":"/posts/note6/:6:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/note6/"},{"categories":["paper"],"content":"6DoF 不只用户的头部，用户的身体也是自由的。同时支持方向与位置的自由。 ","date":"2021-11-11","objectID":"/posts/note6/:7:0","tags":["Immersive Video"],"title":"沉浸式流媒体现有标准","uri":"/posts/note6/"},{"categories":["paper"],"content":"背景 用户使用头戴设备比使用传统显示器观看360度视频内容时的满意度对于扰乱更加敏感。 沉浸式的体验受到不完美的视角预测和高度动态化的网络状况的消极影响。 目前主要面临的挑战有以下4个： ","date":"2021-11-04","objectID":"/posts/note5/:0:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"Viewport预测 ","date":"2021-11-04","objectID":"/posts/note5/:1:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"背景 HMD的本质特征是快速响应用户头部的移动。当用户改变viewport时HMD处理交互并检测相关的viewport来精确播放器的信息，这样视野就能以正常的可视角度被提供给用户。Viewport预测在优化的360度视频推流中非常必要。配备有位置传感器的可穿戴HMD允许客户端更新其视角方向相应的视角场景。 ","date":"2021-11-04","objectID":"/posts/note5/:1:1","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"分类 内容不可知的方式基于历史信息对viewport进行预测。 内容感知的方式需要视频内容信息来预测未来的viewport。 ","date":"2021-11-04","objectID":"/posts/note5/:1:2","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"内容不可知方式 分类 平均线性回归LR 航位推算DR 聚类 机器学习ML 编解码器体系结构 现有成果 Qian’s work——LR 使用平均线性回归和加权线性回归模型来做viewport预测，之后对与预测区域重叠的tile进行整体推流。 当预测后0.5s、1s、2s加权线性回归表现更好 Petrangeli’s work——LR 将被划分成tile的等矩形的帧分成3个区域：viewport区、相邻区、其他区。 结合观察者头部的移动，将可变比特率分配给可见和不可见区域。 作者利用最近（100毫秒）用户观看历史的线性外推来预测未来的注视点。 Mavlankar and Girod’s work——运动向量 使用运动向量比如观察者的平移、倾斜、缩放等方向上的速度和加速度，来执行视角区域预测。 La Fuente’s work——运动向量 考虑了两种预测变体：角速度和角加速度，从用户以前的方向数据来估计未来的头部方向。按照预测结果分配不同的量化参数到每个tile上。 当进行进一步的预测时（超过2s），这种方式限制了预测的精度。 如果视频tile被基于错误的预测而被请求，用户的实际viewport可能会被没有请求因而没有内容的黑色tile所覆盖。 Ban’s work——KNN+LR 使用KNN算法利用跨用户观看历史，使用LR模型利用户个体化的行为。 就视角预测的准确率而言，分别取得了20%和48%的绝对和相对改进。 Liu’s work——cluster 提出了使用数据融合方法，通过考虑几个特征来估计未来视角位置。特征例如：用户的参与度、用户观看同一视频的行为、单个用户观看多个视频的行为、最终用户设备、移动性水平。 Petrangeli’s work——cluster 基于车辆轨迹预测的概念，考虑了类似的轨迹形成一个簇来预测未来的viewport。 结果表明这种方法为更长的视野提高了精确度。 检查了来自三个欧拉角的不同轨迹，这样做可能导致性能不足。 Rossi’s work——cluster 提出了一种聚类的方法，基于球形空间中有意义的viewport重叠来确认用户的簇。 基于Bron-Kerbosch（BK）算法的聚类算法能够识别大量用户，这些用户观看的是相同的60%的3s长球形视频块。 与基准相比，该方法为簇提供了可兼容且重要的几何viewport重叠。 Jiang’s work 背景： LR方法对于长期的预测视野会导致较差的预测精度。长短时记忆（LSTM）是一种递归神经网络（RNN）架构，适用于序列建模和模式开发。 方法： 为了在FoV预测中获取比LR方法更高的精确度，开发了一种使用带有128个神经元的LSTM模型的viewport预测方法。 分析了360度数据集，观察到用户在水平方向头部有快速转向，但是在垂直方向几乎是稳定的。 实验表明，这种方法同时考虑水平和垂直方向的头部移动时，比LR等方法产生了更少的预测错误。 Bao’s work 背景： 对150个用户进行了16个视频剪辑的主观实验，并对其行为进行了分析。 使用3个方向的欧拉角$\\theta$, $\\phi$, $\\psi$来表示用户在3D空间中头部的移动，结果表明不同方向的动作有强自相关性和消极的互相关性。因此多个角度的预测可以分开进行。 方法： 开发两个独立的LSTM模型来分别预测$\\theta$和$\\phi$，之后将预测结果应用于目标区域流来有效利用可用网络资源。 Hou’s work 提出一种基于深度学习的视角产生方法来只对提前预测的360度视频和3自由度的VR应用的viewport tile进行抽取和推流。（使用了大规模的数据集来训练模型） 使用包含多层感知器和LSTM模型来预测6自由度的VR环境中头部乃至身体的移动，预测的视野被预渲染来做到低延迟的VR体验。 Heyse’s work 背景： 在某些例子中，用户的移动在视频的不同部分中非常不稳定。这增加了机器学习方式的训练压力。 方法： 提出了一个基于RL模型的上下文代理，这个模型首先检测用户的显著移动，然后预测移动的方向。这种分层自学习执行器优于球形轨迹外推法（这种方法将用户运动建模为轨迹的一部分，而不是单位球体上的完整轨迹） Qian’s work 提出了一种叫做Flare的算法来最小化实际viewport和预测viewport之间的不匹配。 应用了一种ML方法来执行频繁的viewport预测，包括从130名用户收集的1300条头部运动轨迹的4个间隔。 使用viewport轨迹预测，Flare可以将错误预测替换成最新预测。 Yu and Liu’s work 背景： LSTM网络本身具有耗时的线性训练特性。编解码器的LSTM模型把训练过程并行化，相比于LR和LSTM本身而言，改善了预测精度。 方法： 使用基于注意力的LSTM编解码器网络体系结构来避免昂贵的递归并能更好地捕获viewport变化。 提出的体系结构相比于传统的RNN，获得了更高的预测精度，更低的训练复杂度和更快的收敛。 Jamali’s work 提出使用LSTM编解码器网络来做长期的viewport预测（例如3.5s）。 收集了低延迟异质网络上跨用户的方向反馈来调整高延迟网络上目标用户的预测性能。 ","date":"2021-11-04","objectID":"/posts/note5/:1:3","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"内容感知方式 背景 内容感知方式可以提高预测效率。 具体方法 Aladagli’s work 提出了一个显著性驱动的模型来提高预测精度。 没有考虑用户在360度视频中的视角行为。 viewport预测错误可以通过理解用户对360度视频独特的可见注意力最小化。 Nguyen’s work 背景： 大多数现存的方法把显著性图看作是360度显示中的位置信息来获得更好的预测结果。 通用的显著性和位置信息体系结构基于固定预测模型。 方法： 提出了PanoSalNet来捕获用户在360度帧中独特的可见注意力来改善显著性检测的性能。 同时使用HMD特性和显著性图的固定预测模型获得了可测量的结果。 Xu’s work 提出了两个DRL(Deep Reinforcement Learning)模型用于同时考虑运动轨迹和可见注意力特性的viewport预测网络。 离线模型基于内容流行度检测每个帧里的显著性。 在线模型基于从离线模型获得的显著性图和之前的viewport预测信息预测viewport方向和大小。 这个网络只能预测30ms的下一个viewport位置。 Xu’s work 收集了大规模的被使用带有眼部轨迹跟踪的HMD的45个观测者观察的动态360度视频数据集，提出了基于历史扫描路径和图像特征预测注视位移的方法。 在与当前注视点、viewport和整个图像相关的三个空间尺度上执行了显著性计算。 可能的图像特性被通过向CNN喂图像和相应的显著性图，同时LSTM模型捕获历史信息来抽取出来。 之后将LSTM和CNN特性耦合起来，用于下一次的用户注视信息预测。 Fan’s work 用户更容易被运动的物体吸引，因此除了显著性图之外，Fan等人也考虑了使用预训练 的CNN来估计用户未来注视点的内容运动图。 由于可能存在多个运动，这让预测变得不可靠，因此运动贴图的开发还需要进一步的研究。 Yang’s work 使用CNN模型基于历史观测角度信息预测了单viewport。 接着考虑了一种使用内容不可知和内容感知方法如RNN和CFVT模型的融合层的viewport轨迹预测策略。 融合模型使其同时支持更好地预测并且提高了大概40%的精度。 Ozcinar’s work 将viewport轨迹转换为基于viewport的视觉注意图，然后对不同大小的tile进行推流以保证更高的编码效率。 Li’s work 现有的预测模型对未来的预测能力有限，Li等人提出了两种模型，分别用于viewport相关和基于tile的推流系统。 第一个模型应用了基于用户轨迹的LSTM编解码网络体系结构。 第二个模型应用了卷积LSTM编解码体系结构，使用序列的热图来预测用户的未来方向。 ","date":"2021-11-04","objectID":"/posts/note5/:1:4","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"总结 精确的方向预测使360度视频的客户端可以以高分辨率下载最相关的tile。 当前采用显著性和位置信息的神经网络模型的性能比直接利用当前观察位置进行未来viewport位置估计的简单无运动的基线方法表现差。估计的显著性中的噪音等级限制了这些模型的预测精度。并且这些模型也引入了额外的计算复杂度。 对于360度视频注意点的可靠预测和用户观看可能性与显著性图之间关系的理解，显著性模型必须被改善并通过训练大规模的数据集来适应，尤其是被配备了不同摄像机旋转的镜头所捕获的数据。 另一方面，卷积LSTM编解码器和基于轨迹的预测方法适合长期预测，并能带来相当大的QoE改进，特别是在协作流媒体环境中。 ","date":"2021-11-04","objectID":"/posts/note5/:1:5","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"QoE评估 ","date":"2021-11-04","objectID":"/posts/note5/:2:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"背景 由于全方位视频非常普遍，因此，通过这种类型的视频分发来确定用户的特定质量方面是至关重要的。QoE在视频推流应用中扮演着重要角色。在传统视频推流中，QoE很大程度上被网络负载和分发性能所影响。现有的次优目标度量方法并不适用于全向视频，因为全向视频受网络状况和用户视角行为的影响很大。 ","date":"2021-11-04","objectID":"/posts/note5/:2:1","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"主观质量评估 主观质量评估是估计360度视频推流质量的现实并且可靠的方法。 Upenik’s work 用一台MergeVR HMD执行了主观测试来体验360度图像。 实验数据包括主观分数、视角轨迹、在每个图像上花费的时间由软件上获得。 视角方向信息被用于计算显著性图。 但是这项研究没有考虑对360度视频的评估。 Zhang’s work 为了弥补360度视频和常规视频度量方式之间的性能差距，为全景视频提出了一种主观质量评估方法，称为SAMPVIQ。 23位参与者被允许观看4个受损视频，整体视频质量体验的评分在0～5分之间。 参与者之间存在较大的评分差异。 Xu’s work 提出两种主观测量方式：总体区分平均意见分数(O-DMOS)和矢量区分平均意见分数(V-DMOS)来获得360度视频的质量损失。 类似于传统食品的DMOS度量方式，O-DMOS度量方式计算主观测试序列的总计区分分数。 Schatz’s work 研究了使用HMD观看360度内容时停顿事件的影响。 沉浸式内容的主观质量评估并非不重要，可能导致比实际推荐更多的开放性问题。 通常来讲人们的期望于传统的HAS相似，即如果可能的话，根本没有停顿。 可用的开源工具 AVTrack360，OpenTrack和360player能捕获用户观看360度视频的头部轨迹。 VRate是一个在VR环境中提供主观问卷调查的基于Unity的工具。 安卓应用*MIRO360*，支持未来VR主观测试的指南开发。 Cybersickness Cybersickness是一种获得高QoE的潜在障碍，它能引起疲劳、恶心、不适和呕吐。 Singla’s work 使用受限的带宽和分辨率，在不同的延迟情况下进行了两个主观实验。 开发了主观测试平台、测试方法和指标来评估viewport自适应360度视频推流中的视频感知等级和Cybersickness。 基于tile的推流在带宽受限的情况下表现很好。 47ms的延迟实际上不影响感知质量。 Tran’s work 考虑了几个影响因子例如内容的空间复杂性，数量参数，分辨率特性和渲染模型来评估cybersickness，质量，可用性和用户的存在。 VR环境中快速移动的内容很容易引发cybersickness。 由于高可用性和存在性，用户的cybersickness也可能加剧。 Singla’s work 评估了28名受试者在Oculus Rift和HTC Vive头戴式电脑上观看6个全高清和超高清分辨率YouTube视频时的观看不适感。 HMD的类型轻微地影响感知质量。 分辨率和内容类型强烈影响个人体验。 女性用户感到cybersickness的人数更多。 空间存在感 空间存在感能增强沉浸感。 Zou’s work 方法： 提出了一个主观框架来测量25名受试者的空间存在感。 提出的框架包括三层，从上到下分别为：空间存在层、感知层、科技影响层。 心理上的空间存在感形成了空间存在层。 感知层以视频真实感、音频真实感和交互元素为特征。 科技影响层由几个模块组成，这些模块与感知层相连，以反映传感器的真实性。 Hupont’s work 应用通用感知的原则来研究在Oculus HMD和传统2D显示器上玩游戏的用户的空间存在感。 与2D显示器相比，3D虚拟现实主义显示出更高的惊奇、沉浸感、存在感、可用性和兴奋感。 生理特征度量 Salgado’s work 方法： 捕获多种多样的生理度量，例如心率HR，皮肤电活性EDA、皮肤温度、心电图信号ECG、呼吸速率、血压BVP、脑电图信号EEG来评价沉浸式模拟器的质量。 Egan’s work 基于HR和EDA信号评估VR和非VR渲染模式质量分数。 相比于HR，EDA对质量分数有强烈的影响。 技术因素感知 不同的技术和感知特征，如失真、清晰度、色彩、对比度、闪烁等，用于评估感知视频质量。 Fremerey’s work 确定了可视质量强烈地依赖于应用的运动插值（MI）算法和视频特征，例如相机旋转和物体的运动。 在一项主观实验中，12位视频专家回顾了使用FFmpeg混合、FFmpeg MCI（运动补偿插值）和butterflow插值到90 fps的四个视频序列。作者发现，与其他算法相比，MCI在QoE方面提供了极好的改进。 总结 主观测试与人眼直接相关，并揭示了360度视频质量评估的不同方面的影响。 在这些方面中，空间存在感和由佩戴VR头戴设备观看360度视频导致的cybersickness极为重要，因为这些效果并不在传统的2D视频观看中出现。 主观评估需要综合的手工努力并因此昂贵耗时并易于出错，相对而言，客观评估更易于管理和可行。 ","date":"2021-11-04","objectID":"/posts/note5/:2:2","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"客观质量评估 由于类似的编码结构和2D平面投影格式，对360度内容应用客观质量评估很自然。 计算PSNR 现有投影方式中的采样密度在每个像素位置并不均匀。 Yu’s work 为基于球形的PSNR计算引入S-PSNR和L-PSNR。 S-PSNR通过对球面上所有位置的像素点做同等加权来计算PSNR。 利用插值算法，S-PSNR可以完成对支持多种投影模式的360度视频的客观质量评估。 L-PSNR通过基于纬度和访问频率的像素点加权测量PSNR。 L-PSNR可以测量viewport的平均PSNR而无需特定的头部运动轨迹。 Zakharchenko’s work 提出了一种Craster Parabolic Projection-PSNR (CPP-PSNR) 度量方式来比较多种投影方案，通过不改变空间分辨率和不计算实际像素位置的PSNR，将像素重新映射成CPP投影。 CPP投影方式可能使视频分辨率大幅下降。 Sun’s work 提出了一种叫做weighted-to-spherically-uniform PSNR (WS-PSNR)的质量度量方式，以此来测量原始和受损内容之间的质量变化。 根据像素在球面上的位置考虑权重。 计算SSIM SSIM是另一种质量评估指标，它通过三个因素反映图像失真，包括亮度、对比度和结构。 Chen’s work 为2D和360度视频分析了SSIM结果，引入了球型结构的相似性度量（S-SSIM）来计算原始和受损的360度视频之间的相似性。 在S-SSIM中，使用重投影来计算两个提取的viewport之间的相似性。 Zhou’s work 考虑相似性的权重提出了WS-SSIM来测量投影区域中窗口的相似性。 性能评估表明，与其他质量评估指标相比，WS-SSIM更接近人类感知。 Van der Hooft’s work 提出了ProbGaze度量方式，基于tile的空间尺寸和viewport中的注视点。 考虑外围tile的权重来提供合适的质量测量。 相比于基于中心和基于平均的PSNR和SSIM度量方式，ProbGaze能估计当用户突然改变viewport位置时的视频质量变化。 Xu’s work 引入了两种客观质量评估度量手段：基于内容感知的PSNR和非内容感知的PSNR，用于编码360度视频。 第一种方式基于空间全景内容对像素失真进行加权。 第二种方式考虑人类偏好的统计数据来估计质量损失。 基于PSNR和SSIM方式的改进 尽管各种基于PSNR和SSIM的方式被广阔地应用到了360度视频的质量评估中，但这些方式都没有真正地捕获到感知质量，特别是当HMD被用于观看视频时。因此需要为360度内容特别设计一种优化的质量度量方式。 Upenik’s work 考虑了一场使用4张高质量360度全景图像来让45名受试者在不同的编码设定下评估和比较客观质量度量方式性能的主观实验。 现有的客观度量方式和主观感知到的质量相关性较低。 Tran’s work 论证主观度量和客观度量之间相关性较高，但是使用的数据集较小。 基于ML的方式 基于ML的方式可以弥补客观评估和主观评估之间的差距。 Da Costa Filho’s work 提出了一个有两个阶段的模型。 首先自适应VR视频的播放性能由机器学习算法所确定。 之后模型利用估计的度量手段如视频质量、质量变化、卡顿时间和启动延迟来确定用户的QoE。 Li’s work 引入了基于DRL的质量获取模型，在一次推流会话中同时考虑头部和眼部的移动。 360度视频被分割成几个补丁。 低观看概率的补丁被消除。 参考和受损视频序列都被输入到深度学习可执行文件中，以计算补丁的质量分数。 之后分数被加权并加到一起得到最终的分数。 Yang’s work 考虑了多质量等级的特性和融合模型。 质量特性用region of interest(ROI)图来计算，其中包括像素点等级、区域等级、对象等级和赤道偏差。 混合模型由后向传播的神经网络构造而成，这个神经网络组合了多种质量特性来获取整体的质量评分。 ","date":"2021-11-04","objectID":"/posts/note5/:2:3","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"总结 精确的QoE获取是优化360度视频推流服务中重要的因素，也是自适应分发方案中基础的一环。 单独考虑VR中的可视质量对完整的QoE框架而言并不足够。 为能获得学界的认可，找到其他因素的影响也很必要，例如cybersickness，生理症状，用户的不适感，HMD的重量和可用性，VR音频，viewport降级率，网络特性（延迟，抖动，带宽等），内容特性（相机动作，帧率，编码，投影等），推流特性（viewport偏差，播放缓冲区，时空质量变化等）。 ","date":"2021-11-04","objectID":"/posts/note5/:2:4","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"低延迟推流 ","date":"2021-11-04","objectID":"/posts/note5/:3:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"背景 360度全景视频推流过程中的延迟由几部分组成：传感器延迟、云/边处理延迟、网络延迟、请求开销、缓冲延迟、渲染延迟和反馈延迟。 低延迟的要求对于云VR游戏、沉浸式临场感和视频会议等更为严格。 要求极低的终端处理延迟、快速的云/边计算和极低的网络延迟来确保对用户头部移动做出反馈。 现代HMD可以做到使传感器延迟降低到用户无法感知的程度。 传输延迟已经由5G移动和无线通信技术大幅减少。 但是，对于减少处理、缓冲和渲染延迟的工作也是必要的。 许多沉浸式应用的目标是MTP的延迟少于20ms，理想情况是小于15ms。 ","date":"2021-11-04","objectID":"/posts/note5/:3:1","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"减少启动时间 减少初始化请求的数据量 通常来讲，较小的视频segment能减少启动和下载时间。 Van der Hooft’s work 考虑了新闻相关内容的推流，使用的技术有： 服务端编码 服务端的用户分析 服务器推送策略 客户端积极存储视频数据 取得的效果： 降低了启动时间 允许不同网络设定下的快速内容切换 较长的响应时间降低了性能 Nguyen’s work 基于viewport依赖的自适应策略分析了自适应间隔延迟和缓冲延迟的影响。 使用服务端比特率计算策略来最小化响应延迟的影响。 根据客户端的响应估计可用的网络吞吐量和未来的viewport位置。 服务端的决策引擎推流合适的tile来满足延迟限制。 取得的效果： 对于viewport依赖型推流方案而言，较少的自适应和缓冲延迟不可避免。 ","date":"2021-11-04","objectID":"/posts/note5/:3:2","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"降低由tile分块带来的网络负载 在HTTP/1.1中，在空间上将视频帧分成矩形tile会增加网络负载，因为每个tile会产生独立的网络请求。 请求爆炸的问题导致了较长的响应延迟，但是可以通过使用HTTP/2的服务器推送特性解决。这个特型使服务器能使用一条HTTP请求复用多条消息。 Wei’s work 利用HTTP/2协议来促进低延迟的HTTP自适应推流。 提出的服务端推送的策略使用一条请求同时发送几个segment避免多个GET请求。 Petrangeli’s work 结合特定请求参数与HTTP/2的服务端推送特性来促进360度视频推流。 客户端为一个segment发送一条call，服务器使用FCFS策略传送k个tile。 利用HTTP/2的优先级特性可以使高优先级的tile以紧急的优先级被获取，进而改善网络环境中的高往返时间的性能。 Xu’s work 为360度内容采用了k-push策略：将k个tile推送到客户端，组成一个单独的时间段。 提出的方法与QoE感知的比特率自适应算法一起，在不同的RTT设定下，提高了20%的视频质量，减少了30%的网络传输延迟。 Yahia’s work 使用HTTP/2的优先级和多路复用功能，在两个连续的viewport预测之间，即在交付相同片段之前和期间，组织紧急视频块的受控自适应传输。 Yen’s work 开发了一种支持QUIC的体系结构来利用流优先级和多路复用的特性来实现360度视频的安全和低优先级的传输。 当viewport变化发生时，QUIC能让常规的tile以低优先级推流，viewport内的tile以高优先级推流，都通过一条QUIC连接来降低viewport tile的缺失率。 作者说测试表明基于QUIC的自适应360度推流比HTTP/1.1和HTTP/2的方案表现更好。 ","date":"2021-11-04","objectID":"/posts/note5/:3:3","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"使用移动边缘计算降低延迟 Mangiante’s work 提出了利用基于边缘处理的viewport渲染方案来减少延迟，同时利用终端设备上的电源和计算负载。 但是作者没有给出有效的算法或是建立一个实践执行平台。 Liu’s work 采用远端渲染技术，通过为不受约束的VR系统获取高刷新率来隐藏网络延迟。 采用60GHz的无线链路支持的高端GPU，来加快计算速度和4K渲染，减少显示延迟。 尽管提供了高质量和低延迟的推流，但是使用了昂贵的带宽连接，这通常并不能获得。 Viitanen’s work 引入了端到端的VR游戏系统。通过执行边缘渲染来降低延迟，能源和计算开销。 为1080p 30fps的视频格式实现了端到端的低延迟（30ms）的系统。 前提是有充足的带宽资源、终端设备需要性能强劲的游戏本。 Shi’s work 考虑了不重视viewport预测的高质量360度视频渲染。 提出的MEC-VR系统采用了一个远端服务器通过使用一个自适应裁剪过滤器来动态适应viewport覆盖率，这个过滤器按照观测到的系统延迟增加viewport之外的区域。 基于viewport覆盖率的延迟调整允许客户端容纳和补偿突然的头部移动。 ","date":"2021-11-04","objectID":"/posts/note5/:3:4","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"共享VR环境中的延迟处理 共享VR环境中用户的延迟取决于用户的位置和边缘资源的分发。 Park’s work 通过考虑多个用户和边缘服务器之间的双向通信，提出了一种使用线性蜂窝拓扑中的带宽分配策略，以最小化端到端系统延迟。确定了推流延迟强烈地依赖于： 边缘服务器的处理性能 多个交互用户之间的物理和虚拟空间 Perfecto’s work 集成了深度神经网络和毫米波多播传输技术来降低协同VR环境中的延迟。 神经网络模型估计了用户即将来临的viewport。 用户被基于预测的相关性和位置分组，以此来优化正确的viewport许可。 执行积极的多播资源调度来最小化延迟和拥塞。 ","date":"2021-11-04","objectID":"/posts/note5/:3:5","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"总结 在单用户和多用户的环境中，边缘辅助的解决方式对于控制延迟而言占主要地位。 此外还有服务端的viewport计算、服务端push机制和远程渲染机制都能用于低延迟的控制。 现有的4G网络足以支持早期的自适应沉浸式多媒体，正在成长的5G网络更能满足沉浸式内容的需求。 ","date":"2021-11-04","objectID":"/posts/note5/:3:6","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"360度直播推流 ","date":"2021-11-04","objectID":"/posts/note5/:4:0","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"背景 传统的广播电视频道是直播推流的流行来源。现在私人的360度直播视频在各个社交媒体上也有大幅增长。 因为视频生产者和消费者之间在云端的转码操作，360度视频推流是更为延迟敏感的应用。 现有的处理设备在诸如转码、渲染等实时处理任务上受到了限制。 内容分发 Hu’s work 提出了一套基于云端的直播推流系统，叫做MELiveOV，它使高分辨率的全向内容的处理任务以毛细管分布的方式分发到多个支持5G的云端服务器。 端到端的直播推流系统包括内容创作模块、传输模块和viewport预测模块。 移动边缘辅助的推流设计减少了50%的带宽需求。 Griwodz’s work 为360度直播推流开发了优化FoV的原型，结合了RTP和基于DASH的pull-patching来传送两种质量等级的360度视频给华为IPTV机顶盒和Gear VR头戴设备。 作者通过在单个H.265硬件解码器上多路复用多个解码器来实现集体解码器的想法，以此减少切换时间。 视频转码 Liu’s work 研究表明只转码viewport区域有潜力大幅减少高性能转码的计算需求。 Baig’s work 开发了快速编码方案来分发直播的4K视频到消费端设备。 采用了分层视频编码的方式来在高度动态且不可预测的WiGig和WiFi链路上分发质量可变的块。 Le’s work 使用RTSP网络控制协议为CCTV的360度直播推流提出了实时转码和加密系统。 转码方式基于ARIA加密库，Intel媒体SDK和FFmpeg库。 系统可以管理并行的转码操作，实现高速的转码性能。 内容拼接缝合 相比于其他因素如捕获、转码、解码、渲染，内容拼接在决定整体上的推流质量时扮演至关重要的角色。 Chen’s work 提出了一种内容驱动的拼接方式，这种方式将360度帧的语义信息的不同类型看作事件，以此来优化拼接时间预算。 基于VR帧中的语义信息，tile执行器模块选择合适的tile设计。 拼接器模块然后执行基于tile的拼接，这样，基于可用资源，事件tile有更高的拼接质量。 评估表明系统通过实现89.4%的时间预算，很好地适应了不同的事件和时间限制。 ","date":"2021-11-04","objectID":"/posts/note5/:4:1","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"总结 相比于点播式流媒体，360度直播推流面临多个挑战，例如在事先不知情的情况下处理用户导航、视频的首次流式传输以及实时视频的转码。在多用户场景中，这些挑战更为棘手。 关于处理多个用户的观看模式，可伸缩的多播可以用于在低带宽和高带宽网络上以接近于按需推流的质量等级。 基于ROI的tile拼接和转码可以显著地减少延迟敏感的交互型应用的延迟需求。 ","date":"2021-11-04","objectID":"/posts/note5/:4:2","tags":["Immersive Video"],"title":"自适应360度视频推流挑战","uri":"/posts/note5/"},{"categories":["paper"],"content":"概况 现有的沉浸式流媒体应用都对带宽、QoS和计算需求有着高要求，这主要得益于5G网络。 传统的中心化云计算和云存储体系结构不适于实时的高码率内容分发。 边缘缓存和移动边缘计算成为了推动沉浸式流媒体发展的关键技术。 解决方案 ","date":"2021-10-30","objectID":"/posts/note4/:0:0","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/note4/"},{"categories":["paper"],"content":"360度视频的边缘协助推流 ","date":"2021-10-30","objectID":"/posts/note4/:1:0","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/note4/"},{"categories":["paper"],"content":"背景 主要的视频内容可以被传送到边缘节点乃至下游客户端来满足高分辨率等级和严格的低延迟要求。 在边缘计算中，处理和存储的任务被从核心网转移到边缘节点例如基站、微型数据中心和机顶盒等。 Hou’s work 提出边缘/云服务器渲染可以使计算更加轻便，可以让无线VR/AR体验可行并且便携。 Zhang’s work 为VR多人游戏提出了一种混合边缘云基础架构，中心云负责更新全局游戏事件，边缘云负责管理视图更新和大规模的帧渲染任务，以此来支持大量的在线联机人数的低延迟游戏。 进一步陈述了一种服务器选择算法，它基于QoS和玩家移动的影响确保所有VR玩家之间的公平性。 Lo’s work 考虑了为360度视频渲染提供边缘协助的设备的异质性。 边缘服务器将 HEVC tile流转码为viewport视频流并传输到多个客户端。 最优化算法根据视频质量、HMD类型和带宽动态决定边缘节点服务哪个客户端。 边缘缓存策略 背景 传统视频的缓冲方案并不能直接应用到360度视频上。 为了在启用边缘缓存的网络中促进360度视频的传输，两个传输节点之间的代理缓存被部署来使用户侧的内容可用。 边缘缓存能从实质上减少重复的传输并且可以使内容服务器更加可扩展。 Mahzai’s work 基于其他用户的观看行为为360度视频的流行内容提出了一种缓存策略。 与最不常用 (LFU) 和最近最少使用 (LRU) 缓存策略相比，在缓存使用方面的性能分别提高了至少 40% 和 17%。 Papaioannou’s work 提出了基于tile分辨率和需求统计信息的缓存策略，用最少的错误，提高要求tile的和缓存tile这两种版本的viewport覆盖率。 不同缓存和传输延迟的实验评估表明提高了缓存命中率，特别是对于分层编码的tile。 Liu’s work 背景： 边缘缓存可以被在Evolved Packet Core处执行，因为packet大小很小所以这样可能会产生次优的性能。 另一种替换的方式是在Radio Access Network处缓存数据。但这样由于数据隧道和分包会变得更加复杂。 研究内容： 为移动网络提出了一种同时使用RAN和EPC的基于tile的缓存方案，以此在视频流延迟的约束下节省传输带宽。 为EPC和RAN的缓存节点分别被部署在Packet Data Network Gateway和eNodeBs。 EPC中的内容控制实体负责为tile内容改善缓存利用率。 这种联合的tile缓存设计能以优秀的可伸缩性为回程网络显著地减少带宽压力。 Maniotis’s work 为了利用协作传输的机会，提出了一种在包含宏蜂窝基站(MBS)和多个小基站(SBS)的蜂窝网络中的tile级别的视频流行度感知缓存和传输方案。 应用了一种高级的编码方式来创建灵活的tile编码结构，使在每个SBS中能协同缓存。 这种协同允许在 SBS 只存储可能被观看的图块，而其他图块可以通过回程链路获取。 Chen’s work 为被捕获内容从Drone base station到小基站的联合缓存和分发提出了一种echo-liquid状态的 DRL 模型，使用高频毫米波通信技术。 为了满足即时延迟的目标，基站可以从数据中缓存流行内容。 但是，小基站的广泛部署实际上消耗了很多能源。 Yang’s work 在计算资源受限制的MEC架构中，利用缓存和计算资源来降低对通信资源的要求。 但是这种结构需要资源敏感的任务调度来平衡通信开销和延迟。 Chakareski’s work 为multi-cell网络环境中的AR/VR应用探索了最前沿的缓存、计算和通信机制。 提出的框架允许基站利用适当的计算和缓存资源来最大化总计的回报。 只关注了缓存和渲染，没有考虑用户视角的感受以及处理事件。 Sun’s work 在内容到达终端之前，同时利用FoV(Field of View)缓存和必要的计算操作来节省通信带宽而不牺牲响应时间。 对于同质的FoVs，联合缓存和计算框架执行关于缓存和后期处理的最优决策。 对于异质的FoVs，应用凹凸表达式来得到有吸引力的结果。 Rigazzi’s work 基于一个开源项目Fog05提出了一个三层(3C)解决方案来分发密集的任务（例如编解码和帧重建），穿越中心云层，受约束的雾层和边缘节点层。 利用了系统可伸缩性、互操作性和360度视频推流服务的生命周期循环。 实验性的评估表明在带宽、能源消耗、部署开销和终端复杂性方面取得了显著的减少。 Elbamby’s work 通过在延迟和可靠性的约束下，应用积极的计算和毫米波传输，为交互式的VR游戏提出了一个联合框架。 对视频帧做预计算和存储来减少VR流量。 评估表明这种联合机制可以减少多达30%的端到端延迟。 边缘计算的优势 减少延迟 传统的云端节点距离用户较远，边缘计算使用户可以共享多个服务器池的协同计算资源。 降低能耗 根据网络架构和资源供应将计算卸载到分布式计算集群，能显著提高移动设备的性能。 负载均衡 边缘节点例如基站、小蜂窝和终端设备可以在用户端存储内容，降低了核心网的负载。 现有利用边缘计算的解决方案 大多数任务卸载的MEC方案只致力于优化带宽、能源或延迟。 发展中的方案同时致力于许多其他重要的目标：可靠性、可移动性、QoS、部署成本、安全性。 利用带缓存的边缘计算的能力可以增强可移动性、位置感知能力、高效的数据分发、网络上下文理解和提供服务的安全性。 层级化的边缘-云体系结构对于适应360度视频快速动态传输是必要的。 相比于单静态层，多个动态缓存模型可以帮助管理唐突的viewport和网络变化来改善多用户的viewport命中率。 无论环境怎样，主动缓存都可以通过采用预测机制来预取和缓存部分视频来提高感知质量。 ","date":"2021-10-30","objectID":"/posts/note4/:1:1","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/note4/"},{"categories":["paper"],"content":"360度视频的协同传输 ","date":"2021-10-30","objectID":"/posts/note4/:2:0","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/note4/"},{"categories":["paper"],"content":"背景 360度视频推流有较大的用户需求并且在逐渐增长。 目前推流viewport之外的冗余信息会浪费重要的网络带宽。 相同的360度视频内容，在带宽受限的网络之上被推流给多个用户时，码率的需求变得更难满足。 几个方法应用了360度视频的协同传输，进而改善传输效率。 ","date":"2021-10-30","objectID":"/posts/note4/:2:1","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/note4/"},{"categories":["paper"],"content":"方案 Ahmadi’s work 引入了基于DASH的加权tile方法来优化子用户组请求的tile编码性能。 提出了多播流方案基于被用户看到的可能性对tile分配适当的权重。 接着基于可用带宽和tile权重为每个子用户组选择tile的码率。 实际上因为相邻tile的不同质量导致了空间质量变化，最终造成糟糕的推流体验。 不必要的离散优化问题巨大，不能保证有积极的表现。 Bao’s work 基于动作预测和并发观看用户的信道条件提出了一种多播框架，来只分发可能被看到的360度视频块。 没有在无线多播传输中考虑优化资源分配。 Guo’s work 为每个用户假设了一种随机动作模式和不稳定的信道条件，并且开发了多播机会来避免冗余数据传输。 作者考虑了两个非凸的问题： 在给定视频质量的约束下，最小化平均传输时间和能源消耗。 在给定的传输时间和能源预算下，最大化每个用户的视频质量。 Long’s work 考虑了传输时间、视频质量的平滑性和能源限制，在单服务器多用户无线网络环境中优化多个用户的聚合效用。 为了减少传输复杂性，作者准备了多种质量的tile，并为每组用户将tile划分到不相邻的子集中。 Zhang’s work 引入了一种使用SVC质量自适应方法的协同推流策略，来改善移动自组网环境中，观看360度内容的多个用户间的带宽共享。 当遇到可用网络资源限制时，提出的启发性方式基于被看到的可能性和聚合的组级别偏好设置选择最优的tile子集。 Kan’s work 提出了一种服务端混合多播-单播协同推流方案来分发不同质量的360度视频到多个用户。 基于用户的观看行为对其进行分簇，以此来轻松共享相同的视频内容。 为每个tile联合选择传输模式和适当的码率来提高整体的QoE。 Huang and Zhang’s work 设计了一种MIMO网络中的MAC调度方式。 资源分配策略基于三个主要的函数 基于延迟的Motion-To-Photon(MTP)VR帧权重计算。 基于最大Aggregate Delay-Capacity Utility（ADCU）的用户选择。 用于平衡VR数据传输的极高需求的链路自适应方法。 Li and Gao’s work 提出了多用户VR框架，其中边缘云自适应地存储和重用冗余VR帧，以减少计算和传输负载。 两级cache的设计：用户端的小型本地cache和边缘的大型中央cache。 通过为所有用户产生背景视图和无论何时都重用帧，使得减少了内存需求。 评估表明帧相关数据和计算负载分别减少了95%和90%。 总结 对推流到多个临近用户的流行内容共享例如360度视频是一种自然的选择。 然而非协作式的用户对带宽的竞争会快速使整个网络瘫痪。 为了为多个用户获得改善的QoE，研究者从以下几个方面做了努力： 确定多个用户可能的需求来公平地分配可用的网络资源。 分析跨用户的行为来精确传输要求的子帧到终端用户。 由于侧信道攻击，保护VR帧传输到多个终端用户。 ","date":"2021-10-30","objectID":"/posts/note4/:2:2","tags":["Immersive Video"],"title":"沉浸式流媒体网络问题的相关解决方案","uri":"/posts/note4/"},{"categories":["knowledge"],"content":"常见的坑 所有标准库容器都支持迭代器，而只有少数几种支持下标运算符。 string虽然不是容器，但是支持很多容器的操作。 容器不为空时：begin()返回的是容器中第一个元素的位置；end()返回的是容器中最后一个元素的后一个位置。 容器为空时：begin()和end()返回的都是最后一个元素的后一个位置。 任何可能改变容器大小的操作都会使容器的迭代器失效。 ","date":"2021-10-28","objectID":"/posts/iterator/:1:0","tags":["C++"],"title":"重学C++：容器和迭代器","uri":"/posts/iterator/"},{"categories":["knowledge"],"content":"必须要理解的点 和指针类似的是，迭代器支持对对象的间接访问。 和指针不同的是，获取迭代器不使用取地址符，有迭代器的类型都拥有返回迭代器的成员函数，如begin(), end()。 所有迭代器都支持的运算： 运算符 例子 含义 * *iter 返回迭代器iter指向元素的引用 -\u003e iter-\u003emem 解引用iter并获取该元素名为mem的成员，即(*iter).mem ++ ++iter 令iter指向当前元素的后一个元素 – --iter 令iter指向当前元素的前一个元素 == iter1 == iter2 如果两个迭代器指向相同的元素返回true，否则返回false != iter1 != iter2 上面例子的反面 迭代器的类型有两种：iterator和const_iterator。 vector\u003cint\u003e::iterator itv; // 可用于读写vector\u003cint\u003e中的元素 string::iterator its; // 可用于读写string对象中的元素 vector\u003cint\u003e::const_iterator citv; // 只能读取元素 string::const_iterator cits; // 只能读取元素 begin()和end()返回哪一种取决于对象本身是否被const修饰。 C++11中引入了cbegin()和cend()来专门返回const_iterator。 认定一种类型是迭代器当且仅当它支持一套操作，这套操作能使我们访问容器内的元素或从某一个元素移动到另一个元素。 vector和string的迭代器支持的额外的运算： 运算 含义 iter + n 运算得到一个新迭代器，指向当前元素的后n个元素的位置 iter - n 运算得到一个新迭代器，指向当前元素的前n个元素的位置 iter += n 运算得到的新迭代器赋值给iter iter -= n 同上 iter1 - iter2 两个迭代器之间的距离，可正可负 \u003e, \u003c, \u003c=, \u003e= 同两类型的下标运算符中的数字的关系，位置靠前的较小 ","date":"2021-10-28","objectID":"/posts/iterator/:2:0","tags":["C++"],"title":"重学C++：容器和迭代器","uri":"/posts/iterator/"},{"categories":["knowledge"],"content":"建议 一般不在意迭代器的类型，因此使用auto来标注。 循环结束的判断条件习惯使用迭代器和!=，这样可以不用在意容器类型。 凡是使用了迭代器的循环体中都不能有改变容器大小的操作如push_back()。 ","date":"2021-10-28","objectID":"/posts/iterator/:3:0","tags":["C++"],"title":"重学C++：容器和迭代器","uri":"/posts/iterator/"},{"categories":["knowledge"],"content":"常见的坑 vector的默认初始化是否合法取决于vector内对象所属的类是否要求显式初始化。 使用()和{}对vector执行初始化含义不同。 using std::vector; vector\u003cint\u003e v1{10}; // 存储1个int对象，值为10 vector\u003cint\u003e v2(10); // 存储10个int对象，值为0 vector\u003cint\u003e v3(10, 1); // 存储10个int对象，值都是1 vector\u003cint\u003e v4{10, 1}; // 存储2个int对象，值分别是10和1 使用{}执行列表初始化时按照顺序遵守2个守则： 如果{}内容可以用于初始化，则采用{}默认的初始化含义。 如果{}中的内容无法用{}默认的初始化含义做出解释，则会按照()的初始化含义去解释{}。 using std::vector; using std::string; vector\u003cstring\u003e v1{\"hi\"}; // 存储1个值为hi的string对象 vector\u003cstring\u003e v2{10}; // 存储10个值为空的string对象 vector\u003cstring\u003e v3{10, \"hi\"}; // 存储10个值为hi的string对象 与string相同，vector也有size_type作为其size()的返回值类型。 但是使用时必须首先指定vector由哪个类型定义。 std::vector\u003cint\u003e::size_type a; // 正确 std::vector::size_type a; // 错误 只有vector内元素的类型可以被比较时才能做比较运算，对于自定义类型需要手动定义运算符重载。 增加vector中的元素只能使用push_back()，而不能使用对下标赋值的方式。 ","date":"2021-10-28","objectID":"/posts/vector/:1:0","tags":["C++"],"title":"重学C++：标准库类模板Vector","uri":"/posts/vector/"},{"categories":["knowledge"],"content":"必须理解的点 vector是类模板而非类型。 vector中只能容纳对象，不能容纳引用。 vector对象能高效增长，增加vector中的元素需要使用push_back()成员函数。 vector的成员函数（empty(), size()）和各种运算符（赋值、关系、下标）的操作使用方法和规则基本同string。 ","date":"2021-10-28","objectID":"/posts/vector/:2:0","tags":["C++"],"title":"重学C++：标准库类模板Vector","uri":"/posts/vector/"},{"categories":["knowledge"],"content":"建议 不需要在创建vector时确定其中的元素及其大小。 在循环体内部包含向vector对象添加元素的操作时，不应该使用foreach循环。 ","date":"2021-10-28","objectID":"/posts/vector/:3:0","tags":["C++"],"title":"重学C++：标准库类模板Vector","uri":"/posts/vector/"},{"categories":["knowledge"],"content":"常见的坑 string.size()和string.length()等价。 string.size()和其他STL容器的命名风格相一致（如vector, map）。 string.length()出现主要是因为这样的命名符合人的直觉，有更好的可读性。 string::size_type是无符号类型，和int不同，能存放下任何string对象的大小。 +两边至少有一端需要是string对象，不允许两个字符串字面量单独相加。 using std::string; string a = \"a\"; string b = a + \"b\" + \"c\"; // 正确，从左到右运算时能保证至少一段是string对象 string c = \"b\" + \"c\" + a; // 错误，从左到右运算时第一个+左右都是字符串字面量 ","date":"2021-10-28","objectID":"/posts/string/:1:0","tags":["C++"],"title":"重学C++：标准库类型string","uri":"/posts/string/"},{"categories":["knowledge"],"content":"必须要理解的点 string的初始化方式有两种，一种是默认初始化，另一种是拷贝初始化。 string.size()返回值类型为string::size_type，出现这种类型是为了体现标准库类型和机器无关的特性。 string对象的比较运算完全实现了运算符重载（==, !=, \u003c,\u003c=, \u003e, \u003e=）。 ==表明两个对象的内容和长度完全一致，反之任一不同则!=。 不等关系运算符比较的法则： 如果两个对象长度不同，但是从前到后内容一致，则长度较短的对象较小。 如果两个对象从前到后有对应位置的字符不同，则这个位置的两个字符的大小关系就是两个对象的大小关系。 string对象赋值操作就是内容的替换。 string对象相加操作就是内容的拼接，+=操作同理。 string对象可以与字符串字面量相加。 形如cname的C++头文件兼容形如ctype.h的C头文件，C++头文件中定义的名字可以在std中找到。 ","date":"2021-10-28","objectID":"/posts/string/:2:0","tags":["C++"],"title":"重学C++：标准库类型string","uri":"/posts/string/"},{"categories":["knowledge"],"content":"建议 表达式中出现string.size()函数时就不应该使用int类型，这样可以避免int和unsigned混用的问题。 C++和C兼容的头文件作选择时，选择C++的头文件。 处理string对象中每一个字符时，使用foreach语句。 #include \u003ciostream\u003e #include \u003ccctype\u003e using std::string; string str{\"Some String\"}; for (auto c : str) { std::cout \u003c\u003c c \u003c\u003c std::endl; } // 使用引用来改变原字符串内容 for (auto \u0026c : str) { c = std::toupper(c); } std::cout \u003c\u003c str \u003c\u003c std::endl; 处理string对象中特定字符时使用[]（下标运算符）或者迭代器。 使用[]访问字符之前检查string对象是否为空。 std::string s = \"a\"; if (!s.empty()) { std::cout \u003c\u003c s[0] \u003c\u003c std::endl; } string对象下标使用string::size_type作为类型而非int。 using std::string; string a = \"Hello, world!\"; string::size_type index_of_space = a.find(\" \"); ","date":"2021-10-28","objectID":"/posts/string/:3:0","tags":["C++"],"title":"重学C++：标准库类型string","uri":"/posts/string/"},{"categories":["knowledge"],"content":"常见的坑 auto可以在一条语句中声明多个变量，但是所有变量的类型必须一致。 decltype在分析表达式类型时并不执行表达式。 decltype处理解引用操作之后返回的是引用类型，而引用类型的变量必须初始化。 decltype((variable))的结果永远是引用。 decltype(variable)的结果只有当variable是引用时才是引用。 ","date":"2021-10-26","objectID":"/posts/auto/:1:0","tags":["C++"],"title":"重学C++：类型推导","uri":"/posts/auto/"},{"categories":["knowledge"],"content":"必须要理解的点 auto用于变量初始化时的类型推导，decltype用于分析表达式的类型。 auto对引用类型推导时实际上用的是引用对象的值。 auto与const：详见重学C++：Const二三事。 decltype与const：详见重学C++：Const二三事。 ","date":"2021-10-26","objectID":"/posts/auto/:2:0","tags":["C++"],"title":"重学C++：类型推导","uri":"/posts/auto/"},{"categories":["knowledge"],"content":"建议 auto尽量只在类型较长但比较清晰时使用。 decltype尽量不要使用。 ","date":"2021-10-26","objectID":"/posts/auto/:3:0","tags":["C++"],"title":"重学C++：类型推导","uri":"/posts/auto/"},{"categories":["knowledge"],"content":"常见的坑 仅用const修饰的对象只在单个文件中有效，如果想在多个文件之间共享const对象，必须在对象定义的前面加extern。 允许为一个常量引用绑定非常量的对象、字面量和表达式。 int i = 42; const int \u0026r1 = i; // 正确 const int \u0026r2 = 42; // 正确 const int \u0026r3 = r1 * 2; // 正确 int \u0026r4 = r1 * 2; // 错误 int \u0026r5 = i; r5 = 0; // 正确 r1 = 42; // 错误 指向常量的指针和常量指针： int err_numb = 0; const double pi = 3.1415; int *const cur_err = \u0026err_numb; const double *mut_pi_pointer = \u0026pi; const double *const pi_pointer = \u0026pi; 从声明语句的变量符号开始，自右向左看： cur_err首先是一个不可变对象，其次是一个指向int类型可变对象的指针。 mut_pi_pointer首先是一个可变对象，其次是一个指向double类型不可变对象的指针。 pi_pointer首先是一个不可变对象，其次是一个指向double类型不可变对象的指针。 当typedef遇到const时容易出现错误理解： typedef char *pstring; const pstring cstr = 0; const pstring *ps = 0; pstring是char *的别名，即指向char的指针。 const修饰的是pstring，因此cstr是：初始化值为nullptr的不可变指针。 错误理解会用char *替换掉pstring，即： const char *cstr = 0; 这样从cstr开始自右向左读的话，cstr就会被理解成：指向字符常量的可变指针。 constexpr属于顶层const，因此constexpr修饰指针意味着指针本身不可变。 auto默认会去除顶层const，保留底层const，如果需要顶层const则需要显式加入。 int i = 0; const int ci = i, \u0026cr = ci; auto b = ci; // b是一个初始化值为0的可变int对象 auto c = cr; // c同b auto d = \u0026i; // d是一个初始化为指向可变int类对象i的可变指针对象 auto e = \u0026ci; // e是一个初始化为指向不可变int类对象ci的可变指针对象 const auto f = ci; // f是一个初始化值为0的不可变int对象 decltype不会去除顶层const。 const int ci = 0; decltype(ci) x = 0; // x的类型是const int ","date":"2021-10-26","objectID":"/posts/const/:1:0","tags":["C++"],"title":"重学C++：Const二三事","uri":"/posts/const/"},{"categories":["knowledge"],"content":"必须要理解的点 const对象在创建时必须进行初始化。 常量引用即对const对象的引用。 常量引用绑定不可变对象和可变对象时含义不同。 可变对象 不可变对象 用常量引用绑定 可以 必须 常量引用的含义 不能通过此引用改变对象的值 不可以改变对象的值 常量引用绑定到可变对象上：对原有可操作性质的窄化，减少操作肯定不会引发错误，所以是允许的。 非常量引用绑定到不可变对象上：对原有可操作性质的拓宽，增加不允许的操作会出错、，所以不可变对象必须使用常量引用。 因为指针是对象，而引用不是对象，所以const和指针的组合有2种情况，const和引用的组合只有1种情况。 指针 指向常量的指针（pointer to const）：不能通过此指针修改对应的量。 常量指针（const pointer）：指针本身的值不可变，即不能用指针指向其他对象，这种不可重新绑定的特性类似于引用。 引用 常量引用：不能通过此引用修改对应的量。 顶层const表示指针本身是常量，推广之后可以指任意对象是常量； 底层const表示指针指向的对象是常量，推广之后主要于指针和引用等复合类型的基本类型部分有关。 常量表达式指：值不会改变，在编译过程中就能得到计算结果的表达式。 为什么需要constexpr？ 因为实际中很难判断一个初始值是否为常量表达式。 使用constexpr相当于把验证变量的值是否是一个常量表达式的工作交给了编译器。 用constexpr声明的变量一定是一个变量，并且必须用常量表达式来初始化。 ","date":"2021-10-26","objectID":"/posts/const/:2:0","tags":["C++"],"title":"重学C++：Const二三事","uri":"/posts/const/"},{"categories":["knowledge"],"content":"建议 如果认定变量是一个常量表达式，那就将其声明成constexpr类型。 ","date":"2021-10-26","objectID":"/posts/const/:3:0","tags":["C++"],"title":"重学C++：Const二三事","uri":"/posts/const/"},{"categories":["knowledge"],"content":"常见的坑 \u0026和*在不同的上下文里面其含义并不相同，因此完全可以当成不同的符号看待。 int i = 42; int \u0026r = i; // \u0026在类型名后出现，是声明的一部分，表明r是一个引用 int *p; // *在类型名后出现，是声明的一部分，表明p是一个指针 p = \u0026i; // \u0026在表达式中出现，是取地址符 *p = 43; // *在表达式中出现，是解引用符 int \u0026r2 = *p; // \u0026是声明的一部分，*是解引用符 指针可以用0进行初始化成空指针，但是不可以用0赋值。 指针之间使用==来比较时，如果结果是true，对应多种情况： 都是空指针 都是同一个地址 都指向同一个对象 一个指针指向某一个对象，另一个指针指向另一对象的下一地址 ","date":"2021-10-26","objectID":"/posts/reference-and-pointer/:1:0","tags":["C++"],"title":"重学C++：引用和指针","uri":"/posts/reference-and-pointer/"},{"categories":["knowledge"],"content":"必须要理解的点 引用和指针——都可以用于间接访问对象 引用 指针 复合类型 ✅ ✅ 表示符号 \u0026 * 含义 变量的别名 变量在内存中的地址 初始化和赋值时是否需要类型匹配 必须匹配（除常量引用） 必须匹配（除void*和指向常量的指针） 是否需要初始化 必须初始化 无需初始化 可否重新绑定其他变量 不可以 可以 可否嵌套定义 不可以 可以 引用： 引用只能绑定在对象上，不能绑定在字面量或者表达式上。 引用只是原有对象的别名，并非对象，因此不可以定义引用的引用。 定义引用时并不开辟新的内存空间，因此不可以定义引用的指针。 指针： 指针本身就是一个对象，能执行的操作自由度远超过引用。 可以实现嵌套定义，即指针的指针。 可以实现指针的引用。 int i = 42; int *p; // p是int型指针 int *\u0026r = p; // r是指针p的引用，从r开始自右向左读，\u0026表明r是一个引用，引用的是指针，指针指向的类型是int r = \u0026i; // r是p的别名，即给p赋值为i的地址，即令p指向i *r = 0; // r是p的别名，对r解引用即对p解引用，即将p所指向的地址处变量的值赋值为0 指针初始化和赋值时需要使用\u0026运算符取得对象的地址。 指针值的情况： 指向一个对象。 指向紧邻对象所占空间的下一个位置。 空指针，没有指向任何对象。 无效指针，除上述情况之外。 对第4种无效指针的操作是未定义的，后果无法预计。 2、3两种值虽然有效，但是因为没有指向任何对象，所以对其操作的后果同样无法预计。 void*眼中内存空间仅仅是内存空间，并不能访问内存空间中的对象。 ","date":"2021-10-26","objectID":"/posts/reference-and-pointer/:2:0","tags":["C++"],"title":"重学C++：引用和指针","uri":"/posts/reference-and-pointer/"},{"categories":["knowledge"],"content":"建议 初始化所有的指针，并且在对象定义完成之后再定义指向它的指针。 避免使用0和NULL初始化空指针，应该使用nullptr。 在使用指针之前检查其是否为nullptr。 记住赋值改变的永远是等号左侧的对象。 面对复杂的指针或引用的声明语句时，从变量名开始自右向左阅读来弄清楚其真实含义。 ","date":"2021-10-26","objectID":"/posts/reference-and-pointer/:3:0","tags":["C++"],"title":"重学C++：引用和指针","uri":"/posts/reference-and-pointer/"},{"categories":["paper"],"content":"概述 360 度视频的推流手段逐渐从视角独立型方案变成基于 tile 的视角依赖型方案。 相比于常规视频，360 度视频被编码成全向的场景。 自适应 360 度视频推流利用 DASH 框架来实现比特率的自适应。 ","date":"2021-10-25","objectID":"/posts/note3/:1:0","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/note3/"},{"categories":["paper"],"content":"分类 ","date":"2021-10-25","objectID":"/posts/note3/:2:0","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/note3/"},{"categories":["paper"],"content":"Viewport-Independent Streaming 服务端的任务 使用如 ERP、CMP 等视角独立型的投影方式，360 度视频被投影到一个球体上。 客户端的任务 投影之后的视频直接被传送到客户端，并不需要来自传感器的方向信息。 客户端需要支持对应的投影格式。 客户端像处理传统视频一样完成比特率自适应。 基于网络特征向将要到来的 segment 请求相同投影格式的表示 DASH 插件需要支持相同质量视频的推流。 应用 视角独立型推流主要用于体育、教育和旅游视频内容。 优点 简单 缺点 相比于视角依赖型方案视频编码效率低了 30%。 为不可见的区域要求大量带宽和解码资源。 ","date":"2021-10-25","objectID":"/posts/note3/:2:1","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/note3/"},{"categories":["paper"],"content":"Viewport-Dependent Streaming 终端设备的任务 只接受特定的视频帧内容，包括等于或大于视角角度的可见信息。 监测相关的视角作为用户头部移动的回应，并且向服务端发送信号来精确播放器信息。 为服务端准备和用户方向相关的几个自适应集。 客户端的任务 根据网络情况和估计的视角位置决定获取哪个自适应集。 难点 可视区域的确定 与用户头部移动的同步 质量调整 提供平滑的播放体验 现有的工作 各种投影方式在实际推流中表现如何？ 相比于金字塔格式，为视角依赖型投影方案提出的多分辨率变体有最好的研究和开发(RD)性能。 偏移 CMP 获得了 5.6%到 16.4%的平均可见质量。 提出的框架可以基于已知的网络资源和未来的视角位置适应视角的尺寸和质量。 相比于理想的下载过程，这种二维自适应策略可以花费 20%的额外网络带宽下载超过 57%的额外视频块。 如何在网络资源受限的情况下提供高质量的推流？ 为视角依赖型推流产生不同质量的 segment。 当流中只有有限的 representation 时，利用 Quality Emphasized Regions 策略来缩放特定区域的分辨率。 在拥塞网络条件下，执行了基于网络回应的视角大小和比特率的联合适应，结果显示，相比于传送全部的 360 度场景，动态的视角覆盖率提供了更好的画面质量。 这种基于网络回应的自适应也确保基于整体拥塞变化做调整时能改善视频质量。 为立体视频的背景和前景视图采用不对称质量。 可以分别为背景块和前景块分别节省 15%和 41%的比特率。 DASH 需要做什么？ manifest 中需要包含视角位置信息和投影元数据。 优化获取 random access point 的周期来优化视角分辨率自适应体验。 考虑低延迟和活跃的视角切换。 ","date":"2021-10-25","objectID":"/posts/note3/:2:2","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/note3/"},{"categories":["paper"],"content":"Tile-based Streaming 传统视频被分成多个块，360 度视频在块的基础上还被分成多个大小相等或者不等的 tile，以此更加精确地调整画面的细节质量。 分块策略 基本完全交付 高级完全交付 部分交付 分块模式 1x1，3x2，5x3，6x4，8x5 其中 6x4 的模式实现了较好的带宽消耗和编码效率的折中。 在不同的带宽条件下，基本完全交付策略获得了大约 65%的带宽节约。 具体方案 ClusTile 基于分簇的方式，推送满足最小带宽需求的 tile 来克服编码效率和计算开销。 相比于传统和高级的基于 tile 的推流方案，分别实现了 72%和 52%的带宽节约。 当实际看到的和下载的 tile 有差异时，基于分簇的 tile 选取可能会导致选择不当。 Ghosh’s work 提议以最低可获得的质量下载周围和远处的 tile。 相比于其他算法，视角及其周边区域的可变质量提高了 20%的 QoE 水平。 Ozcinar’s work 介绍了一种自适应 360° 视频流框架。 利用视觉注意力度量来计算每个帧的最佳平铺模式。 使用选中的模式，为不同区域的 tile 分配非统一的比特率。 比特率的选取取决于估计的视角和网络状况。 因为很大部分的带宽被用于传输非视角内的 tile，框架难以优化视角内的质量。 Xie’s work 提出了一套优化框架，以此来最小化预取 tile 的错误，改善与不同比特率相关联的 tile 边界的平滑程度。 定义了两个 QoE 函数，目标是最小化： 预期质量失真$\\Phi(X)$ 当考虑 tile 看到概率时视角的空间质量方差$\\Psi(X)$： $$ \\Phi(X) = \\frac{\\sum_{i=1}^{N}\\sum_{j=1}^{M}D_{i,j} * x_{i,j} * p_{i,j}}{\\sum_{i=1}^{N}\\sum_{j=1}^{M}x_{i,j} * s_{i}} $$ $$ \\Psi(X) = \\frac{\\sum_{i=1}^{N}\\sum_{j=1}^{M}x_{i,j}*p_i * (D_{i,j} - s_i * \\Phi(X))^{2}}{\\sum_{i=1}^{N}\\sum_{j=1}^{M}x_{i,j}*s_i} $$ 基于目标缓冲区的自适应方法用于在需要短期视口预测的小缓冲区下进行平滑播放 在自适应的第 k 步，当第 k 个 segment 集合下载完成时，缓冲区占用率$b_k$由下面的式子给出： $$ b_k = b_{k-1} - \\frac{R_k*T}{C_k} + T $$ 为了避免用尽所有块，缓冲区的占用率被通过设定一个目标缓冲区水平$B_{target}$所控制，即$b_k = B_{target}$。 平均空间质量方差是 0.97，比其他基于 tile 的策略小。 所提出的概率自适应框架在感知质量上实现了约 39% 的增益，平均降低了 46% 的空间质量方差。 Vander Hooft’s work 将 360 度帧划分成视角内区域和视角外区域。 首先为所有区域都选择最低质量，然后提高视角内 tile 的质量。 如果带宽依然可用，接着提高剩下的 tile 的质量。 启发式的方式在带宽可用的基础上积极提高视角内 tile 的质量。 没有考虑视角比特率调整时视角预测的错误。 Nguyen’s work 提出了一种新的自适应机制，它在每个 segment 中同时考虑头部移动和视角的预测错误，动态地决定视角内的比特率。 联合适应扩展块的覆盖范围和比特率。 在不同记录的用户头部运动下的实验评估表明，在不获取非视角内区域过多带宽利用率的情况下，视角内容质量有所提高。 DASH SRD 扩展 DASH 的 SRD 扩展提供了多种版本的 tile 的关联来节省更多的比特率。 Le Feuvre and Concolato’s work 他们应用了这个 SRD 特性，引入了同时为独立的和运动受限的 HEVC tile 的不同优先级设定，以此来高效地实现基于 tile 的方案。 使用开源的 GPAC 多媒体框架开发了一个 DASH 客户端，以此来执行带有可配置参数的基于 tile 的推流。 D’Acunto’s work 提出了一种 MPEG-DASH SRD 方法来促进可缩放和可平移视频的平滑推流。 总是下载低分辨率的 tile 来避免用户移动视角时的重新缓冲。 当前视野区域被上采样并展示给用户，以此来支持高质量的缩放功能。 用JavaScript实现了 SRD 视频播放器。 Hosseini’s work 基于 SRD 实现了视角内容、相邻 tile 和剩余 tile 的优先级推流。 用 6 个 3D 网格构建了一套 3D 座标系来在 3D 空间中平滑地表示 tile。 相比于基础的方式，这种区分质量的推流方案节省了 72%的带宽。 Kim and Yang’s work 使用改进的 MPEG-DASH SRD 来在质量可变的 tile 层中作选择。 基于他们之前的工作设计并实现了一个支持多层渲染的 360° VR 播放器，以支持高度不可预测的头部运动数据的高分辨率和低延迟流。 Motion-Constrained TileSet 在 HEVC 中，运动约束贴图集(MCTS)是将整个帧表示为子视频的相邻分割，并为自由选择的贴图集提供解码支持。 Zare’s work 将 MCTS 的概念应用到了全景视频推流中。 将两个质量版本的视频分割成 tile，以原始的分辨率推流视角内的 tile，以低分辨率推流剩余的 tile。 它已经表明，选定图块的可变比特率会降低 30% 到 40% 的比特率。 Skupin’s work 陈述了一种使用 HEVC 编码器的基于 tile 的可变分辨率的推流系统。 使用立方贴图投影的 360 度视频被分割成 24 个网格，每个代表了一个独立的比特流。 两种不同质量的版本被推流到客户端，例如 8 个 tile 以高质量推送，16 个 tile 以低质量推送。 Son’s work 在基于视角的移动 VR 推流中，为独立的 tile 提取和传输实现了基于 MCTS 的 HEVC 和可缩放的 HEVC 编解码器。 节省了超过 47%的带宽。 相比于原始的 HM 和 SHM 编码器表现不佳，因为 MCTS 限制了时间运动信息。 Lee’s work 用 MCTS 编码 360 度视频 tile，并使用显著性检测网络将混合质量的视频 tile 推流给终端用户。 通过显著性模型改进 MCTS 的使用，可以在不增加任何复杂性的情况下灵活地对感兴趣的 tile 区域进行解码支持。 Scalable Video Code 可伸缩视频编码 SVC 是实现 viewport 自适应的一种替代策略。 基础层总被需要并且能从客户端预取来避免重新缓冲事件。 提高层改善 viewport 质量并且可以在带宽充足的时候被请求。 SVC 促进了一种高效的网络内缓存支持来减少多个客户端请求相同内容时的分发开销。 Nasrabadi’s work 使用了一种可伸缩编码方案来解决 360 度视频推流的重新缓冲的问题。 存在质量波动的问题，因为没有使用任何机制来处理 viewport 的预测错误。 Nguyen’s work 建议使用 SVC 协同 viewport 预测来克服网络信道和头部运动的随机性。 实验表明，所提出的平铺层更新和后期平铺终止特征可使 viewport 质量提高 17%。 AI 方法的应用 背景：传统视频推流中使用强化学习来高效调整视频比特率和实现长期的 QoE 回报。 和传统视频内容不同，360 度视频包含几个新的方面比如 tile 大小、viewport 预测等。 直接将现有的强化学习自适应策略应用到 360 度视频上可能会降低推流性能。 Fu’s work 为 360 度视频提出了称为360SRL的一种序列化强化学习方法，它基于之前决策的 QoE 回报而非估计的带宽状况做出自适应决策。 360SRL 使用基于 tile 的推流模拟器来增强训练阶段。 跟踪驱动的评估表明，360SRL 比基线适应方法取得了 12%的 QoE 改善。 Jiang’s work 基于历史带宽、缓冲区空间、tile 大小和 viewport 预测错误等，利用强化学习来做 viewport 和非 viewport 内 tile 的比特率选择。 所提出系统的架构由状态缓冲区、视口预测 (VPP) 和 tile 比特率选择 (TBS) 代理组成。 状态缓冲区向 VPP 和 TBS 代理提供用户查看模式和网络状态。 VPP 代理然后使用 LSTM 模型估计下一个 viewport 位置。 TBS 代理由 Asynchronous Advantage Actor-Critic (A3C)算法训练以执行合适的比特率决策。 Quan’s work 通过卷积神经网络(CNN)提取像素运动来分析用户 QoE，并使用它对 tile 动态分组，从而在视频质量和编码效率之间提供重要的平衡。 使用了基于强化学习的自适应代理，它可以智能地使每个图块的质量适应动态环境。 使用真实 LTE 带宽跟踪验证该方案，在感知质量方面表现出了卓越的性能，同时也节省了带宽资源。 背景：深度学习使强化学习能够使用多方面的状态和动作空间进一步优化聚合回报。 Kan and Xiao’s work 设计了一套深度强化学习的框架，基于对环境因素的探索和开发，自适应地调整推流策略。 这两种方案都采用 DRL 的 A3C 算法来进行比特率","date":"2021-10-25","objectID":"/posts/note3/:2:3","tags":["Immersive Video"],"title":"自适应360度视频推流方案","uri":"/posts/note3/"},{"categories":["paper"],"content":"概述 自适应方案可以在处理不同目标对象时帮助改善推流体验。 目标主要包括视频质量、功耗、负载均衡等在移动无线网和有线网接入的情形。 适应性的视频比特率需要同时匹配网络条件和质量目标的需求。 ","date":"2021-10-21","objectID":"/posts/note2/:1:0","tags":["Immersive Video"],"title":"自适应视频推流方案","uri":"/posts/note2/"},{"categories":["paper"],"content":"分类 ","date":"2021-10-21","objectID":"/posts/note2/:2:0","tags":["Immersive Video"],"title":"自适应视频推流方案","uri":"/posts/note2/"},{"categories":["paper"],"content":"服务端适应 大多数服务端适应的方案要求客户端发送系统或网络相关信息。 质量导向的适应方案（Quality-Oriented Adaptive Scheme/QOAS） 向终端用户提供了高知觉质量的媒体内容。 QOAS 是 C-S 架构，决策在服务器端产生。 QOAS 基于客户知觉质量的反馈，提供对推流质量等级的调整。 智能优先级适应方案（intelligent Prioritized Adaptive Scheme/iPAS） 专用于 802.11 网络。 iPAS 服务器上的基于固有印象的带宽分配模块被用于组合 QoS 相关的参数和视频内容特征来进行内容的优先级分类和带宽份额分配。 通过区分多媒体流，iPAS 提供可用无线信道的优先级分配。 设备导向的适应方案（Device-Oriented Adaptive multimedia Scheme/DOAS） 专用于 LTE 网络，建立在 LTE 下行链路调度机制之上。 DOAS 专门根据设备特性实现适配，尤其为多屏终端用户提供了卓越的 QoE。 ","date":"2021-10-21","objectID":"/posts/note2/:2:1","tags":["Immersive Video"],"title":"自适应视频推流方案","uri":"/posts/note2/"},{"categories":["paper"],"content":"客户端适应 基于吞吐量的自适应方案 这类方案基于估计的网络吞吐量从服务端选择视频的比特率。 HTTP 客户端通过之前的观察记录来估计网络的吞吐量。 通过测量端获取时间（segment fetch time/SFT）来代表发起和收到回复的瞬时 HTTP GET 请求之间的时间段，以此来确定一个推流会话中吞吐量的变化，进而独立地做出适应决策。 在分布式网络中，同时考虑并发和顺序的 SFT。通过比较实际的和理想的 SFT 来选择未来的 segment 的质量等级。 FESTIVE 算法 适用于多个 HAS 客户端共享一个常见的拥塞带宽链路的情形。 以效率、稳定性、公平性为度量因素的适应性算法。 探索了一种为分段调度、吞吐量估计和比特率选择而生的健壮的机制。 包含一个随机调度器来调度下一个视频块的下载。 多个客户端共享容量为$W$的满带宽链路，每个客户端$x$在$t$时刻播放的视频比特率为$b_x,_t$ ，需要避免以下 3 种问题： Inefficiency：多个 HAS 客户端必须能选择最可能的表示来提高 QoE。 $$ Inefficiency = \\frac{|\\sum_{x}b_x,_t - W|}{W} $$ 低Inefficiency值表明多个客户端对带宽实现了最有效的利用。 Unfairness：可用带宽应该被均等地分配。 $$ Unfairness = \\sqrt{1-JainFair} $$ 低Unfairness值表明多个客户端有相近的比特率。 Instability：不必要的比特率切换会损害推流体验 $$Instability = \\frac{\\sum_{d=0}^{k-1}|b_{x,t-d} - b_{x,t-d-1}|*w(d)}{\\sum_{d=1}^{k}b_{x,t-d} * w(d)}$$ Probe AND Adapt(PANDA)算法 用于检测网络状况，考虑未来比特率选择的平均目标数据比特率。 目标是当多个 HAS 客户端共享一个拥塞带宽信道时，通过正确探测网络，进而最小化比特率震荡。 PANDA 算法在性能上击败了 FESTIVE 算法，并且 PANDA 算法在这些解决方案中表现出了最好的适应性，在不同带宽情况和播放器设置下实现了最优的效率、公平性和稳定性。 整体上的推流质量不只依赖于本地的吞吐量测量，还依赖服务端的网络容量。 利用服务器发起的推送机制来降低 DASH 内容推流到移动客户端的端到端延迟。 利用HTTP/2的流终止特性来实现中间质量调整。 基于估计的用户 QoE，功耗和可用资源来改善用户端的推流体验。 虽然有证据表明性能得到了提高，但是评估工作只是在受控的 LAN 环境下有效。 Cross Session Stateful Predictor(CS2P)方案 一种数据驱动的吞吐量估计方案，以克服不准确的 HAS 流量预测问题。 将共享相似特性的推流会话分簇，然后对每个簇使用隐马尔科夫模型预测相应的吞吐量样本。 在一个大规模数据集上实验性的评估表明：CS2P 高效地估计了可用的网络吞吐量，进而改善了整体上的视频比特率的适应性。 CFA 和 Pytheas 等方案和 CS2P 类似，也使用数据驱动的控制器来估计可用的吞吐量。 但是这些工作不支持异构系统并且需要额外的训练复杂性，使其不够具有吸引力。 基于吞吐量的适应性方案主要的挑战在于对吞吐量的精确估计。 为 360 度视频采用一个没有经过精巧设计的吞吐量估计机制可能会导致不稳定性和较差的 QoE，在高度动态化的无线和蜂窝网络中尤甚。 基于缓冲区的自适应方案 客户端会在播放视频时根据当前缓冲区的占用情况请求将要到来的 segment。 如何克服不完整的网络信息的限制 在多客户端启用缓存的环境中，结合客户端测量工具集和补偿算法构造模型。 这个模型可以高效探测比特率切换时间并通过选择切换适当的比特率来进行补偿，最终实现了可达 20%的比特率改善。 Buffer Based Adaptation(BBA)方法 应用于 Netfix 客户端时可以减少可达 20%的重新缓冲事件。 BBA 方法考虑的缓冲区较大，因此对于比较短的视频不一定有这样的性能。 Buffer Occupancy-based Lyapunov Algorithm(BOLA) 把比特率适应性问题看作是与播放质量和重新缓冲时间相关的最优化问题。 BOLA 旨在通过把缓冲区大小保持在设定的目标水平来避免重新缓冲。 对于缓冲区级别的突然下降，BOLA 通过请求最低可用视频比特率来避免停顿事件的频率。 如何优化缓冲区利用率 Adaptation and Buffer Management Algorithm(ABMA+) 基于重新缓冲事件的可能性确定未来 representation 的下载时间。 通过基于预先计算的缓冲区大小和 segment 下载时间选择最大比特率来确保流畅的播放。 这样可以实现低计算开销的良好部署。 Scalable Video Coding(SVC)/Bandwidth Independent Efficient Buffering(BIEB) 基于层分发获取视频块，进而维持稳定的缓冲区大小来避免频繁的中断。 没有考虑 QoE 模型中的卡顿和质量切换。 涉及额外的编码和处理开销。 使用 PID 控制器的控制论方法 强制执行缓冲区设置点来使缓冲区保持在最佳水平。 略微降低视频比特率，以防止不必要的视频比特率调整。 在多个客户端竞争的情况下，不能保证公平性。 如何降低 DASH 流的排队延迟 DASH 流会经历最长可达 1s 的排队延迟和严重拥塞，导致缓冲区膨胀问题，而这会严重损害实时多媒体服务的 QoE。 旨在减少网络拥塞的主动队列管理 (AQM) 策略并没有充分减少这种不必要的延迟。 DASH 客户端根据网络设备的队列大小动态接收窗口大小可以显著减轻缓冲区膨胀效应。 由于长期的 viewport 预测的高度不确定性，充足的缓冲区空间对于 360 度视频的流畅播放来说并不可行。 通常小于 3s 的缓冲区大小对于短期的 viewport 预测来讲比较适合。 由于小缓冲区很有可能造成播放卡顿，因此较短持续时间的 segment 可以被用于基于 tile 的流中，但是相比于长持续时间的 segment，这样也会降低编码效率。 混合自适应方案 客户端同时考虑吞吐量和播放缓冲信号来确定即将到来的 segments 的视频比特率。 Model Predictive Control(MPC) 利用良好定义的参数集合来估计可用的网络和缓冲区资源，进而为高 QoE 的比特率做出最优调整的控制论方法。 提出的 QoE 模型采用视频的平均质量$R_k$，平均比特率切换，重新缓冲事件，和初始延迟$T_s$作计算： $$ QoE_1^K = \\sum_{k=1}^{K}q(R_k) - \\lambda\\sum_{k=1}^{K-1}|q(R_{k+1}) - q(R_k)| - \\mu\\sum_{k=1}^{K}(d_k(R_k)/C_k - B_k)_+ - \\mu_sT_s $$ $C_k$：第 k 个块的可用带宽，$B_k$：第 k 个块的可用缓冲区大小 $\\lambda, \\mu, \\mu_s$：可以根据用户兴趣进行调整的权重 MPC 用调和平均的方法来估计吞吐量，并且能够明确管理复杂的控制对象。 只研究了单播放器的情况，因此没有公平性的考量。 Throughput and Buffer Occupancy-based Adaptation(TBOA) 选择合适的视频比特率来获得单个或多个客户端环境中改进的推流体验。 激进地提高了比特率来最高效地利用可用的带宽。 等待缓冲区超过某个级别，然后降低比特率以获得稳定的性能。 为缓冲区等级设置三个阈值，例如： $0 \u003c B_{min} \u003c B_{low} \u003c B_{high}$ 目标区间在$B_{low}$和$B_{high}$之间。 算法努力使最优区间$B_{opt}满足$ $B_{opt} = B_{low} + B_{high} \\over 2$。 通过控制$B_{low}$和$B_{high}$的阈值，使缓冲区和比特率的变化稳定来应对未知的 TCP 吞吐量。 算法表现的流畅而公平，但是没有把用户满意度的度量考虑在内。 fuzzy logic-based DASH 控制重新缓冲事件和视频推流的质量。 考虑了平均吞吐量的估计方法，获得了更高的视频比特率和更少的质量波动。 没有考虑 QoE 度量。 为了更好地调整比特率做出的改进： 用 Kaufman’s Adaptive Moving Average/KAMA 测量法估计吞吐量。 用 Grey Prediction Model/GPM 来估计缓冲区等级。 竞争流模拟环境中，改进所取得的效果： 平均情况下达到 50%的公平性。 最好情况下达到 17%的更好的接收质量。 Spectrum-based Quality Adaptation(SQUAD)算法 解决吞吐量预测和缓冲区等级估计的不连续性。 吞吐量和缓冲区等级反馈信号都被用于选择恰当的质量。 在一开始获取最低质量的 segment 来减少启动时间。 在视频质量切换频率和幅度方面性能显著提高。 尚未有方案讨论如何在视频质量和带宽利用率之间做出很好的平衡。 Throughput Friendly DASH/TFDASH 获得多个竞争客户端情形下的公平性、稳定性和效率。 通过避免 OFF 端获得了最大并且公平的带宽利用率。 双阈值的缓冲区保证播放时的稳定性","date":"2021-10-21","objectID":"/posts/note2/:2:2","tags":["Immersive Video"],"title":"自适应视频推流方案","uri":"/posts/note2/"},{"categories":["paper"],"content":"360 度流媒体视频框架 ","date":"2021-10-20","objectID":"/posts/note1/:1:0","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/note1/"},{"categories":["paper"],"content":"视频采集和拼接 使用不同的 360 度视频采集相机可以将视频内容存储为 3D 的球形内容 ","date":"2021-10-20","objectID":"/posts/note1/:1:1","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/note1/"},{"categories":["paper"],"content":"使用不同的投影策略实现降维 策略主要分为 2 种：视角独立型和视角依赖型 视角独立型 整个 3D 的视频内容被按照统一的质量投影到 2D 平面上 主要包括等距长方形投影和立方贴图投影 等距长方形投影(ERP) 使用左右偏向和俯仰值将观察者周围的球体展平到二维表面上 视角范围：左 180 度～右 180 度、上 90 度～下 90 度 缺点： 极点处会使用比赤道处更多的像素进行表示，会消耗有限的带宽 由于图像失真导致压缩效率不足 立方贴图投影(CMP) 六面立方体组合用于将球体的像素映射到立方体上的相关像素 在游戏中被广泛应用 优点： 节省空间，相比于等距长方形投影视频体积能减少 25% 缺点： 只能渲染有限的用户视野 视角依赖型 视角内的内容比之外的内容有更高保真度的表示 主要包括金字塔投影、截断方形金字塔投影(TSP)和偏移立方贴图投影 金字塔投影 球体被投影到一个金字塔上，基础部分有最高的质量，大多数的投影区域属于用户的视角方向 优点： 节省空间，降低 80%的视频体积 缺点： 用户以 120 度旋转视角时，视频的质量会像旋转 180 度一样急速下降 截断方形金字塔投影 大体情况和金字塔投影相同，区别在与使用了被截断的方形金字塔 优点： 减少了边缘数据，提高了高码率视频的推流性能 缺点： 使边缘更加锐利 偏移立方贴图投影 与原始的立方贴图投影类似，球体的像素点被投影到立方体的 6 个面上 优点： 视角方向的内容会有更高的质量，提供平滑的视频质量变化 缺点： 存储开销很大 ","date":"2021-10-20","objectID":"/posts/note1/:1:2","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/note1/"},{"categories":["paper"],"content":"编码视频内容 目前主要的编码方式有 AVC/H.264 和 HEVC/H.265。 H.264 使用 16x16 的宏块结构对帧编码。 因为使用了编码器的动作预测的特性，编码的数据大小得到减少。 H.265 相比于同质量的 H.264 编码方式，H.265 编码减少了 50%的比特率。 H.265 支持 tiling 特性来实现高效视频推流。 每个 tile 在物理上被分割然后在普通的流中拼接，并且使用一个解码器来解码。 VVC 相比于 H.265，下一代标准 VVC 有望提高 30%的压缩效率。 ","date":"2021-10-20","objectID":"/posts/note1/:1:3","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/note1/"},{"categories":["paper"],"content":"分包和传输 分包 使用 DASH 协议分包。 传输 依赖于雾计算和边缘计算等技术可以缩短分发中心和客户端之间的距离进而实现快速响应和低缓冲时间。 ","date":"2021-10-20","objectID":"/posts/note1/:1:4","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/note1/"},{"categories":["paper"],"content":"渲染和展示 客户端处理 主流方案是使用客户端处理，但是由于会处理不属于用户视角范围内的视频内容，所以会造成计算资源的浪费。 云端处理 另一种方案是使用云端处理，只有用户视角内的视频内容会被传输到客户端，没有更多的带宽和客户端硬件资源要求。 ","date":"2021-10-20","objectID":"/posts/note1/:1:5","tags":["Immersive Video"],"title":"360度流媒体面临的挑战、机遇和解决方案","uri":"/posts/note1/"},{"categories":["knowledge"],"content":"常见的坑 int, short, long, long long都是带符号的，在前面添加unsigned就能得到无符号类型。 字符型被分为3种：char, signed char, unsigned char，前两种并不等价。 虽然有三种类型，但是实际上只有两种表现形式：有符号的和无符号的。 有符号类型在与无符号类型运算时会隐式转换为无符号类型。 虽然变量初始化时候使用了=号，但是初始化和变量赋值并不相同。 变量默认初始化： 变量类型 位置在函数内部 位置在函数外部 内置类型 undefined 0 自定义类型 由类决定 由类决定 #include \u003ciostream\u003e int default_initialize(int a) { // 输出必定是0 std::cout \u003c\u003c a \u003c\u003c std::endl; int b; return b; } int main() { int a; // 输出是随机值 std::cout \u003c\u003c default_initialize(a) \u003c\u003c std::endl; } 如果在函数体内部试图初始化一个extern标记的变量会引发错误。 在嵌套作用域中，内层作用域中的定义可以覆盖外层作用域中声明的变量。 可以显式使用域操作符::来指明使用哪层的变量。 ","date":"2021-10-18","objectID":"/posts/cpp-types/:1:0","tags":["C++"],"title":"重学C++：类型系统基础","uri":"/posts/cpp-types/"},{"categories":["knowledge"],"content":"必须要理解的点 字面量的意思就是从这个表示形式就能推断其对应类型的量，不同表示形式的字面量和不同类型是多对一的关系。 变量的组成部分：类型和值。说白了就是一个定性一个定量。 类型决定变量在内存里面的存储方式，包括大小和布局方式，以及能参与的运算。 值在实际代码运行过程中则被各种函数使用参与运算。 变量声明和定义： 声明的意思就是：我要用这个变量。 定义的意思就是：我要对这个操作的变量做出定义，规定其具体的细节。 声明 定义 规定变量的类型和名字 ✅ ✅ 申请空间 ✅ 初始化 ✅ 执行多次 ✅ 用extern标记未初始化的变量来表明只对变量作声明： extern int i; //只声明不定义 int i; //声明并且定义 extern int i = 10; //声明并且定义 Q：为什么会有声明和定义这两个概念？ A：因为C++支持分离式编译机制，这允许程序被分割成若干个文件，每个文件可以被独立编译。如果要在多个文件中使用同一个变量，就必须要将声明和定义分离。变量的定义必须且只能出现在一个文件中，其他用到这个变量的文件必须对其进行声明，且绝对不能进行重复定义。 名字的作用域： 同一个名字在不同的作用域中可以指向不同的实体。 名字的有效区域始于声明语句，以声明语句所在的作用域末端结束。 ","date":"2021-10-18","objectID":"/posts/cpp-types/:2:0","tags":["C++"],"title":"重学C++：类型系统基础","uri":"/posts/cpp-types/"},{"categories":["knowledge"],"content":"建议 明确数值不可能为负时使用unsigned类型。 使用int执行整数运算，范围不够时使用long long。 使用double执行浮点数运算。 算术表达式中不要使用bool和char。 避免写出依赖实现环境的代码，否则代码不可移植。 避免有符号类型和无符号类型之间的隐式类型转换。 C++11中引入了列表初始化，例如： // 传统的初始化方式 int units_sold = 0; int units_sold(0); // 现代的初始化方式 int units_sold{0}; int units_sold = {0}; 列表初始化在用于内置类型变量时，如果初始值存在丢失信息的风险，编译器会报错。 long double pi = 3.1415926536; int a{pi}, b = {pi}; // 错误：没有执行类型转换，因为可能丢失信息 int a(pi), b = pi; // 正确：执行了隐式类型转化，丢失了信息 对每个内置类型的变量都执行显式默认初始化以防止undefined行为。 在变量第一次使用的地方进行定义操作。 ","date":"2021-10-18","objectID":"/posts/cpp-types/:3:0","tags":["C++"],"title":"重学C++：类型系统基础","uri":"/posts/cpp-types/"},{"categories":["development"],"content":"原仓库地址：Immersive-Video-Sample 修改之后的仓库：Immersive-Video-Sample ","date":"2021-10-09","objectID":"/posts/immersive-video-deploy/:0:0","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/immersive-video-deploy/"},{"categories":["development"],"content":"Server 端搭建 ","date":"2021-10-09","objectID":"/posts/immersive-video-deploy/:1:0","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/immersive-video-deploy/"},{"categories":["development"],"content":"修改 Dockerfile 手动设置 wget 和 git 的 http_proxy 旧 package 目录 not found，修改为新 package 目录 因为找不到 glog 库因此加入软链接操作 ln -s /usr/local/lib64/libglog.so.0.6.0 /usr/local/lib64/libglog.so.0 ","date":"2021-10-09","objectID":"/posts/immersive-video-deploy/:1:1","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/immersive-video-deploy/"},{"categories":["development"],"content":"重新编译内核 运行脚本时显示 libnuma 错误因此推断与 numa 设置有关 执行numactl -H显示只有一个 node，报错输出显示需要至少两个 numa 节点 查询资料之后获知可以使用 fakenuma 技术创造新节点，但是 Ubuntu 默认的内核没有开启对应的内核参数 手动下载 Linux 内核源代码到/usr/src/目录 wget https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.11.1.tar.gz 解压 tar xpvf linux-5.11.1.tar.gz 复制现有内核配置 cd linux-5.11.1 \u0026\u0026 cp -v /boot/config-$(uname -r) .config 安装必要的包 sudo apt install build-essential libncurses-dev bison flex libssl-dev libelf-dev 进入内核配置界面 sudo make menuconfig 按下/键分别查询CONFIG_NUMA和CONFIG_NUMA_EMU位置 手动勾选对应选项之后保存退出 重新编译并等待安装结束 sudo make -j $(nproc) \u0026\u0026 sudo make modules_install \u0026\u0026 sudo make install 修改grub启动参数加入 fake numa 配置 sudo vim /etc/default/grub 找到对应行并修改为 GRUB_CMDLINE_LINUX=\"numa=fake=2\" 更新grub并重启 sudo update-grub \u0026\u0026 sudo reboot 执行numactl -H检查 numa 节点数目为 2 重新执行脚本如图说明一切正常 ","date":"2021-10-09","objectID":"/posts/immersive-video-deploy/:1:2","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/immersive-video-deploy/"},{"categories":["development"],"content":"Client 端搭建 需要 Ubuntu18.04 环境，虚拟机中安装之后按照 README 命令，执行脚本一切正常 ","date":"2021-10-09","objectID":"/posts/immersive-video-deploy/:2:0","tags":["Immersive-Video"],"title":"部署 Immersive Video OMAF-Sample","uri":"/posts/immersive-video-deploy/"},{"categories":["knowledge"],"content":"Delete old sync files sudo rm /var/lib/pacman/sync/* ","date":"2021-06-11","objectID":"/posts/how-to-fix-gpgme-error/:1:0","tags":["linux"],"title":"修复 Archlinux 上出现的 GPGME Error","uri":"/posts/how-to-fix-gpgme-error/"},{"categories":["knowledge"],"content":"Re init pacman-key sudo pacman-key --init ","date":"2021-06-11","objectID":"/posts/how-to-fix-gpgme-error/:2:0","tags":["linux"],"title":"修复 Archlinux 上出现的 GPGME Error","uri":"/posts/how-to-fix-gpgme-error/"},{"categories":["knowledge"],"content":"Populate key sudo pacman-key --populate ","date":"2021-06-11","objectID":"/posts/how-to-fix-gpgme-error/:3:0","tags":["linux"],"title":"修复 Archlinux 上出现的 GPGME Error","uri":"/posts/how-to-fix-gpgme-error/"},{"categories":["knowledge"],"content":"Re sync sudo pacman -Syyy Now you can update successfully! ","date":"2021-06-11","objectID":"/posts/how-to-fix-gpgme-error/:4:0","tags":["linux"],"title":"修复 Archlinux 上出现的 GPGME Error","uri":"/posts/how-to-fix-gpgme-error/"},{"categories":["development"],"content":"Get Correct Version microsoft-edge-dev --version The output is Microsoft Edge 91.0.831.1 dev in my case. ","date":"2021-03-26","objectID":"/posts/python-selenium-settings-on-microsoft-edge-dev-on-linux/:1:0","tags":["Python"],"title":"在 microsoft-edge-dev 上设置 Python selenium","uri":"/posts/python-selenium-settings-on-microsoft-edge-dev-on-linux/"},{"categories":["development"],"content":"Get Corresponding WebDriver Find the corresponding version at msedgewebdriverstorage and download the zip. Extract it to you path like /usr/local/bin or $HOME/.local/bin. ","date":"2021-03-26","objectID":"/posts/python-selenium-settings-on-microsoft-edge-dev-on-linux/:2:0","tags":["Python"],"title":"在 microsoft-edge-dev 上设置 Python selenium","uri":"/posts/python-selenium-settings-on-microsoft-edge-dev-on-linux/"},{"categories":["development"],"content":"Write Code Following is a example. from msedge.selenium_tools import EdgeOptions, Edge options = EdgeOptions() options.use_chromium = True options.binary_location = r\"/usr/bin/microsoft-edge-dev\" options.set_capability(\"platform\", \"LINUX\") webdriver_path = r\"/home/ayamir/.local/bin/msedgewebdriver\" browser = Edge(options=options, executable_path=webdriver_path) browser.get(\"http://localhost:8000\") assert \"Django\" in browser.title ","date":"2021-03-26","objectID":"/posts/python-selenium-settings-on-microsoft-edge-dev-on-linux/:3:0","tags":["Python"],"title":"在 microsoft-edge-dev 上设置 Python selenium","uri":"/posts/python-selenium-settings-on-microsoft-edge-dev-on-linux/"},{"categories":["development"],"content":"Launch it ","date":"2021-03-26","objectID":"/posts/python-selenium-settings-on-microsoft-edge-dev-on-linux/:4:0","tags":["Python"],"title":"在 microsoft-edge-dev 上设置 Python selenium","uri":"/posts/python-selenium-settings-on-microsoft-edge-dev-on-linux/"},{"categories":["knowledge"],"content":"文件和目录的权限 下图为使用exa命令的部分截图 上图中的 Permission 字段下面的字母表示权限 第一个字母表示 文件类型 ： 属性 文件类型 - 普通文件 d 目录文件 l 符号链接 符号链接文件剩余的属性都是 rwxrwxrwx，是伪属性值，符号链接指向的文件属性才是真正的文件属性 c 字符设备文件 表示以字节流形式处理数据的设备，如 modem b 块设备文件 表示以数据块方式处理数据的设备，如硬盘驱动或光盘驱动 剩下的 9 个位置上的字符称为 文件模式 ，每 3 个为一组，分别表示文件所有者、文件所属群组以及其他所有用户对该文件的读取、写入和执行权限 属性 文件 目录 r 允许打开和读取文件 如果设置了执行权限，允许列出目录下的内容 w 允许写入或截断文件，但是不允许重命名或删除文件 如果设置了执行权限，那么允许目录中的文件被创建、被删除和被重命名 x 允许把文件当作程序一样来执行 允许进入目录 ","date":"2021-03-15","objectID":"/posts/linux-authority/:1:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"id：显示用户身份标识 一个用户可以拥有文件和目录，同时对其拥有的文件和目录有控制权 用户之上是群组，一个群组可以由多个用户组成 文件和目录的访问权限由其所有者授予群组或者用户 下图为 Gentoo Linux 下以普通用户身份执行 id 命令的结果 uid 和 gid 分别说明了当前用户的用户编号与用户名、所属用户组的编号与组名 groups 后的内容说明了用户还属于哪些组，说明了其对应的编号和名称 许多类 UNIX 系统会将普通用户分配到一个公共的群组中如：users 现代 Linux 操作是创建一个独一无二的只有一个用户的同名群组 ","date":"2021-03-15","objectID":"/posts/linux-authority/:2:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"chmod：更改文件模式 chmod 支持两种标识方法 八进制表示法 八进制 二进制 文件模式 0 000 --- 1 001 \u0026##x2013;x 2 010 -w- 3 011 -wx 4 100 r-- 5 101 r-x 6 110 rw- 7 111 rwx 常用的模式有 7,6,5,4,0 符号表示法 符号 含义 u user：表示文件或目录的所有者 g group：文件所属群组 o others：表示其他用户 a all：u+g+o 如果没有指定字符默认使用 all ’+’表示添加一种权限 ’-’表示删除一种权限 例如： 符号 含义 u+x 所有者+可执行 u-x 所有者-可执行 +x 所有用户+可执行 o-rw 其他用户-读写 go=rw 群组用户和其他用户权限更改为读，写 u+x,go=rx 所有者+可执行，群组用户和其他用户权限更改为读，可执行 ’-R’=’\u0026##x2013;recursive’表示递归设置 ","date":"2021-03-15","objectID":"/posts/linux-authority/:3:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"umask：设置文件默认权限 使用八进制表示法表示从文件模式属性中删除一个位掩码 掩码的意思：用掩码来取消不同的文件模式 umask 可以看到输出为： 0022 不同 linux 发行版默认的文件权限不同，这里的输出是 Gentoo Linux 上普通用户对应的的输出 0022：先不看第一个 0,后面的 0|2|2 用二进制展开结果是：000|010|010 原始文件模式 --- rw- rw- rw- 掩码 000 000 000 010 结果 --- rw- rw- r-- 掩码中 1 对应位处的权限会被取消，0 则不受影响 所以会有这样的结果： 再来谈最前面的 0:因为除了 rwx 之外还有较少用到的权限设置 setuid 位:4000(8 进制) 设置此位到一个可执行文件时，有效用户 ID 将从实际运行此程序的用户 ID 变成该程序拥有者的 ID 设置场景：应用于由 root 用户拥有的程序，当普通用户运行一个具有 setuid 位的程序时，这个程序会以超级用户的权限执行，因此可以访问普通用户无法访问到的文件和目录 设置程序 setuid： chmod u+s program_name 结果： -rwsr-xr-x 可以看到第二组权限中第一个符号是 s setgid 位:2000(8 进制) 有效组 ID 从该用户的实际组 ID 更改为该文件所有者的组 ID 设置场景：当一个公共组下的成员需要访问共享目录下的所有文件时可以设置此位 对一个目录设置 setgid 位，则该目录下新创建的文件将由该目录所在组所有 chmod g+s dir_name 结果： drwxrwsr-x 可以看到第二组权限中最后一个符号是 s(替换了 x) sticky 位:1000(8 进制) 标记一个可执行文件是“不可交换的”，linux 中默认会忽略文件的 sticky 位，但是对目录设置 sticky 位，能阻止用户删除或者重命名文件，除非用户是这个目录的所有者，文件所有者或者 root 用来控制对共享目录的访问 chmod +t dir_name 结果： drwxrwxrwt 可以看到第三组权限中最后一个符号是 t(替换了 x) ","date":"2021-03-15","objectID":"/posts/linux-authority/:4:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"su：以另一个用户身份运行 shell ","date":"2021-03-15","objectID":"/posts/linux-authority/:5:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"使用 su 命令登录 su [-[l]] [user] 如果包含“-l”选项，得到的 shell session 会是 user 所指定的的用户的登录 shell 即 user 所指定的用户的运行环境将会被加载，工作目录会更改为此用户的主目录 ","date":"2021-03-15","objectID":"/posts/linux-authority/:5:1","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"使用 su 命令执行单个命令 su -c 'comand' 命令内容必须用 ’’ 引用起来（也可以是双引号） ","date":"2021-03-15","objectID":"/posts/linux-authority/:5:2","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"sudo：以另一个用户身份执行命令 ","date":"2021-03-15","objectID":"/posts/linux-authority/:6:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"sudo 和 su 的区别 sudo 比 su 有更丰富的功能，而且可以配置 通过修改配置文件来配置 sudo EDITOR=vim visudo 执行上面的命令可以用 vim 来编辑 sudo 的配置文件 常用的场景是在将用户加入到 wheel 组之后使 wheel 组的用户能够访问 root 权限 使用 sudo 命令输入的不是 root 的密码，而是自己的密码 可以使用 `sudo -l`来查看通过 sudo 命令能获得的权限 ","date":"2021-03-15","objectID":"/posts/linux-authority/:6:1","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"chown：更改文件所有者 ","date":"2021-03-15","objectID":"/posts/linux-authority/:7:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"用法 chown [owner][:[group]] file ... 第一个参数决定 chown 命令更改的是文件所有者还是文件所属群组，或者对两者都更改 参数 结果 bob 文件所有者=\u003ebob bob:users 文件所有者=\u003ebob 文件所属群组=\u003eusers :admins 文件所属群组=\u003eadmins bob: 文件所有者=\u003ebob 文件所属群组=\u003ebob 登录系统时的组 图中使用 root 用户在/home/ayamir 目录下创建了一个 foo.txt 文件，最后将此文件的所有者和所属组都改为了 ayamir（rg 是ripgrep） ","date":"2021-03-15","objectID":"/posts/linux-authority/:7:1","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"chgrp：更改文件所属群组 这个命令是历史遗留问题，在早期的 UNIX 版本中，chown 只能更改文件的所有者，而不能改变文件的所属群组，因此出现了这个命令，事实上现在的 chown 已经能实现 chgrp 的功能，因此没必要再使用这个命令（其使用方式几乎与 chown 命令相同） ","date":"2021-03-15","objectID":"/posts/linux-authority/:8:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"passwd：更改用户密码 ","date":"2021-03-15","objectID":"/posts/linux-authority/:9:0","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"一般用法 passwd [user] 用来更改 user 用户的密码，如果想修改当前用户的密码则不需要指定 user 执行之后会提示输入旧密码和新密码，新密码需要再确认输入一次 拥有 root 用户权限的用户可以设置所有用户的密码 上图为 Gentoo Linux 下使用 passwd 命令修改 ayamir 用户密码的过程，这里可以看到 passwd 会强迫用户使用强密码，会拒绝短密码或容易猜到的密码（其他发行版可能输出会不一样） ","date":"2021-03-15","objectID":"/posts/linux-authority/:9:1","tags":["linux"],"title":"Linux 权限相关命令解读","uri":"/posts/linux-authority/"},{"categories":["knowledge"],"content":"Arch Linux DNS 设置 安装dnsmasq sudo pacman -S dnsmasq 配置/etc/resolv.conf中的域名代理服务器 # Tencent nameserver 119.29.29.29 nameserver 182.254.118.118 # Ali nameserver 223.5.5.5 nameserver 223.6.6.6 # OpenDNS IPv4 nameservers nameserver 208.67.222.222 nameserver 208.67.220.220 # OpenDNS IPv6 nameservers nameserver 2620:0:ccc::2 nameserver 2620:0:ccd::2 # Google IPv4 nameservers nameserver 8.8.8.8 nameserver 8.8.4.4 # Google IPv6 nameservers nameserver 2001:4860:4860::8888 nameserver 2001:4860:4860::8844 # Comodo nameservers nameserver 8.26.56.26 nameserver 8.20.247.20 # Generated by NetworkManager nameserver 192.168.1.1 防止/etc/resolv.conf被修改 sudo chattr +i /etc/resolv.conf 减少主机名查找时间 sudo echo \"options timeout:1\" \u003e /etc/resolv.conf.tail 启动dnsmasq sudo systemctl enable dnsmasq.service --now ","date":"2021-01-26","objectID":"/posts/dns-settings-on-archlinux/:1:0","tags":["linux"],"title":"在 Linux 上手动设置 DNS","uri":"/posts/dns-settings-on-archlinux/"},{"categories":null,"content":"个人概况 20 岁，北京邮电大学 18 级软件工程专业本科生，已推免至北京邮电大学网络智能中心。 目前主要的研究方向是全景视频的传输（VR or Metaverse）。 Life is hard, so any rational person will give up if you don’t have Passion. ","date":"0001-01-01","objectID":"/about/:1:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"友链 cl 的博客：https://clslaid.icu/ 黄鸡的博客：https://hyiker.com/ z217 的博客：https://z217blog.cn/ Tackoil 的博客：https://tackoil.github.io/ Leeshy 的博客: https://leeshy-tech.github.io/ ","date":"0001-01-01","objectID":"/about/:2:0","tags":null,"title":"","uri":"/about/"}]
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Paper - 分类 - Ayamir&#39;s Blog</title>
        <link>http://localhost:1313/categories/paper/</link>
        <description>Paper - 分类 - Ayamir&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miracle_l@bupt.edu.cn (Ayamir)</managingEditor>
            <webMaster>miracle_l@bupt.edu.cn (Ayamir)</webMaster><lastBuildDate>Sun, 20 Mar 2022 22:09:11 &#43;0800</lastBuildDate><atom:link href="http://localhost:1313/categories/paper/" rel="self" type="application/rss+xml" /><item>
    <title>Note for DQB</title>
    <link>http://localhost:1313/posts/papers/note-for-dqb/</link>
    <pubDate>Sun, 20 Mar 2022 22:09:11 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-dqb/</guid>
    <description><![CDATA[<h1 id="整体概况">整体概况</h1>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9317771" target="_blank" rel="noopener noreffer">Modeling the Perceptual Quality for Viewport-Adaptive Omnidirectional Video Streaming Considering Dynamic Quality Boundary Artifact</a>
Level：IEEE TCSVT 2021</p>
<p>DQB: Dynamic Quality Boundary，指在基于分块的 FoV 自适应全景视频推流过程中低质量分块区域的暴露和质量切换现象。</p>
<p>DQB 现象实际上就是 FoV 内分块间的质量差异和随时间变化的分块质量变化。
这篇论文主要的贡献在于深入研究了这种现象，并且针对此提出了可以利用现存的 QoE 评估指标的模型，并且可以实际应用。</p>
<h1 id="model-的建立">Model 的建立</h1>
<ol>
<li>执行一系列主观评估，由低质量分块的比例和质量导致的感知质量的降低可以基于主观实验结果完成建模。</li>
<li>结合剩下分块的感知质量可以完成单帧质量模型的建模。</li>
<li>最后将一段时间内的所有帧的感知质量池化，就完成了整个的模型。</li>
</ol>
<h2 id="主观实验的设定">主观实验的设定</h2>
<ol>
<li>获得 FoV 内帧的感知质量（低质量分块和高质量分块同时存在）</li>
<li>获取整个视频的感知质量（与上面的实验过程相近，只是过程中没有暂停）</li>
<li>获取整个视频的感知质量（没有引入 DQB，所有分块质量相同）</li>
</ol>
<p>实验结果</p>
<p></p>
<h2 id="帧质量感知模型">帧质量感知模型</h2>
<p>从上面的实验结果可以看出来高质量区域与低质量区域的质量差距 $d_n$ 越大，DQB 效应越显著（符合直觉）。将这部分影响因素看作是感知质量的主要影响因素：</p>
<p>$$
d_n = Q_{H, n} - Q_{L, n}
$$</p>
<p>$Q_{H, n}$ 和 $Q_{L, n}$ 分别表示第 $n$个 帧高质量分块和低质量分块的感知质量。
这两个质量从主观实验 3 的主观质量获得，在之后的训练过程中可以被客观质量评估的结果所替换。</p>
<p>为了调查质量差异 $d_n$ 和感知质量降低 $D_n$ 之间的关系，通过使用实验 1 的帧质量分数计算得出第$n$个帧的感知质量降低：</p>
<p>$$
D_n = Q_{H, n} - Q_{HL, n}
$$</p>
<p>$Q_{HL, n}$是实验 1 中评分得到的第$n$个帧的 FoV 内感知质量。</p>
<p>在 6 个视频上的实验结果如下图：</p>
<p></p>
<p>可以看到二者的关系可以近似为线性相关，即：</p>
<p>$$
D_n = k_1 d_n
$$</p>
<p>$k_1$ 作为线性回归的参数，可以计算出来。</p>
<p>但是对于不同取值的 $p_n$ ， $k_1$ 的取值也相当不同，两者之间的关系可以见下图：</p>
<p></p>
<p>数学表示可以建模为：</p>
<p>$$
k_1 = a_1 \cdot ln(a_2 \cdot p_n + a_3) \cdot sgn(p_n - P)
$$</p>
<p>$sgn$ 是符号函数，$a_1, a_2, a_3$ 可以从回归中计算出来， $P$ 表示低质量分块的比例。按照图中的回归结果，$P = 0.118$ 时，用户几乎没办法注意到低质量区域的存在。</p>
<p>最终，由低质量区域暴露引起的感知质量降低 $D_n$ 可以计算为：</p>
<p>$$
D_n = a_1 \cdot ln(a_2 \cdot p_n + a_3) \cdot (Q_{H, n} - Q_{L, n}) \cdot sgn(p_n - P)
$$</p>
<p>那么实际的感知质量 $Q_n$ 可以计算为：</p>
<p>$$
Q_n = Q_{H, n} - D_n
$$</p>
<h2 id="时间池化">时间池化</h2>
<p>可以采用下面两种方式之一完成</p>
<h3 id="exp-minkowski-basedhttpsieeexploreieeeorgdocument6603210"><a href="https://ieeexplore.ieee.org/document/6603210" target="_blank" rel="noopener noreffer"><code>Exp Minkowski-Based</code></a></h3>
<p>单个帧的感知质量由衰减指数加权，衰减指数表示在主观评估中观察到的<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/3299/1/Viewer-response-to-time-varying-video-quality/10.1117/12.320109.short?SSO=1" target="_blank" rel="noopener noreffer">近因效应</a>。</p>
<p>最终整个视频的感知质量 $PQ$ 可以计算为：</p>
<p>$$
PQ = \Big[\frac{1}{N} \sum_{n=1}^{N} exp\big( \frac{n-N}{\delta} \big) \cdot {Q_n}^p \Big]^{1/p}
$$</p>
<p>$N$ 是整个视频的帧数。</p>
<p>$p$ 是 <code>Minkowski</code>指数，高 $p$ 值强调了最高质量帧的影响。</p>
<p>$\delta$ 是控制近因效应强度的指数时间常数，以帧的数量的形式给出，高 $\delta$ 值对应较弱的近因效应。</p>
<p>$p$ 和 $\delta$ 的值可以通过对主观帧质量和视频序列的整体质量进行回归得到。</p>
<h3 id="quality-contribution-basedhttpsieeexploreieeeorgdocument6235989"><a href="https://ieeexplore.ieee.org/document/6235989" target="_blank" rel="noopener noreffer"><code>Quality Contribution-Based</code></a></h3>
<p>之前的研究表明，传统视频在时间维度上的感知质量降低主要与每帧的显示时长相关。</p>
<p>FoV 自适应的全景视频也与之类似，感知质量与降低质量帧和高质量帧的持续时间相关。因此采用<code>Quality Contribution</code>的概念来描述每帧对视频感知质量的影响（考虑每帧的空间感知质量和显示时长）。</p>
<p>时间池化是由相应的显示时长加权的每帧的质量贡献的函数，特别的，质量贡献是从 MOS 和显示持续时间之间初步找到的对数关系所导出的：</p>
<p>$$
C_n = Q_n \cdot (p_1 + p_2 \cdot log(T))
$$</p>
<p>$C_n$ 是第 $n$ 帧的贡献， $T$ 是每帧的显示时长， $T = Max(T, 33.3ms)$，即当帧率不低于 30fps 时，时间不连续性可以忽略。</p>
<p>接着，二级时间池化法用于池化单帧的分布。这种方法将 FoV 内的帧以注视水平划分为短时帧组(GoFs)，并以 GoF 的质量作为长期时间池化的基本单位来评估感知质量。</p>
<p>给出每帧的质量贡献之后，每个 GoF 的质量可以计算为</p>
<p>$$
Q_{GoF} = \frac{\sum_{n \in N} \big( C(n) \cdot T(n) \big)}{\sum_{n \in N} T(n)}
$$</p>
<p>接下来组合 GoF 的质量得到长期时间池化，即可以获得感知质量。</p>
<p>质量严重受损的帧会影响相邻帧的感知质量，视频中质量最差的部分主要决定整个视频的感知质量。因此提出选择计算出的质量低于平均值 75%的 GoF，以此计算平均质量并作为整个视频的感知质量。</p>
]]></description>
</item>
<item>
    <title>Note for Toward Immersive Experience</title>
    <link>http://localhost:1313/posts/papers/note-for-toward-immersive-experience/</link>
    <pubDate>Wed, 09 Mar 2022 11:20:37 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-toward-immersive-experience/</guid>
    <description><![CDATA[<h1 id="overview">Overview</h1>
<p>Link: <a href="https://ieeexplore.ieee.org/document/9679801" target="_blank" rel="noopener noreffer">Toward Immersive Experience: Evaluation for Interactive Network Services</a></p>
<p>Level: IEEE Network 2022</p>
<p>Keywords: QoE Metrics</p>
<h1 id="background">Background</h1>
<p>Compared with traditional QoE for regular video/audio services, the existing work on IE is still in its infancy. This work aims at providing systematic and comprehensive research on IE for interactive network services, mainly studying the following three fundamental and challenging issues.</p>
<ul>
<li><em>What is the essential difference between IE and traditional QoE?</em></li>
<li><em>Which categories of factors mainly influence IE?</em></li>
<li><em>How to evaluate IE in an efficient and intelligent manner?</em></li>
</ul>
<h1 id="ie-versus-traditional-qoe">IE versus traditional QoE</h1>
<h2 id="theoretical-definitions">Theoretical definitions</h2>
<p>Existing concepts of IE can be classified into two categories.</p>
<ul>
<li>The subjective sense of being surrounded or experiencing multi-sensory stimulation when interacting with the virtual environment.</li>
<li>The user&rsquo;s psychological state of deep involvement, engagement, absorption, or engrossment.</li>
</ul>
<p>Traditional QoE:</p>
<ul>
<li>A subjective measure from the user perspective of the overall value of the provided service and application.</li>
</ul>
<p>We can summary two significant points as follows to distinguish IE and traditional QoE:</p>
<ul>
<li>Both IE and traditional QoE are devoted to characterizing user&rsquo;s subjective experience for network services.</li>
<li>In terms of application scenarios, IE concentrates on the evaluation of network services equipped with interactive characteristics while traditional QoE is generally appropriate for regular audio/video services.</li>
</ul>
<p>IE is much more complex, fine-grained and multi-dimensional perception, which is produced through the interplay between multi-sensory data and diverse cognitive processes.</p>
<h2 id="technical-challenges">Technical challenges</h2>
<ul>
<li>Growing data volume</li>
<li>Stricter delay constraint</li>
<li>Increasing data dimension</li>
</ul>
<h2 id="ifs-on-ie">IFs on IE</h2>
<p></p>
<h2 id="network-aware-ifs">Network-aware IFs</h2>
<p>Actually, when heterogeneous streams are delivered to the network, their transmission quality is dependent on the outside network conditions(e.g., delay, jitter, throughput, and so on), as well as the streaming strategy (e.g., encoding, transmission protocol, and so on) inside streams, which ultimately impact end users&rsquo; IE. To this end, we can further subdivide this category into two classes including network QoS and stream-related IFs.</p>
<ul>
<li>
<p>QoS:</p>
<ul>
<li>low latency</li>
<li>high throughput</li>
<li>high reliability</li>
<li>temporal synchronization among heterogeneous streams</li>
</ul>
</li>
<li>
<p>stream-related IFs</p>
<ul>
<li>the form of data compression strategy</li>
<li>resource scheduling scheme</li>
</ul>
</li>
</ul>
<h2 id="user-aware-ifs">User-aware IFs</h2>
<p>IE may be influenced by human users while human users can perceive IE, for which we can subdivide this category into three classes based on such correlations.</p>
<ul>
<li>User profile</li>
<li>Physiological IFs</li>
<li>Psychological IFs</li>
</ul>
<p>It is obvious that users with diverse user profiles have distinctive influences on IE.</p>
<p>The psychology and physiology of users can highly reflect the IE for the application.</p>
<ul>
<li>For psychological IFs, they are able to directly demonstrate a user&rsquo;s positive or negative feedback for interactive network services. However, this can hardly be simply measured.</li>
<li>For physiological IFs, some of them(e.g., heart rate, blood pressure) can be objectively measured by affordable medical sensors.</li>
</ul>
<h2 id="device-aware-ifs">Device-aware IFs</h2>
<p>With regard to device-aware IFs, two broad classes can be gotten according to internal systems(e.g., CPU) and external specifications(e.g., screen size, FOV) of the device.</p>
<p>IE management in the device level mainly lies in two aspects.</p>
<ul>
<li>The selection of terminal type(e.g., mobile phone, laptop, VR/AR glasses)</li>
<li>The corresponding possession of hardware(e.g., CPU, GPU, battery).</li>
</ul>
<h2 id="context-aware-ifs">Context-aware IFs</h2>
<p>Typically, IE for interactive network services is generated by interacting with the virtual environment. To this end, we can derive two primary classes.</p>
<ul>
<li>Virtual context: focuses on the specific virtual application scenario.</li>
<li>Physical context: focuses on its surrounding physical environment.</li>
</ul>
<p>We can provide constructive suggestions for different contexts. For example, online virtual games are  appropriate to play outside for the broad horizon, but watching a 3D film is more proper inside the home.</p>
<p>We can suggest appropriate application types with different technical requirement to guarantee users&rsquo; IE according to existing network resources and the surrounding environment.</p>
<p></p>
<h1 id="light-weight-ie-evaluation">Light-weight IE evaluation</h1>
<p>We proposed two light-weight IE evaluation approaches by respectively exploiting the AI technology and exploring the mathematical relationship among IFs and IE, which are appropriate for different cases according to the data amount.</p>
<h2 id="ai-based">AI-based</h2>
<p>Existing popular studies focusing on DL-based models(e.g., DNNs, LSTMs) can hardly satisfy the stringent delay requirement.</p>
<p>We employ a multi-view learning combining with lightweight ML methods(e.g., SVM, decision tree) for fast and accurate IE evaluation.</p>
<p></p>
<p>The raw data through multi-view learning is first represented by multiple feature extractors according to their heterogeneous properties. Each modality is regraded as a particular view for multi-modal applications. Motivations are:</p>
<ol>
<li>It can provide efficient dimension reduction via subspace mapping. Subspace learning-based approaches can map the high-dimensional raw data to a latent subspace, in which its dimensionality is lower than that of raw data.</li>
<li>Multi-view learning is more applicable to the IE context with abundant infomation, which can overcome the weakness of ML-based methods regarding evaluation accuracy for interactive network services.</li>
<li>Multi-view learning can take full advantage of the associated and complementary features from redundant views for evaluation performance improvement.</li>
</ol>
<h2 id="statistical-function-based">Statistical function-based</h2>
<p>AI-based approach may achieve better evaluation performances under large amounts of data, they lack strong interpretability and cannot explicitly explain the inherent relations among IFs and IE.</p>
<p>We introduced statistical function-based approach to analyze the mathematical relationship among IFs and IF under limited data.</p>
<p>Existing statistical function-based approaches for user experience evaluation are broadly divided into three categories:</p>
<ul>
<li>Exponential model</li>
<li>Logarithmic model</li>
<li>Linear regression model</li>
</ul>
<p>Notably, in order to further improve evaluation performance for interactive network services via statistical function-based approaches, two fundamental and significant issues need to be concerned as follows:</p>
<ol>
<li>How to comprehensively explore diverse and various IFs for accurate IE evaluation?</li>
<li>How to conduct an efficient dimension reduction method for fast IE evaluation?</li>
</ol>
<h1 id="case-study">Case study</h1>
<h2 id="multi-view-generation">Multi-view generation</h2>
<p>We can construct multiple views from expert prior knowledge or via the random subspace method, which is a random sampling algorithm for automatic feature set partitioning. Here we partition multi-modal data into three specific views according to different modalites.(e.g., audio, video, and haptic signals).</p>
<h2 id="view-combination">View combination</h2>
<p>Then we adopt subspace learning-based approaches to obtain an appropriate subspace from the above-mentioned multiple views. Importantly, canonical correlation analysis in subspace learning plays a significant role in dimension reduction, and outputs the optimal projection for each view.</p>
<h3 id="ie-evaluation">IE evaluation</h3>
<p>Finally, based on the optimal and combined projection subspace, decision tree is deployed here to evaluation IE.</p>
<p>The key point is find a general and robust evaluation approach:
$$
f: X \rarr Y
$$
Result is:
$$
Y = X^{\top} {\beta} + {\epsilon}
$$
${\epsilon}$ is the noise, ${\beta}$ can be considered as influencing degree of various IFs to the IE.</p>
<p>IE evaluation for multi-modal applications must satisfy more stringent delay requirements in the context of higher-dimensional data. So we apply the <a href="https://www.doi.org/10.1080/10618600.1998.10474784" target="_blank" rel="noopener noreffer">LASSO estimation</a>, which is equipped with sparse solutions for the linear regression model, is incorporated to alleviate the issue of high-dimensional data for fast IE evaluation.</p>
<p>Dataset: <a href="http://8.133.175.194/" target="_blank" rel="noopener noreffer">VisTouch</a></p>
<p>Compare obejcts:</p>
<ul>
<li>Ridge regression</li>
<li>Exponential model</li>
</ul>
<p>Performance metric: MAE</p>
<p>Test result:</p>
<p></p>
]]></description>
</item>
<item>
    <title>Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)</title>
    <link>http://localhost:1313/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/</link>
    <pubDate>Sun, 27 Feb 2022 10:39:45 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/</guid>
    <description><![CDATA[<h1 id="bitrate-adaptation-schemes">Bitrate Adaptation Schemes</h1>
<h2 id="client-based">Client-based</h2>
<p>Recently, most of the proposed bitrate adaptation schemes reside at the client side, according to the specifications in the DASH standard.</p>
<p></p>
<p>Purposes:</p>
<ol>
<li>Minimal rebuffering events when the playback buffer depletes.</li>
<li>Minimal startup delay especially in case of live video streaming.</li>
<li>A high overall playback bitrate level with respect to network resources.</li>
<li>Minimal video quality oscillations, which occur due to frequent switching.</li>
</ol>
<h3 id="available-bandwidth-based">Available bandwidth-based</h3>
<p>The client makes its representation decisions based on the measured available network bandwidth, which is usually calculated as the size of the fetched segment(s) divided by the transfer time.</p>
<p>This scheme suffers from poor QoE due to a lack of a reliable bandwidth estimation methods, which results in frequent buffer underruns.</p>
<h4 id="general-context">General context</h4>
<ul>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/1943552.1943575" target="_blank" rel="noopener noreffer">Based on segment fetch time(SFT)</a> measures the time starting from sending the HTTP GET request to receiving the last byte of the segment. Sequential and parallel segment fetching method in CDNs, by using metric that compares the expected segment fetch time(ESFT) with the measured SFT to determine if the selected segment bitrate matches the network capacity.</p>
<p><a href="https://ieeexplore.ieee.org/document/6333880/" target="_blank" rel="noopener noreffer">Based on the bitrate observed for the last segment downloaded</a> and the estimated throughput that was calculated during the previous estimation.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/6774592" target="_blank" rel="noopener noreffer">Probe AND Adapt</a> tries to eliminate the ON-OFF steady state issue as well as reduce bitrate oscillations when multiple clients share the same bottleneck link.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/2789168.2790118" target="_blank" rel="noopener noreffer">piStream</a> enables clients to estimate bandwidth based on a resource monitor module that act as a physical-layer daemon.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/2155555.2155580" target="_blank" rel="noopener noreffer">SVC with DASH</a> prefetches base layers of future segments or downloads enhancement layers for existing segments using a bandwidth-sloping-based heuristic.</p>
</li>
</ul>
<h4 id="mobile-context">Mobile context</h4>
<h5 id="static">Static</h5>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/2964284.2964313" target="_blank" rel="noopener noreffer">DASH2M</a> uses HTTP/2 server push and stream terminate properties to reduce the battery consumption of the mobile device. Adaptive k-push scheme propose to increase/decrease k according to a bandwidth increase/decrease while keeping in mind the overall power consumption in a push cycle.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2990505" target="_blank" rel="noopener noreffer">LOw-LatenceY Prediction-based adaPtation(LOLYPOP)</a> leverages TCP throughput predictions on multiple times scales to achieve low latency and improve QoE.</li>
</ul>
<h5 id="motive">Motive</h5>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/2964284.2964333" target="_blank" rel="noopener noreffer">GeoStream</a>: introduce the use of geostatistics to estimate future bandwidth in unknown locations.</li>
</ul>
<h3 id="playback-buffer-based">Playback Buffer-Based</h3>
<p>The client uses the playout buffer occupancy as a criterion to select the next segment bitrate during video playback.</p>
<p>This scheme suffers from many limitations including low overall QoE and instability issues, especially in the case of long-term bandwidth fluctuations. SVC-based approaches have limitations related to the complexity of SVC.</p>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/7177435" target="_blank" rel="noopener noreffer">Base</a> combines the buffer size with a tool-set of client metrics for accurate rate selection and smooth switching.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2619239.2626296" target="_blank" rel="noopener noreffer">BBA</a> aims to maximize the average video quality and avoid unnecessary rebuffering events, but suffers from QoE degradation during long-term bandwidth fluctuations.</li>
<li><a href="https://ieeexplore.ieee.org/document/7524428" target="_blank" rel="noopener noreffer">BOLA</a> uses online control algorithm that treats bitrate adaptation as a utility maximization problem. Provide strong theorectical proof that it is near optimal, design a QoE model that incorporates both the average playback quality and the rebuffering time. It is implemented and available in the <code>dash.js</code> player.</li>
<li><a href="https://ieeexplore.ieee.org/document/6573184" target="_blank" rel="noopener noreffer">BIEB</a> maximizes video quality based on SVC priority while reducing the number of quality oscillations and avoiding stalls and frequent bitrate switching. it maintains a stable buffer occupancy before increasing the quality.</li>
<li><a href="https://dl.acm.org/doi/10.1145/3123266.3123390" target="_blank" rel="noopener noreffer">QUEuing Theory approach to DASH Rate Adaptation(QUETRA)</a> allows to calculate the expected buffer occupancy given a bitrate choice, network throughput, and buffer capacity.</li>
</ul>
<h3 id="mixed-adaptation">Mixed Adaptation</h3>
<p>The client makes its bitrate selection based on a combination of metrics including available bandwidth, buffer occupancy, segment size and/or duration.</p>
<h4 id="simple-client">Simple client</h4>
<ul>
<li>Control-theoretic based: <a href="https://dl.acm.org/doi/10.1145/2829988.2787486" target="_blank" rel="noopener noreffer">FastMPC</a>, <a href="https://ieeexplore.ieee.org/document/6410740" target="_blank" rel="noopener noreffer">Another</a></li>
<li>Optimization problem: <a href="https://dl.acm.org/doi/10.1145/2557642.2557658" target="_blank" rel="noopener noreffer">Streaming video over HTTP with Consistent Quality</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/2413176.2413190" target="_blank" rel="noopener noreffer">Towards agile and smooth video adaptation in dynamic HTTP streaming</a> aims to balance bandwidth utilization and smoothness in DASH in both single and multiple CDN(s) scenarois.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2910017.2910593" target="_blank" rel="noopener noreffer">SQUAD</a> is a lightweight bitrate adaptation algorithm that uses the available bandwidth and buffer information to increase the average bitrate while minimizing the number of quality switches.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2155555.2155582" target="_blank" rel="noopener noreffer">Multi-path solution for abr in wireless networks</a> avoids the problems of TCP congestion control by using parallel TCP streams.</li>
<li><a href="https://ieeexplore.ieee.org/document/7247436" target="_blank" rel="noopener noreffer">SARA</a> is Segment-Aware Rate Adaptation algorithm based on the segment size variation, the available bandwidth estimate and the buffer occupancy. It extends MPD file to include the size of every segment.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2910017.2910596" target="_blank" rel="noopener noreffer">ABMA+</a> selects the highest segment representation based on the estimated <em>probability of video rebuffering</em>. It makes use of buffer maps, which define the playout buffer capacity that is required under certain conditions to satisfy a rebuffering threshold and to avoid heavy online calculations.</li>
<li><a href="https://dl.acm.org/doi/10.1145/3204949.3204961" target="_blank" rel="noopener noreffer">GTA</a> uses a cooperative game in coalition formation then formulates the bitrate selection problem as a bargaining process and consensus mechanism. GTA improves QoE and video stability without increasing the stall rate or startup delay.</li>
</ul>
<h4 id="multiple-clients">Multiple clients</h4>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/6691442" target="_blank" rel="noopener noreffer">ELASTIC</a> generates a long-lived TCP flow and avoids the ON-OFF steady state behavior which leads to bandwidth overestimations. Ensure bandwidth fairness between competing clients based on network feedback assistance, but without taking the QoE into consideration. In addition, it ignores quality oscillations in its bitrate decisions.</li>
<li><a href="https://ieeexplore.ieee.org/document/6229732" target="_blank" rel="noopener noreffer">Adaptation algorithm for HAS</a> uses current buffer occupancy level to estimate available bandwidth and average bitrate of the different bitarte levels from MPD as metrics in its bitrate selection.</li>
<li><a href="https://ieeexplore.ieee.org/document/6704839" target="_blank" rel="noopener noreffer">FESTIVE</a> contains:
<ul>
<li>a bandwidth estimator module</li>
<li>a bitrate selection and update method with stateful player</li>
<li>a randomized scheduler which incorporates the buffer size to schedule the download of the next segment.</li>
</ul>
</li>
<li><a href="https://ieeexplore.ieee.org/document/8101529" target="_blank" rel="noopener noreffer">TSDASH</a> uses a logarithmic-increase-multiplicative-decrease (LIMD) based bandwidth probing algorithm to estimate the available bandwidth and a dual-threshold buffer for the bitrate adaptation.</li>
</ul>
<h3 id="mdp-based">MDP-Based</h3>
<p>The video streaming process is formulated as a finite MDP to be able to make adaptation decisions under fluctuating network conditions.</p>
<p>This scheme may suffer from instability, unfairness and underutilization when the number of clients increases, probably because such factors are not taken into account in the MDP models and due to clients&rsquo; decentralized ON-OFF patterns.</p>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/6774598" target="_blank" rel="noopener noreffer">Real-time best-action search algorithm over multiple access networks</a> uses both Bluetooth and WiFi links to simultaneously download video segments. However, this scheme shows limitations during user mobility which negatively affect QoE.</li>
<li><a href="https://ieeexplore.ieee.org/document/7305810" target="_blank" rel="noopener noreffer">Optimizing in Vehicular environment</a> introduces a three-variant of RL-based algorithms which take advantage of the historical bandwidth samples to build an accurate bandwidth estimation model.</li>
<li><a href="https://ieeexplore.ieee.org/document/6838245" target="_blank" rel="noopener noreffer">Multi-agent Q-Learning-based for fairness</a> uses a central manager in charge of collecting QoE statistics and coordination between the competing clients. The algorithm ensures a fair QoE distribution and improves QoE while avoiding suboptimal decisions.(without considering stalls and quality switches)</li>
<li><a href="https://dl.acm.org/doi/10.1145/2910017.2910603" target="_blank" rel="noopener noreffer">Online learning adaptation</a> aims to select the optimal representation and maximize the long-term expected QoE. The reward function is calculated from a combination of quality oscillations, segment quality and stalls experienced by the client. It exploits a parallel learning technique to avoid slow convergence and suboptimal solutions.</li>
<li><a href="https://ieeexplore.ieee.org/document/7393865" target="_blank" rel="noopener noreffer">mDASH</a> aims to improve QoE during long-term bandwidth variations. It takes buffer size, bandwidth conditions and bitrate stability as Markov state variables.</li>
<li><a href="https://dl.acm.org/doi/10.1145/3098822.3098843" target="_blank" rel="noopener noreffer">Pensive</a> does not rely on pre-programmed models or assumptions about the environment, but gradually learns the best policy for bitrate decisions through observation and experience.</li>
<li><a href="https://ieeexplore.ieee.org/document/8048013" target="_blank" rel="noopener noreffer">D-DASH</a> combines DL and RL to improve QoE, achieves a good trade-off between policy optimality and convergence speed during the decision process.</li>
</ul>
<h2 id="server-based">Server-Based</h2>
<p>Server-based schemes use a bitrate shaping method at the server side and do not require any cooperation from the client. The switching between the bitrates is implicitly controlled by the bitrate shaper. The client still makes its own decisions, but the decisions are more or less determined by the shaping method on the server.</p>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/2155555.2155557" target="_blank" rel="noopener noreffer">Traffic shaping</a> analyzes instability and unfairness issues in the presence of multiple HAS players competing for the available bandwidth. This method can be deployed at a home gateway to improve fairness, stability and convergence delay, and to eliminate the OFF periods during the steady states.</li>
<li><a href="https://ieeexplore.ieee.org/document/7524620" target="_blank" rel="noopener noreffer">Tracker-assisted adaptation</a> uses a architecture which consists of clients communicating with a server through a shared proxy and a server having a tracker functionality that manages the clients&rsquo; statuses and helps them share knowledge about their statues.</li>
<li><a href="https://dl.acm.org/doi/10.1145/1943552.1943573" target="_blank" rel="noopener noreffer">Quality Adaptation Controller</a> aims to control the size of the server sending buffer in order to adjust and select the most appropriate bitrate level for each DASH player. It maintains the playback buffer occupancy of each player as stable as possible and to match bitrate level decisions with the available bandwidth.</li>
<li><a href="https://ieeexplore.ieee.org/document/7983147/" target="_blank" rel="noopener noreffer">Multi-Source Stream system</a>: the client fetches the segments from multi-source stream servers.</li>
</ul>
<p></p>
<p>Cons:</p>
<ol>
<li>Produce high overhead on the server side with a high complexity</li>
<li>These schemes also need modifications to the MPD or a custom server software to implement the bitrate adaptation logic.(a violation of the DASH-standard design principles)</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)</title>
    <link>http://localhost:1313/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/</link>
    <pubDate>Sat, 26 Feb 2022 11:26:06 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/</guid>
    <description><![CDATA[<h1 id="paper-overview">Paper Overview</h1>
<p>Link: <a href="https://ieeexplore.ieee.org/document/8424813" target="_blank" rel="noopener noreffer">https://ieeexplore.ieee.org/document/8424813</a></p>
<p>Level: IEEE Communications Surveys &amp; Tutorials 2019</p>
<h1 id="background">Background</h1>
<h2 id="traditional-non-has-ip-based-streaming">Traditional non-HAS IP-based streaming</h2>
<ol>
<li>
<p>The client receives media that is typically <em>pushed</em> by a media server using <strong>connection-oriented</strong> protocol such as Real-time Messaging Protocol(RTMP/TCP) or <strong>connectionless</strong> protocol such as Real-time Transport Protocol(RTP/UDP).</p>
</li>
<li>
<p>Real-time Streaming Protocol(RTSP) is a common protocol to control the media servers, which is responsible for setting up a streaming session and keeping the state information during this session, but is not responsible for actual media delivery(task for protocol like RTP).</p>
</li>
<li>
<p>The media server performs rate adaption and data delivery scheduling based on the RTP Control Protocol(RTCP) reports sent by the client.</p>
</li>
<li>
<p>When it comes to NAT and firewall, additional protocols or configurations are needed during the session establishment.</p>
</li>
</ol>
<p>The characteristics result in complex and expensive servers. These scalability and vendor dependency issues as well as high maintenance costs have resulted in deployment challenges for protocols like RTSP.</p>
<h2 id="has">HAS</h2>
<p>Around 2005, HTTP adaptive streaming(HAS) became popular and dominant, which treated the media content like regular Web content and delivered it in small pieces over HTTP protocol.</p>
<ol>
<li>HTTP as application and TCP as the transport-layer protocol.</li>
<li>Client <em>pull</em> the data from a standard HTTP server, which simply hosts the media content.</li>
<li>HAS solutions employ dynamic adaptation with respect to varying network conditions to provide a seamless streaming experience.</li>
<li>The original file/stream is partitioned into <em>segments</em> (also called <em>chunks</em>) of equi-length playback time. Multiple versions(also called representations) of each segment are generated that vary in bitrate/resolution/quality using an encoder or a transcoder.</li>
<li>The server generates an index file, which is a manifest that lists the available representations including HTTP urls to identify the segments along with their availability times.</li>
<li>The client first receives the manifest that contains the metadata for video, audio, subtitles and other features, then constantly measures certain parameters: available network bandwidth, buffer status, battery and CPU levels, etc. According to these parameters, the HAS client repeatedly fetches the most suitable next segment among the available representations from the server.</li>
</ol>
<p>Advantages:</p>
<ol>
<li>It use HTTP to deliver video segments, which simplifies the traversal through NATs and firewalls.</li>
<li>At the server side, it use conventional Web servers or caches available within the networks of ISPs and CDNs.</li>
<li>At the client side, it requests and fetches each segment independently from others and maintains the playback session state, whereas the server is not required to maintain any state.</li>
<li>It doesn&rsquo;t require a persistent connection between the client and server, which improves system scalability and reduces implementation and deployment costs.</li>
</ol>
<h2 id="comparison-summary">Comparison Summary</h2>
<p></p>
<p></p>
<h1 id="challenges">Challenges</h1>
<h2 id="multi-client-competitionstability-issues">Multi-Client Competition/Stability Issues</h2>
<p>A centralized management controller can enhance the overall video quality, while improve QoE.</p>
<p>A robust HAS scheme should achieve 3 main objectives:</p>
<ol>
<li><em>Stability</em>: HAS clients should avoid frequent bitrate switching.</li>
<li><em>Fairness</em>: Multiple HAS clients competing for available bandwidth should equally share network resources based on viewer, content and device characteristics.</li>
<li><em>High Utilization</em>: While the clients attempt to be stable and fair, network resources should be used as efficiently as possible.</li>
</ol>
<p>A streaming session consists of 2 states: buffer-filling state and steady state.</p>
<ul>
<li>
<p>The buffer-filling state aims to fill the playback buffer and reach a certain threshold where the playback can be initiated or resumed.</p>
</li>
<li>
<p>The steady state is to keep the buffer level above a minimum threshold despite bandwidth fluctuation or interruptions. The steady state consists of 2 activity periods referred to as ON and OFF.</p>
<p>The client requests a segment every $T_s$ time units, where $T_s$ represents the content time duration of each segment, and sum of ON and OFF period durations equals $T_s$.</p>
<ul>
<li>ON period: client downloads the current segment and notes the achieved throughput value that will be later used in selecting the appropriate bitrate for future segments.</li>
<li>OFF period: client becomes idle temporarily.</li>
</ul>
</li>
</ul>
<p></p>
<p>There are different cases during competition process.</p>
<ol>
<li>
<p>The ON periods of clients don&rsquo;t overlap during the current segment download, each client will overestimate the available bandwidth. So longer download time will cause the initially non-overlapping ON periods to eventually start overlapping.</p>
<p></p>
</li>
<li>
<p>As the amount of overlap increases, the clients will have lower bandwidth estimations and start selecting segments that have lower bitrate. These segment will take less time to download, causing the amount of overlap among the ON periods to precedurally shorten, until the process reverts to its initial situation.</p>
<p></p>
</li>
<li>
<p>The cycle repeats itself, causing periodic up and down shift in the selected bitrates, leading to unstable video quality, unfairness, and underutilization.</p>
<p></p>
</li>
</ol>
<h2 id="consistent-quality-streaming">Consistent-Quality Streaming</h2>
<p>The correlation between video bitrate and its perceptual quality is non-linear.</p>
<ul>
<li>Different video content types have unique characteristics.</li>
<li>Differences of inter-stream and intra-stream video scene complexity across content.</li>
</ul>
<p></p>
<p></p>
<h2 id="qoe-optimization-and-measurement">QoE Optimization and Measurement</h2>
<p>HAS scheme uses application control loop, which also interacts with a lower-layer control loop(such as TCP congestion control). It plays a key role in determining the viewer QoE.</p>
<p></p>
<p>Factors influencing QoE are categorized as:</p>
<ol>
<li>Perceptual, directly perceived by the viewer.</li>
<li>Technical, indirectly affecting the QoE.</li>
</ol>
<h3 id="perceptual">Perceptual</h3>
<p>Perceptual factors include the video image quality, initial delay, stalling duration and frequency.</p>
<p>The impact of these factors differs depending on the users subjectivity.</p>
<p>Most users consider initial delays less critical than stalling.</p>
<h3 id="technical">Technical</h3>
<p>Technical factors include the algorithms, parameters, and hardware/software used in streaming system.</p>
<p>Specifically, factors are:</p>
<ul>
<li>Server side: encoding parameters, video qualities and segment size.</li>
<li>Client side: adaptation parameters and environment that clients reside in.</li>
</ul>
<h3 id="qoe-measurement">QoE measurement</h3>
<ol>
<li>Objective matrics: Peak Signal-to-Noise Ratio(PSNR), Structural SIMilarity(SSIM and SSIMplus), Perceived Video Quality(PVQ) and Statistically Indifferent Quality Variation(SIQV).</li>
<li>Subjective matrics: Mean Opinion Score(MOS).</li>
<li>Quality-of-Service (QoS)-derived matrics: startup delay, average video bitrate, quality switches and rebuffering events.</li>
</ol>
<p>Try to optimize each metric is difficult because it may result in conflicts.</p>
<h2 id="inter-destination-multimedia-synchronization">Inter-Destination Multimedia Synchronization</h2>
<p>Online communities are drifting towards watching online videos together in a synchronized manner.</p>
<p>Having Multiple streaming clients distributed in different geographical locations poses challenges in delivering video content simultaneously, while keeping the playback state of each client the same.</p>
<p>Typically, IDMS solutions involve a master node to which clients synchronize their playout to.</p>
<p>Rainer et proposed an IDMS architecture for DASH by using a distribute control scheme where peers can communicate and negotiate a reference placback timestamp in each session.</p>
<p>In another work, Rainer et provided a crowdsourced subjective evaluation to find a asynchronism threshold at which QoE was not significantly affected.</p>
]]></description>
</item>
<item>
    <title>Note for Content Based Vp for Live Streaming (2)</title>
    <link>http://localhost:1313/posts/papers/note-for-content-based-vp-for-live-streaming-2/</link>
    <pubDate>Tue, 25 Jan 2022 11:59:24 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-content-based-vp-for-live-streaming-2/</guid>
    <description><![CDATA[<h1 id="liveobj">LiveObj</h1>
<p><code>LiveDeep</code>方法利用卷积层从视频内容中提取深层特征，不受动态背景的影响。然而在整个推流会话中需要更新一个带有大量权重的巨大的神经网络模型。同时因为没有历史视频和用户的轨迹的数据，模型需要在运行时从随机权重开始训练。而这会导致两个问题：</p>
<ol>
<li>模型需要花很长时间从一次预测错误中恢复；</li>
<li>在初始化的阶段预测率成功率很低；</li>
</ol>
<p>为了解决这两个问题，提出预训练的模型来分析视频内容，对视频的语义进行层次化。</p>
<p><strong>基于对内容的分析，进一步设计了一个轻量级用户模型，将用户偏好映射到不同的视频内容。</strong></p>
<h2 id="用户观看行为分析">用户观看行为分析</h2>
<p>在直播推流中，不能通过分析其他用户的行为模式来得到特定用户的<code>ROI</code>，因此只能直接从视频内容本身入手。</p>
<p>通过对视频内容从空间和时间两个维度的分析得出结论：用户的<code>ROI</code>与物体的大小无关，而是很大程度上依赖于物体在视频中的语义，即用户倾向于观看有意义的事物。</p>
<p>这一结论可以给出推断<code>FoV</code>的直觉：基于检测视频中有意义的物体。</p>
<h2 id="methods">Methods</h2>
<p>首先提出两种直观的通过分析视频内容的视点预测方法，进一步总结这些方法的局限性，并逐步切换到对<code>LiveObj</code>的讨论。</p>
<h3 id="basic-method">Basic method</h3>
<p><em>Basic</em>方法检测视频中所有的对象并使用其中心作为预测的中心。</p>
<p>给出每个帧中的 $k$ 个物体， $\vec{O} = [o_1, o_2, o_3, &hellip;, o_k]$ ，其中每个 $o_i(i = 1, &hellip;, k)$ 表示物体的中心坐标： $o_i = &lt;o^{(x)}_i, o^{(y)}_i&gt;$ 。</p>
<p>最终的预测中心点坐标可以计算出来：
$$
C_x = \frac{1}{k} \sum^{k}_{i=1} o^{(x)}_i;\ C_y = \frac{1}{k} \sum^{k}_{i=1} o^{(y)}_i
$$</p>
<h3 id="over-cover-method">Over-Cover method</h3>
<p>受<code>LiveMotion</code>方法的启发，其创建了不规则的预测<code>FoV</code>来覆盖更多的潜在的区域，<em>Over-Cover</em>的方式预测的<code>FoV</code>会覆盖所有包含物体的区域。</p>
<p>采用<code>YOLOv3</code>来处理帧并检测物体，接着每个检测到的对象生成与该对象共享相同中心的预测子视图，所有子视图的聚合形成最终的预测视口。</p>
<h3 id="summary-for-intuitive-methods">Summary for intuitive methods</h3>
<p><em>Basic</em>方式可能会在多个物体的场景中无法正确选择目标；</p>
<p><em>Over-Cover</em>方式覆盖所有可能的目标来满足较高的精度，但会导致更高的带宽使用量；</p>
<p><em>Velocity</em>方式能很快的适应用户偏好的变化，但是预测精度在长期预测的情况下会显著下降；</p>
<h2 id="liveobj-method">LiveObj Method</h2>
<p><em>Over-Cover</em>方法将所有检测到的目标合并到预测的<code>FoV</code>中而导致冗余问题，而用户一次只能观看其中的几个。</p>
<p>为了解决这个问题，提出基于用户的反馈选择最吸引人的目标，例如用户当前的<code>FoV</code>来形成预测的<code>FoV</code>。</p>
<p>基于这种想法而提出<code>LiveObj</code>，一种基于轨迹的 VP 方式，通过从<em>Over-Cover</em>方法的结果中过滤掉用户更小可能性看到的目标来缩小最终的<code>FoV</code>。</p>
<p></p>
<ul>
<li><em>Object Detection</em>：处理视频帧并检测目标；</li>
<li><em>User View Estimation</em>：分析用户反馈并用<em>Velocity</em>的方式估计<code>FoV</code>；</li>
<li><em>Object tracking</em>：追踪用户观看的目标；</li>
<li><em>RL-based modeling</em>：接受估计出的<code>FoV</code>和被追踪的目标，最终更新每个分块的状态（选中或未选中）</li>
</ul>
<h3 id="object-detection-and-tracking">Object Detection and Tracking</h3>
<ol>
<li>
<p>Detection：<code>YOLOv3</code>；</p>
</li>
<li>
<p>Tracking：追踪的基本假设是用户会在接下来的一段时间内接着观看当前看着的目标。追踪任务在直播推流的运行时完成。因此每隔几秒收集用户反馈，并进一步推断用户之前正在观看的目标，然后据此更新追踪目标。</p>
<p>追踪算法：</p>
<p></p>
</li>
</ol>
<h3 id="user-view-estimation">User View Estimation</h3>
<p>分析用户的反馈处于两个目的：</p>
<ol>
<li>估计未来的用户的<code>FoV</code>；</li>
<li>校准当前用户<code>FoV</code>以及要跟踪的对象；</li>
</ol>
<p>给出用户反馈（即过去片段中实际的<code>FoV</code>），首先更新用户<code>FoV</code>并分析用户的行为模式，并根据此模式计算出下一帧中的预期用户速度。然后识别更新后的<code>FoV</code>中的对象，这些对象确定为<code>ROI</code>，对象追踪步骤将这些更新用于未来的片段来提高预测精度。</p>
<h3 id="rl-based-modeling">RL-based Modeling</h3>
<p>因为预测的误差和用户实际<code>FoV</code>的变化，可能会导致追踪的目标从<code>FoV</code>中消失，而这会使整个预测算法完全失效。所以提出一个基于 RL 的模型来为每个分块建立用户行为模型，旨在最小化预测误差。</p>
<p>出发点是<strong>不同的分块有不同的概率包含有意义的目标，并且更可能包含有意义目标的分块通常对目标检测错误更敏感。</strong></p>
<p>将上面的观察形式化为一个策略学习过程 $M$：
$$
M = &lt;S, A, P_{s, a, s&rsquo;}, R&gt;
$$
其中 $S$ 和 $A$ 表示状态和动作， $P_{s, a, s&rsquo;}$ 是给定状态 $s$ 的情况下选择动作 $a$ 的概率，转移之后的状态为 $s&rsquo;$ ，$R$ 表示奖励函数。</p>
<p>系统的目标是通过设定不同的 $P_{s, a, s&rsquo;}$ 的值，来学习每个分块对目标检测误差的不同的敏感性。</p>
<p>状态-价值函数用于估计在为所有可能的状态 $s \in S$ 选择动作 $a$ 时的价值，形式化为：
$$
v = E[Q_{s, a} | S_t = s]
$$</p>
<p>$$
Q_{s, a} = R^a_s + \gamma \sum_{s&rsquo; \in S} P_{s, a, s&rsquo;} v
$$</p>
<p>其中：$\gamma$ 是奖励参数。</p>
<p>最终的目标是通过计算每个 $P_{s, a, s&rsquo;}$ 找到最大的 $max(Q_{s, a})$。</p>
<p>而这一过程很耗费时间，因此使用修改之后的<code>Q-learning</code>过程，用贪心的方式来解决最优化问题。</p>
<p><code>Q-learning</code>过程在直播推流中有别于传统点播中的应用：</p>
<ol>
<li>预测同时基于当前的输入（目标追踪和<code>FoV</code>估计的结果）和历史状态（分块是否被选择）；</li>
<li>奖励基于用户的反馈在线生成，并且会在整个推流会话中变化，而不是预先设定好的奖励矩阵 $R$ ；</li>
<li>由于直播推流中内容的不可提前获取性， $Q$ 表必须在每次预测中更新；</li>
</ol>
<p>特别的，为每个分块都创建一个 $Q$ 表，对于每个 $Q$ 表有 4 种类型：</p>
<ul>
<li><em>object only</em>;</li>
<li><em>object and viewport</em>;</li>
<li><em>viewport only</em>;</li>
<li><em>no objects or viewport</em>;</li>
</ul>
<p>将这 4 种类型和 2 种中历史状态（选中或未选中）组合之后，得到每个表中状态 $s$ 的 8 个选项组合；</p>
<p>对每个状态而言，有 2 种动作（选中或不选中），因此每个表有 8 个状态和 2 个动作。</p>
<p>对每个表的奖励基于用户是否看到了分块而更新。</p>
<p>基于状态 $s$ 的对动作 $a$ 的选择转化成了：在相同输入的情况下找到 $max(Q(s, s&rsquo;))$；</p>
<p></p>
<h1 id="liveroi">LiveROI</h1>
<p><code>LiveObj</code>的基础是对象检测算法，用于分析视频内容的敏感性。但是其检测性能可能会受到算法、对象的缩放程度和全景视频导致的扭曲失真的影响，进而引起预测误差。类似于<code>LiveObj</code>的出发点，<code>LiveROI</code>的目标是通过使用动作识别来对视频内容进行分析，这会降低预测性能与前面所提因素的敏感性。</p>
<p>使用<code>3D-CNN</code>等预先训练的模型来分析每个分块上的视频内容，以完成动作识别。同时基于<code>NLP</code>技术，使用轻量级用户模型将用户偏好映射到不同的视频内容。</p>
<h2 id="用户对视频内容的偏好">用户对视频内容的偏好</h2>
<p>最基本的研究问题是：找到直播视频内容中的有效特征和信号或用户的行为，这些与用户的未来的<code>FoV</code>有强相关关系，因此可以将其作为预测因子。</p>
<p>通过对两个固定主题的视频的实验可以得出：</p>
<ol>
<li>用户花绝大多数的时间在视频中有意义的部分；</li>
<li><code>ROI</code>在空间上只占整个帧很小的部分；</li>
</ol>
<h2 id="liveroi-method">LiveROI Method</h2>
<p>融合视频内容感知和用户偏好反馈（即以用户头部运动轨迹的形式）来预测实时 VR 视频流中的<code>FoV</code>。</p>
<p>主要想法是使用 CV 算法去理解每个分块的内容，除此之外，采用实时的用户反馈方便分块的选择。</p>
<p>需要满足的条件是：所有分块上的视频处理开销应该保持较小，以避免视频冻结和累计的实时延迟。</p>
<p>使用<code>3D-CNN</code>进行视频理解，重点是识别视频中隐含的有意义的动作，动作识别结果用于以自然语言的格式描述视频内容。这种 3D-CNN 模型可以在公共数据集上进行训练，因此具有通用性，以适应各种类型的动作和视频，这使得它可以用于实时 VR 流传输，因为在流传输会话之前没有关于视频内容的先验知识。</p>
<p>但是具有有意义动作的区域可能不是用户最后会确定的<code>FoV</code>，尤其是在目标视频中存在多个有意义动作的情况下。</p>
<p>为了解决这一问题，通过收集用户关于偏好视频内容的实时描述，进一步设计了基于“词/短语”的用户偏好模型。</p>
<p>采用<strong>词语嵌入</strong>的方法，通过比较两个来源短语的语义相似度，确定最佳匹配区域作为预测<code>FoV</code>，以此来桥接动作识别结果和用户偏好模型。</p>
<h3 id="workflow">Workflow</h3>
<p></p>
<p><code>3D-CNN</code>的输入数据包含一批 $T$ 张图像，因此统一在一个视频片段中子采样 $T$ 帧。</p>
<p>每个子采样的帧都划分成 $M \times N$ 个分块，VP 问题定义为确定要包含在<code>FoV</code>中的分块。</p>
<p>为了避免由于分块带来的潜在的信息损失（有意义的动作被划分成多个分块），每个用于动作识别的输入图像是从比原始分块边界更大的区域中所提取出来的，但是将共享与原始分块相同的中心。</p>
<p><code>3D-CNN</code>模块的输出是动作识别结果，即结果矩阵。</p>
<p>面对 $M \times N$ 个分块，为了满足性能要求，将每个分块的动作识别过程视为相互独立的过程，创建 $m \times n$ 个线程来实现并行识别，每个线程向结果矩阵输出对应分块的结果向量。</p>
<p>在预测的最后一步，生成包含所有分块的预测分数的得分向量。进一步对所有的分数向量进行排序，并定位第 $M$ 个值，该值设定为选择分块进入预测<code>FoV</code>中的阈值。通过控制 $M$ 的大小可以控制预测的<code>FoV</code>的大小，分数向量中的分数表示用户对分块内容的感兴趣程度。</p>
<p>为了计算分数向量，进一步设计用户向量，其中包含描述用户偏好的词或短语。考虑到推流过程中用户可能会改变兴趣，用户向量会基于用户实时轨迹更新。</p>
<p>在给定用户向量和结果矩阵中的词或短语的情况下，考虑到非自然语言中的两个不同的词可能具有相近的含义，不直接进行词比较，而是使用词分析来计算其相关性。</p>
<h3 id="cnn-model">CNN Model</h3>
<p>采用<code>ECO lite</code>模型完成 VR 直播推流中的动作识别。所有来自同一视频片段的图像都被储存在一个缓冲帧集合中。</p>
<p><code>ECO lite</code>模型为 2D CNN 提取特征图的任务收集工作帧集合（分别由前一视频片段和当前视频片段的缓冲帧集合的后半部分和前半部分组成），在下一个阶段，从每个片段获得的特征图被堆叠到更高的表示中，之后被送到之后的 3D CNN 中用于最终的动作预测。具体的识别过程中同样使用多线程并行处理，处理 1 帧图像是每次创建和分块数相同的线程，为每个分块都初始化一个<code>ECO lite</code>模型。</p>
<p>显然预训练的模型不能为直播推流提供正确的推理结果，但是它可以看作是对视频内容的验证，即：给定一种类型的视频内容，其实其本身被误分类了，但在同一个模型之下它总是会被分类进在整个推流过程中都有相近分数的簇中。</p>
<p>利用这个特性，基于动作识别模型提供的对视频内容的描述，进一步设计动态的用户模型来映射用户偏好到不同的视频内容上。</p>
<h3 id="nlp-model">NLP Model</h3>
<p>为了桥接动作识别和用户偏好向量，必须分析词/短语之间的相似性。</p>
<p>然而现有的 ML 算法不能直接处理生数据，因为输入必须是数值。为了解决这个问题，采用单词嵌入技术，使用多种语言模型以数值向量的形式来表示单词，以此来确保有相近意义的词有相近密度的表示。</p>
<p>具体处理时使用<a href="https://github.com/artetxem/phrase2vec" target="_blank" rel="noopener noreffer"><code>Phrase2Vec</code></a>作为 NLP 模块的模型（作为<code>Word2Vec</code>的扩展，能更好的分析两个短语之间的相似性）。</p>
<h2 id="用户模型与预测">用户模型与预测</h2>
<p></p>
<p>图 5.3 阐明了基于结果向量和用户向量的预测过程。由动作识别得出的结果向量，包括一个动作向量 $A$ 和一个权重向量 $W$ 。用户向量包括偏好向量 $P$ 和可能性向量 $L$ 。$A$ 和 $P$ 包含词和短语，描述了视频内容和用户偏好。 $W$ 和 $L$ 分别由表示神经网络对动作结果的置信度和用户对视频内容的参考可能性的值组成。</p>
<p>假设每帧 25 个分块，CNN 模块的输出结果是 25 个 $A$ 向量和 25 个 $W$ 向量；对与用户偏好，只使用 1 个 $P$ 向量和 1 个 $L$ 向量。</p>
<p>最终的分数向量 $S$ 计算为每个 $A$ 和 唯一的 $P$ 之间的相关性。结果也受相应的 $W$ 和 $L$ 的影响而调整。</p>
<p>假设余弦相似性函数为 $\rho$ ，那么 $A$ 和 $P$ 中的每个 $a_i$ 和 $p_i$ 的计算可以表示为：
$$
{\rho}_i (a_i, p_i) = Phrase2Vec(a_i, p_i)
$$
设定每个向量中包含 5 个元素，分数向量 $S$ 计算为：
$$
S = L \cdot W \cdot \sum {\rho} (A, P)
$$
对应于 25 个分块，最终的分数向量中包含 25 个元素。 $s_k$ 表示 $k_{th}$ 分块的分数值，详细算法：</p>
<p></p>
<p>分数向量更新完毕之后就可以获得每个分块内容和用户偏好之间的相关性，用帧上每个分块的亮度来做可视化：</p>
<p></p>
<p>将分数向量中的元素从高到低排序，选定 $\frac{1}{3}$ 作为阈值，将前 $\frac{1}{3}$ 的分块看作相同的分数等级作为最后的预测区域。</p>
<p>为了应对推流过程中用户偏好的变化，为分数向量的计算设计动态加权的用户偏好向量。</p>
<p>设定用户偏好向量 $P$ 的大小与动作向量 $A$ 的大小相同，一旦系统获取到用户实际的<code>FoV</code>位置，就计算其视野中心并定位到相应的分块，使用前一视频片段中该选中分块的动作向量 $A&rsquo;$ 来更新用户的偏好向量。</p>
]]></description>
</item>
<item>
    <title>Content Based VP for Live Streaming (1)</title>
    <link>http://localhost:1313/posts/papers/note-for-content-based-vp-for-live-streaming-1/</link>
    <pubDate>Sat, 22 Jan 2022 18:03:09 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-content-based-vp-for-live-streaming-1/</guid>
    <description><![CDATA[<h1 id="livemotion">LiveMotion</h1>
<h2 id="motivation">Motivation</h2>
<p>基于视频中物体的运动模式来做对应的<code>FoV</code>预测。</p>
<p>将用户的<code>FoV</code>轨迹与视频内容中运动物体的轨迹结合到一起考虑：</p>
<p></p>
<p>细节可以参见：<a href="https://ayamir.github.io/posts/note-for-content-motion-viewport-prediction/" target="_blank" rel="noopener noreffer">note-for-content-motion-viewport-prediction</a>.</p>
<h1 id="livedeep">LiveDeep</h1>
<p>受限于<code>Motion</code>识别算法，前面提出的<code>LiveMotion</code>只能作用于有清晰并且容易分别的前景背景边界的视频，其健壮性并不能满足全景直播推流的场景。</p>
<h2 id="method">Method</h2>
<p><code>LiveDeep</code>处理问题的场景为：</p>
<ol>
<li>视频内容在线生成；</li>
<li>没有历史用户数据；</li>
<li>预测需要满足实时性的要求；</li>
</ol>
<p><code>LiveDeep</code>的设计原则：</p>
<ol>
<li><em>online</em>：在线训练在线预测；</li>
<li><em>lifelong</em>：模型在整个视频播放会话中更新；</li>
<li><em>real-time</em>：预测带来的处理延迟不能影响推流延迟；</li>
</ol>
<p><code>CNN</code>的设计：</p>
<ol>
<li>在推流会话的运行时收集并标注训练数据；</li>
<li>以交替迭代的方式进行基于当前视频片段的推理和基于之前视频片段的训练；</li>
<li>子采样少部分的代表帧来运行 VP 以满足实时性的要求；</li>
</ol>
<h2 id="framework">Framework</h2>
<p></p>
<h3 id="setup">Setup</h3>
<ol>
<li>分包器将视频按照 DASH 标准将视频分段，每个段作为训练模型和预测的单元；</li>
<li>考虑到不同的视频可能具有不同的帧速率，在每个单元中统一采样 $k$ 帧而非以固定的采样率采样；</li>
<li>将每帧图像划分成 $x \times y$ 个分块，最终每个单元中要处理的分块数为 $k \times x \times y$ ；</li>
<li>训练集来自于用户的实时反馈，根据实际<code>FoV</code>和预测<code>FoV</code>之间的差距来标注数据；</li>
<li>用户的轨迹数据来自于用户的实时头部轨迹，采样的帧与<code>CNN</code>模块采样的帧同步；</li>
</ol>
<h3 id="details">Details</h3>
<ol>
<li>在用于训练的图像还没有被标注之前并不能直接预测，所以 CNN 模块只能以随机的权重给出预测结果。用预测结果与实际结果计算出损失值之后以此来更新 CNN 模型；</li>
<li>LSTM 模型只能以用户观看到视频之后的实际轨迹作为训练的输入输入数据；</li>
<li>对下一个片段而言，首先使用两个模块独立做出预测。每个模块的预测都基于子采样之后的 $k$ 个帧；</li>
<li>为了产生对整个片段的预测结果，假设相邻的帧之间共享相同的视野中心（时空局部性）；</li>
<li>取两个模块预测输出的共同的部分作为最终的预测结果；</li>
</ol>
<h2 id="cnn-module">CNN Module</h2>
<p></p>
<p>使用经典的 CNN：VGG 作为骨干网络，修改最后一层，只输出两类：感兴趣的和不感兴趣的。</p>
<h3 id="推理和视口生成">推理和视口生成</h3>
<p>直观上的想法是选择被分类为感兴趣的部分，并且这些所选部分在原始帧中的位置将指示其他帧中可能感兴趣的<code>FoV</code>。</p>
<p>实际上存在的问题是：几乎所有的部分都被分类为感兴趣的一类，最终结果是整个帧被选择作为预测的结果。</p>
<p>所以不直接使用 CNN 网络的输出，而是在被分类为感兴趣的部分中进一步细分。通过对输出的分数排序并选择前 $M$ 份比例的输出作为最终的结果，这样通过控制 $M$ 的大小可以调整精度和消耗的带宽。</p>
<h3 id="训练过程">训练过程</h3>
<p>在传统的监督训练中，训练时间取决于可接受的最低损失值和 epoch 的值。为了满足实时性，<code>LiveDeep</code>采用较高的最低损失值和较低的最大 epoch 值。</p>
<ul>
<li>
<p><em><strong>High acceptable loss value</strong></em>：因为直接对从被分类为感兴趣的部分中去获取最终结果，所以通过实验证明，损失值应该要比常规的 CNN 更高：设定为 0.2。</p>
</li>
<li>
<p><em><strong>The number of epochs</strong></em>：因为直播推流的特殊性，重复的训练并不能持续降低损失，所以采用较小的值：10。</p>
</li>
<li>
<p><em><strong>The batch size</strong></em>：受限于训练的图像，将其设定为训练图像的个数即： $k \times x \times y$。</p>
</li>
<li>
<p><em><strong>Dynamic learning rate</strong></em>：</p>
<p></p>
</li>
</ul>
<h2 id="lstm-module">LSTM Module</h2>
<p>单纯的<code>CNN</code>模型可能会导致对视频内容有强记忆性，而这会使模型在面对新视频内容时需要花较长的时间去接受用户偏好，即对于用户偏好的快速切换不能做出即时响应。而<code>LSTM</code>的模块用于弥补这一缺陷；</p>
<p>采用与原始的<code>LSTM</code>模型相同的训练过程：先用收集的训练数据训练模型然后推断未来的数据。</p>
<p>收集用户在过去的视频片段中的用户轨迹，包括从 $k$ 个子采样帧中的 $k$ 个采样点，因此作为训练数据，同时将每个采样点中每个帧的索引指定为时间戳。最终模型的输出是预测出的分块的索引。</p>
<h2 id="混合模型">混合模型</h2>
<p>将<code>CNN</code>模块得到的输出作为主要的结果，接着结合<code>LSTM</code>模块的输出结果作为最终的预测结果。</p>
]]></description>
</item>
<item>
    <title>Note for Popularity Aware 360-Degree Video Streaming</title>
    <link>http://localhost:1313/posts/papers/note-for-popularity-aware-360-degree-video-streaming/</link>
    <pubDate>Tue, 18 Jan 2022 16:07:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-popularity-aware-360-degree-video-streaming/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/abstract/document/9488856/" target="_blank" rel="noopener noreffer">Popularity-Aware 360-Degree Video Streaming</a></p>
<p>Level：IEEE INFOCOM 2021</p>
<p>Keywords：Dynamic tiling, Cross-user division, Heuristic QoE optimization</p>
<h2 id="motivation">Motivation</h2>
<p>将视频划分成分块进行编码之后，会降低编码效率，并增大服务端的存储压力。（细节可以参见<a href="https://ayamir.github.io/posts/note-for-optile/" target="_blank" rel="noopener noreffer">Optile</a>）</p>
<p>而分块时根据用户的 ROI 来确定不同的大小，并在客户端预取，这可以节省带宽。</p>
<p>用户的 ROI 推断利用跨用户的偏好来确定，即所谓的<code>Popularity-Aware</code>。</p>
<h2 id="model-and-formulation">Model and Formulation</h2>
<h3 id="video-model">Video Model</h3>
<p>视频从时间上被分成固定长度的片段，接着每个片段被从空间上划分成 $C$ 个分块。</p>
<p>除了常规的分块之外， $M$ 个宏块也被建构出来。</p>
<p>每个常规分块和宏块都被编码成 $V$ 个不同的码率质量等级并存储在服务端。</p>
<p>整个推流过程可以看作是一系列连续的下载任务。</p>
<p>客户端在每次下载任务中的目标是：选择恰当分块（宏块或者常规分块的集合）的恰当质量。</p>
<p>用 $L$ 表示客户端请求分块时，缓冲区中已经下载但还没有查看的视频的视频长度，为了避免缓冲事件，分块需要在缓冲区被清空即 $L = 0$ 之前被下载完毕。</p>
<h3 id="qoe-model">QoE Model</h3>
<p>$$
Q(V_k) = Q_{0}(V_k) - {\omega}_v I_v (V_k) - {\omega}_r I_r (V_k)
$$</p>
<p>$V_k$ 表示下载的第 $k$ 段视频质量； $Q_0$ 表示平均质量； $I_v$ 表示由质量变化导致的质量损害； $I_r$ 表示由缓冲事件导致的质量损害； ${\omega}_v$ 和 ${\omega}_r$ 分别表示质量变化和缓冲的加权因子；</p>
<ul>
<li>
<p>平均质量：
$$
Q_0(V_k) = q(\overline{V_k})
$$
$\overline{V_k}$ 表示<code>FoV</code>内的平均视频质量； $q(\cdot)$ 表示视频质量和用户实际感知质量之间的映射函数；</p>
</li>
<li>
<p>质量变化：两个连续段之间的质量差异和<code>FoV</code>内不同空间位置 tile 的质量差异会导致用户不适。
$$
I_v(V_k) = |Q_0(V_k) - Q_0(V_{k-1})| + \widehat{V_k}
$$
$|Q_0(V_k) - Q_0(V_{k-1})|$ 表示连续段间的<code>FoV</code>内时间质量差异； $\widehat{V_k}$ 表示一个视频段的<code>FoV</code>内空间质量差异；</p>
</li>
<li>
<p>缓冲：
$$
L_r(V_k) = {(\frac{S(V_k)}{R} - L, 0)}_+
$$
$S(V_k)$ 表示段数据量大小； $R$ 表示下载吞吐量； ${(x)}_+ = max \lbrace x, 0 \rbrace$ ；</p>
</li>
</ul>
<h3 id="formulation">Formulation</h3>
<p>用 ${\beta}^v_m ({\beta}^v_c)$ 表示对应的宏块或常规块是否被下载：</p>
<p>${\beta}^v_m = 1$ 表示下载编码的质量等级为 $v$ 的宏块，消耗的带宽为 $B^v_m$ ，反之 $ {\beta}^v_m = 0$ 表示不下载；</p>
<p>${\beta}^v_c = 1$ 表示下载编码的质量等级为 $v$ 的常规块，消耗的带宽为 $B^v_c$，反之 ${\beta}^v_m = 0$ 表示不下载；</p>
<p>客户端应该优先下载覆盖用户<code>FoV</code> 的宏块，如果没有这样的宏块则去下载对应的常规块的集合。</p>
<p>优化目标：
$$
max\ Q(\lbrace v | {\forall}_{m, v} {\beta}^v_m = 1 \rbrace) + Q(\lbrace v | {\forall}_{c, v} {\beta}^v_c = 1 \rbrace)
$$
同时需要满足以下 3 个约束：
$$
\sum^{M}_{m=1} \sum^{V}_{v=1} {\beta}^v_m + 1(\sum^{C}_{c=1} \sum^{V}_{v=1} {\beta}^v_c) = 1
$$</p>
<p>$$
\sum^{V}_{v=1} {\beta}^v_c \le 1,\ for\ c = 1, &hellip;, C
$$</p>
<p>$$
\sum^{M}_{m=1} \sum^{V}_{v=1} {\beta}^v_m B^v_m + \sum^{C}_{c=1} \sum^{V}_{v=1} {\beta}^v_c B^v_c \le R \cdot L
$$</p>
<p>$Q(\cdot)$ 是公式 1 中定义的质量； $R$ 是网络带宽； $1(x) = 1 \iff x &gt; 0$ ；$1(x) = 0 \iff x \le 0$ ；</p>
<p>约束 1 强制为观看区域下载宏块或常规块的集合，只下载宏块的一个质量版本；</p>
<p>约束 2 规定只下载常规块的一个质量版本；</p>
<p>约束 3 保证视频数据可以在开始播放之前被完全下载；</p>
<p>给出用户的观看区域之后，候选的宏块或对应的常规块集合也可以求出。</p>
<p>将<code>QoE</code>最大化的问题分解成两个子问题：</p>
<ul>
<li>确定宏块的质量等级；</li>
<li>确定常规块的质量等级；</li>
</ul>
<p>最后的解取这两种方案能取得更大<code>QoE</code>的那种。</p>
<p>如果<code>QoE</code>模型不考虑常规块之间的质量差异，则整体的<code>QoE</code>等价于下载的常规块的平均质量等级。</p>
<p>确定常规块质量等级的问题则可以简化为：
$$
max\ \sum_{c \in C} \sum^{V}_{v=1} Q({\beta}^v_c v)
$$
需要满足以下 2 个约束：
$$
\sum^{V}_{v=1} {\beta}^v_c = 1,\ for\ c \in C
$$</p>
<p>$$
\sum_{c \in C} \sum^{V}_{v=1} {\beta}^v_c B^v_c \le R \cdot L
$$</p>
<p>$C$ 表示覆盖观看区域的常规块集合。</p>
<p>简化之后的子问题可以通过对多项选择背包问题的简化，证明为是<code>NP-hard</code>问题，基于此提出启发式算法。</p>
<h2 id="基于宏块的流行性感知推流">基于宏块的流行性感知推流</h2>
<h3 id="基于观看区域确定宏块">基于观看区域确定宏块</h3>
<p>不同用户对相同视频的观看有着相似的 ROI，其视野中心是相近的，因此首先确定其视野中心并聚类到一起。</p>
<p>不能直接应用的知名聚类算法：</p>
<ul>
<li>需要事先确定簇（即宏块）数量的算法（事先并不能确定需要多少宏块）：<code>K-means</code></li>
<li>簇会越聚越大的算法（这样会失去节约带宽的优点）：<code>DBSCAN</code></li>
</ul>
<p>提出的算法用 2 个参数 $\lambda$ 和 $\gamma$ 来保证彼此相近的两个视野中心被归入同一簇，同时基于簇的宏块不至于太大。</p>
<ul>
<li>被归入同一簇的视野中心之间的距离应该小于等于 $\lambda$；</li>
<li>同一个簇的任意两个视野中心之间的距离应该小于等于 $\gamma$；</li>
</ul>
<p>为了确定这两个参数，还需要考虑常规块的大小带来的影响。</p>
<p>算法描述：</p>
<p>给出用 $P$ 表示的点集，其中每个点表示一个用户的视野中心位置；</p>
<p>用 $N_p = \lbrace q | q \in P \land q \neq p \land dist(p, q) \le \lambda \rbrace$ 来表示与点 $p$ 之间欧式距离小于 $\lambda$ 的点集（即为临近点集）；</p>
<ol>
<li>初始化拥有最多临近点的点所在的簇，例如： $p = {argmax}_{p \in P} |N_p|$；</li>
<li>添加临近簇内任何点的点到簇中，扩张过程直到找不到符合条件的点位置；</li>
<li>检查簇中任意两个点之间的距离是否大于 $\gamma$ ，如果存在这种情况就使用<code>K-means</code>算法将这个簇分成两个子簇；</li>
<li>从 $P$ 中移除簇中的点；</li>
<li>重复 1-4 的过程直到 $P = \empty$；</li>
</ol>
<p></p>
<h3 id="宏块优化">宏块优化</h3>
<p>通过简单地覆盖簇中用户的所有观看区域来为每个簇建构宏块可能会导致建构出不必要的大宏块，因此需要确定恰当的宏块大小。</p>
<ul>
<li>
<p>首先需要确定哪些用户的观看区域应该被用于构建宏块，这样用户下载宏块时的带宽使用率小于下载一组常规块时的带宽使用率：$B_m$ 和 $B_c$ 分别表示覆盖相同观看区域的宏块和常规块的数据量大小。</p>
</li>
<li>
<p>为了解决用户头部运动的随机性，宏块应该在覆盖用户观看区域之外加上一些边界区域。边界区域可以基于用户观看中心的变化来确定，变化通过在推流观看过程中以固定采样率记录。</p>
<p>一个视频片段中 $x(y)$ 坐标的变化定义为 $x(y)$ 坐标的标准差。</p>
<p></p>
<p>实验发现：在一个视频片段中，用户的 $x(y)$ 坐标的变化很小。</p>
<p>分别用 $A_x$ 和 $A_y$ 表示 $x$ 和 $y$ 方向上的变化，构建的宏块应该覆盖用户的观看区域，并为 $x(y)$ 方向加上 $\frac{A_x}{2}(\frac{A_y}{2})$ 的边缘区域。</p>
</li>
</ul>
<p>宏块构造问题的形式化：</p>
<p>为每个用户 $i$ 引入二元变量 ${\alpha}_i$ ，${\alpha}_i = 1$ 表示此用户的观看区域用于构建宏块，反之则没有；</p>
<p>实际应用中即为：如果 ${\alpha}_i = 1$ ，则用户 $i$ 可以下载宏块；否则用户只能下载对应的常规块集合。</p>
<p>问题的目标是：在下载宏块或相同质量等级的常规块集合时，最小化所有用户的总带宽使用量。
$$
\underset{\lbrace {\alpha}_i \rbrace}{min}\ \sum^{N_j}_{i=1} {\alpha}_i B_m + (1-{\alpha}_i) B_c
$$
$N_j$ 表示在 $j^{th}$ 簇中的用户数量；解决问题之后，可以用所有 ${\alpha}_i = 1$ 的用户观看区域构建宏块；</p>
<p>尽管暴力枚举法可以完成最优求解，但是其时间复杂度为 $O(2^{N_j})$ ，为了减少实际建构宏块的时间，提出一种类似于<a href="https://en.wikipedia.org/wiki/Random_sample_consensus" target="_blank" rel="noopener noreffer">随机采样一致性算法</a>的迭代算法，每次迭代中，所做工作如下：</p>
<ol>
<li>随机选取用户观察区域的子集。</li>
<li>编码宏块，用 $B_m$ 表示构建的宏块的带宽使用量。</li>
<li>检查建构的宏块是否覆盖用户 $i \in \lbrace 1, &hellip;N_j \rbrace$ ，是则${\alpha}_i = 1$；否则 ${\alpha}_i = 0$。</li>
<li>检查总共的带宽使用量是否比之前迭代的更小，是则用当前迭代建构的宏块更新最终的宏块；否则继续迭代。</li>
</ol>
<p>为了避免预测失败时用户看到空白区域，在下载观看区域的高质量宏块或常规块集合之外，也以最低质量下载其余的常规块。</p>
<h3 id="流行性感知推流">流行性感知推流</h3>
<p>服务端基于多个用户的历史观看信息建构宏块，同时也使用常规块的划分方案编码视频。</p>
<p>客户端在推流过程中选择恰当块（宏块或常规块集）的恰当的质量等级来最大化用户的<code>QoE</code>。</p>
<p>流行性感知的推流算法首先为每个视频段预测用户的观看区域，之后预取相应的宏块或常规块集。</p>
<ul>
<li>使用<a href="https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db" target="_blank" rel="noopener noreffer">岭回归</a>做 VP，输入用户在一系列历史帧中的观看区域中心坐标，输出未来帧中用户的观看区域位置。</li>
<li>基于预测的观看区域，算法确定是否存在覆盖预测区域及其边缘区域的宏块，是则搜索并下载满足条件的最高质量的宏块；否则下载相应区域的常规块集。</li>
</ul>
<p></p>
<p>选择常规块集时首先为所有要选择的块确定满足贷款限制的最高质量等级，分配完之后如果还有剩余的带宽，算法会根据常规块与视野中心距离的远近程度提高一个质量等级，越近越优先提高。同时考虑到空间质量差异会降低<code>QoE</code>，所以提高质量的行为只有在超过半数的常规块满足条件时才会执行。</p>
]]></description>
</item>
<item>
    <title>Note for srlABR Cross User</title>
    <link>http://localhost:1313/posts/papers/note-for-srlABR-cross-user/</link>
    <pubDate>Sat, 15 Jan 2022 18:46:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-srlABR-cross-user/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9234071" target="_blank" rel="noopener noreffer">Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network</a></p>
<p>Level：IEEE Transactions on Broadcasting 2021</p>
<p>Keywords：Cross-user vp, Sequetial RL ABR</p>
<h2 id="主要工作">主要工作</h2>
<ul>
<li>使用跨用户注意力网络<code>CUAN</code>来做 VP；</li>
<li>使用<code>360SRL</code>来做 ABR</li>
<li>将上面两者集成到了推流框架中；</li>
</ul>
<h2 id="vp">VP</h2>
<h3 id="motivation">Motivation</h3>
<p>形式化 VP 问题如下：</p>
<p>给出 $p^{th}$ 用户的 $1-t$ 时间内的历史视点坐标 $L^{p}_{1:t} = \lbrace l^p_1, l^p_2, &hellip;, l^p_t \rbrace$ ，其中 $l^p_t = (x_t, y_t), x_t \in [-180, 180]; y_t \in [-90, 90]$ ；</p>
<p>同一视频的不同用户视点表示为 $L^{1:M}_{1:t+T}$ ， $M$ 表示其他用户的数量；</p>
<p>目标是预测未来的 $T$ 个时刻的视点位置 $L^p_i, i = t+1, &hellip;, t+T$ ；</p>
<p>最终可以用数学公式表达为：
$$
\underset{F}{min} \sum^{t+T}_{k = t+1} {\parallel l^p_k - \hat{l}^p_k \parallel}_1
$$</p>
<p>现有的用<code>KNN</code>做的跨用户预测基于 LR 的模型，而 LR 的模型很容易产生偏差，所以为了增强<code>KNN</code>的性能，同时考虑单用户的历史视点轨迹和跨用户的视点轨迹。</p>
<ul>
<li>提出一种注意力机制来自动提取来自其他用户视口的有用信息；</li>
<li>对于与当前用户有相似偏好的用户轨迹信息给与更多的注意；</li>
<li>相似性通过基于过去时间段内其他用户的轨迹计算出来；</li>
</ul>
<h3 id="design">Design</h3>
<p></p>
<ol>
<li>
<p>轨迹编码器模块从用户的历史视点位置提取时间特征；</p>
<p>使用<code>LSTM</code>来编码用户的观看路径；</p>
<p>为了预测 ${(t+1)}^{th}$ 帧的视点位置，首先向<code>LSTM</code>输入 $p^{th}$ 用户的历史视点坐标：
$$
f^{p}_{t+1} = h(l^p_1, l^p_2, &hellip;, l^p_t)
$$
$h(\cdot)$ 是<code>LSTM</code>的输入输出函数；</p>
<p>接着使用相同的<code>LTSM</code>编码其他用户的观看轨迹：
$$
f^{i}_{t+1} = h(l^i_1, l^i_2, &hellip;, l^i_{t+1}), i \in \lbrace 1, &hellip;, M \rbrace
$$</p>
</li>
<li>
<p>注意力模块从其他用户的视点轨迹中提取与 $p^{th}$ 用户相关的信息</p>
<p>首先推导出 $p^{th}$ 用户和其他用户之间的相关系数：
$$
s^{pi}_{t+1} = z(f^{i}_{t+1}, l^{p}_{t+1}), i \in \lbrace 1, &hellip;, M \rbrace \cup \lbrace p \rbrace;
$$
$s^{th}_{t+1}$ 表示 $p^{th}$ 用户和 $i^{th}$ 用户之间的相似性；$z()$ 由内积运算建模（还可用其他方式建模比如多个 FC 层）；</p>
<p>接着将相关系数规范化：
$$
{\alpha}^{pi}_{t+1} = \frac{e^{s^{pi}_{t+1}}}{\sum_{i \in \lbrace 1,&hellip; M \rbrace \cup {\lbrace p \rbrace}^{e^{s^{pi}_{t+1}}}}}
$$
最后得到融合特征：
$$
g^{p}_{t+1} = \sum_{i \in {\lbrace 1,&hellip;M \rbrace \cup \lbrace p \rbrace}} {\alpha}^{pi}_{t+1} \cdot f^{i}_{t+1}
$$
融合特征被最后用于 VP。</p>
</li>
<li>
<p>VP 模块预测 ${(t+1)}^{th}$ 帧的视点位置</p>
<p>$$
\hat{l}^{p}_{t+1} = r(g^{p}_{t+1})
$$
函数 $r(\cdot)$ 由一层 FC 建模。值得注意的是，对应于未来 T 帧的视点是以滚动方式预测的。</p>
</li>
</ol>
<h3 id="loss">Loss</h3>
<p>损失函数定义为预测的视点位置和实际视点位置之间的所有绝对差异的总和：
$$
L = \sum^{t+T}_{i=t} {|\hat{l}^p_i - l^p_i|}_1
$$</p>
<h3 id="details">Details</h3>
<ul>
<li>使用<code>PyTorch</code>实现；</li>
<li>函数 $h(\cdot)$ 由两个堆叠的<code>LSTM</code>层组成，两者都有 32 个神经元；</li>
<li>函数 $r(\cdot)$ 包含一个带有 32 个神经元的 FC 层，接着是<code>Tanh</code>函数；</li>
<li>历史视点和未来视点的长度设定为 1 秒和 5 秒；</li>
<li>每次迭代从数据集中随机产生 2048 个样本；</li>
<li>所有训练变量的优化函数采用<code>Adam</code>；</li>
<li>$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$；</li>
<li>$learning\ rate = 10^{-3}, training\ epoch = 50$；</li>
</ul>
<h2 id="abr">ABR</h2>
<h3 id="formulation">Formulation</h3>
<p>全景视频被切分成 $m$ 个长度为 $T$ 秒的视频片段，每个视频片段空间上划分成 $N$ 个分块，分别以 $M$ 个不同的码率等级编码。因此对于每段有 $N \times M$ 个可选的编码块。</p>
<p>ABR 的目标是为每个片段找到最优的码率集 $X = \lbrace x_{i, j} \rbrace \in Z^{N \times M}$ （ $x_{i, j} = 1$ 意味着为 $i^{th}$ 块选择 $j^{th}$ 的码率等级）：
$$
\underset{X}{max} \sum^{m}_{t=1} Q_t
$$
$Q_t$ 表示 $t^{th}$ 段的 QoE 分数，与以下几个方面有关：</p>
<ul>
<li>
<p>VIewport Quality：
$$
Q^1_t = \sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot p_i \cdot r_{i,j}
$$
$p_i$ 表示 $i^{th}$ 分块的规范化观看概率； $r_{i,j}$ 记录块 $(i, j)$ 的码率；</p>
</li>
<li>
<p>Viewport Temporal Variation：
$$
Q^2_t = |Q^1_t - Q^{1}_{t-1}|
$$</p>
</li>
<li>
<p>Viewport Spatial Variation：
$$
Q^3_t = \frac{1}{2} \sum^{N}_{i=1} \sum_{u \in U_i} p_i \cdot p_u \sum^{M}_{j=1} |x_{i,j} \cdot r_{i,j} - x_{u,j} \cdot r_{u,j}|
$$
$U_i$ 表示 $i^{th}$ 个分块的 1 跳邻居中的 tile 索引<a href="https://ieeexplore.ieee.org/document/8486606" target="_blank" rel="noopener noreffer">[1]</a>；</p>
</li>
<li>
<p>Rebuffering：
$$
Q^4_t = max(\frac{\sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot r_{i,j} \cdot T}{\xi_t} - b_{t-1}, 0)
$$
$\xi_t$ 表示网络吞吐量； $b_{t-1}$ 表示播放器的缓冲区占用率；</p>
<p>最终的 QoE 可以由上面的指标定义：
$$
Q_t = Q^1_t - \eta_1 \cdot Q^2_t - \eta_2 \cdot Q^3_t - \eta_3 \cdot Q^4_t
$$
$\eta_*$ 是可调节的参数，与不同的用户偏好对应。</p>
</li>
</ul>
<h3 id="sequential-rl-based-abr">Sequential RL-Based ABR</h3>
<p>假设基于 tile 的全景推流 ABR 过程也是 MDP。</p>
<p></p>
<p>细节在<a href="https://ayamir.github.io/posts/note-for-360srl/" target="_blank" rel="noopener noreffer">360SRL</a>中已经说明清楚。</p>
]]></description>
</item>
<item>
    <title>Note for 360SRL</title>
    <link>http://localhost:1313/posts/papers/note-for-360srl/</link>
    <pubDate>Thu, 13 Jan 2022 12:08:36 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-360srl/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/8784927" target="_blank" rel="noopener noreffer">360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming</a></p>
<p>Level：ICME 2019</p>
<p>Keywords：ABR、RL、Sequential decision</p>
<h2 id="创新点">创新点</h2>
<ul>
<li>在 MDP 中，将 N 维决策空间内的一次决策转换为 1 维空间内的 N 次级联顺序决策处理来降低复杂度。</li>
</ul>
<h2 id="问题定义">问题定义</h2>
<p>原始的全景视频被划分成每段固定长度为 $T$ 的片段，</p>
<p>每个片段包含 $N$ 个分块，并以 $M$ 的码率等级独立编码，</p>
<p>因此对每个片段，有 $N \times M$ 种可选的编码块。</p>
<p>为了保证播放时的流畅性，需要确定最优的预取集合：</p>
<p>${a_0, &hellip;, a_i, &hellip;, a_{N-1}}, i \in \lbrace 0, &hellip;, N-1 \rbrace, a_i \in \lbrace 0, &hellip;, M-1 \rbrace $</p>
<p>分别用 $q_{i, a_i}$ 和 $w_{i, a_i}$ 表示码率选择为 $a^{th}_i$ 的 $i^{th}$ 分块的质量和相应的分块片段大小。</p>
<p>用 $p_i \in [0, 1]$ 表示 $i^{th}$ 块的被看到的可能性。</p>
<h2 id="顺序-abr-决策">顺序 ABR 决策</h2>
<p></p>
<h2 id="代理设计">代理设计</h2>
<h3 id="状态">状态</h3>
<p>对于 $i^{th}$ 维，输入状态包括原始的环境状态 $s_t$ ；</p>
<p>与之前维度的动作集合相关的信号： $u^{i}_{s_t} = \lbrace Th, C_i, p_{0:i-1}, q_{0:i-1}, b_t, p_i, S_i, Q_{t-1} \rbrace$</p>
<p>$Th$ ：表示过去 m 次下载一个段的平均吞吐量；</p>
<p>$C_i \in R^M$ ：表示 $i^{th}$ 个分块的可用块大小向量；</p>
<p>$p_{0:i-1}$ 和 $q_{0:i-1, a^{0:i-1}_{t}}$ 分别表示选中的码率集合和看到之前 $i-1$ 个分块的概率集；</p>
<p>$b_t$ 是缓冲区大小；</p>
<p>$p_i$ 是 $i^{th}$ 个分块被看到的可能性；</p>
<p>$S_i$ 是之前选择的 $i-1$ 个分块的块大小之和： $S_i = \sum^{i-1}_{h=0} C_{h, a^h_t}$ ；</p>
<p>$Q_{t-1}$ 记录了最后一个段中 $N$ 个分块的平均视频质量；</p>
<h3 id="动作">动作</h3>
<p>动作空间离散，代理输出定义为价值函数：$f(u^i_{s_t}, a^i_t)$</p>
<p>表示所选状态的价值 $a^i_t \in \lbrace 0, &hellip;, M-1 \rbrace$ 处于状态 $u_{s_t}^i$ .</p>
<h3 id="回报">回报</h3>
<p>回报定义为下列因素的加权和：</p>
<p>平均视频质量 $q^{avg}_t$，空间视频质量方差 $q^{s_v}_t$，时间视频质量方差 $q^{t_v}_t$ ，重缓冲时间 $T^r_t$</p>
<p>$$
q^{avg}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot q_{i, a_i}
$$</p>
<p>$$
q^{s_v}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot |q_{i, a_i} - q^{avg}_t|
$$</p>
<p>$$
q^{t_v}_t = |q^{avg}_{t-1} - q^{avg}_t|
$$</p>
<p>$$
T^r_t = max \lbrace T_t - b_{t-1}, 0 \rbrace
$$</p>
<p>$$
R_t = w_1 \cdot q^{avg}_t - w_2 \cdot q^{s_v}_t - w_3 \cdot q^{t_v}_t - w_4 \cdot T^r_t
$$</p>
<h2 id="训练方法">训练方法</h2>
<p>使用<code>DQN</code>作为基本的算法来学习动作-价值函数 $Q(s_t, a_t; \theta)$ ，其中 $\theta$ 作为参数，对应的贪心策略为 $\pi(s_t; \theta) = \underset{\theta}{argmax} Q(s_t, a_t; \theta)$ 。</p>
<p><code>DQN</code>网络的关键想法是更新最小化损失函数的方向上的参数：
$$
L(\theta) = E[y_t - Q(s_t, a_t; \theta)]
$$</p>
<p>$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \pi(s_{t+1}; {\theta}&rsquo;); {\theta}&rsquo;)
$$
${\theta}&rsquo;$ 表示固定且分离的目标网络的参数；</p>
<p>$r(\cdot)$ 是即时奖励函数，即上面公式 5 中的 $R_t$ ；</p>
<p>$\gamma \in [0, 1]$ 是折扣因子；</p>
<p>为了缓解过拟合，引入 <code>double-DQN</code> 的结构，所以公式 7 被重写为：
$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, {\pi}(s_{t+1}; \theta); {\theta}&rsquo;)
$$
利用公式 6 和公式 8 可以得出 $i^{th}$ 维的暂时损失函数：
$$
l^i_t = Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$
其中 $Q_{target}$ 满足：</p>
<p>$$
Q_{target} = r_t + {\gamma}_u \cdot Q(u^0_{s_{t+1}}, \pi(u^0_{s_{t+1}}; 0); {\theta}&rsquo;)
$$</p>
<p>${\gamma}_u$ 和 ${\gamma}_b$ 分别代表”Top MDP“和”Bottom MDP“的折扣因子，训练中设定 ${\gamma}_b = 1$ 。</p>
<p>观察公式 9 和公式 10 可以看出每维都有相同的目标函数，意味着无法区别每个独立维度的动作 $a^i_t$ 对 $r_t$ 的贡献。</p>
<p>为了克服限制，根据某个分块的动作 $a^i_t$ 与其观看概率成正比的先验知识，向 $l^i_t$ 添加一个额外的 $r^i_{extra}$ ：
$$
l^i_t = r^i_{extra} + Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$</p>
<p>$$
r^i_{extra} =
\begin{cases}
0, p_i &gt; P ;
\
-a^i_t, p_i \le P
\end{cases}
$$</p>
<p>通过设定一个观看概率的阈值 $P$ ，对观看概率低于 $P$ 但选择了高码率的分块施加 $-a^i_t$ 的奖励。</p>
<p>因此最终的平均损失可以形式化为：
$$
l^{avg}_t = \frac{1}{N} \sum^{N-1}_{i=0} l^i_t
$$
接着使用梯度下降法来更新模型，学习率设定为 $\alpha$：
$$
\theta \larr \theta + \alpha \triangledown l^{avg}_t
$$
同时，在训练阶段利用经验回放法来提高<code>360SRL</code>的泛化性。</p>
<p></p>
<p></p>
<h2 id="实现细节">实现细节</h2>
<p></p>
<p>特征从输入状态中通过特征提取网络提取出来。</p>
<p>初始的 4 个输入通过带有 128 个过滤器的 1 维卷积层被传递，4 个输入核心大小分别为 $1 \times m$ 、 $1 \times M$ 、 $1 \times N$ 、 $1 \times M$ ，后续这 4 个输入被喂给有 128 个神经元的全连接层；</p>
<p>随后特征映射被连接成一个张量，接着是具有 1024 个神经元和 256 个神经元的前向网络；</p>
<p>整个动作-价值网络的输出是 M 维的向量。</p>
<p>特征提取层和前向网络层都使用 <code>Leaky-ReLU</code>作为激活函数，最后是层归一化层。</p>
]]></description>
</item>
<item>
    <title>Note for Content Assisted Prediction</title>
    <link>http://localhost:1313/posts/papers/note-for-content-assisted-prediction/</link>
    <pubDate>Thu, 06 Jan 2022 15:17:33 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-content-assisted-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://www.researchgate.net/publication/333971523_Content_Assisted_Viewport_Prediction_for_Panoramic_Video_Streaming" target="_blank" rel="noopener noreffer">Content Assisted Viewport Prediction for Panoramic Video Streaming</a></p>
<p>Level：IEEE CVPR 2019 CV4ARVR</p>
<p>Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion</p>
<h2 id="主要工作">主要工作</h2>
<h3 id="基于轨迹预测">基于轨迹预测</h3>
<p>输入：历史窗口轨迹</p>
<p>模型：64 个神经元的单层 LSTM，在输入层后面加上一个额外的减法层进行点归一化，以及一个加法层来恢复输出之前的值；用 ADAM 进行优化，MAE 作为损失函数。</p>
<h3 id="跨用户热图">跨用户热图</h3>
<p>除了观看者自己的历史 FOV 轨迹之外，其他观看者对同一视频帧的观看方向也有启发性。</p>
<p></p>
<p>对视频的每一帧，首先收集用户的观看方向（坐标使用原始的来自三个方向的欧拉角表示，而非经纬度）。</p>
<p>接着将坐标投影到用经纬度表示的 180x360 像素的平面图上，对于图中的每个像素点，可以数出其被看到的次数；并对周围像素应用二维高斯光滑。</p>
<p>上面的过程可以为视频生成热图：</p>
<p></p>
<h3 id="视频帧的显著图">视频帧的显著图</h3>
<p>鉴于观看相同的全景视频时跨用户行为的共性，进一步假设是内容促使多个观众观看公共区域，因此提取出每个帧的显著图可能会表明用户的 RoI。</p>
<p>对特定的视频帧，应用经典的特征密集型方法——Ittykoch，它首先根据强度、边缘、颜色和方向将图像分解为多个特征通道，然后将它们组合成识别显著区域。</p>
<p>除了在静态视频帧上检测显著性之外，进一步进行背景减法来减少不太可能感兴趣的区域：应用基于高斯混合的背景/前景分割算法，高级思想是在连续帧之间临时过滤变化的像素点。</p>
<p>结合上面这两个过程可以为视频帧提取时间显著图。</p>
<p></p>
<h3 id="多模态融合">多模态融合</h3>
<p></p>
<p>使用包含 3 个 LSTM 分支的深度学习模型来融合上述的几种预测方式的结果。</p>
<p>基于轨迹的 LSTM（图中绿色分支）从历史窗口 $hw$ 中接受 $n$ 个坐标的输入，接着预测未来窗口 $pw$ 中的 $m$ 个坐标，用 $trj_y_{i}$ 表示；</p>
<p>基于热图的 LSTM（图中蓝色分支）将每个预测步骤对应的视频帧的热图作为输入，并在 $pw$ 中输出第 2 组 $m$ 个坐标的预测，用 $ht_y_{i}$ 表示：</p>
<p>对于每个热图，让其通过 3 个卷积层，每个卷积层后面都有一个最大池化层。然后，在此图像特征提取之后，应用展平步骤和 1 个密集层来回归坐标（经纬度表示）。</p>
<p>基于显著图的 LSTM 采用与热图相似的架构，将显著图作为输入，在 $pw$ 中输出第 3 组 $m$ 个坐标的预测，用 $sal_y_{i}$ 表示。</p>
<p>对热图和显著图的分支，应用 <code>TimeDistributed</code>层，以便其参数在预测步骤中保持一致。</p>
<p>最终在每个预测步骤连接 $trj_y$ ， $ht_y$，和 $sal_y$ ，并产生一个最终输出 $y$ 。</p>
<p>每个模型的损失函数采用 MAE，优化函数采用 ADAM。</p>
<p>为每个分支的输出以及最终的输出都检查损失，单独和联合地去调整其参数。</p>
<h2 id="评估">评估</h2>
<p>使用 2 折的交叉验证。</p>
<h3 id="超参数">超参数</h3>
<ol>
<li>$pw$ 的大小：0.1s，1.0s，2.0s；</li>
<li>$hw$ 的大小：0.05s，0.6s，1.0s；（分别与上面的 $pw$ 对应）</li>
<li>用于训练的用户数：[3, 10, 30]</li>
</ol>
<p></p>
<h3 id="结果与分析">结果与分析</h3>
<ol>
<li>所有模型的预测精度随着 $pw$ 的增长而下降，表明长期预测问题更难解决；</li>
<li>所有模型的精度预测误差几乎是纬度预测误差的二倍，可能由于运动区域在水平方向的翻倍；</li>
<li>线性回归模型只有在 $pw$ 很短的时候预测精确，随着 $pw$ 的增长，其预测精度会迅速下降；</li>
<li>基于 LSTM 的轨迹模型始终优于所有 $pw$ 的基线模型，但更多的训练观众无助于显着提高准确性。</li>
<li>跨用户的热图和显著图可以帮助长期的预测，可以提供合理的离线全视频 FOV 预测，并具有一致的性能（因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入），当 $pw$ 增长时，其预测精度超过了基于历史轨迹的模型；</li>
<li>结合 3 种模型之后，可以平衡来自历史轨迹、跨用户兴趣和内容显著性的输入，不论 $pw$ 长或短都能产生优化的预测结果；</li>
</ol>
<h3 id="例外情况">例外情况</h3>
<p></p>
<p>M3 在经度上的表现并不适用于上面图中标示的两个视频（Mega.Coaster 和 GTR.Drives.First.Ever）</p>
<p>原因分析：</p>
<p>这两个视频的共同特点是在驾驶路径的一侧具有高运动内容的驾驶内容，因此用户在观看这些视频时，大多数 FOV 始终以行驶轨迹为中心。因此用户不太可能改变其观看方向，这导致即使 $pw = 2.0s$ 时，单一基于轨迹的模型的预测精度也更高。相比之下，从对内容角度出发的分析无济于事，但可能会引入观众可能会忽略的变道，进而造成预测误差。</p>
]]></description>
</item>
<item>
    <title>Note for GPAC</title>
    <link>http://localhost:1313/posts/papers/note-for-gpac/</link>
    <pubDate>Thu, 30 Dec 2021 10:23:26 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-gpac/</guid>
    <description><![CDATA[<h2 id="dash-客户端自适应逻辑">Dash 客户端自适应逻辑</h2>
<ol>
<li><em>tile priority setup</em>：根据定义的规则对 tile 进行优先级排名。</li>
<li><em>rate allocation</em>：收集网络吞吐量信息和 tile 码率信息，使用确定的 tile 优先级排名为其分配码率，努力最大化视频质量。</li>
<li><em>rate adaption</em>：在播放过程中，执行码率自适应算法，基于播放速度、质量切换的次数、缓冲区占用情况等。</li>
</ol>
<h3 id="tile-priority-setup">tile priority setup</h3>
<ol>
<li>
<p>Dash 客户端加载带有 SRD 信息的 MPD 文件时，首先确定使用 SRD 描述的 tile 集合。</p>
</li>
<li>
<p>确定 tile 之间的编码依赖（尤其是使用 HEVC 编码的 tile 时）</p>
</li>
<li>
<p>为每个独立的 tile 向媒体渲染器请求一个视频对象，并向其通知 tile 的 SRD 信息。</p>
</li>
<li>
<p>渲染器根据需要的显示大小调整 SRD 信息之后，执行视频对象的最终布局。</p>
</li>
<li>
<p>一旦 tile 集合被确定，客户端向每个 tile 分配优先级。（每次码率自适应执行的时候都需要分配 tile 优先级）</p>
<p></p>
</li>
</ol>
<h3 id="rate-allocation">Rate allocation</h3>
<ol>
<li>首先需要估计可用带宽（tile 场景和非 tile 场景的估计不同）</li>
<li>在一个视频段播放过程中，客户端需要去下载多个段（并行-HTTP/2）</li>
<li>带宽可以在下载单个段或多个段的平均指标中估计出来。</li>
<li>一旦带宽估计完成，码率分配将 tile 根据其优先级进行分类。</li>
<li>一开始所有的 tile 都分配成最低的优先级对应的码率，然后从高到低依次增长优先级高的 tile 的码率。</li>
<li>一旦每个 tile 的码率分配完成，将为目标带宽等于所选比特率的每个 tile 调用常规速率自适应算法</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for MPC</title>
    <link>http://localhost:1313/posts/papers/note-for-mpc/</link>
    <pubDate>Thu, 23 Dec 2021 10:39:32 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-mpc/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/2785956.2787486" target="_blank" rel="noopener noreffer">A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP</a></p>
<p>Level：ACM SIGCOMM 15</p>
<p>Keywords：Model Predictive Control，ABR，DASH</p>
<h2 id="motivation">Motivation</h2>
<p>关于码率自适应的逻辑，现有的解决方案还没有形成清晰的、一致的意见。不同类型的方案之间优化的出发点并不相同，比如基于速率和基于缓冲区，而且没有广泛考虑各方面的因素并形成折中。</p>
<p>文章引入了控制论中的方法，将各方面的影响因素形式化为<em>随机优化控制</em>问题，利用<strong>模型预测控制 MPC</strong>将两种不同出发点的解决方案结合到一起，进而解决其最优化的问题。而仿真结果也证明，如果能运行一个最优化的 MPC 算法，并且预测误差很低，那么 MPC 方案可以优于传统的基于速率和基于缓冲区的策略。</p>
<h2 id="背景">背景</h2>
<ul>
<li>播放器端为 QoE 需要考虑的问题：
<ol>
<li>最小化冲缓冲事件发生的次数；</li>
<li>在吞吐量限制下尽可能传输码率较高的视频；</li>
<li>最小化播放器开始播放花费的时间（启动时间）；</li>
<li>保持播放过程平滑，尽可能避免大幅度的码率变化；</li>
</ol>
</li>
<li>这些目标相互冲突的原因：
<ol>
<li>最小化重缓冲次数和启动时间会导致只选择最低码率的视频；</li>
<li>尽可能选择高码率的视频会导致很多的重缓冲事件；</li>
<li>保持播放过程平滑可能会与最小的重缓冲次数与最大化的平均码率相冲突；</li>
</ol>
</li>
</ul>
<h2 id="控制论模型">控制论模型</h2>
<h3 id="视频推流模型">视频推流模型</h3>
<ol>
<li>
<p>参数形式化</p>
<ul>
<li>
<p>将视频建模成连续片段的集合，即：$V = \lbrace 1, 2, &hellip;, K \rbrace$，每个片段长为$L$秒；</p>
</li>
<li>
<p>每个片段以不同码率编码，$R$ 作为所有可用码率的集合；</p>
</li>
<li>
<p>播放器可以选择以码率$R_k \in R$ 下载第$k$块片段，$d_k(R_k)$ 表示以码率$R_k$编码的视频大小；</p>
<ul>
<li>对于恒定码率 CBR 的情况，$d_k(R_k) = L \times R_k$；</li>
<li>对于变化码率 VBR 的情况，$d_k \sim R_k$；</li>
</ul>
</li>
<li>
<p>选择的码率越高，用户感知到的质量越高：</p>
<p>$q(\cdot):R \rightarrow \R_+$ 是一个不减函数，是选择的码率 $R_k$ 到用户感知到的视频质量 $q(R_k)$ 的映射；</p>
</li>
<li>
<p>片段被下载到<em>回访缓冲</em>中，其中包含下载了的但还没看过的片段。</p>
</li>
<li>
<p>$B(t) \in [0, B_{max}]$ 表示 $t$ 时刻缓冲区的占用， $B_{max}$ 表示内容提供商的策略和播放器的存储限制；</p>
</li>
</ul>
</li>
<li>
<p>播放过程形式化</p>
<p>在 $t_k$ 时刻，视频播放器开始下载第 $k$ 个块，这个块的下载时间可以计算为： $d_k(R_k) / C_k$； $C_k$ 表示下载过程中经历的平均下载速度；</p>
<p>一旦第 $k$ 个块下载完毕，播放器等待 $\Delta t_k$ 时间并在 $t_{k+1}$ 时刻下载下一个块 $k+1$ ；</p>
<p>假设等待时间 $\Delta t_k$ 很短并且不会导致重缓冲事件，用 $C_t$ 表示 $t$ 时刻的网络吞吐量：
$$
t_{k+1} = t_k + \frac{d_k(R_k)}{C_k} + \Delta t_k
$$</p>
<p>$$
C_k = \frac{1}{t_{k+1} - t_k - \Delta t_k} \int_{t_k}^{t_{k+1} - \Delta t_k} C_t dt
$$</p>
<p>$B(t)$ 的变化取决于下载的块和播放的块的数量：</p>
<p>在第 $k$ 个块下载完毕之后缓冲区占用增长 $L$ 秒；用户观看一个块之后缓冲区占用减少 $L$ 秒；</p>
<p>$B_k = B(t_k)$ 表示播放器开始下载第 $k$ 个块时的缓冲区占用；</p>
<p>缓冲区占用的动态变化可以表示为：
$$
B_{k+1} = \big( (B_k - \frac{d_k(R_k)}{C_k})_+ + L - \Delta t_k \big)_+
$$
其中 $(x)_+ = max\lbrace x, 0 \rbrace $ 确保其非负；</p>
<p>如果 $B_k &lt; d_k(R_k) / C_k$ ，表示缓冲区在播放器还在下载第 $k$ 个块时变空，而这会导致重缓冲事件；</p>
<p></p>
<p>等待时间 $\Delta t_k$ 的确定也称为<em>块调度</em>问题，本文中假设播放器在第 $k$ 个块下载完毕之后尽可能快地去下载第 $k+1$ 个块（除了缓冲区满了的情况，播放器等待缓冲区中的块被消耗之后再下载新的块）：
$$
\Delta t_k = \Big( \big( B_k - \frac{d_k(R_k)}{C_k} \big)_+ + L - B_max \Big)_+
$$</p>
</li>
</ol>
<h3 id="qoe-最大化问题">QoE 最大化问题</h3>
<p>QoE 的组成部分：</p>
<ol>
<li>
<p>平均视频质量：在所有块中每个块平均的质量，计算为：
$$
\frac{1}{K} \sum^K_{k=1} q(B_k)
$$</p>
</li>
<li>
<p>平均质量变化：相邻块之间质量变化的平均值，计算为：
$$
\frac{1}{K-1} \sum^{K-1}_{k=1} | q(R_{k+1}) - q(R_k) |
$$</p>
</li>
<li>
<p>重缓冲总计时间：对每个块而言，当轮到其被消耗时但下载块的过程还没完成即出现了重缓冲，总时间计算为：
$$
\sum^K_{k=1} (\frac{d_k(R_k)}{C_k} - B_k)_+
$$</p>
</li>
<li>
<p>启动延迟 $T_s$ ，假设 $T_s \ll B_{max}$ 。</p>
</li>
</ol>
<p>对不同用户而言，上述 4 种因素的重要程度不同。使用上述分量的加权，定义视频块 $1$ 到 $K$ 的 QoE：
$$
QoE^K_1 = \sum^K_{k=1} q(R_k) - \lambda \sum^K_{k=1} | q(R_{k+1}) - q(R_k) | - \mu \sum^K_{k=1} (\frac{d_k(R_k)}{C_k} - B_k)_+ - \mu_s T_s,\
\lambda, \mu, \mu_s \nless 0
$$
相对较小的 $\lambda$ 表示用户不太关心视频质量变化； $\lambda$ 越大表明越需要使视频质量变得光滑。</p>
<p>相对较大的 $\mu$ 表示用户很在意重缓冲；</p>
<p>在这里文章倾向于启动延迟很低，所以采用大 $\mu_s$ ；</p>
<p>QoE 的最大化：</p>
<p>输入：吞吐量迹 ${C_t, t \in [t_1, t_{K+1}]}$</p>
<p>输出：码率选择 $R_1, &hellip;, R_K$；启动时间 $T_s$ ；</p>
<p>需要注意：当最大化的决策发生在播放过程中时，启动时间便不再存在；</p>
<p></p>
<h3 id="算法">算法</h3>
<p>上图中的 QoE 最大化问题是一种随机优化控制问题，随机性源自可获得的吞吐量 $C_t$ 。</p>
<p>$t_k$ 时刻播放器选择码率 $R_k$ ，只有过去的吞吐量 $\lbrace C_t, t \le t_k \rbrace$ 可知，未来的值 ${C_t, t &gt; t_k}$ 未知。</p>
<p>但是，<em>吞吐量预测器</em>可以用于获取对吞吐量的预测，定义其为 $\lbrace \hat{C_t}, t &gt; t_k \rbrace$ 。</p>
<p>基于这样的预测和缓冲区的信息（精确可知），<em>码率选择器</em>对下个块 $k$ 的码率选择可以表示为：
$$
R_k = f \big( B_k, \lbrace \hat{C_t}, t &gt; t_k \rbrace, \lbrace R_i, i &lt; k \rbrace \big)
$$
文章只关注码率自适应算法，假设已经得到了预测值，并根据预期预测误差对其进行了表征，即：</p>
<p>我们着重于 $f(\cdot)$ 的设计以及预测误差对比较控制算法性能的影响。</p>
<p>现有的两类自适应算法：基于速率和基于缓冲区，分别可以表示为：
$$
R_k = f \big( \lbrace \hat{C_t}, t &gt; t_k \rbrace, \lbrace R_i, i &lt; k \rbrace \big)
$$</p>
<p>$$
R_k = f(B_k, \lbrace R_i, i &lt; k \rbrace)
$$</p>
<p>前者只基于吞吐量的预测结果而不管缓冲区状况；后者只基于缓冲区而不管未来的吞吐量可能状况；</p>
<p>这两种方法在原则上都只是次优的，理想情况下我们想要同时考虑缓冲区占用和吞吐量预测结果。</p>
<p></p>
<h2 id="mpc-for-optimal-bitrate-adaptation">MPC for Optimal Bitrate Adaptation</h2>
<h3 id="why-mpc">Why MPC</h3>
<p>MPC 天然适合码率自适应问题。</p>
<ul>
<li>
<p><strong>Strawman solutions</strong></p>
<p>码率自适应问题本质是<em>随机控制优化</em>问题，就这一点而言，有两个知名控制算法：</p>
<ol>
<li>Proportional-integral-derivation(PID) control.</li>
<li>Markov Decision Process(MDP) based control.</li>
</ol>
<p>PID 相较 MDP 而言计算起来更加简单，只能用于使系统稳定，不能显式地优化 QoE 目标；此外 PID 被设计用于有连续的时间和连续的状态空间的问题中，用于当前这种高度离散化的问题中会导致性能亏损和不稳定。</p>
<p>应用 MDP 的话可以将吞吐量和缓冲区状态形式化为马氏过程，然后使用诸如值迭代和策略迭代等标准算法求出最优解。</p>
<p>（然而，这有一个很强的假设，即吞吐量动态遵循马尔可夫过程，不清楚这在实践中是否成立。我们将 MDP 的潜在用途和吞吐量动态分析作为未来的工作。）</p>
</li>
<li>
<p><strong>Case for MPC</strong></p>
<p>理想情况下，如果给出未来吞吐量的完美数据，那么启动时间 $T_s$ 和最优码率选择 $R_1, &hellip; R_K$ 可以一下子就计算出来；</p>
<p>实际情况中，虽然不能得到未来吞吐量的完美预测，但是我们可以假设吞吐量在较短的时间段 $[t_k, t_{k+N}]$ 内不会剧烈变化。</p>
<p>基于此，可以使用当前视界中的预测来应用第 1 个码率 $R_k$ ，之后将视界向前移动到 $[t_{k+1}, t_{k+N+1}]$ 。</p>
<p>而这种方案就称为 MPC。MPC 的一般好处在于，MPC 可以利用预测在约束条件下在线优化动态系统中的复杂控制目标。</p>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for TBRA</title>
    <link>http://localhost:1313/posts/papers/note-for-tbra/</link>
    <pubDate>Tue, 21 Dec 2021 10:11:23 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-tbra/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3474085.3475590" target="_blank" rel="noopener noreffer">TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming</a></p>
<p>Level：ACM MM 21</p>
<p>Keywords：Adaptive tiling and bitrate，Mobile streaming</p>
<h2 id="创新点">创新点</h2>
<h3 id="背景">背景</h3>
<p>现有的固定的 tile 划分方式严重依赖 viewport 预测的精度，然而 viewport 预测的准确率往往变化极大，这导致基于 tile 的策略实际效果并不一定能实现其设计初衷：保证 QoE 的同时减少带宽浪费。</p>
<p>考虑同样的 viewport 预测结果与不同的 tile 划分方式组合的结果：</p>
<p></p>
<p>从上图可以看到：</p>
<ul>
<li>如果采用$6 \times 6$的分块方式，就会浪费 26，32 两个 tile 的带宽，同时 15，16，17 作为本应在实际 viewport 中的 tile 并没有分配最高的优先级去请求。</li>
<li>如果采用$5 \times 5$的分块方式，即使预测的结果与实际的 viewport 有所出入，但是得益于 tile 分块较大，所有应该被请求的 tile 都得到了最高的优先级，用户的 QoE 得到了保证。</li>
</ul>
<p>另一方面，基于 tile 的方式带来了额外的编解码开销（可以看这一篇论文：<a href="https://ayamir.github.io/2021/12/note-for-optile/" target="_blank" rel="noopener noreffer">note-for-optile</a>），而这样的性能需求对于移动设备而言是不可忽略的。</p>
<h3 id="创新">创新</h3>
<p>除了考虑常见的因素如带宽波动和缓冲区占用之外，提出同时自适应分块策略和码率分配以应对变化的 viewport 预测性能和受限的移动设备的解码能力。</p>
<h3 id="论文组织">论文组织</h3>
<ol>
<li>首先使用现实世界的轨迹分析了典型的 viewport 预测算法并确定了其性能的不确定性。</li>
<li>接着讨论了不同的分块策略在 tile 选择和解码效率上的影响。</li>
<li>自适应的分块策略可以适应 viewport 预测的错误，并能保证 tile 选择的质量。</li>
<li>为解码时间建构了分析模型，可以在给定受限的计算资源时用于选择恰当的分块策略和码率。</li>
<li>形式化了优化模型，讨论了自适应算法的细节。</li>
<li>评估证明了方案的优越性。</li>
</ol>
<h2 id="motivation">Motivation</h2>
<h3 id="分块策略对-tile-选择的影响">分块策略对 tile 选择的影响</h3>
<p>实现 4 种轻量的 viewport 预测算法：线性回归 LR、岭回归 RR、支持向量回归、长短期记忆 LSTM。</p>
<p>设置历史窗口大小为 2s，预测窗口大小为 1s；viewport 的宽度和高度分别为 100°和 90°。</p>
<p>默认的分块策略为$6 \times 6$；头部移动数据集来自<a href="https://dl.acm.org/doi/10.1145/3204949.3208139" target="_blank" rel="noopener noreffer">公开数据集</a>。</p>
<h4 id="viewport-预测的不准确性">viewport 预测的不准确性</h4>
<p>研究表明，用户的头部运动主要发生在水平方向而较少发生在垂直方向，所以只分析水平方向的预测。</p>
<p>实际的商业移动终端只有有限的传感和处理能力，并不能支持高频的 viewport 预测采样。</p>
<p>视频内容的不同类型会显著影响预测的精度，基于录像环境（室内或户外）和相机的运动状态分类。</p>
<ul>
<li>
<p>改变采样频率会直接影响 viewport 预测的精度，频率越低，精度越低。</p>
</li>
<li>
<p>相机运动的 viewport 预测错误率比相机静止的明显更高。</p>
</li>
</ul>
<h4 id="通过分块容忍预测错误">通过分块容忍预测错误</h4>
<p>因为不管 tile 的哪个部分被包含在预测的 viewport 中，只要包含一部分就会请求整个 tile，所以增大每个 tile 的尺寸能吸收预测错误。</p>
<p>实验验证：</p>
<p>设定从$4 \times 4$到$10 \times 10$的分块方式，使用不同的预测误差来检查分块设定可以容纳的最大预测误差，同时保持 tile 选择结果的相同质量。</p>
<p>用$F_1$分数来表示 tile 选择的质量：$F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall}$。</p>
<p>实验结果表明更大的 tile 尺寸更能容忍预测错误。</p>
<h3 id="分块策略对解码复杂性的影响">分块策略对解码复杂性的影响</h3>
<p>虽然当前的移动设备硬件性能发展迅速，但是实时的高码率高分辨率全景视频的解码任务还是充满挑战。</p>
<p>分块对于编码的影响：</p>
<ul>
<li>tile 越小，帧内和帧间内容的相关区域就越小，编码效率越低。</li>
</ul>
<p>直接影响解码复杂性的因素：</p>
<ul>
<li>tile 的数量。</li>
<li>视频的分辨率。</li>
<li>用于解码的资源。</li>
</ul>
<p>固定其中 1 个因素改变另外 2 个因素来检查其对解码的影响：</p>
<p></p>
<p>根据对图的观察可以得出这 3 个因素在经验上是相互独立的，因为这三幅图之中的图像几乎相同。</p>
<p>分别用$F_n(x), F_r(x), F_c(x)$表示 tile 数量、分辨率、线程数量为$x$时，解码时间与基线时间的比值。</p>
<p>将这 3 个比值作为 3 个乘子建立分析模型：
$$
D = D_0 \cdot F_n(x_1) \cdot F_r(x_2) \cdot F_c(x_3)
$$
上式表示计算整体的解码时间，其中 tile 数量为$x_1$、分辨率为$x_2$、线程数量为$x_3$；$D_0$时解码的基线时间。</p>
<p>这个模型将用于帮助做出分块和码率适应的决策。</p>
<p>注意在实际情况中，可供使用的计算资源（线程数）是受限的，需要根据设备当前可用的计算资源来分配。</p>
<h2 id="tbra-的设计">TBRA 的设计</h2>
<ul>
<li>$S = \lbrace s_1, s_2, &hellip; \rbrace$ 表示 360°视频分块方式的集合；</li>
<li>对于分块方式$s_i$，$|s_i|$ 表示这种方案中 tile 的数量；</li>
<li>当 $i &lt; j$ 时，假设 $|s_i| &lt; |s_j|$；</li>
<li>对于分块方式$s$， $b_{i, j}$ 表示第 $i$ 块的 tile $j$，$i \le 块的数量, j \le |s|$；</li>
<li>目标是确定分块方式$s$，并为每个 tile 确定其码率$b_{i, j}$；</li>
</ul>
<h3 id="分块自适应">分块自适应</h3>
<h4 id="自适应的概念">自适应的概念</h4>
<p>分块尺寸大小会导致 viewport 容错率和传输效率的变化。</p>
<ul>
<li>分块尺寸小，极端情况下每个像素点作为一个 tile，viewport 容错率最小，但是传输效率达到 100%；</li>
<li>分块尺寸大，极端情况下整个视频帧作为一个 tile，viewport 容错率最大，但是传输效率最小；</li>
</ul>
<p>优化的目标就是在这两种极端条件中找到折中的最优解。</p>
<h4 id="分块选择">分块选择</h4>
<p>以$\overline{r_d}, d \in \lbrace left, right, up, down \rbrace$为半径扩大预测区域；$e_d$表示过去 n 秒中方向 $d$ 的预测错误平均值；
$$
\overline{r_d} = (1-\alpha) \cdot \overline{r_d} + \alpha \cdot e_d
$$
预测区域的扩展被进一步用于 tile 选择，受过去预测精度的动态影响。</p>
<p>下一步检查不同分块方式，进而找到 QoE 和传输效率之间的折中。</p>
<p>对于每个分块方式，比较基于扩展的预测区域的 tile 选择的质量。使用 2 个比值作为 QoE 和传输效率的度量：
$$
Miss\ Ratio = \frac{of\ missed\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p>$$
Waste\ ratio = \frac{of\ unnecessary\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p></p>
<p>这 2 个比值的 tradeoff 可以在上图中清晰地看出。</p>
<p>使用分块方式对应的惩罚$Tiling\ i_{penalty}$来评估其性能：
$$
Tiling\ i_{penalty} = \beta \cdot Miss\ Ratio + |1/cos(\phi_i)| \cdot Waste\ Ratio
$$
$\phi_i$ 是 viewport $i$ 的中心纬度坐标，它表明随着 viewport 的垂直移动，浪费率的权重会发生变化。（因为投影方式是 ERP）</p>
<p>检查完所有的方式之后，最终选择惩罚最小的分块方式。</p>
<h3 id="码率自适应">码率自适应</h3>
<h4 id="视频质量">视频质量</h4>
<p>$w_{i, j}$表示在第 $i$ 个视频块播放时，tile $j$ 的权重；在当前方案中 $w_{i, j} = 0\ or\ 1$ 取决于 tile 是否在预测的 viewport 中。</p>
<p>$q(b_{i, j})$ 是 tile 比特率选择 $b_{i, j}$ 与用户实际感知到的质量之间的非递减映射函数。</p>
<p>第 $i$ 个视频块的质量等级可以定义为：</p>
<p>$$
Q^{(1)}_i = \sum^n_{j=1} w_{i, j} q(b_{i, j})
$$</p>
<p>使用最新研究的<a href="https://ieeexplore.ieee.org/document/8979422/citations?tabFilter=papers" target="_blank" rel="noopener noreffer">主观视频质量模型</a>：
$$
subjective\ PSNR:\ q_i = PSNR_i \cdot [M(v_i)]^{\gamma} [R(v_i)]^{\delta}
$$
$M(v_i)$ 是检测阈值；$R(v_i)$ 是视网膜滑移率；$v_i$ 是第播放 $i$ 个视频块时 viewport 的移动速度；$\gamma = 0.172, \delta = -0.267$</p>
<h4 id="质量变化">质量变化</h4>
<p>连续视频块之间的强烈质量变化会损害 QoE，定义质量变化作为响铃两个视频块之间质量的变化：
$$
Q^{(2)}_i = |Q^{(1)}_1 - Q^{(1)}_{i-1}|,\ i \in [2, m]
$$</p>
<h4 id="重缓冲时间">重缓冲时间</h4>
<p>参数设置：</p>
<ul>
<li>$C_i$ 表示下载视频块 $i$ 的预计吞吐量；</li>
<li>$B_i$ 表示客户端开始下载视频块 $i$ 时缓冲区的占用率；</li>
<li>$B_{default}$ 表示在启动阶段默认的缓冲区填充等级，记 $B_{default} = B_1$；</li>
<li>下载第 $i$ 个视频块需要时间 $\sum^n_{j=1} b_{i, j} / C_i$ ；</li>
<li>每个视频块的长度为 $L$ ；</li>
</ul>
<p>缓冲区的状态应该在每次视频块被下载的时候都得到更新，则下一个视频块 $i+1$ 的缓冲区占用情况可以计算为：
$$
B_{i+1} = max\lbrace B_1 - \sum^n_{j=1} b_{i, j} / C_i,\ 0\rbrace + L
$$
下载第 $i$ 个视频块时的重缓冲时间可以计算为：</p>
<p>$$
Q^{(3)}_i = max \lbrace \sum^n_{j=1} b_{i, j} / C_i - B_i,\ 0 \rbrace + t_{miss}
$$</p>
<p>第一部分是下载时间过长且缓冲区耗尽，视频无法播放情况下的重新缓冲时间；</p>
<p>第二部分 $t_{miss}$ 表示下载缺失的 tile 所花费的时间（在视频块播放过程中被看到但是之前没有分配码率的 tile）。</p>
<h4 id="优化目标">优化目标</h4>
<p>第 $i$ 个视频块的整体优化目标可以定义为前述 3 个指标的加权和：
$$
Q_i = pQ^{(1)}_i - qQ^{(2)}_i - rQ^{(3)}_i
$$
各个系数的符号分配表示：最大化视频质量、最小化块间质量变化、最小化重缓冲时间。</p>
<p>传统意义上使用所有视频块的平均 QoE 作为优化对象，但实际上很难获得从块 $1$ 到块 $m$ 的整个视界的完美的未来信息。</p>
<p>为了处理预测长期吞吐量和用户行为的难度，采用<a href="https://dl.acm.org/doi/10.1145/2785956.2787486" target="_blank" rel="noopener noreffer">基于 MPC 的框架</a>，在有限的范围内优化多个视频块的 QoE，最终的目标函数可以形式化为：
$$
\underset{b_{i, j}, i \in [t, t+k-1], j \in [1, n]}{max} \sum^{t+k-1}_{i=t} Q_i
$$
因为短期内的 viewport 预测性能和网络状况可以很容易得到，QoE 优化可以通过使用窗口 $[t, t+k-1]$ 内的预测信息；</p>
<p>接着将视界向前移动到 $[t+1, t+k]$ ，更新新的优化窗口的信息，为下一个视频块执行 QoE 优化，直到最后一个窗口。</p>
<p>使用基于 MPC 的公式的优点：由于受限的问题规模，每个优化问题的实例都是实际可解的。</p>
<h4 id="高效求解">高效求解</h4>
<p>提出的公式天然适合在线求解，得益于短窗口的实例问题规模很小，QoE 优化可以通过详尽搜索定期解决。</p>
<p>但是因为优化过程需要高频调用，所以对于大的搜索空间还是充满挑战。</p>
<p>为了支持实时优化，需要对搜索空间进行高效剪枝，确定几点约束：</p>
<ul>
<li>
<p>解码时间需要被约束；</p>
<p>解码时间应该短于回放长度。</p>
<p>给定移动设备上可用的计算资源，可以得到支持的最大解码线程数。</p>
<p>基于解码时间的分析模型，由于解码复杂度和分辨率的单调性，可以找到设备能够限定时间内解码的最大质量水平，这会将码率选择限制在有界搜索空间内。</p>
</li>
<li>
<p>码率选择应该考虑吞吐量的限制：$\sum^n_{j=1} b_{i, j} \le LC_i$ ；</p>
<p>不会主动耗尽缓冲区，无需让其处理吞吐量的波动。</p>
</li>
<li>
<p>码率选择应该考虑 tile 的分类；</p>
<p>tile 的码率不应该低于同一个视频块中更低权重 tile 的码率： $b_{i, j} \ge b_{i, j&rsquo;}, \forall w_{i, j} &gt; w_{i, j&rsquo;}$ 。</p>
</li>
<li>
<p>属于相同类别的 tile 比特率选择应该是同一个等级；</p>
<p>这使码率自适应在 tile 类的级别上执行而非单个 tile 的级别，大大减小了搜索空间的规模。</p>
</li>
<li>
<p>当优化窗口中的吞吐量和用户行为保持稳定时，同一个窗口中的 tile 应该有相同的结果。</p>
</li>
</ul>
<h3 id="tbra-workflow">TBRA workflow</h3>
<p></p>
<p>这样的方式需要在服务端存储大量的按照不同分块方式划分的不同码率版本的视频块，这一点可以进一步研究。</p>
<p>但是对于移动终端设备而言，这样的解决方案只引入了可以忽略不计的开销。</p>
<p>观察到 tile 自适应问题具有全局最优通常就是局部最优的特点，因此可以大大减少计算量。</p>
<p>基于 MPC 的优化 workflow 还可以有效地解决码率自适应问题。</p>
]]></description>
</item>
<item>
    <title>Note for Content Motion Viewport Prediction</title>
    <link>http://localhost:1313/posts/papers/note-for-content-motion-viewport-prediction/</link>
    <pubDate>Mon, 20 Dec 2021 10:47:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-content-motion-viewport-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/abs/10.1145/3328914" target="_blank" rel="noopener noreffer">Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking</a></p>
<p>Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019</p>
<p>Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model</p>
<h2 id="workflow">Workflow</h2>
<ul>
<li>Tracking：VR motion 追踪算法：应用了高斯混合模型来检测物体的运动。</li>
<li>Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户 viewport 来自动更正潜在的预测错误。</li>
<li>Update：viewport 动态更新算法：动态调整预测的 viewport 大小去覆盖感兴趣的潜在 viewport，同时尽可能保证最低的带宽消耗。</li>
<li>Evaluation：经验用户/视频评估：构建 VR viewport 预测方法原型，使用经验 360°视频和代表性的头部移动数据集评估。</li>
</ul>
<h2 id="全景直播推流的预备知识">全景直播推流的预备知识</h2>
<h3 id="vr-推流直播">VR 推流直播</h3>
<p></p>
<p>相比于传统的 2D 视频推流的特别之处：</p>
<ul>
<li>VR 系统是交互式的，viewport 的选择权在客户端；</li>
<li>呈现给用户的最终视图是整个视频的一部分；</li>
</ul>
<h3 id="用户头部移动的模式">用户头部移动的模式</h3>
<p>在大量的 360°视频观看过程中，用户主要的头部移动模式有 4 种，使用$i-j\ move$来表示；</p>
<p>其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。</p>
<ul>
<li>$1-1\ move$：单个物体以单一方向移动；</li>
<li>$1-n\ move$：单个物体以多个方向移动；</li>
<li>$m-n\ move$：多个物体以多个方向移动；</li>
<li>$Arbitrary\ move$：用户不跟随任何感兴趣的物体而移动，viewport 切换随机；</li>
</ul>
<p></p>
<p>现有的直播 VR 推流中的 viewport 预测方法是基于速度的方式，这种方式只对$1-1\ move$这一种模式有效。</p>
<p>本方案的目标是提出对 4 种模式都有效的预测策略。</p>
<h2 id="系统架构">系统架构</h2>
<p></p>
<h3 id="理论创新">理论创新</h3>
<ul>
<li>
<p>核心功能模块：</p>
<ol>
<li>
<p>motion detection：区分运动物体与静止的背景。</p>
</li>
<li>
<p>feature selection：选择代表性的特征并对运动物体做追踪。</p>
<p>这两个模块使系统能识别用户可能感兴趣的 viewport。</p>
</li>
</ol>
</li>
<li>
<p>使用贝叶斯方法分析用户观看行为并形式化用户的兴趣模型。</p>
<ol>
<li>
<p>使用错误恢复机制来使当预测错误被检测到时的预测 viewport 去适应实际的 viewport，尽管不能消除预测错误但是能避免在此基础上进一步的预测错误。</p>
</li>
<li>
<p>使用动态 viewport 更新算法来产生大小可变的 viewport，通过同时考虑跟踪到的 viewport 轨迹和用户当前的速度（矢量）。</p>
<p>这样，即使用户的运动模式很复杂也能有更高的概率去覆盖潜在的视图。</p>
</li>
</ol>
</li>
</ul>
<h3 id="具体实施">具体实施</h3>
<ul>
<li>
<p>虽然提出的运动追踪和错误处理机制是计算密集型的任务，但是这些组件都部署在 video packager 中，运行在服务端。</p>
</li>
<li>
<p>将生成 VR 视图的工作负载移动到服务端，进一步减少了客户端的计算开销以及网络开销。</p>
</li>
</ul>
<h2 id="形式化">形式化</h2>
<h3 id="基于运动轨迹的-viewport-预测">基于运动轨迹的 viewport 预测</h3>
<p>使用<a href="https://ieeexplore.ieee.org/document/1333992" target="_blank" rel="noopener noreffer">GMM</a>完成运动检测，使用<a href="https://ieeexplore.ieee.org/document/323794" target="_blank" rel="noopener noreffer">Shi-Tomasi algorithm</a>解决运动轨迹跟踪问题。</p>
<p></p>
<ol>
<li>
<p>运动检测</p>
<p>GMM 前景提取</p>
</li>
<li>
<p>特征选取与过滤</p>
<p>采用 Shi-Tomasi algorithm 从视频中检测代表性的特征，直接检测得到的代表性特征数量较多而难以追踪。</p>
<p>采用两种过滤的方法来减少要追踪的特征数量。</p>
<ul>
<li>
<p>比较当前帧和前一帧的特征，只保留其共有的部分。</p>
</li>
<li>
<p>采用第 1 步中运动检测的方式，只保留运动的部分。</p>
</li>
</ul>
</li>
<li>
<p>viewport 生成</p>
<p>经过选择和过滤之后的特征通常分布在不能被单一用户视图所覆盖的广阔区域中。</p>
<p>在整个 360°视频中可能存在多个运动的物体，即$m-n\ move$。</p>
<p>提出一种系统的方式来产生用户最可能跟随观看的 viewport。</p>
<p>直觉是用户更可能将大部分注意力放在两种类型的物体上：</p>
<ul>
<li>离用户更近的物体。</li>
<li>就物理形状而言更“重要”的物体。</li>
</ul>
<p>这两种类型的物体大多包含最密集和最大量的特征，因此通过所有特征的重心来计算预测用户视图的中心。</p>
<p>对于剩余的特征列表：$\vec{F} = [f_1, f_2, f_3, &hellip;, f_k]$，其中$f_i(i = 1 &hellip; k)$表示特征$f_i = &lt;f^{(x)}_i, f^{(y)}_i&gt;$的像素点坐标，则预测出的 viewport 中心坐标可以计算出来：
$$
l_x = \frac{1}{k} \sum^k_{i=1} f^{(x)}_i;\ l_y = \frac{1}{k} \sum^k_{i=1} f^{(y)}_i.
$$
考虑到即使预测的 viewport 中包含用户观看的物体，预测得到的 viewport 也可能会与实际的 viewport 存在差异。</p>
<p>所以预测的 viewport 可能比实际的 viewport 要大，所以使用缩放因子$S_c$来产生预测的 viewport。</p>
<p>给出用户 viewport 的大小$S_{user}$，预测的 viewport 可以通过$S_{pre} = S_c \cdot S_{user}$计算出来。</p>
</li>
</ol>
<h3 id="基于用户反馈的错误恢复">基于用户反馈的错误恢复</h3>
<p>video packager 可以通过 HMD 和 web 服务器通过反向路径从用户处检索用户实际视图的反馈信息。</p>
<p>基于反馈的错误恢复机制在以下两种场景中表现良好：</p>
<ol>
<li>
<p>没有运动的物体</p>
<p>如果没有检测到运动的物体，则用户很可能是在观看静止的物体，这会导致基于运动目标的 viewport 预测失败。</p>
<p>在这种场景中，可以认为视频内容已经不再是决定用户 viewport 的因素，而只取决于用户自身的行为。</p>
<p>因此采用基于速度的方式来预测 viewport。（这样的决策可以在运动检测模块没有检测到运动物体时就做出）</p>
<p>一旦从反馈路径上得到用户信息，可以产生用户 viewport 位置向量：$\vec{L} = [l_1, l_2, l_3, &hellip;, l_M]$，其中$l_i$表示第$i$个帧中用户 viewport 的位置，$M$表示视频播放缓冲区中的帧数。那么可以计算 viewport 速度：
$$
\vec{V} = \frac{\vec{(l_2 - l_1)} + \vec{(l_3 - l_2)} &hellip;.(l_M - l_{M-1})}{M-1} = \frac{(\vec{l_M - l_1})}{M-1}
$$
下一帧的预测位置$L_{M=1}$也可以计算出来：
$$
l_{M+1} = l_M + \vec{V}
$$</p>
</li>
<li>
<p>预测视图与实际视图的不匹配</p>
<p>一旦运动追踪策略检测到用户实际的视图和预测的视图不同，就会触发恢复机制去追踪用户实际在看着的物体。</p>
<p>可以使用运动追踪方式确定用户实际观察的物体的速度。</p>
<p>给出前一帧匹配的特征$\vec{FA} = [fA_1, fA_2, fA_3, &hellip;, fA_p]$和当前帧的特征$\vec{FB} = [fB_1, fB_2, fB_3, &hellip;, fB_p]$，可以计算出速度：
$$
V_x = \frac{1}{p} (\sum^p_{i=1} fB^{(x)}_i - \sum^p_{i=1}fA^{(x)}_i),\
V_y = \frac{1}{p} (\sum^p_{i=1} fB^{(y)}_i - \sum^p_{i=1}fA^{(y)}_i),
$$
假设预测的 viewpoint 是$(l_x, l_y)$，修改之后的 viewpoint 是$(l_x + V_x,\ l_y + V_y)$。</p>
</li>
</ol>
<h3 id="动态-viewport-更新">动态 viewport 更新</h3>
<p>前述的错误恢复机制发生在 viewport 预测错误出现之后，任务是避免未来更多的错误。</p>
<p>动态的 viewport 更新则努力避免 viewport 预测错误。</p>
<p>关键思想是扩大预测的 viewport 大小，以高概率去覆盖$m-n\ move$和$arbitrary\ move$下所有潜在的运动目标；更重要的是动态调整视图的大小去获得更高效的带宽利用率。</p>
<ul>
<li>
<p>对于一个 360°全景视频，将 360°的帧均分为$N = n \times n$个网格，每个网格看作是一个 tile，预测的 viewport 即为$N$个 tile 的子集。</p>
</li>
<li>
<p>使用贝叶斯方法分析用户的观看行为，每个 tile 分配一个独立的贝叶斯模型，所以每个 tile 可以独立更新。</p>
</li>
<li>
<p>设$X$表示用户 viewport，$Y$表示静态内容，$Z$表示运动物体。</p>
</li>
<li>
<p>未来的用户 viewport 可以以条件概率计算为$P(X|Y,\ Z)$，$Y$与$Z$相互独立。</p>
</li>
<li>
<p>用户的 viewport 可以通过反馈信息得出$P(X)$；用户观看静态特征可以表示为$P(X|Y)$；用户观看动态特征可以表示为$P(X|Z)$。</p>
</li>
<li>
<p>$P(X|Y, Z)$可以计算为：
$$
P(X|Y, Z) = \frac{P(Y|X) \cdot P(Z|X) \cdot P(X)}{P(Y, Z)}
$$</p>
</li>
<li>
<p>只要用户开始观看，对于 tile $T_i$，就能得到其先验概率$P(Y_i|X_i)$和$P(Z_i|X_i)$，进而根据贝叶斯模型计算出$P(X|Y, Z)$。</p>
</li>
</ul>
<p>为每个 tile 定义两种属性：</p>
<ol>
<li>当前状态：表示此 tile 是否属于预测的 viewport（属于标记为$PREDICTED$，不属于标记为$NONPREDICTED$）。</li>
<li>生存期：表示此 tile 会在 view port 中存在多长时间（例如定义 3 种等级：$ZERO$，$MEDIUM$，$HIGH$，实际的定义划分可以根据具体的用户和视频设定）。</li>
</ol>
<h2 id="预测步骤">预测步骤</h2>
<p>按照形式化中提出的 3 步，分为系统初始化、帧级别的更新、缓冲区级别的更新。</p>
<ol>
<li>
<p>系统初始化</p>
<p>初始化阶段中，view 更新算法将所有的$N$个 tile 标注为$PREDICTED$，并将生存期设置为$MEDIUM$，即系统向用户发送完整的一帧作为自举。</p>
<p>这样设定的原因在于：当用户第一次启动视频会话时，允许“环视”类型的移动，这可能会覆盖 360°帧的任意 viewport。</p>
</li>
<li>
<p>帧级别的更新</p>
<p>给定一帧，应用修改后的 motion 追踪算法在运动区域中选择特征，而不使用特征的密度做进一步的过滤。</p>
<p>使用有多个 tile 的多个视图来覆盖一个放大的区域，该区域包含作为预测 viewport 的移动对象上的所有特征，这样就能适应$m-n\ move$中的用户行为。</p>
<p>设计帧级别的算法标记选择的 tile 作为$PREDICTED$并设置其生存期为$HIGH$（直觉上讲运动中的物体或用户所感兴趣的静态特征会更以长时间保留在 viewport 之中）。</p>
</li>
<li>
<p>缓冲区级别的更新</p>
<p>以缓冲区长度为间隔检索用户的实际视图，基于此可以对 tile 的两种属性做出调整。</p>
<ol>
<li>对于与用户实际视图重叠的 tile，设置为$PREDICTED$和$HIGH$。</li>
<li>对于用户实际视图没有出现但出现在预测的视图中的 tile，生存期减 1，如果生存期减为$ZERO$，就重设其状态为$NONPREDICTED$，将其从预测的 viewport 中移除。</li>
</ol>
<p></p>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for RnnQoE</title>
    <link>http://localhost:1313/posts/papers/note-for-rnnQoE/</link>
    <pubDate>Thu, 16 Dec 2021 19:53:10 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-rnnQoE/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9580281" target="_blank" rel="noopener noreffer">QoE-driven Mobile 360 Video Streaming: Predictive
View Generation and Dynamic Tile Selection</a></p>
<p>Level：ICCC 2021</p>
<p>Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles</p>
<h2 id="系统建模与形式化">系统建模与形式化</h2>
<h3 id="视频划分">视频划分</h3>
<p>先将视频划分成片段：$\Iota = {1, 2, &hellip;, I}$表示片段数为$I$的片段集合。</p>
<p>接着将片段在空间上均匀划分成$M \times N$个 tile，FOV 由被用户看到的 tile 所确定。</p>
<p>使用 ERP 投影，$(\phi_i, \theta_i),\ \phi_i \in (-180\degree, 180\degree], \theta_i \in (-90\degree, 90\degree]$来表示用户在第$i$个片段中的视点坐标。</p>
<p>播放过程中记录用户头部运动的轨迹，积累的数据可以用于 FOV 预测。</p>
<p>跨用户之间的 FOV 轨迹可以用于提高预测精度。</p>
<h3 id="qoe-模型">QoE 模型</h3>
<ul>
<li>
<p>前提</p>
<p>视频编解码器预先确定，无法调整每个 tile 的码率。</p>
</li>
<li>
<p>实现</p>
<ol>
<li>每个 tile 都以不同的码率编码成不同的版本。</li>
<li>每个 tile 都有两种分辨率的版本。</li>
</ol>
</li>
<li>
<p>QoE 内容</p>
<p>客户端收到的视频质量和观看时的卡顿时间。</p>
</li>
<li>
<p>质量形式化</p>
<p>对于每个片段$i \in \Iota$，$S_i = {\tau_{i, j}}_{j=1}^{M \times N}$是用来表示用户实际看到的 tile 的集合的向量。</p>
<p>$\tau_{i, j} = 1$表示第$i$个段中的第$j$个 tile 被看到；$\tau_{i, j} = 0$表示未被看到。</p>
<p>同样的， $\tilde{S}_i = {\tilde{\tau}_{i, j}}_{j = 1}^{M \times N}$ 表示经过 FOV 预测和 tile 选择之后成功被传送到用户头戴设备上的 tile 集合的向量。</p>
<p>$\tilde{\tau}_{i, j} = 1$表示第$i$个段中的第$j$个 tile 被用户接收；$\tilde{\tau}_{i, j} = 0$表示未被接收。</p>
<p>第$i$个段的可感知到的质量可以表示为：</p>
<p>$$
Q_i = \sum_{j = 1}^{M \times N} p_{i, j}b_{i, j}\tau_{i, j}\tilde{\tau}_{i, j}
$$</p>
<p>$b_{i, j}$表示第$i$个片段的第$j$个 tile 的码率；$p_{i, j}$表示对不同位置 tile 所分配的权重；</p>
</li>
<li>
<p>关于权重$p_{i, j}$</p>
<p>研究表明用户在全景视频 FOV 中的注意力分配并不是均等的，越靠近 FOV 中心的 tile 对用户的 QoE 贡献越大。</p>
<p>下面讨论单个片段的情况：用$(\phi_j, \theta_j)$表示 tile 中心点的坐标，并映射到笛卡尔坐标系上$(x_j, y_j, z_j)$：</p>
<p>$$
x_j = cos\theta_jcos\phi_j,\ y_j = sin\theta_j,\ z_j = -cos\theta_jsin\phi_j
$$</p>
<p>则两个 tile 之间的半径距离$d_{j, j&rsquo;}$可以表示为：</p>
<p>$$
d_{j, j&rsquo;} = arccos(x_j x_{j&rsquo;} + y_j y_{j&rsquo;} + z_j z_{j&rsquo;})
$$</p>
<p>对于第$i$个片段，假设用户 FOV 中心的 tile 为$j^*$，那么第$j$个 tile 的权重可以计算出来：</p>
<p>$$
p_{i, j} = (1 - d_{j, j^*} / \pi) \tau_{i, j}
$$</p>
</li>
<li>
<p>卡顿时间形式化</p>
<p>当$\tilde{\tau}_{i, j}$与$\tau_{i, j}$出现分歧时，用户就不能成功收到请求的 tile，头戴设备中显示的内容就会被冻结，由此导致卡顿。</p>
<p>对于任意的片段$i \in \Iota$，相应的卡顿时间$D_i$可以计算出来：</p>
<p>$$
D_i = \frac{\sum_{j = 1}^{M \times N} b_{i, j} \cdot [\tau_{i, j} - \tilde{\tau}_{i, j}]^+}{\xi}
$$</p>
<p>$[x]^+ = max \lbrace x, 0 \rbrace $；$\xi$表示可用的网络资源（已知，并且在推流过程中保持为常数）</p>
<p>卡顿发生于在播放时，用户 FOV 内的 tile 还没有被传输到用户头戴设备中的时刻，终止于所有 FOV 内 tile 被成功传送的时刻。</p>
</li>
<li>
<p>质量与卡顿时间的结合</p>
<p>$$
max\ QoE = \sum_{i = 1}^I (Q_i - wD_i)
$$</p>
<p>$w$表示卡顿事件的惩罚权重。例如，w＝1000 意味着 1 秒视频暂停接收的 QoE 惩罚与将片段的比特率降低 1000 bps 相同。</p>
</li>
</ul>
<h2 id="联合-viewport-预测与-tile-选择">联合 viewport 预测与 tile 选择</h2>
<p>联合框架包括 viewport 预测和动态 tile 选择两个阶段。</p>
<p>viewport 预测阶段集成带有注意力机制的 RNN，接收用户的历史头部移动信息作为输入，输出每个 tile 出现在 FOV 中的可能性分布。</p>
<p>选择 tile 阶段为预测的输出建立的上下文空间，基于上下文赌博机学习算法来选择 tile 并确定所选 tile 的质量版本。</p>
<p></p>
<p></p>
<h3 id="viewport-预测">Viewport 预测</h3>
<p>FOV 预测问题可以看作是序列预测问题。</p>
<p>不同用户观看相同视频时的头部移动轨迹有强相关性，所以跨用户的行为分析可以用于提高新用户的 viewport 预测精度。</p>
<p>被广泛使用的 LSTM 的变体——Bi-LSTM（Bi-directional LSTM）用于 FOV 预测。</p>
<ol>
<li>
<p>输入参数构造</p>
<p>为了构造 Bi-LSTM 学习网络，需要对不同用户的 viewpoint 特性作表征。</p>
<ul>
<li>
<p>在用户观看事先划分好的$I$个片段时，记录每个片段对应的 viewpoint 坐标：</p>
<p>$\Phi_{1:I} = {\phi_i}^I_{i = 1},\ \Theta_{1:I} = {\theta_i}^I_{i=1}$</p>
</li>
<li>
<p>预测时使用的历史信息的窗口大小记为$k$；</p>
</li>
<li>
<p>对于第$i, (i &gt; k)$个片段，相应的 viewpoint 特性由$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$所给出；</p>
</li>
<li>
<p>列索引$m_i$和行索引$n_i$作为 viewpoint tile $(\phi_i, \theta_i)$的标签，由<a href="https://zh.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener noreffer">独热编码</a>表示；</p>
</li>
<li>
<p>通过滑动预测的窗口，所看到的视频片段的特性和标签可以被获取。</p>
</li>
</ul>
</li>
<li>
<p>LSTM 网络构造</p>
<p>整个网络包含 3 层：</p>
<ul>
<li>遗忘门层决定丢弃哪些信息；</li>
<li>更新门层决定哪类信息需要存储；</li>
<li>输出门层过滤输出信息。</li>
</ul>
<p>为了预测用户在第$i$个段的 viewpoint，LSTM 网络接受$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$作为输入；输出行列索引；</p>
<p>$$
m_i = LSTM(\theta_{i-k}, &hellip;, \phi_{i-1}; \alpha)
$$</p>
<p>$$
n_i = LSTM(\theta_{i-k}, &hellip;, \theta_{i-1}; \beta)
$$</p>
<p>$\alpha, \beta$是学习网络的参数；分类交叉熵被用作网络训练的损失函数。</p>
</li>
<li>
<p>Bi-LSTM 的特殊构造</p>
<ul>
<li>
<p>将公共单向的 LSTM 划分成 2 个方向。</p>
<p>当前片段的输出利用前向和反向信息，这为网络提供了额外的上下文，加速了学习过程。</p>
</li>
<li>
<p>双向的 LSTM 不直接连接，不共享参数。</p>
</li>
<li>
<p>每个时间槽的输入会被分别传输到前向和反向的 LSTM 中，并分别根据其状态产生输出。</p>
</li>
<li>
<p>两个输出直接连接到 Bi-LSTM 的输出节点。</p>
</li>
<li>
<p>引入注意力机制为每步时间自动分配权重，从大量信息中选择性地筛选出重要信息。</p>
</li>
<li>
<p>将 Softmax 层堆叠在网络顶部，以获取不同 tile 的 viewpoint 概率。</p>
</li>
</ul>
</li>
</ol>
<p></p>
<h3 id="动态-tile-选择">动态 tile 选择</h3>
<p>使用上下文赌博机学习算法来补偿 viewport 预测错误对 QoE 造成的影响。</p>
<ul>
<li>
<p>上下文赌博机学习算法概况</p>
<p>上下文赌博机学习算法是一个基于特征的 exploration-exploitation 技术。</p>
<p>通过在多条手臂上重复执行选择过程，可以获得在不同上下文中的每条手臂的回报。</p>
<p>通过 exploration-exploitation，目标是最大化累积的回报。</p>
</li>
<li>
<p>组成部分形式化</p>
<ol>
<li>
<p>上下文</p>
<p>直觉上讲，当预测的 viewpoint 不够精确时，需要扩大 FOV 并选择更多的 tile 进行传输。</p>
<p>为了做出第$i$个片段上的预测 viewpoint 填充决策，定义串联的上下文向量：</p>
<p>$c_i = [f^s_i, f^c_i]$，$f^s_i$表示自预测的上下文，$f^c_i$表示跨用户之间的预测上下文。</p>
<p>预测输出的用户$u$的 viewpoint tile 索引用$[\tilde{m}^u_{i-1}, \tilde{n}^u_{i-1}]$表示；</p>
<p>实际的用户$u$的 viewpoint tile 索引用$[m_{i-1}^u, n_{i-1}^u]$表示；</p>
<p>那么对第$i$个片段而言，自预测的上下文可以计算出来：</p>
<p>$$
f_i^s = [|m_{i-1}^u - \tilde{m}^u_{i-1}|, |n_{i-1}^u - \tilde{n}^u_{i-1}|]
$$
跨用户的上下文信息获取：使用 KNN 准则选择一组用户，其在前$k$个片段中的轨迹最接近用户$u$的轨迹。</p>
<p>使用$\zeta$表示获得的用户集合，使用</p>
<p>$$E_{\zeta_u}(m_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u} |m_i^u - \tilde{m}_i^u|$$</p>
<p>$$E_{\zeta_u}(n_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u}|n_i^u - \tilde{n}_i^u|$$</p>
<p>表示预测误差，则：</p>
<p>$$
f_i^u = [E_{\zeta_u}(m_i), E_{\zeta_u}(n_i)]
$$</p>
</li>
<li>
<p>手臂</p>
<p>根据从第一个阶段得到的每个 tile 的可能性分布，所有的 tile 可以用倒序的方式排列。</p>
<p>最高可能性的 tile 被看作 FOV 的中心，高码率以此 tile 为中心分配。</p>
<p>剩余的带宽用于扩展 FOV，低可能性的 tile 被顺序选择来扩展 FOV 直至带宽耗尽。</p>
<p>手臂的状态$a \in {0, 1}$表示 tile 选择的策略：</p>
<ul>
<li>
<p>$a = 0$表示 viewpoint 预测准确，填充 tile 分配了高质量；</p>
</li>
<li>
<p>$a = 1$表示 viewpoint 预测不准确，填充 tile 分配的质量较低，为了传送尽可能多的 tile 而减少卡顿；</p>
</li>
</ul>
</li>
<li>
<p>回报</p>
<p>给定上下文$c_i$，选择手臂$a$，预期的回报$r_{i, a}$建模为$c_i$和$a$组合的线性函数：</p>
<p>$$
\Epsilon[r_{i, a}|c_{i, a}] = c_{i, a}^T \theta_a^*
$$</p>
<p>未知参数$\theta_a$表示每个手臂的特性，目标是为第$i$个片段选择最优的手臂：</p>
<p>$$
a_i^* = \underset{a}{argmax}\ c_{i, a}^T \theta_a^*
$$</p>
<p>使用<a href="https://zhuanlan.zhihu.com/p/38875273" target="_blank" rel="noopener noreffer">LinUCB</a>算法做出特征向量的精确估计并获取$\theta_a^*$。</p>
<p></p>
</li>
</ol>
</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>评估准备</p>
<ul>
<li>使用现有的<a href="https://github.com/xuyanyu-shh/VR-EyeTracking" target="_blank" rel="noopener noreffer">viewpoint 轨迹数据集</a>，所有视频被编码为至少每秒 25 帧，长度为 20 到 60 秒；</li>
<li>视频每个片段被划分为$6 \times 12$的 tile，每个的角度是$30\degree \times 30\degree$；</li>
<li>初始 FOV 设定为$90\degree \times 90\degree$，在 viewpoint 周围是$3 \times 3$的 tile；</li>
<li>每个片段的长度为 500ms；</li>
<li>默认的预测滑动窗口大小$k = 5$；</li>
<li>HD 和 LD 版本分别以按照 HEVC 的$QP={32, 22}$的参数编码而得到；</li>
<li>训练集和测试集的比例为$7:3$；</li>
<li>Bi-LSTM 层配置有 128 个隐单元；</li>
<li>batch 大小为 64；</li>
<li>epoch 次数为 60；</li>
</ul>
</li>
<li>
<p>性能参数</p>
<ul>
<li>
<p>预测精度</p>
</li>
<li>
<p>视频质量</p>
<p>由传送给用户的有效码率决定：例如实际 FOV 中的 tile 码率总和</p>
</li>
<li>
<p>卡顿时间</p>
</li>
<li>
<p>规范化的 QoE</p>
<p>实际取得的 QoE 与在 viewpoint 轨迹已知情况下的 QoE 的比值</p>
</li>
</ul>
</li>
<li>
<p>对比目标</p>
<ul>
<li>预测阶段——预测精度
<ol>
<li>LSTM</li>
<li>LR</li>
<li>KNN</li>
</ol>
</li>
<li>取 tile 的阶段——规范化的 QoE
<ol>
<li>两个阶段都使用纯 LR</li>
<li>只预测而不做动态选择</li>
</ol>
</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for OpTile</title>
    <link>http://localhost:1313/posts/papers/note-for-optile/</link>
    <pubDate>Mon, 13 Dec 2021 16:19:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-optile/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3123266.3123339" target="_blank" rel="noopener noreffer">OpTile: Toward Optimal Tiling in 360-degree Video Streaming</a></p>
<p>Level：ACM MM 17</p>
<p>Keyword：Dynamic tile division, Optimize encoding efficiency, Optimize tile size</p>
<h2 id="背景知识">背景知识</h2>
<h3 id="编码过程概述">编码过程概述</h3>
<ol>
<li>
<p>对一帧图像中的每一个 block，编码算法在当前帧的已解码部分或由解码器缓冲的临近的帧中搜索类似的 block。</p>
<p>当编码器在邻近的帧中找到一个 block 与当前 block 紧密匹配时，它会将这个类似的 block 编码进一个动作向量中。</p>
</li>
<li>
<p>编码器计算当前 block 和引用 block 之间像素点的差异，通过应用变换（如离散余弦变换），量化变换系数以及对剩余稀疏矩阵系数集应用无损熵编码（如 Huffman 编码）对计算出的差异进行编码。</p>
</li>
</ol>
<h3 id="对编码过程的影响">对编码过程的影响</h3>
<ol>
<li>基于 tile 的方式会减少可用于拷贝的 block 数量，增大了可供匹配的 tile 之间的距离。</li>
<li>不同的投影方式会影响编码变换输出的系数稀疏性，而这会降低视频编码效率。</li>
</ol>
<h3 id="投影过程">投影过程</h3>
<p>因为直接对 360 度图像和视频的编码技术还没有成熟，所以 360 度推流系统目前还需要先将 3D 球面投影到 2D 平面上。</p>
<p>目前应用最广的投影技术主要是 ERP 和 CMP，分别被 YouTube 和 Meta 采用。</p>
<h4 id="erp-投影">ERP 投影</h4>
<p>基于球面上点的左右偏航角$\theta$与上下俯仰角$\phi$将其映射到宽高分别为$W$和$H$的矩形上。</p>
<p>对于平面坐标为$(x, y)$的点，其球面坐标分别为：</p>
<p>$$
\theta = (\frac{x}{W} - 0.5) * 360
$$</p>
<p>$$
\phi = (0.5 - \frac{y}{H}) * 180
$$</p>
<h4 id="cmp-投影">CMP 投影</h4>
<p>将球面置于一个立方体中，光线从球心向外发射，并分别与球面和立方体相交于两点，这两点之间便建立了映射关系。</p>
<p>之后将立方体 6 个平面拼接成矩形，就可以使用标准的视频编码方式进行压缩。</p>
<p>关于投影方式还可以参考这里的讲解：<a href="https://zhuanlan.zhihu.com/p/106922217" target="_blank" rel="noopener noreffer">谈谈全景视频投影方式</a></p>
<h3 id="tile-方式的缺点">tile 方式的缺点</h3>
<ul>
<li>
<p>降低编码效率</p>
<p>tile 划分越细，编码越低效</p>
</li>
<li>
<p>增加更大的整体存储需求</p>
</li>
<li>
<p>可能要求更多的带宽</p>
</li>
</ul>
<h2 id="optile-的设计">OpTile 的设计</h2>
<p>直觉上需要增大一些 tile 的大小来使与这些 tile 相关联的片段能捕获高效编码所需的类似块。</p>
<p>同时也需要 tile 来分割视频帧来减少传输过程中造成的带宽浪费。</p>
<ul>
<li>
<p>为了明白哪些片段的空间部分可以被高效独立编码，需要关于 tile 的存储大小的不同维度的信息。</p>
</li>
<li>
<p>为了找到切分视频的最好位置，需要在片段播放过程中用户 viewport 运动轨迹的偏好。</p>
</li>
</ul>
<p>将编码效率和浪费数据的竞争考虑到同一个问题之中，这个问题关注的是<strong>一个片段中所有可能的视图的分布</strong>。</p>
<p>片段的每个可能的视图可以被 tile 的不同组合所覆盖。</p>
<p>目标是为一个片段选择一个 tile 覆盖层，以<strong>最小化固定时间段内视图分布的总传输带宽</strong>。</p>
<ul>
<li>目标分离的部分考虑整个固定时间段的表示（representation）的存储开销。</li>
<li>目标的存储部分与下载的带宽部分相竞争。例如，如果一个不受欢迎的视频一年只观看一次，那么我们更喜欢一个紧凑的表示，我们可以期望向用户发送更多未观看的像素。</li>
</ul>
<h2 id="问题形式化">问题形式化</h2>
<table>
<thead>
<tr>
<th style="text-align:center">segment/片段</th>
<th style="text-align:center">推流过程中可以被下载的连续播放的视频单元</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">basic sub-rectangle/基本子矩形</td>
<td style="text-align:center">推流过程中可以被下载的片段中最小的空间划分块</td>
</tr>
<tr>
<td style="text-align:center">solution sub-rectangle/解子矩形</td>
<td style="text-align:center">片段中由若干基本子矩形组成的任何矩形部分</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">$x$</th>
<th style="text-align:center">用于表示子矩形在解中的存在的二元向量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$c^{(stor)}$</td>
<td style="text-align:center">每个子矩形存储开销相关的向量</td>
</tr>
<tr>
<td style="text-align:center">$c^{(view)}$</td>
<td style="text-align:center">给定一个 segment 中用户 viewport 的分布，$c^{(view)}$指相关子矩形的预期下载字节</td>
</tr>
<tr>
<td style="text-align:center">$\alpha$</td>
<td style="text-align:center">分配到$c^{(view)}$的权重，以此来控制相对于传输一个片段的存储开销</td>
</tr>
</tbody>
</table>
<p>考虑将 1 个矩形片段划分成 4 个基本子矩形，其对应的坐标如下：</p>
<p></p>
<p>4 个基本子矩形可以有 9 种分配方式，成为解子矩形，如下（因为需要保持对应的空间关系，所以只有 9 种）：</p>
<p></p>
<ul>
<li>
<p>$x$的形式化</p>
<p>可以用向量$x$来分别表示上图中子矩形在解中的存在：</p>
<p>$$
[1 \times 1\ at\ (0, 0),\ 1 \times 1\ at\ (0, 1),\ 1 \times 1\ at\ (1, 0),
\
1 \times 1\ at\ (1, 1),\ 1 \times 2\ at\ (0, 0),\ 1 \times 2\ at\ (1, 0),
\
2 \times 1\ at\ (0, 0),\ 2 \times 1\ at\ (0,1),\ 2 \times 2\ at\ (0,0).]
$$</p>
<p>（$x$中每个二元变量的的组成：$1 \times 1$表示子矩形的形状，$(0,0)$表示所处的位置）</p>
<p>要使$x$有效，<strong>每个基本子矩形必须被$x$中编码的子矩形精确覆盖一次</strong>。例如：</p>
<ul>
<li>
<p>$[0, 0, 0, 0, 1, 1, 0, 0, 0]$=&gt;有效（第 5 和第 6 次序的位置分别对应$e$和$f$子矩形，恰好覆盖了所有基本子矩形 1 次）</p>
</li>
<li>
<p>$[0,0,0,1,1,0,0,0,0]$=&gt;无效（第 4 和第 5 次序的位置分别对应$d$和$e$子矩形，没有覆盖到$(1,0)$基本子矩形）</p>
</li>
<li>
<p>$[0,0,0,1,1,1,0,0,0]$=&gt;无效（第 4、第 5 和第 6 次序的位置分别对应$d$、$e$和$f$子矩形，$(1,1)$基本子矩形被覆盖了两次）</p>
</li>
</ul>
</li>
<li>
<p>$c^{(stor)}$的形式化</p>
<p>与每个$x$相对应的向量$c^{(stor)}$长度与$x$相等，其中每个元素是$x$中对应位置的子矩形的存储开销的估计值。</p>
</li>
<li>
<p>$c^{(view)}$的形式化</p>
<p>考虑分发子矩形的网络带宽开销时，需要考虑所有可能被分发的 360 度表面的视图。</p>
<p>为了简化问题，将片段所有可能的视图离散化到一个大小为$V$的集合中。</p>
<p>集合中每个元素表示一个<strong>事件</strong>，即向观看 360 度视频片段的用户显示基本子矩形的唯一子集。</p>
<p>注意到片段中被看到的视频区域可以包含来自多个视角的区域。</p>
<p>将之前离散化好的大小为$V$的集合中每个元素与可能性相关联：$[p_1, p_2, &hellip;, p_V]$。</p>
<p>考虑为给定的解下载视图$V$的开销，作为需要为该视图下载的数据量：</p>
<p>$$
quantity = x^{\top}diag(d_v)c^{(stor)}
$$</p>
<p>$d_v$是一个二元向量，其内容是按照$x$所描述的表示方案，对所有覆盖视图的子矩形的选择。</p>
<p>例如对于 ERP 投影中位置坐标为$yaw = 0, pitch = 90$即处于等矩形顶部的图像，对应的$d_{view-(0, 90)} = [1, 1, 0, 0, 1, 0, 1, 1, 1]$</p>
<p>（即上面图中$a, b, e, g, h, i$位置的子矩形包含此视图所需的基本子矩形）。</p>
<p>给出一个片段中的用户 viewport 分布，$c^{(view)}$的元素是相关联的子矩形预期的下载字节。</p>
<p>$$
c^{(view)} = \sum_v p_v diag(d_v) c^{(stor)}
$$</p>
<p>最后，将优化问题的基本子矩形覆盖约束编码为矩阵$A$。</p>
<p>$A$是一个列中包含给定子矩形解所覆盖的基本子矩形信息的二元矩阵。</p>
<p>对于$2 \times 2$的矩形片段，其$A$有 4 行 9 列，例子如下：</p>
<p></p>
<p>因此最终的问题可以形式化为一个整数线性程序：</p>
<p></p>
<ul>
<li>
<p>$c^{(stor)}$</p>
<p>可以理解为存储一段$\Delta t$时间长的片段的子矩形的存储开销；</p>
</li>
<li>
<p>$c^{(view)}$</p>
<p>可以理解为传输一个视图所需要的所有的子矩形的传输开销。</p>
</li>
<li>
<p>$\alpha$</p>
<p>控制相比于传输一个片段的相对存储开销，同时应该考虑片段的流行度。</p>
<p>即$\alpha$应该与所期望的片段在$\Delta t$的时间间隔内的下载次数成比例，$\alpha$应该可以通过经验测量以合理的精度进行估计。</p>
<p>可以通过将$x$的二元离散限制放松到$0 \le x_i \le 1\ \forall i$构成一个线性程序，其解为整数。</p>
<p>（对于有 33516 个变量的$x$，其解可以在单核 CPU 上用 7~10 秒求出）</p>
</li>
</ul>
</li>
</ul>
<h2 id="开销向量建构">开销向量建构</h2>
<p>首先需要建构出存储开销向量$c^{(stor)}$，但是对于有$n$个基本子矩形的子矩形，其建构复杂度为$O(n^2)$。</p>
<p>因此对每个子矩形进行编码来获得存储开销并不可行，所以利用视频压缩与运动估计之间的强相关性来预测$c^{(stor)}$的值。</p>
<ol>
<li>
<p>给定一个视频，首先暂时将其分成长度为 1 秒的片段，每个片段被限定为只拥有 1 个 GOP，片段的大小表示为$S_{orig}$。</p>
</li>
<li>
<p>接着抽取出每个片段中的动作序列用于之后的分析。</p>
</li>
<li>
<p>将片段从空间上划分成基本子矩形，每个基本子矩形包含$4 \times 4 = 16$个宏块（例如：$64 \times 64$个像素点）。</p>
</li>
<li>
<p>独立编码每个基本子矩形，其大小表示为$S_i$。</p>
</li>
<li>
<p>通过分析动作向量信息，可以推断出如果对基本子矩形$i$进行独立编码，指向基本子矩形$i$的原始运动向量应该重新定位多少。</p>
<p>将其表示为$r_i$。</p>
</li>
<li>
<p>每个运动向量的存储开销可以计算为：</p>
<p>$$
o = \frac{\sum_i S_i - S_{orig}}{\sum_i r_i}
$$</p>
<p>即：存储开销的整体增长除以被基本子矩形边界所分割的运动向量数。</p>
</li>
<li>
<p>如果基本子矩形被融合进更大的子矩形$t$，使用$m_t$来表示由于融合操作而无须再进行重定位的运动向量的数量：</p>
<p>$$
m_t = \sum_{i \in t} r_i - r_t
$$</p>
<p>$i \in t$表示基本子矩形位于子矩形$t$中。</p>
</li>
<li>
<p>为了估计任意子矩形$t$的大小，使用下面 5 个参数：
$$
\sum_{i \in t} S_i,\ \sum_{i \in t} r_i,\ m_t,\ o,\ n
$$
$n$表示$t$中基本子矩形的数量。</p>
</li>
</ol>
<p>实际操作：</p>
<ol>
<li>
<p>创建了来自 4 个单视角 360 度视频的 6082 个 tile 数据集。4 个视频都以两种分辨率进行编码：$1920 \times 960$和$3980 \times 1920$。</p>
</li>
<li>
<p>为了产生 tile，从视频中随机选取片段，随机选取 tile 的位置，宽度和高度。</p>
<p>设置 tile 的 size 最大为$12 \times 12$个基本子矩形。</p>
<p>对于每个选择的 tile，为其建构一个数据集元素：</p>
<ol>
<li>计算上面提到的 5 参数的特性向量。</li>
<li>使用 FFmpeg 编码 tile 的视频段来得到存储该段需要的空间。</li>
</ol>
</li>
<li>
<p>使用多层感知机 MLP 来估计 tile 的大小。</p>
<p>MLP 中包含 50 个节点的单隐层，激活函数为 ReLU 函数，300 次迭代的训练过程使用<a href="https://zhuanlan.zhihu.com/p/29672873" target="_blank" rel="noopener noreffer">L-BFGS 算法</a>。</p>
<p>为了评估 MLP 的预测效果，使用 4 折的交叉验证法。</p>
<p>每次折叠时先从 3 个视频训练 MLP，接着使用训练好的模型去预测第 4 个视频的 tile 大小。</p>
</li>
</ol>
<h2 id="实现">实现</h2>
<p></p>
<p>将视频划分成 1 秒长的片段，之后为每个片段解决整数线性问题来确定最优的 tile 划分策略。</p>
<ol>
<li>使用 MLP 模型估计每个 tile 的存储开销。</li>
<li>根据视图的集合$d$及其对应的可能性分布$p$，来估计视图的下载开销$c^{(view)}$。</li>
<li>构造矩阵$A$时，限制最大的 tile 大小为$12 \times 12$的基本子矩形（如果设置每个基本子矩形包含$64 \times 64$的像素，tile 的最大尺寸即为$768 \times 768$的像素）。</li>
<li>使用<a href="https://www.gnu.org/software/glpk/" target="_blank" rel="noopener noreffer">GNU Linear Programming Kit</a>来解决问题。</li>
<li>将所有可能的解子矩形编码进一个二元向量$x$中来表示解。</li>
<li>GLPK 的解表明一个可能的解子矩形是否应该被放入解中。</li>
<li>基于最终得到的解，划分片段并使用 ffmepg 以同样参数的 x264 格式进行编码。</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>度量指标</p>
<ol>
<li>服务端存储需求。</li>
<li>客户端需要下载的字节数。</li>
</ol>
</li>
<li>
<p>数据来源</p>
<p>数据集：<a href="http://dash.ipv6.enstb.fr/headMovements/" target="_blank" rel="noopener noreffer">dash.ipv6.enstb.fr</a></p>
</li>
<li>
<p>评估准备</p>
<p>下载 5 个使用 ERP 投影的视频，抽取出测试中用户看到的对应部分。</p>
<p>每个视频都有$1920 \times 960$和$3840 \times 1920$的两种分辨率的版本。</p>
<p>$1920 \times 960$视频的基本子矩形尺寸为$64 \times 64$的像素。</p>
<p>$3840 \times 1920$视频的基本子矩形尺寸为$128 \times 128$的像素。</p>
<p>将视频划分成 1 秒长的片段，对每个片段都产生出 MLP 所需的 5 元组特性。</p>
<p>之后使用训练好的 MLP 模型来预测所有可能的 tile 的大小。</p>
</li>
<li>
<p>数据选择</p>
<ol>
<li>
<p>从数据集中随机选择出 40 个用户的集合。</p>
</li>
<li>
<p>假设 100°的水平和垂直 FOV，并使用 40 个用户的头部方向来为每个片段产生$p_v$和$d_v$。</p>
<p>即：分块的决策基于每个片段的内容特征信息与用户的经验视图模式。</p>
</li>
</ol>
</li>
<li>
<p>参数设定：$\alpha = 0,1,1000$.</p>
</li>
<li>
<p>对比实验：</p>
<p>一组使用由 ILP 得出的结构进行分块；</p>
<p>另外一组：</p>
<ul>
<li>
<p>$1920 \times 960$的视频片段分别使用$64 \times 64$，$128 \times 128$，$256 \times 256$，$512 \times 512$的方案固定大小分块。</p>
</li>
<li>
<p>$3840 \times 1920$的视频片段分别使用$128 \times 128$，$256 \times 256$，$512 \times 512$，$1024 \times 1024$的方案固定大小分块。</p>
</li>
</ul>
</li>
<li>
<p>划分结果对比</p>
<p></p>
</li>
</ul>
<h3 id="服务端的存储大小">服务端的存储大小</h3>
<p></p>
<p>按照$\alpha = 0$方案分块之后的视频大小几乎与未分块之前的视频大小持平，有时甚至略微小于未分块前的视频大小。</p>
<p>因为所有分块方案都使用相同的编码参数，所以重新编码带来的有损压缩并不会影响竞争的公平性。</p>
<p>如果将$\alpha$的值调大，存储的大小会略微增大；固定分块大小的方案所得到的存储大小也会随 tile 变小而变大。</p>
<h3 id="客户端的下载大小">客户端的下载大小</h3>
<ul>
<li>
<p>预测完美的情况——下载的 tile 没有任何浪费</p>
<p></p>
<p>$\alpha= 1000$的情况下，OpTile 的表现总是最好的。</p>
</li>
<li>
<p>正常预测的情况</p>
<p>预测的方法：假设用户的头部方向不会改变，预测的位置即为按照当前方向几秒之后的位置。</p>
<p></p>
<p>相比于完美假设的预测，所有分块方案的下载大小都增大了。</p>
<p>$\alpha = 1000$的方案在两个视频的情况下都取得了最小的下载大小。在剩下的 3 个视频中，OpTile 方案的下载大小比起最优的固定分块大小方案不超过 25%。</p>
<p>尽管固定分块大小的方案可能表现更好，但是这种表现随视频的改变而变化显著。</p>
<p><strong>因为固定分块的方案没有考虑视频内容的特性与用户的观看行为。</strong></p>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for RainbowDQN and Multitype Tiles</title>
    <link>http://localhost:1313/posts/papers/note-for-rainbowDQN&#43;tiles/</link>
    <pubDate>Sat, 11 Dec 2021 16:14:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-rainbowDQN&#43;tiles/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Level：IEEE Transaction on multimedia 21</p>
<p>Keyword：Rainbow-DQN, Multi-type tiles, Full streaming system</p>
<h2 id="问题形式化">问题形式化</h2>
<h3 id="模型">模型</h3>
<ol>
<li>
<p>原始视频用网格划分成$N$块 tile，每个 tile 都被转码成$M$个不同的质量等级$q_i$。</p>
</li>
<li>
<p>基于传输控制模块得出的结果，播放器请求$t_i$个 tile 的$q_i$质量的版本并将其存储在缓冲区中，对应的缓冲区大小为$l_i$。</p>
</li>
<li>
<p>用户 Viewport 的信息用$V$表示，可以确定 FOV 的中心。</p>
</li>
<li>
<p>根据$V$可以将 tile 划分成 3 种类型：FOV、OOS、Base。</p>
</li>
<li>
<p>FOV 中的 tile 被分配更高的码率；</p>
<p>OOS 按照与$V$的距离逐步降低质量等级$q_i$；</p>
<p>Base 总是使用低质量等级$q_{Base}$但使用完整的分辨率。</p>
</li>
<li>
<p>传输的 tile 在同步完成之后交给渲染器渲染。</p>
</li>
<li>
<p>播放器根据各项指标计算可以评估播放性能：</p>
<p>$&lt;V, B, Q, F, E&gt;$：viewport 信息$V$，网络带宽$B$，FOV 质量$Q$，重缓冲频率$F$，传输效率$E$。</p>
</li>
<li>
<p>传输控制模块用于确定每个 tile 的质量等级$q_i$和缓冲区大小$l_i$。</p>
</li>
<li>
<p>传输控制模块优化的最终目标是获取最大的性能：
$$
performance = E_{max},\ QoE \in accept\ range
$$</p>
</li>
</ol>
<h3 id="带宽评估">带宽评估</h3>
<ol>
<li>
<p>收集每个 tile 的下载日志来评估带宽。</p>
</li>
<li>
<p>使用<a href="https://zhuanlan.zhihu.com/p/32335746" target="_blank" rel="noopener noreffer">指数加权移动平均算法 EWMA</a>使评估结果光滑，来应对网络波动。</p>
</li>
<li>
<p>第$t$次评估结果使用$B_t$表示，用下式计算：
$$
B_t = \beta B_{t-1} + (1-\beta)b_t
$$
$b_t$是 B 的第$t$次测量值；$\beta$是 EWMA 的加权系数。</p>
</li>
<li>
<p>$t=0$时，$B_0$被初始化为 0；所以在初始的$t$比较小的时候，$B_t$与理想值相比就很小。</p>
<p>这种影响会随着$t$增大而减少。</p>
</li>
<li>
<p>为了优化启动过程，对公式做出修改：
$$
B_t = \frac{\beta B_{t-1} + (1-\beta)b_t}{1 - \beta^t}
$$
$t$较小的时候，分母会放大$B_t$；$t$较大时，分母趋近于 1，影响随之消失。</p>
</li>
</ol>
<h3 id="fov-表示和预测">FOV 表示和预测</h3>
<ol>
<li>
<p>3D 虚拟相机用于渲染视频，处于全景视频球面上的某条轨道，其坐标可以表示为$(\theta, \phi)$，可以直接从系统中获取。</p>
<p>相机始终朝向球的中心，所以用户的 FOV 中心坐标$(\theta^{&rsquo;}, \phi^{&rsquo;})$可以用$(\theta, \phi)$表示：
$$
\begin{cases}
\theta^{&rsquo;} = (\theta + \pi)\ mod\ 2\pi,\ 0 \le \theta \le 2\pi
\
\phi^{&rsquo;} = \pi - \phi,\ 0 \le \phi \le \pi
\end{cases}
$$</p>
</li>
<li>
<p>2D 网格中 tile 坐标$(u, v)$可以通过球面坐标使用 ERP 投影获得
$$
\begin{cases}
u = \frac{\theta^{&rsquo;}}{2\pi} \cdot W, 0 \le u \le W.
\
v = \frac{\phi^{&rsquo;}}{\pi} \cdot H, 0 \le v \le H.
\end{cases}
$$
$W$和$H$分别表示使用 ERP 投影得到的矩形宽度和高度</p>
</li>
<li>
<p>短期的 FOV 预测基于目前和历史的 FOV 信息。</p>
<p>使用$(U_t, V_t)$表示$t$时刻的 FOV 中心位置；$U_{t1:t2}$和$V_{t1:t2}$分别表示从$t1$到$t2$过程中$U$和$V$的序列；
$$
\begin{cases}
\hat{U}<em>{t+T_f} = f_U (U</em>{t-T_p:t}).
\
\hat{V}<em>{t+T_f} = f_V (V</em>{t-T_p:t}).
\end{cases}
$$
$T_p$是过去记录的滑动窗口；$T_f$是短期的预测窗口；$f_U$和$f_V$分别对应$U$和$V$方向上的映射函数；</p>
<p>因为是时间序列回归模型，所以映射函数使用 LSTM。</p>
</li>
</ol>
<h3 id="qoe-评估">QoE 评估</h3>
<p>QoE 由 3 个部分组成：平均 FOV 质量$Q$、重缓冲频率$F$与 FOV 内 tile 的质量变化（因为平均分配所以不考虑）。</p>
<ol>
<li>
<p>FOV 质量$Q$</p>
<p>第$t$次的 FOV 质量评估表示为$Q_t$：
$$
Q_t = \frac{\beta Q_{t-1} + (1-\beta) \frac{1}{k} \cdot \sum_{j=1}^{k} max{q_j, q_b}}{1 - \beta^t}
$$
$q_j$表示第$j$条 FOV tile 流的质量；$k$表示 FOV 内 tile 的数量；</p>
<p>为了避免评估结果的大幅波动，使用了 EWMA 来光滑结果。</p>
<p>当第$j$条 tile 流因为缓冲区不足不能成功播放时，$q_j = q_{Base}$（这表明了 Base tile 在提高 QoE 中的作用）。</p>
</li>
<li>
<p>重缓冲频率$F$</p>
<p>在基于 tile 的传输中，每条流都属于一个缓冲区。所以当 FOV 中 tile 的缓冲区处于饥饿状态时，重缓冲就会发生。</p>
<p>重缓冲频率描述了 FOV 内的 tile 流在一段时间内的重新缓冲频率。</p>
<p>第$t$次重缓冲频率的评估表示为$F_t$：
$$
F_t = \frac{\beta F_{t-\tau} + (1-\beta) \frac{f_t}{\tau}}{1 - \beta^{\tau}}
$$
$f_t$表示播放失败的次数；$\tau$表示一段时间；</p>
</li>
</ol>
<h3 id="传输效率评估">传输效率评估</h3>
<p>第$t$次传输效率评估表示为$E_t$，$E_t$通过传输的 FOV 内 tile 占总 tile 的比率来计算：
$$
E_t = \frac{\beta E_{t-1} + (1-\beta) \frac{total^{FOV}}{total^{ALL}}}{1 - \beta^t}
$$
$total^{FOV}$表示 FOV 内 tile 的数据量；$total^{ALL}$表示 tile 的总共数据量；</p>
<p>效率计算并不在传输过程中完成，因为需要获取哪些 tile 在 FOV 中的信息，效率评估滞后于播放过程。</p>
<h3 id="问题形式化-1">问题形式化</h3>
<p>传输控制的任务：确定所有 tile 流的质量等级$\chi$和缓冲区大小$\psi$。
$$
\chi = &lt;q_1, q_2, &hellip;, q_N&gt;
\
\psi = &lt;l_1, l_2, &hellip;, l_N&gt;
\
&lt;Q, F, E&gt; = \xi (B, V, \chi, \psi)
$$
$\chi$和$\psi$与带宽$B$和 Viewport 轨迹$V$一起作用于系统$\xi$，最终影响 FOV 质量$Q$，重缓冲频率$F$和传输效率$E$。</p>
<p>进一步，将目标形式化为获得每条 tile 流的$q_i$和$l_i$通过限制 QoE 满足可接受的范围、在此基础上最大化传输效率：
$$
\underset{\chi, \psi}{argmax} \sum_{t=0}^{+\infty} E_t,
$$</p>
<p>$$
s.t.:\ 0 \le q_i \le M,
$$</p>
<p>$$
0 \le l_i \le L,
$$</p>
<p>$$
Q^{min} \le Q_t \le M,
$$</p>
<p>$$
0 \le F_t \le F^{max}.
$$</p>
<p>$q_i$和$l_i$分别受限于质量版本数$M$和最大缓冲区大小$L$；</p>
<p>$Q_t$受限于最低 QoE 标准$Q^{min}$；</p>
<p>$F_t$受限于最大能忍受的重缓冲频率$F^{max}$。</p>
<h2 id="系统架构">系统架构</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>将原始视频转码为有不同比特率的多个版本。</li>
<li>转码后的视频被划分成多个 tile。</li>
<li>传输协议使用 MPEG-DASH。</li>
</ol>
<h3 id="客户端">客户端</h3>
<h4 id="评估器">评估器</h4>
<ul>
<li>任务：获取 QoE、FOV 预测、传输效率、网络带宽</li>
<li>组成：
<ul>
<li>QoE 评估器：评估当前 FOV 质量=&gt;Q 和重缓冲频率=&gt;F（近似为 Q+F=QoE）</li>
<li>FOV 预测器：基于历史 FOV 信息预测短期未来的 FOV=&gt;P</li>
<li>根据下载和播放日志：计算传输效率=&gt;E 并估计带宽=&gt;B</li>
</ul>
</li>
</ul>
<h4 id="控制器">控制器</h4>
<ul>
<li>任务：控制传输过程中的推流</li>
<li>目标：保证 QoE 在可接受的范围之内、最大化传输效率</li>
<li>详细：基于 FOV 预测将 tile 划分成 3 种类型：FOV、OOS、Base</li>
<li>输入：Q、F、E、B（QoE+传输效率和带宽）</li>
<li>过程：Rainbow-DQN</li>
<li>输出：决定每个 tile 流的码率和缓冲区大小（作为下载器的输入）</li>
</ul>
<h4 id="下载器">下载器</h4>
<ul>
<li>输入：tile 码率和缓冲区大小</li>
<li>过程：基于 HTTP/2 进行并行下载</li>
<li>输出：下载好的 tile</li>
</ul>
<h4 id="视频缓冲区">视频缓冲区</h4>
<ul>
<li>任务：解码、同步、存储下载好的 tile 等待渲染器消耗，大小供控制器调节</li>
<li>随着 FOV 的切换缓冲区内容可能被循环利用</li>
</ul>
<h4 id="全景渲染器">全景渲染器</h4>
<ul>
<li>任务：将同步好的 tile 拼接，tile 质量：FOV&gt;OOS&gt;Base</li>
<li>投影方式：ERP</li>
</ul>
<h2 id="控制器-1">控制器</h2>
<h3 id="控制过程">控制过程</h3>
<ol>
<li>
<p>设定 QoE 的可接受范围。</p>
</li>
<li>
<p>将网络带宽和用户 FOV 设定为外部因素而非环境</p>
<p>为什么：因为这两个因素变化太快，在面对不同传输条件时，直接作为环境会导致决策过程的不稳定性并且难以收敛。</p>
</li>
<li>
<p>最优化的对象只是最大化累积的传输效率。</p>
<p>为什么：简单</p>
</li>
</ol>
<h3 id="tile-聚合和决策">tile 聚合和决策</h3>
<ol>
<li>
<p>tile 分类原则：</p>
<ul>
<li>
<p>控制器无需为每个 tile 独立决定码率 Q 和缓冲区大小 L</p>
</li>
<li>
<p>FOV 内的 tile 应该被分配相近的码率，FOV 内的 tile 应该聚集成一组，OSS 和 Base 同理</p>
<p>为什么：避免相邻 tile 的锐利边界，只考虑 3 组而非所有 tile 降低了计算复杂性和决策延迟</p>
<p>（能否实现独立的 tile 码率计算或更细粒度的划分值得调研？与内容感知的方案结合？）</p>
</li>
</ul>
</li>
<li>
<p>基于距离的 tile 分类实现方式：</p>
<ul>
<li>
<p>使用评估器预测出的 FOV 坐标来分类 FOV 和 OOS 的 tile</p>
</li>
<li>
<p>tile 出现在未来 FOV 的可能性由距离计算</p>
<p>tile 中心点坐标$(\omega_i, \mu_i)$、FOV 坐标$(\hat{U}, \hat{V})$</p>
<p>距离的变化区间内存在一个临界点，临界点之内的划分为 FOV，之外的划分为 OOS</p>
<ul>
<li>
<p>度量距离的方式：
$$
\Delta Dis_U = min{|\omega_i - \hat{U}|, |1+\omega_i - \hat{U}|}
$$</p>
<p>（这里为何不直接使用$|\omega_i - \hat{U}|$？）
$$
Dis_i =
\begin{cases}
{\sqrt{({\Delta Dis_{U}})^2 + {(\mu_i - \hat{V})}^2},\  \frac{R}{H} \le \hat{V} \le 1 - \frac{R}{H}}
\
{\Delta Dis_U + |\mu_i - \hat{V}|,\ Others}
\end{cases}
$$</p>
</li>
<li>
<p>因为 ERP 的投影方式会在两级需要更多的 tile，因此使用一个矩形来代表两极的 FOV</p>
<p>（可以深入调研 ERP 在两极处的处理方式）</p>
</li>
<li>
<p>$Dis_i$使用曼哈顿距离来测量。临界点初始化为$2\cdot R$，并随着 FOV 中心和两极的垂直距离增长。</p>
</li>
<li>
<p>FOV 看作是半径为 R 的圆，使用欧式距离测量。临界点初始化为$R$</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>聚合 tile 的决策</p>
<ul>
<li>使用 2 个变量：$K$作为 FOV 和非 FOV 的 tile 的带宽分配比率；$Len$作为 tile 缓冲区的大小。
<ul>
<li>
<p>$K$确定之后，分配给 FOV 内 tile 的带宽被均匀分配（可否非均匀分配）</p>
<p>$K$不直接与网络状况相关因此可以保持控制的稳定性</p>
</li>
<li>
<p>$Len$：所有传输的 tile 的缓冲区长度$l_i$都被设为$Len$  （文中并没有这样做的原因解释）</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="基于-drl-的传输控制算法">基于 DRL 的传输控制算法</h3>
<p>相关术语解释：<a href="https://www.jianshu.com/p/1dfd84cd2e69" target="_blank" rel="noopener noreffer">Rainbow DQN</a>、<a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e" target="_blank" rel="noopener noreffer">RL Dictionary</a>、<a href="https://zhuanlan.zhihu.com/p/38358183" target="_blank" rel="noopener noreffer">PER</a>、<a href="https://zhuanlan.zhihu.com/p/34747205" target="_blank" rel="noopener noreffer">TD-Error</a></p>
<ol>
<li>
<p>控制过程</p>
<ol>
<li>
<p>首先调整 buffer 长度 Len，并划分 FOV 与非 FOV 的带宽分配。</p>
</li>
<li>
<p>等 viewport 预测完成之后，tile 被分类为属于 FOV 和 OOS 的 tile。</p>
</li>
<li>
<p>FOV 的带宽被平均分给其中每一个 tile 并决定 FOV 内 tile 的质量等级$q_i$。</p>
<p>非 FOV 的带宽按照与 FOV 的距离分配，每超过一个距离单位$Dis_i$就降低一级质量$q_i$。</p>
</li>
<li>
<p>最终的输出是请求序列，每个请求序列中包括质量等级$q_i$和预期的缓冲区大小$l_i$。</p>
</li>
<li>
<p>根据输出做出调整之后，接收奖励反馈并不断完成自身更新。</p>
</li>
</ol>
</li>
<li>
<p>状态设计</p>
<p>状态设计为 5 元组：$&lt;K, Len, Q, F, E&gt;$（传输控制参数$K$，$Len$、QoE 指标：FOV 质量 Q 和重缓冲频率$F$、传输效率$E$）</p>
<p>没有直接使用带宽$B$和 viewport 轨迹$V$，因为：</p>
<ol>
<li>随机性强与变化幅度较大带来的不稳定性（如何定义随机性强弱和变化幅度大小？）</li>
<li>希望设计的模型有一定的通用性，可以与不同的网络情况和用户轨迹相兼容</li>
</ol>
</li>
<li>
<p>动作设计</p>
<p>两种动作：调整$K$和$Len$（两者的连续变化区间被离散化，调整的每一步分别用$\Delta k$和$\Delta l$表示）</p>
<p>调整的方式被形式化为二元组：$&lt;n_1, n_2&gt;$，$n_1$和$n_2$分别用于表示$K$和$Len$的调整</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">-n</th>
<th style="text-align:center">0</th>
<th style="text-align:center">n</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">K</td>
<td style="text-align:center">减少 n$\Delta k$</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">增加 n$\Delta k$</td>
</tr>
<tr>
<td style="text-align:center">Len</td>
<td style="text-align:center">减少 n$\Delta l$</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">增加 n$\Delta l$</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>奖励函数</p>
<p>因为 QoE 的各项指标权重难以确定，没有使用传统的基于加权的方式。</p>
<p>设定了<strong>能接受的 QoE 范围</strong>和<strong>在此基础上最大化的传输效率</strong>作为最后的<strong>性能</strong>指标，形式化之后如下：
$$
Reward =
\begin{cases}
-INF,\ F \ge F^{max}
\
-INF,\ E \le E^{min}
\
E,\ Others
\end{cases}
$$</p>
<p>$-INF$意味着终止当前 episode；动作越能使系统满足高 QoE 的同时高效运行，得分越高；</p>
<p>为了最大化传输效率，使用$E$作为奖励回报。</p>
<p>FOV 质量$Q$并没有参与到奖励函数中，因为：<strong>高 Q 意味着高性能，但是低 Q 不一定意味着低性能</strong>，详细解释如下：</p>
<ul>
<li>在带宽不足的情况下，低 Q 可能已经是这种条件下的满足性能的最好选择。</li>
<li>高传输效率意味着传输了更多的 FOV 数据，也能满足高 FOV 质量的目标。</li>
</ul>
</li>
<li>
<p>模型设计
基于 Rainbow-DQN 模型：</p>
<ul>
<li>
<p>输入是 5 元组$&lt;K, Len, Q, F, E&gt;$。</p>
</li>
<li>
<p>神经网络使用 64 维的 3 隐层模型。</p>
</li>
<li>
<p>为了提高鲁棒性，神经网络的第 3 层使用 Dueling DQN 的方式，将 Q 值$Q(s, a)$分解为状态价值$V(s)$和优势函数$A(s,a)$：
$$
Q(s, a) = V(s) + A(s, a)
$$</p>
<p>$V(s)$表示系统处于状态$s$时的性能；$A(s,a)$表示系统处于状态$s$时动作$a$带来的性能；</p>
</li>
<li>
<p>为了避免价值过高估计，使用 Double DQN 的方式，设计了两个独立的神经网络：评估网络和目标网络。</p>
<p>评估网络用于动作选择；目标网络是评估网络从最后一个 episode 的拷贝用于动作评估。</p>
</li>
<li>
<p>为了缓解神经网络的不稳定性（更快收敛），使用大小为$v$的回放池来按照时间序列保存客户端的经验。</p>
<p>因为网络带宽和 FOV 轨迹在短期内存在特定的规律性，回放池中有相似状态和相似采样时间的样本更加重要，出现了优先级</p>
<p>所以使用优先经验回放 PER，而优先级使用时间查分误差 TD-error 定义
$$
\delta_i = r_{i+1} + \gamma Q(s_{i+1}, arg\underset{a}{max}Q(s_{i+1}, a; \theta_i); \theta_i^{&rsquo;}) - Q(s_i, a_i; \theta_i)
$$</p>
<p>$r_i$是奖励；$\gamma$是折扣因子</p>
</li>
<li>
<p>损失函数使用均方误差定义
$$
J = \frac{1}{v} \sum_{i=1}^{v} \omega_i(\delta_i)^2
$$</p>
<p>$\omega_i$是回放缓冲中第 i 个样本的重要性采样权重</p>
</li>
</ul>
</li>
</ol>
<h2 id="实验验证">实验验证</h2>
<ol>
<li>
<p>环境设定</p>
<ul>
<li>
<p>传输控制模块：基于<a href="https://tensorforce.readthedocs.io/en/latest/" target="_blank" rel="noopener noreffer">TensorForce</a>（配置教程：<a href="https://zhuanlan.zhihu.com/p/60241809" target="_blank" rel="noopener noreffer">用 TensorForce 快速搭建深度强化学习模型</a>）；</p>
<p>开发工具集：<a href="https://gym.openai.com/" target="_blank" rel="noopener noreffer">OpenAI Gym</a></p>
</li>
<li>
<p>数据来源：使用全景视频播放设备收集，加入高斯噪声来产生更多数据。</p>
</li>
</ul>
</li>
<li>
<p>结果分析</p>
<ul>
<li>
<p>与其他 DQN 算法的对比——DQN、Double DQN、Dueling DQN</p>
<ul>
<li>
<p>对比训练过程中每个 episode 中的最大累计奖励：$MAX_{reward}$</p>
</li>
<li>
<p>对比模型收敛所需要的最少 episode：$MIN_{episode}$</p>
<p>相同的带宽和 FOV 轨迹</p>
</li>
</ul>
</li>
<li>
<p>与其他策略对比性能——高 QoE 和高传输效率</p>
<ul>
<li>
<p>随机控制策略：随机确定 K 和 Len</p>
</li>
<li>
<p>固定分配策略：固定 K 和 Len 的值</p>
</li>
<li>
<p>只预测 Viewport 策略：使用 LSTM 做预测，不存在 OSS 与 Base，所有带宽都用于 FOV</p>
<p>带宽和 FOV 轨迹的均值和方差相等</p>
</li>
</ul>
</li>
<li>
<p>与其他全景视频推流系统的对比</p>
<ul>
<li>
<p>DashEntire360：使用 Dash 直接传送完整的 360 度视频，使用线性回归来估计带宽并动态调整视频比特率</p>
</li>
<li>
<p>360ProbDash：在 DashEntire360 的基础上划分 tile 基于 Dash 传输，使用可能性模型为 tile 分配比特率</p>
</li>
<li>
<p>DRL360：使用 DRL 来优化多项 QoE 指标</p>
<p>实现三种系统、使用随机网络带宽和 FOV 轨迹。</p>
<p>使用 DRL360 中提出的方式测量 QoE：
$$
V_{QoE} = \eta_1 Q - \eta_2 F - \eta_3 A
$$</p>
<p>$A$是 viewport 随时间的平均变化，反映 FOV 质量 Q 的变化；</p>
<p>$\eta_1, \eta_2, \eta_3$分别是 3 种 QoE 指标的非负加权，使用 4 种加权方式来训练模型并对比：</p>
<p>$&lt;1, 1, 1&gt;$，$&lt;1, 0.25, 0.25&gt;$，$&lt;1, 4, 1&gt;$，$&lt;1,1,4&gt;$</p>
</li>
</ul>
</li>
<li>
<p>在不同环境下的性能评估——带宽是否充足、FOV 轨迹是否活跃（4 种环境）</p>
</li>
</ul>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for 360ProbDASH</title>
    <link>http://localhost:1313/posts/papers/note-for-360ProbDASH/</link>
    <pubDate>Thu, 09 Dec 2021 10:20:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-360ProbDASH/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3123266.3123291" target="_blank" rel="noopener noreffer">360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming</a></p>
<p>Level: ACM MM 17</p>
<p>Keyword:</p>
<p>Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation</p>
<h2 id="工作范围与目标">工作范围与目标</h2>
<p>应用层-&gt;基于 tile-&gt;viewport 预测的可能性模型+预期质量的最大化</p>
<ul>
<li>
<p>针对小 buffer 提出了<code>target-buffer-based rate control</code>算法来避免重缓冲事件（避免卡顿）</p>
</li>
<li>
<p>提出 viewport 预测的可能性模型计算 tile 被看到的可能性（避免边缘效应）</p>
</li>
<li>
<p>形式化 QoE-driven 优化问题：</p>
<p>在传输率受限的情况下最小化 viewport 内的质量失真和空间质量变化（获取受限状态下最好的视频质量）</p>
</li>
</ul>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>形式化参数</p>
<p>$M*N$个 tile，M 指 tile 序列的序号，N 指不同的码率等级</p>
<p>$r_{i, j}$指比特率，$d_{i, j}$指失真，$p_{i}$指被看到的可能性（$\sum_{i=1}^{N}p_{i} = 1$）</p>
<p>$\Phi(X)$指质量失真，$\Psi(X)$指质量变化</p>
</li>
<li>
<p>目标</p>
<p>找到推流段的集合：$X = {x_{i, j}}$，其中${x_{i, j}} = 1$指被第$&lt;i, j&gt;$个 tile 被选中；$x_{i, j} = 0$则是未选中。
$$
\underset{X}{min}\ \Phi(X) + \eta \cdot \Psi(X) \
s.t. \sum_{i=1}^{N}\sum_{j=1}^{M}x_{i, j}\cdot r_{i, j} \le R, \
\sum_{j=1}^{M}x_{i, j} \le 1, x_{i, j} \in {0, 1}, \forall i.
$$
整个公式即为前所述的问题的形式化表达的公式化结果。</p>
</li>
<li>
<p>模型细节</p>
<ol>
<li>
<p>$\Phi(X)$和$\Psi(X)$的计算=&gt;通过考虑球面到平面的映射</p>
<p>通过计算球面上点的 Mean Squared Error 来得到 S-PSNR 进而评估质量：$d_{i, j}$来表示第${&lt;i, j&gt;}$个段的 MSE</p>
<p>
$$
\phi_i = \frac{\pi}{2} - h_i \cdot \frac{\pi}{H}, \Delta\phi = \Delta h \cdot \frac{\pi}{H}, \
\theta_i = w_i \cdot \frac{2\pi}{W}, \ \Delta\theta = \Delta w \cdot \frac{2\pi}{W},
$$
$H$和$W$分别指按照 ERP 格式投影之后的视频高度和宽度</p>
<p>第$i$个 tile 的空间面积用$s_i$表示：
$$
s_i\ =\ \iint_{\Omega_i}Rd\phi Rcos\phi d\theta \
=\Delta\theta R^2[sin(\phi_i + \Delta\phi) - sin\phi_i],
$$
$R$指球的半径（$R = W/2\pi$），所以整体的球面质量失真$D_{i, j}$可以计算出来：
$$
D_{i, j} = d_{i, j} \cdot s_i,
$$
结合每个 tile 被看到的概率$p_i$可以得出$\Phi(X)$和$\Psi(X)$
$$
\Phi(X)=\frac{\sum_{i=1}^N\sum_{j=1}^MD_{i, j}\cdot x_{i,j}\cdot p_i}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i},\
\Psi(X) = \frac{\sum_{i=1}^N\sum_{j=1}^Mx_{i, j}\cdot p_i \cdot\ (D_{i,j}-s_{i} \cdot \Phi(X))^2}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i}.
$$</p>
</li>
<li>
<p>Viewport 的可能性模型</p>
<ol>
<li>
<p>方向预测=&gt;<strong>线性回归模型</strong></p>
<p>将用户的欧拉角看作是$yaw(\alpha)$，$pitch(\beta)$和$rool(\gamma)$，应用线性回归做预测
$$
\begin{cases}
\hat{\alpha}(t_0 + \delta) = m_{\alpha}\delta+\alpha(t_0),\
\hat{\beta}(t_0 + \delta) = m_{\beta}\delta+\beta(t_0),\
\hat{\gamma}(t_0 + \delta) = m_{\gamma}\delta+\gamma(t_0).
\end{cases}
$$</p>
</li>
<li>
<p>预测错误的分布=&gt;<strong>高斯分布</strong>，根据公式均值和标准差都能从统计信息中计算出来</p>
<p>收集 5 名志愿者的头部移动轨迹并投影到 3 个方向上绘制成图，实验结果为预测错误呈现高斯分布（样本数可能不够？）</p>
<p>
$$
\begin{cases}
P_{yaw}(\alpha) = \frac{1}{\sigma_{\alpha}\sqrt{2\pi}}exp{-\frac{[\alpha-(\hat{\alpha}+\mu_{\alpha})]^2}{2\sigma_{\alpha}^2}},\
P_{pitch}(\beta) = \frac{1}{\sigma_{\beta}\sqrt{2\pi}}exp{-\frac{[\beta-(\hat{\beta}+\mu_{\beta})]^2}{2\sigma_{\beta}^2}},\
P_{roll}(\gamma) = \frac{1}{\sigma_{\gamma}\sqrt{2\pi}}exp{-\frac{[\gamma-(\hat{\gamma}+\mu_{\gamma})]^2}{2\sigma_{\gamma}^2}}.
\end{cases}
$$
3 个方向各自<strong>独立</strong>，因此最终的预测错误$P_E(\alpha,\beta,\gamma)$可以表示为：
$$
P_E(\alpha, \beta, \gamma) = P_{yaw}(\alpha)P_{pitch}(\beta)P_{roll}(\gamma).
$$</p>
</li>
<li>
<p>球面上点被看到的可能性</p>
<p>球面坐标为$(\phi, \theta)$点的可能性表示为$P_s(\phi, \theta)$</p>
<p>因为一个点可能在多个不同的 viewport 里面，所以定义按照用户方向从点$(\phi, \theta)$出发能看到的点集$L(\phi, theta)$</p>
<p>因此空间点$s$被看到的可能性可以表示为：
$$
P_s(\phi, \theta) = \frac{1}{|L(\phi, \theta)|}\sum_{(\alpha, \beta, \gamma) \in L(\phi, \theta)}P_E(\alpha, \beta, \gamma),
$$</p>
</li>
<li>
<p>球面上 tile 被看到的可能性</p>
<p>tile 内各个点被看到的可能性的<strong>均值</strong>即为 tile 被看到的可能性（可否使用其他方式？）
$$
p_i = \frac{1}{|U_i|} \sum_{(\phi, \theta) \in U_i} P_s(\phi, \theta).
$$
$U_i$表示 tile 内的空间点集</p>
</li>
</ol>
</li>
<li>
<p><code>Target-Buffer-based</code> Rate Control</p>
<p>因为长期的头部移动预测会产生较高的预测错误，所以不能采用大缓冲区（没有 cite 来证明这一点）</p>
<p></p>
<p>将处于相同时刻的段集合成一个块存储在缓冲区中。</p>
<p>在自适应的第 k 步，定义$d_k$作为此时的 buffer 占用情况（等到第 k 个块被下载完毕）
$$
b_k = b_{k-1} - \frac{R_k \cdot T}{C_k} + T
$$
$C_k$表示平均带宽，$R_k$表示总计的码率</p>
<p>为了避免重新缓冲设定目标 buffer 占用$B_{target}$，并使 buffer 占用保持在$B_{target}$（$b_k = B_{target}$）</p>
<p>因此总计的码率需要满足：
$$
R_k = \frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T),
$$
这里的$C_k$表示可以从历史的段下载信息中估计出来的带宽</p>
<p>设定$R$的下界$R_{min}$之后（没有说明为何需要设定下界），公式 12 可以修正为如下：
$$
R_k = max{\frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T), R_{min}}.
$$</p>
</li>
</ol>
</li>
</ol>
<h2 id="实现">实现</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>
<p>视频裁剪器</p>
<p>将视频帧切割成 tile</p>
</li>
<li>
<p>编码器</p>
<p>对 tile 进行划分并将其编码成多种码率的段</p>
</li>
<li>
<p>MPD 产生器</p>
<p>添加<strong>SRD 特性</strong>来表示段之间的空间关系</p>
<p>添加经度和<strong>纬度</strong>属性来表示</p>
<p>添加<strong>质量失真</strong>和<strong>尺寸</strong>属性</p>
</li>
<li>
<p>Apache HTTP 服务器</p>
<p>存储视频段和 mpd 文件，向客户端推流</p>
</li>
</ol>
<h3 id="客户端">客户端</h3>
<ol>
<li>
<p>基础：dash.js</p>
</li>
<li>
<p>额外的模块</p>
<ul>
<li>
<p><code>QoE-driver Optimizer</code>
$$
Output = HTTP\ GET请求中的最优段
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Target\ buffer\ based\ Rate\ Controller\
Viewport\ Probabilistic\ Model\
QR\ Map
\end{cases}
$$</p>
</li>
<li>
<p><code>Target-buffer-based Rate Controller</code>
$$
Output = 总计的传输码率，按照公式13计算而来
$$</p>
<p>$$
Input = Output\ of\ {Bandwidth\ Estimation\ module
$$</p>
</li>
<li>
<p><code>Viewport Probabilistic Model</code>
$$
Output = 每个tile被看到的可能性，按照公式10计算而来
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Orientation\ Prediction\ module\
SRD\ information
\end{cases}
$$</p>
</li>
<li>
<p><code>QR Map</code>QR=&gt;Quality-Rate
$$
Output = 所有段的QR映射
$$</p>
<p>$$
Input = MPD中的属性
$$</p>
</li>
<li>
<p><code>Bandwidth Estimation</code>（没有展开研究，因为不是关键？）
$$
Output = 前3秒带宽估计的平均值
$$</p>
<p>$$
Input = 下载段过程中的吞吐量变化
$$</p>
<p>可以通过<code>onProgess()</code>的回调函数<code>XMLHttpRequest API</code>获取</p>
</li>
<li>
<p><code>Orientation Prediction</code>
$$
Output = 用户方向信息的预测结果（yaw, pitch, roll）
$$</p>
<p>$$
Input = Web\ API中获取的DeviceOrientation信息，使用线性回归做预测
$$</p>
</li>
</ul>
</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>整体设定</p>
<ol>
<li>将用户头部移动轨迹编码进播放器来模拟用户头部移动</li>
<li>积极操控网络状况来观察不同方案对网络波动的反应</li>
</ol>
</li>
<li>
<p>详细设定</p>
<ul>
<li>
<p>服务端</p>
<ol>
<li>
<p>视频选择</p>
<p>2880x1440 分辨率、时长 3 分钟、投影格式 ERP</p>
</li>
<li>
<p>切分设置</p>
<p>每个块长 1s（$T=1$）、每个块被分成 6x12 个 tile（$N=72$）</p>
<p>每个段的码率设置为${20, 50, 100, 200, 300}$，单位 kpbs</p>
</li>
<li>
<p>视频编码</p>
<p><a href="http://www.videolan.org/developers/x264.html" target="_blank" rel="noopener noreffer">开源编码器 x264</a></p>
</li>
<li>
<p>视频分包</p>
<p><a href="https://gpac.wp.mines-telecom.fr/mp4box/" target="_blank" rel="noopener noreffer">MP4Box</a></p>
</li>
<li>
<p>注意事项</p>
<p>每个段的确切尺寸可能与其码率不同，尤其对于长度较短的块。</p>
<p>为了避免这影响到码率自适应，将段的确切尺寸也写入 MPD 文件中</p>
</li>
</ol>
</li>
<li>
<p>客户端</p>
<ol>
<li>
<p>缓冲区设定（经过实验得出的参数）</p>
<p>$B_{max}=3s$，$B_{target}=2.5s$，$R_{min}=200kbps$，$权重\eta=0.0015$</p>
</li>
</ol>
</li>
<li>
<p>高斯分布设定</p>
<table>
<thead>
<tr>
<th style="text-align:center">Yaw</th>
<th style="text-align:center">Pitch</th>
<th style="text-align:center">Roll</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\mu_{\alpha}=-0.54,\ \sigma_{\alpha}=7.03$</td>
<td style="text-align:center">$\mu_{\beta}=0.18,\ \sigma_{\beta}=2.55$</td>
<td style="text-align:center">$\mu_{\gamma}=2.16,\ \sigma_{\gamma}=0.15$</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li>
<p>比较对象</p>
<ul>
<li>ERP：原始视频格式</li>
<li>Tile：只请求用户当前 viewport 的 tile，不使用 viewport 预测，作为 baseline</li>
<li>Tile-LR：使用线性回归做预测，每个 tile 的码率被平均分配</li>
</ul>
</li>
<li>
<p>性能指标</p>
<ul>
<li>卡顿率：卡顿时间占播放总时长的比例</li>
<li>Viewport PSNR：直接反应 Viewport 内的视频质量</li>
<li>空间质量差异：Viewport 内质量的协方差</li>
<li>Viewport 偏差：空白区域在 Viewport 中的比例</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for Dante</title>
    <link>http://localhost:1313/posts/papers/note-for-dante/</link>
    <pubDate>Wed, 08 Dec 2021 22:14:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note-for-dante/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3232565.3234686" target="_blank" rel="noopener noreffer">https://dl.acm.org/doi/10.1145/3232565.3234686</a></p>
<p>Level: SIGCOMM 18</p>
<p>Keyword: UDP+FOV-aware+FEC</p>
<h2 id="工作范围">工作范围</h2>
<p></p>
<h2 id="目标">目标</h2>
<p>在给定序列的帧中，为<strong>每个 tile</strong>设定 FEC 冗余，根据其被看到的可能性的加权最小化平均质量降低。</p>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>输入
估计的丢包率$p$、发送速率$f$、有$n$个 tile 的$m$个帧($&lt;i, j&gt;$来表示第$i$个帧的第$j$个 tile</p>
<p>第$&lt;i, j&gt;$个 tile 的大小$v_{i, j}$、第$&lt;i, j&gt;$个 tile 被看到的可能性$\gamma_{i, j}$、</p>
<p>如果第$&lt;i, j&gt;$ 个 tile 没有被恢复的质量降低率、最大延迟$T$</p>
</li>
<li>
<p>输出</p>
<p>第$&lt;i, j&gt;$个 tile 的 FEC 冗余率$r_{i, j} = \frac{冗余包数量}{原始包数量}$</p>
</li>
<li>
<p>最优化问题的形式化
$$
minimize\  \sum_{0&lt;i\le m}\sum_{0&lt;j\le n} \gamma_{i, j}d_{i, j}(p, r_{i, j})
$$</p>
<p>$$
subject\ \ to\ \  \frac{1}{f}\sum_{0&lt;i\le m}\sum_{0&lt;j\le n}v_{i, j}(1+r_{i, j}) \le T
$$</p>
<p>$$
r_{i, j} \le 0
$$</p>
<p>（1）：最小化最终被看到的 tile 的质量衰减的加权和，权重按照被看到的可能性分配。</p>
<p>（2）：经过重新编码的包和原始的包需要在 T 时刻之前发出。</p>
<p>​      Dante 将 1 个 GOP(Group of Pictures)中的所有帧当作一批处理，$T$作为 GOP 的持续时间</p>
<p>​      $f$：使用 TCP Friendly Rate Control algorithm，基于估计的丢包率和网络延迟来计算得出</p>
<p>（3）：确保冗余率总是非负的。</p>
</li>
<li>
<p>关键变量是$d_{i, j}(p, r)$：丢包率是 p 情况下，采用 r 作为冗余率的第$&lt;i, j&gt;$个 tile 的质量衰减
$$
d_{i, j}(p, r) = \delta_{i, j},\ if\ r &lt; \frac{1}{1-p}; 0, otherwise.
$$</p>
<p>假设帧中有 k 个原始包，质量衰减发生在丢失的包不能被恢复的情况下。</p>
<p>FEC 可以容忍 $r \cdot k$ 个丢包=&gt;即当 $p(r<em>k+k)$ 大于  $r</em>k$  时会发生质量衰减。</p>
</li>
<li>
<p>过多的丢包会导致依赖链上所有帧的质量衰减，因此考虑帧之间的依赖关系之后，可以重新计算质量衰减：</p>
<p>$$
d^{*}<em>{i, j}(p, r) = \sum</em>{0&lt;c\le i}w_{c, i}d_{c, j}(p, r)
$$</p>
<p>$w_{c, i}$ 编码帧 i 对帧 c 的依赖作为单独的第 c 个帧的质量衰减的权重；</p>
<p>最终第 i 个帧的第 j 个 tile 的最终质量衰减就是所有依赖的质量衰减的和。</p>
</li>
</ol>
<h2 id="fec-冗余的自适应逻辑">FEC 冗余的自适应逻辑</h2>
<ol>
<li>
<p>关于$d_{i, j}(p, r)$ ：因为是分段函数，所以其值会因为 r 和 p 的大小关系而急剧改变。</p>
<p>利用背包问题的思想可以将其规约成 NP 完全问题：</p>
<p>将每个 tile 看作是一个物品，共有 m*n 个。</p>
<p><strong>如果$r_{i, j} &lt; \frac{1}{1-p}$ ，则表示不把第&lt;i,j&gt;和物品放入背包；否则就是将其放入背包。</strong></p>
<p>公式 1 可以转化为：最大化所有物品二元变量的线性组合；</p>
<p>公式 2 可以转化为：二元变量的另一个线性组合必须低于阈值约束。</p>
<p>因此整个问题就能被完全转化为<strong>0-1 背包</strong>问题</p>
</li>
<li>
<p>算法</p>
<p></p>
<p>整体上是背包问题的标准解法，能以线性复杂度（因为变量只是 B)解决问题。</p>
</li>
</ol>
<h2 id="原型设计">原型设计</h2>
<p></p>
<ul>
<li>使用基于 TCP 和 UDP 的两条连接来分别传输控制信息（双向：到客户端的播放会话的起至点和到服务端的网络信息反馈）和视频数据包</li>
<li>服务端根据反馈的网络信息，在每个 GOP 的边界时刻运行算法 1 来确定下一个 GOP 的帧和 tile 的 FEC 冗余。
确定之后服务端使用 RS 码来插入冗余包，和原始视频数据包一起重新编码，并使用基于 TFRC 的发送率发送数据。</li>
<li>Dante 的实现是对应用程序级比特率适配策略的补充，并且可以通过对视频播放器进行最小更改来替换现有的底层传输协议来部署。</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>环境：使用 Gilbert 模型来模拟实现丢包事件（而非使用统一随机丢包）</p>
<p>创造了两种网络条件 good（丢包率 0.5%）和 bad（丢包率 2%）</p>
</li>
</ul>
<h2 id="局限性">局限性</h2>
<ul>
<li>效果主要依赖于 Viewport 预测的结果是否准确</li>
</ul>
]]></description>
</item>
<item>
    <title>沉浸式流媒体传输的实际度量</title>
    <link>http://localhost:1313/posts/papers/note11/</link>
    <pubDate>Mon, 22 Nov 2021 15:21:59 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>http://localhost:1313/posts/papers/note11/</guid>
    <description><![CDATA[<h2 id="度量指标">度量指标</h2>
<ol>
<li>viewport 预测精度。
<ul>
<li>使用预测的 viewport 坐标和实际用户的 viewport 坐标的大圈距离来量化。</li>
</ul>
</li>
<li>视频质量。
<ul>
<li>viewport 内部的 tile 质量（1～5）。</li>
<li>tile 在最高质量层之上花费的时间。</li>
<li>根据用户视线的分布而提出的加权质量度量。</li>
</ul>
</li>
</ol>
<h2 id="度量参数">度量参数</h2>
<ol>
<li>分块策略</li>
<li>带宽</li>
<li>延迟</li>
<li>viewport 预测</li>
<li>HTTP 版本</li>
<li>持久化的连接数量</li>
</ol>
]]></description>
</item>
</channel>
</rss>

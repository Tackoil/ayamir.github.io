<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - Ayamir&#39;s Blog</title>
        <link>https://ayamir.github.io/posts/</link>
        <description>所有文章 | Ayamir&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miracle_l@bupt.edu.cn (Ayamir)</managingEditor>
            <webMaster>miracle_l@bupt.edu.cn (Ayamir)</webMaster><lastBuildDate>Sat, 15 Jan 2022 18:46:02 &#43;0800</lastBuildDate><atom:link href="https://ayamir.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>Note for srlABR Cross User</title>
    <link>https://ayamir.github.io/posts/note-for-srlabr-cross-user/</link>
    <pubDate>Sat, 15 Jan 2022 18:46:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-srlabr-cross-user/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9234071" target="_blank" rel="noopener noreffer">Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network</a></p>
<p>Level：IEEE Transactions on Broadcasting 2021</p>
<p>Keywords：Cross-user vp, Sequetial RL ABR</p>
<h2 id="主要工作">主要工作</h2>
<ul>
<li>使用跨用户注意力网络<code>CUAN</code>来做VP；</li>
<li>使用<code>360SRL</code>来做ABR</li>
<li>将上面两者集成到了推流框架中；</li>
</ul>
<h2 id="vp">VP</h2>
<h3 id="motivation">Motivation</h3>
<p>形式化VP问题如下：</p>
<p>给出 $p^{th}$ 用户的 $1-t$ 时间内的历史视点坐标 $L^{p}_{1:t} = \lbrace l^p_1, l^p_2, &hellip;, l^p_t \rbrace$ ，其中 $l^p_t = (x_t, y_t), x_t \in [-180, 180]; y_t \in [-90, 90]$ ；</p>
<p>同一视频的不同用户视点表示为 $L^{1:M}_{1:t+T}$ ， $M$ 表示其他用户的数量；</p>
<p>目标是预测未来的 $T$ 个时刻的视点位置 $L^p_i, i = t+1, &hellip;, t+T$ ；</p>
<p>最终可以用数学公式表达为：
$$
\underset{F}{min} \sum^{t+T}_{k = t+1} {\parallel l^p_k - \hat{l}^p_k \parallel}_1
$$</p>
<p>现有的用<code>KNN</code>做的跨用户预测基于LR的模型，而LR的模型很容易产生偏差，所以为了增强<code>KNN</code>的性能，同时考虑单用户的历史视点轨迹和跨用户的视点轨迹。</p>
<ul>
<li>提出一种注意力机制来自动提取来自其他用户视口的有用信息；</li>
<li>对于与当前用户有相似偏好的用户轨迹信息给与更多的注意；</li>
<li>相似性通过基于过去时间段内其他用户的轨迹计算出来；</li>
</ul>
<h3 id="design">Design</h3>
<p></p>
<ol>
<li>
<p>轨迹编码器模块从用户的历史视点位置提取时间特征；</p>
<p>使用<code>LSTM</code>来编码用户的观看路径；</p>
<p>为了预测 ${(t+1)}^{th}$ 帧的视点位置，首先向<code>LSTM</code>输入 $p^{th}$ 用户的历史视点坐标：
$$
f^{p}_{t+1} = h(l^p_1, l^p_2, &hellip;, l^p_t)
$$
$h(\cdot)$ 是<code>LSTM</code>的输入输出函数；</p>
<p>接着使用相同的<code>LTSM</code>编码其他用户的观看轨迹：
$$
f^{i}<em>{t+1} = h(l^i_1, l^i_2, &hellip;, l^i</em>{t+1}), i \in \lbrace 1, &hellip;, M \rbrace
$$</p>
</li>
<li>
<p>注意力模块从其他用户的视点轨迹中提取与 $p^{th}$ 用户相关的信息</p>
<p>首先推导出 $p^{th}$ 用户和其他用户之间的相关系数：
$$
s^{pi}<em>{t+1} = z(f^{i}</em>{t+1}, l^{p}<em>{t+1}), i \in \lbrace 1, &hellip;, M \rbrace \cup \lbrace p \rbrace;
$$
$s^{th}</em>{t+1}$ 表示 $p^{th}$ 用户和 $i^{th}$ 用户之间的相似性；$z()$ 由内积运算建模（还可用其他方式建模比如多个FC层）；</p>
<p>接着将相关系数规范化：
$$
{\alpha}^{pi}<em>{t+1} = \frac{e^{s^{pi}</em>{t+1}}}{\sum_{i \in \lbrace 1,&hellip; M \rbrace \cup {\lbrace p \rbrace}^{e^{s^{pi}<em>{t+1}}}}}
$$
最后得到融合特征：
$$
g^{p}</em>{t+1} = \sum_{i \in {\lbrace 1,&hellip;M \rbrace \cup \lbrace p \rbrace}} {\alpha}^{pi}<em>{t+1} \cdot f^{i}</em>{t+1}
$$
融合特征被最后用于VP。</p>
</li>
<li>
<p>VP模块预测 ${(t+1)}^{th}$ 帧的视点位置</p>
<p>$$
\hat{l}^{p}<em>{t+1} = r(g^{p}</em>{t+1})
$$
函数 $r(\cdot)$ 由一层FC建模。值得注意的是，对应于未来 T 帧的视点是以滚动方式预测的。</p>
</li>
</ol>
<h3 id="loss">Loss</h3>
<p>损失函数定义为预测的视点位置和实际视点位置之间的所有绝对差异的总和：
$$
L = \sum^{t+T}_{i=t} {|\hat{l}^p_i - l^p_i|}_1
$$</p>
<h3 id="details">Details</h3>
<ul>
<li>使用<code>PyTorch</code>实现；</li>
<li>函数 $h(\cdot)$ 由两个堆叠的<code>LSTM</code>层组成，两者都有32个神经元；</li>
<li>函数 $r(\cdot)$ 包含一个带有32个神经元的FC层，接着是<code>Tanh</code>函数；</li>
<li>历史视点和未来视点的长度设定为1秒和5秒；</li>
<li>每次迭代从数据集中随机产生2048个样本；</li>
<li>所有训练变量的优化函数采用<code>Adam</code>；</li>
<li>$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$；</li>
<li>$learning\ rate = 10^{-3}, training\ epoch = 50$；</li>
</ul>
<h2 id="abr">ABR</h2>
<h3 id="formulation">Formulation</h3>
<p>全景视频被切分成 $m$ 个长度为 $T$ 秒的视频片段，每个视频片段空间上划分成 $N$ 个分块，分别以 $M$ 个不同的码率等级编码。因此对于每段有 $N \times M$ 个可选的编码块。</p>
<p>ABR的目标是为每个片段找到最优的码率集 $X = \lbrace x_{i, j} \rbrace \in Z^{N \times M}$ （ $x_{i, j} = 1$ 意味着为 $i^{th}$ 块选择 $j^{th}$ 的码率等级）：
$$
\underset{X}{max} \sum^{m}_{t=1} Q_t
$$
$Q_t$ 表示 $t^{th}$ 段的QoE分数，与以下几个方面有关：</p>
<ul>
<li>
<p>VIewport Quality：
$$
Q^1_t = \sum^{N}<em>{i=1} \sum^{M}</em>{j=1} x_{i,j} \cdot p_i \cdot r_{i,j}
$$
$p_i$ 表示 $i^{th}$ 分块的规范化观看概率； $r_{i,j}$ 记录块 $(i, j)$ 的码率；</p>
</li>
<li>
<p>Viewport Temporal Variation：
$$
Q^2_t = |Q^1_t - Q^{1}_{t-1}|
$$</p>
</li>
<li>
<p>Viewport Spatial Variation：
$$
Q^3_t = \frac{1}{2} \sum^{N}_{i=1} \sum_{u \in U_i} p_i \cdot p_u \sum^{M}_{j=1} |x_{i,j} \cdot r_{i,j} - x_{u,j} \cdot r_{u,j}|
$$
$U_i$ 表示 $i^{th}$ 个分块的1跳邻居中的tile索引<a href="https://ieeexplore.ieee.org/document/8486606" target="_blank" rel="noopener noreffer">[1]</a>；</p>
</li>
<li>
<p>Rebuffering：
$$
Q^4_t = max(\frac{\sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot r_{i,j} \cdot T}{\xi_t} - b_{t-1}, 0)
$$
$\xi_t$ 表示网络吞吐量； $b_{t-1}$ 表示播放器的缓冲区占用率；</p>
<p>最终的QoE可以由上面的指标定义：
$$
Q_t = Q^1_t - \eta_1 \cdot Q^2_t - \eta_2 \cdot Q^3_t - \eta_3 \cdot Q^4_t
$$
$\eta_*$ 是可调节的参数，与不同的用户偏好对应。</p>
</li>
</ul>
<h3 id="sequential-rl-based-abr">Sequential RL-Based ABR</h3>
<p>假设基于tile的全景推流ABR过程也是MDP。</p>
<p></p>
<p>细节在<a href="https://ayamir.github.io/posts/note-for-360srl/" target="_blank" rel="noopener noreffer">360SRL</a>中已经说明清楚。</p>
]]></description>
</item>
<item>
    <title>Note for 360SRL</title>
    <link>https://ayamir.github.io/posts/note-for-360srl/</link>
    <pubDate>Thu, 13 Jan 2022 12:08:36 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-360srl/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/8784927" target="_blank" rel="noopener noreffer">360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming</a></p>
<p>Level：ICME 2019</p>
<p>Keywords：ABR、RL、Sequential decision</p>
<h2 id="创新点">创新点</h2>
<ul>
<li>在MDP中，将N维决策空间内的一次决策转换为1维空间内的N次级联顺序决策处理来降低复杂度。</li>
</ul>
<h2 id="问题定义">问题定义</h2>
<p>原始的全景视频被划分成每段固定长度为 $T$ 的片段，</p>
<p>每个片段包含 $N$ 个分块，并以 $M$ 的码率等级独立编码，</p>
<p>因此对每个片段，有 $N \times M$ 种可选的编码块。</p>
<p>为了保证播放时的流畅性，需要确定最优的预取集合：</p>
<p>${a_0, &hellip;, a_i, &hellip;, a_{N-1}}, i \in \lbrace 0, &hellip;, N-1 \rbrace, a_i \in \lbrace 0, &hellip;, M-1 \rbrace $</p>
<p>分别用 $q_{i, a_i}$ 和 $w_{i, a_i}$ 表示码率选择为 $a^{th}_i$ 的 $i^{th}$ 分块的质量和相应的分块片段大小。</p>
<p>用 $p_i \in [0, 1]$ 表示 $i^{th}$ 块的被看到的可能性。</p>
<h2 id="顺序abr决策">顺序ABR决策</h2>
<p></p>
<h2 id="代理设计">代理设计</h2>
<h3 id="状态">状态</h3>
<p>对于 $i^{th}$ 维，输入状态包括原始的环境状态 $s_t$ ；</p>
<p>与之前维度的动作集合相关的信号： $u^{i}_{s_t} = \lbrace Th, C_i, p_{0:i-1}, q_{0:i-1}, b_t, p_i, S_i, Q_{t-1} \rbrace$</p>
<p>$Th$ ：表示过去 m 次下载一个段的平均吞吐量；</p>
<p>$C_i \in R^M$ ：表示 $i^{th}$ 个分块的可用块大小向量；</p>
<p>$p_{0:i-1}$ 和 $q_{0:i-1, a^{0:i-1}_{t}}$ 分别表示选中的码率集合和看到之前 $i-1$ 个分块的概率集；</p>
<p>$b_t$ 是缓冲区大小；</p>
<p>$p_i$ 是 $i^{th}$ 个分块被看到的可能性；</p>
<p>$S_i$ 是之前选择的 $i-1$ 个分块的块大小之和： $S_i = \sum^{i-1}_{h=0} C_{h, a^h_t}$ ；</p>
<p>$Q_{t-1}$ 记录了最后一个段中 $N$ 个分块的平均视频质量；</p>
<h3 id="动作">动作</h3>
<p>动作空间离散，代理输出定义为价值函数：$f(u^i_{s_t}, a^i_t)$</p>
<p>表示所选状态的价值 $a^i_t \in \lbrace 0, &hellip;, M-1 \rbrace$ 处于状态 $u_{s_t}^i$ .</p>
<h3 id="回报">回报</h3>
<p>回报定义为下列因素的加权和：</p>
<p>平均视频质量 $q^{avg}_t$，空间视频质量方差 $q^{s_v}_t$，时间视频质量方差 $q^{t_v}_t$ ，重缓冲时间 $T^r_t$</p>
<p>$$
q^{avg}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot q_{i, a_i}
$$</p>
<p>$$
q^{s_v}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot |q_{i, a_i} - q^{avg}_t|
$$</p>
<p>$$
q^{t_v}_t = |q^{avg}_{t-1} - q^{avg}_t|
$$</p>
<p>$$
T^r_t = max \lbrace T_t - b_{t-1}, 0 \rbrace
$$</p>
<p>$$
R_t = w_1 \cdot q^{avg}_t - w_2 \cdot q^{s_v}_t - w_3 \cdot q^{t_v}_t - w_4 \cdot T^r_t
$$</p>
<h2 id="训练方法">训练方法</h2>
<p>使用<code>DQN</code>作为基本的算法来学习动作-价值函数 $Q(s_t, a_t; \theta)$ ，其中 $\theta$ 作为参数，对应的贪心策略为 $\pi(s_t; \theta) = \underset{\theta}{argmax} Q(s_t, a_t; \theta)$ 。</p>
<p><code>DQN</code>网络的关键想法是更新最小化损失函数的方向上的参数：
$$
L(\theta) = E[y_t - Q(s_t, a_t; \theta)]
$$</p>
<p>$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \pi(s_{t+1}; {\theta}'); {\theta}')
$$
${\theta}'$ 表示固定且分离的目标网络的参数；</p>
<p>$r(\cdot)$ 是即时奖励函数，即上面公式5中的 $R_t$ ；</p>
<p>$\gamma \in [0, 1]$ 是折扣因子；</p>
<p>为了缓解过拟合，引入 <code>double-DQN</code> 的结构，所以公式7被重写为：
$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, {\pi}(s_{t+1}; \theta); {\theta}')
$$
利用公式6和公式8可以得出 $i^{th}$ 维的暂时损失函数：
$$
l^i_t = Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$
其中 $Q_{target}$ 满足：</p>
<p>$$
Q_{target} = r_t + {\gamma}_u \cdot Q(u^0_{s_{t+1}}, \pi(u^0_{s_{t+1}}; 0); {\theta}')
$$</p>
<p>${\gamma}_u$ 和 ${\gamma}_b$ 分别代表”Top MDP“和”Bottom MDP“的折扣因子，训练中设定 ${\gamma}_b = 1$ 。</p>
<p>观察公式9和公式10可以看出每维都有相同的目标函数，意味着无法区别每个独立维度的动作 $a^i_t$ 对 $r_t$ 的贡献。</p>
<p>为了克服限制，根据某个分块的动作 $a^i_t$ 与其观看概率成正比的先验知识，向 $l^i_t$ 添加一个额外的 $r^i_{extra}$ ：
$$
l^i_t = r^i_{extra} + Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$</p>
<p>$$
r^i_{extra} =
\begin{cases}
0, p_i &gt; P ;
\
-a^i_t, p_i \le P
\end{cases}
$$</p>
<p>通过设定一个观看概率的阈值 $P$ ，对观看概率低于 $P$ 但选择了高码率的分块施加 $-a^i_t$ 的奖励。</p>
<p>因此最终的平均损失可以形式化为：
$$
l^{avg}_t = \frac{1}{N} \sum^{N-1}_{i=0} l^i_t
$$
接着使用梯度下降法来更新模型，学习率设定为 $\alpha$：
$$
\theta \larr \theta + \alpha \triangledown l^{avg}_t
$$
同时，在训练阶段利用经验回放法来提高<code>360SRL</code>的泛化性。</p>
<p></p>
<p></p>
<h2 id="实现细节">实现细节</h2>
<p></p>
<p>特征从输入状态中通过特征提取网络提取出来。</p>
<p>初始的4个输入通过带有128个过滤器的1维卷积层被传递，4个输入核心大小分别为 $1 \times m$ 、 $1 \times M$ 、 $1 \times N$ 、 $1 \times M$ ，后续这4个输入被喂给有128个神经元的全连接层；</p>
<p>随后特征映射被连接成一个张量，接着是具有1024个神经元和256个神经元的前向网络；</p>
<p>整个动作-价值网络的输出是M维的向量。</p>
<p>特征提取层和前向网络层都使用 <code>Leaky-ReLU</code>作为激活函数，最后是层归一化层。</p>
]]></description>
</item>
<item>
    <title>Summary for VP</title>
    <link>https://ayamir.github.io/posts/summary-for-vp/</link>
    <pubDate>Fri, 07 Jan 2022 23:08:36 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/summary-for-vp/</guid>
    <description><![CDATA[<h2 id="vp-在传输中所处的作用">VP 在传输中所处的作用</h2>
<p>基于 tile 的全景视频传输方式之所以热门，就是因其可以通过只传输用户 FoV 内的分块而大幅减少观看过程中消耗的带宽。</p>
<p>所以对用户 FoV 的预测是首先要处理的因素，如果 VP 精度很高，那么所有的带宽都可以用很高的码率去传输 FoV 内的分块。</p>
<h2 id="两种方式的基本假设">两种方式的基本假设</h2>
<ul>
<li>
<p>基于轨迹的方法的基本假设</p>
<p>相对于当前时刻，前 $hw$ (history window)内用户的 FoV 位置对未来可预测的 $pw$ (predict window)内用户的 FoV 位置有影响，比如用户只有很小可能性会在很短的一段单位时间内做 180 度的转弯，而更小角度的调整则更可能发生。</p>
</li>
<li>
<p>基于内容的方法的基本假设</p>
<p>用户的FoV变化是因为对视频内容感兴趣，即ROI与FoV之间有相关关系，比如在观看篮球比赛这样的全景视频时，用户的FoV更可能专注于篮球。</p>
<p>按照提取ROI的来源不同可以分为两种类型：</p>
<ol>
<li>从视频内容本身出发，使用CV方法去猜测ROI；</li>
<li>从用户观看视频的热图出发，相当于得到了经过统计之后的平均FoV分布，以此推测其他用户的ROI；</li>
</ol>
</li>
</ul>
<p>基于轨迹的方式是要在最表层的历史和预测的轨迹之间学习，即假设两者之间只有时空关系。</p>
<p>跨用户的方式则假设由用户群体所得出的热图可以用来预测单个用户的FoV，即利用共性来推断个性。</p>
<p>基于内容的方式直接提取视频显著图来推断FoV，即进一步假设共性与视频内容本身有关系。</p>
<h2 id="跨用户预测的概念">跨用户预测的概念</h2>
<ul>
<li>
<p>基本假设</p>
<p>就单个用户而言，在观看视频过程中其FoV的变化看似随机，但是其行为可能从用户群体的角度去看是跨用户相通的，即多个用户在观看视频时可能会表现出相似的，可以学习的行为模式，这种行为模式可以帮助提高VP的精度。</p>
</li>
<li>
<p>实际应用</p>
<p>基于轨迹的跨用户：如果训练的模型是基于轨迹的离线模型如LSTM，那么实际上训练好的模型已经学习到了这种跨用户的行为模式；而如果采用的是边训练边预测的模型如LR（输入历史窗口的经纬度数据，输出预测窗口的经纬度数据），那么这样的模型就是纯粹的单用户模型。</p>
<p>基于内容的跨用户：将用户在观看视频帧时的注意点作为研究对象，找到用户群体在面对同一帧视频时共同关注的空间区域，而这就是用户间相似的行为模式。这种与内容相结合的跨用户方式即为实际研究中所指的跨用户的研究方式。（实际上就是基于内容的研究方法，只不过出发点不是视频本身，而是用户在观看视频时的FoV）</p>
</li>
</ul>
<h2 id="实际应用">实际应用</h2>
<p></p>
<ul>
<li>
<p>图中3个黄色矩形表示3种方法：</p>
<ol>
<li>
<p>ROI extract：基于内容的预测</p>
</li>
<li>
<p>Multiple watchers' FoV：跨用户的预测</p>
</li>
<li>
<p>Multiple watchers' trajectories：基于轨迹的预测</p>
</li>
</ol>
</li>
<li>
<p>绿色渐变矩形表示直接使用用户当前的历史轨迹数据去训练模型，接着做出预测。</p>
</li>
</ul>
<h2 id="研究方法">研究方法</h2>
<ul>
<li>
<p>基于轨迹的方法</p>
<p>在线训练：输入历史窗口的位置信息，不断迭代修正模型，输出预测窗口的位置信息。</p>
<p>离线训练：输入任何采样条件下的多对hw和pw信息来拟合模型。</p>
</li>
<li>
<p>跨用户的方法</p>
<p>求出多个用户在同一帧上的热图，以此作为FoV预测的依据。</p>
</li>
<li>
<p>基于内容的方法</p>
<p>提取视频帧中的显著图，以此作为FoV预测的依据。</p>
</li>
</ul>
<h2 id="优点">优点</h2>
<ol>
<li>使用回归实现的在线训练模型实现简单，反应迅速，有优秀的短期预测精度。</li>
<li>跨用户的热图可以帮助长期的预测，可以提供合理的离线全视频FOV预测，并具有一致的性能。（因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入）</li>
<li>显著图对于ROI集中突出的预测效果较好。</li>
</ol>
<h2 id="缺点">缺点</h2>
<ol>
<li>使用回归实现的在线训练模型在预测窗口增大时，性能会显著下降。</li>
<li>提取显著图的方式一方面训练开销比较大，另一方面对于ROI不够集中突出的视频效果并不好。</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for Content Assisted Prediction</title>
    <link>https://ayamir.github.io/posts/note-for-content-assisted-prediction/</link>
    <pubDate>Thu, 06 Jan 2022 15:17:33 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-content-assisted-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://www.researchgate.net/publication/333971523_Content_Assisted_Viewport_Prediction_for_Panoramic_Video_Streaming" target="_blank" rel="noopener noreffer">Content Assisted Viewport Prediction for Panoramic Video Streaming</a></p>
<p>Level：IEEE CVPR 2019 CV4ARVR</p>
<p>Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion</p>
<h2 id="主要工作">主要工作</h2>
<h3 id="基于轨迹预测">基于轨迹预测</h3>
<p>输入：历史窗口轨迹</p>
<p>模型：64个神经元的单层LSTM，在输入层后面加上一个额外的减法层进行点归一化，以及一个加法层来恢复输出之前的值；用ADAM进行优化，MAE作为损失函数。</p>
<h3 id="跨用户热图">跨用户热图</h3>
<p>除了观看者自己的历史FOV轨迹之外，其他观看者对同一视频帧的观看方向也有启发性。</p>
<p></p>
<p>对视频的每一帧，首先收集用户的观看方向（坐标使用原始的来自三个方向的欧拉角表示，而非经纬度）。</p>
<p>接着将坐标投影到用经纬度表示的180x360像素的平面图上，对于图中的每个像素点，可以数出其被看到的次数；并对周围像素应用二维高斯光滑。</p>
<p>上面的过程可以为视频生成热图：</p>
<p></p>
<h3 id="视频帧的显著图">视频帧的显著图</h3>
<p>鉴于观看相同的全景视频时跨用户行为的共性，进一步假设是内容促使多个观众观看公共区域，因此提取出每个帧的显著图可能会表明用户的RoI。</p>
<p>对特定的视频帧，应用经典的特征密集型方法——Ittykoch，它首先根据强度、边缘、颜色和方向将图像分解为多个特征通道，然后将它们组合成识别显著区域。</p>
<p>除了在静态视频帧上检测显著性之外，进一步进行背景减法来减少不太可能感兴趣的区域：应用基于高斯混合的背景/前景分割算法，高级思想是在连续帧之间临时过滤变化的像素点。</p>
<p>结合上面这两个过程可以为视频帧提取时间显著图。</p>
<p></p>
<h3 id="多模态融合">多模态融合</h3>
<p></p>
<p>使用包含3个LSTM分支的深度学习模型来融合上述的几种预测方式的结果。</p>
<p>基于轨迹的LSTM（图中绿色分支）从历史窗口 $hw$ 中接受 $n$ 个坐标的输入，接着预测未来窗口 $pw$ 中的 $m$ 个坐标，用 $trj_y_{i}$ 表示；</p>
<p>基于热图的LSTM（图中蓝色分支）将每个预测步骤对应的视频帧的热图作为输入，并在 $pw$ 中输出第2组 $m$ 个坐标的预测，用 $ht_y_{i}$ 表示：</p>
<p>对于每个热图，让其通过3个卷积层，每个卷积层后面都有一个最大池化层。然后，在此图像特征提取之后，应用展平步骤和1个密集层来回归坐标（经纬度表示）。</p>
<p>基于显著图的LSTM采用与热图相似的架构，将显著图作为输入，在 $pw$ 中输出第3组 $m$ 个坐标的预测，用 $sal_y_{i}$ 表示。</p>
<p>对热图和显著图的分支，应用 <code>TimeDistributed</code>层，以便其参数在预测步骤中保持一致。</p>
<p>最终在每个预测步骤连接 $trj_y$ ， $ht_y$，和 $sal_y$ ，并产生一个最终输出 $y$ 。</p>
<p>每个模型的损失函数采用MAE，优化函数采用ADAM。</p>
<p>为每个分支的输出以及最终的输出都检查损失，单独和联合地去调整其参数。</p>
<h2 id="评估">评估</h2>
<p>使用2折的交叉验证。</p>
<h3 id="超参数">超参数</h3>
<ol>
<li>$pw$ 的大小：0.1s，1.0s，2.0s；</li>
<li>$hw$ 的大小：0.05s，0.6s，1.0s；（分别与上面的 $pw$ 对应）</li>
<li>用于训练的用户数：[3, 10, 30]</li>
</ol>
<p></p>
<h3 id="结果与分析">结果与分析</h3>
<ol>
<li>所有模型的预测精度随着 $pw$ 的增长而下降，表明长期预测问题更难解决；</li>
<li>所有模型的精度预测误差几乎是纬度预测误差的二倍，可能由于运动区域在水平方向的翻倍；</li>
<li>线性回归模型只有在 $pw$ 很短的时候预测精确，随着 $pw$ 的增长，其预测精度会迅速下降；</li>
<li>基于 LSTM 的轨迹模型始终优于所有 $pw$ 的基线模型，但更多的训练观众无助于显着提高准确性。</li>
<li>跨用户的热图和显著图可以帮助长期的预测，可以提供合理的离线全视频FOV预测，并具有一致的性能（因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入），当 $pw$ 增长时，其预测精度超过了基于历史轨迹的模型；</li>
<li>结合3种模型之后，可以平衡来自历史轨迹、跨用户兴趣和内容显著性的输入，不论 $pw$ 长或短都能产生优化的预测结果；</li>
</ol>
<h3 id="例外情况">例外情况</h3>
<p></p>
<p>M3在经度上的表现并不适用于上面图中标示的两个视频（Mega.Coaster和GTR.Drives.First.Ever）</p>
<p>原因分析：</p>
<p>这两个视频的共同特点是在驾驶路径的一侧具有高运动内容的驾驶内容，因此用户在观看这些视频时，大多数FOV始终以行驶轨迹为中心。因此用户不太可能改变其观看方向，这导致即使 $pw = 2.0s$ 时，单一基于轨迹的模型的预测精度也更高。相比之下，从对内容角度出发的分析无济于事，但可能会引入观众可能会忽略的变道，进而造成预测误差。</p>
]]></description>
</item>
<item>
    <title>Note for GPAC</title>
    <link>https://ayamir.github.io/posts/note-for-gpac/</link>
    <pubDate>Thu, 30 Dec 2021 10:23:26 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-gpac/</guid>
    <description><![CDATA[<h2 id="dash客户端自适应逻辑">Dash客户端自适应逻辑</h2>
<ol>
<li><em>tile priority setup</em>：根据定义的规则对tile进行优先级排名。</li>
<li><em>rate allocation</em>：收集网络吞吐量信息和tile码率信息，使用确定的tile优先级排名为其分配码率，努力最大化视频质量。</li>
<li><em>rate adaption</em>：在播放过程中，执行码率自适应算法，基于播放速度、质量切换的次数、缓冲区占用情况等。</li>
</ol>
<h3 id="tile-priority-setup">tile priority setup</h3>
<ol>
<li>
<p>Dash客户端加载带有SRD信息的MPD文件时，首先确定使用SRD描述的tile集合。</p>
</li>
<li>
<p>确定tile之间的编码依赖（尤其是使用HEVC编码的tile时）</p>
</li>
<li>
<p>为每个独立的tile向媒体渲染器请求一个视频对象，并向其通知tile的SRD信息。</p>
</li>
<li>
<p>渲染器根据需要的显示大小调整SRD信息之后，执行视频对象的最终布局。</p>
</li>
<li>
<p>一旦tile集合被确定，客户端向每个tile分配优先级。（每次码率自适应执行的时候都需要分配tile优先级）</p>
<p></p>
</li>
</ol>
<h3 id="rate-allocation">Rate allocation</h3>
<ol>
<li>首先需要估计可用带宽（tile场景和非tile场景的估计不同）</li>
<li>在一个视频段播放过程中，客户端需要去下载多个段（并行-HTTP/2）</li>
<li>带宽可以在下载单个段或多个段的平均指标中估计出来。</li>
<li>一旦带宽估计完成，码率分配将tile根据其优先级进行分类。</li>
<li>一开始所有的tile都分配成最低的优先级对应的码率，然后从高到低依次增长优先级高的tile的码率。</li>
<li>一旦每个tile的码率分配完成，将为目标带宽等于所选比特率的每个tile调用常规速率自适应算法</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for MPC</title>
    <link>https://ayamir.github.io/posts/note-for-mpc/</link>
    <pubDate>Thu, 23 Dec 2021 10:39:32 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-mpc/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/2785956.2787486" target="_blank" rel="noopener noreffer">A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP</a></p>
<p>Level：ACM SIGCOMM 15</p>
<p>Keywords：Model Predictive Control，ABR，DASH</p>
<h2 id="motivation">Motivation</h2>
<p>关于码率自适应的逻辑，现有的解决方案还没有形成清晰的、一致的意见。不同类型的方案之间优化的出发点并不相同，比如基于速率和基于缓冲区，而且没有广泛考虑各方面的因素并形成折中。</p>
<p>文章引入了控制论中的方法，将各方面的影响因素形式化为<em>随机优化控制</em>问题，利用<strong>模型预测控制MPC</strong>将两种不同出发点的解决方案结合到一起，进而解决其最优化的问题。而仿真结果也证明，如果能运行一个最优化的MPC算法，并且预测误差很低，那么MPC方案可以优于传统的基于速率和基于缓冲区的策略。</p>
<h2 id="背景">背景</h2>
<ul>
<li>播放器端为QoE需要考虑的问题：
<ol>
<li>最小化冲缓冲事件发生的次数；</li>
<li>在吞吐量限制下尽可能传输码率较高的视频；</li>
<li>最小化播放器开始播放花费的时间（启动时间）；</li>
<li>保持播放过程平滑，尽可能避免大幅度的码率变化；</li>
</ol>
</li>
<li>这些目标相互冲突的原因：
<ol>
<li>最小化重缓冲次数和启动时间会导致只选择最低码率的视频；</li>
<li>尽可能选择高码率的视频会导致很多的重缓冲事件；</li>
<li>保持播放过程平滑可能会与最小的重缓冲次数与最大化的平均码率相冲突；</li>
</ol>
</li>
</ul>
<h2 id="控制论模型">控制论模型</h2>
<h3 id="视频推流模型">视频推流模型</h3>
<ol>
<li>
<p>参数形式化</p>
<ul>
<li>
<p>将视频建模成连续片段的集合，即：$V = \lbrace 1, 2, &hellip;, K \rbrace$，每个片段长为$L$秒；</p>
</li>
<li>
<p>每个片段以不同码率编码，$R$ 作为所有可用码率的集合；</p>
</li>
<li>
<p>播放器可以选择以码率$R_k \in R$ 下载第$k$块片段，$d_k(R_k)$ 表示以码率$R_k$编码的视频大小；</p>
<ul>
<li>对于恒定码率CBR的情况，$d_k(R_k) = L \times R_k$；</li>
<li>对于变化码率VBR的情况，$d_k \sim R_k$；</li>
</ul>
</li>
<li>
<p>选择的码率越高，用户感知到的质量越高：</p>
<p>$q(\cdot):R \rightarrow \R_+$ 是一个不减函数，是选择的码率 $R_k$ 到用户感知到的视频质量 $q(R_k)$ 的映射；</p>
</li>
<li>
<p>片段被下载到<em>回访缓冲</em>中，其中包含下载了的但还没看过的片段。</p>
</li>
<li>
<p>$B(t) \in [0, B_{max}]$ 表示 $t$ 时刻缓冲区的占用， $B_{max}$ 表示内容提供商的策略和播放器的存储限制；</p>
</li>
</ul>
</li>
<li>
<p>播放过程形式化</p>
<p>在 $t_k$ 时刻，视频播放器开始下载第 $k$ 个块，这个块的下载时间可以计算为： $d_k(R_k) / C_k$； $C_k$ 表示下载过程中经历的平均下载速度；</p>
<p>一旦第 $k$ 个块下载完毕，播放器等待 $\Delta t_k$ 时间并在 $t_{k+1}$ 时刻下载下一个块 $k+1$ ；</p>
<p>假设等待时间 $\Delta t_k$ 很短并且不会导致重缓冲事件，用 $C_t$ 表示 $t$ 时刻的网络吞吐量：
$$
t_{k+1} = t_k + \frac{d_k(R_k)}{C_k} + \Delta t_k
$$</p>
<p>$$
C_k = \frac{1}{t_{k+1} - t_k - \Delta t_k} \int_{t_k}^{t_{k+1} - \Delta t_k} C_t dt
$$</p>
<p>$B(t)$ 的变化取决于下载的块和播放的块的数量：</p>
<p>在第 $k$ 个块下载完毕之后缓冲区占用增长 $L$ 秒；用户观看一个块之后缓冲区占用减少 $L$ 秒；</p>
<p>$B_k = B(t_k)$ 表示播放器开始下载第 $k$ 个块时的缓冲区占用；</p>
<p>缓冲区占用的动态变化可以表示为：
$$
B_{k+1} = \big( (B_k - \frac{d_k(R_k)}{C_k})_+ + L - \Delta t_k \big)_+
$$
其中 $(x)_+ = max\lbrace x, 0 \rbrace $ 确保其非负；</p>
<p>如果 $B_k &lt; d_k(R_k) / C_k$ ，表示缓冲区在播放器还在下载第 $k$ 个块时变空，而这会导致重缓冲事件；</p>
<p></p>
<p>等待时间 $\Delta t_k$ 的确定也称为<em>块调度</em>问题，本文中假设播放器在第 $k$ 个块下载完毕之后尽可能快地去下载第 $k+1$ 个块（除了缓冲区满了的情况，播放器等待缓冲区中的块被消耗之后再下载新的块）：
$$
\Delta t_k = \Big( \big( B_k - \frac{d_k(R_k)}{C_k} \big)_+ + L - B_max \Big)_+
$$</p>
</li>
</ol>
<h3 id="qoe最大化问题">QoE最大化问题</h3>
<p>QoE的组成部分：</p>
<ol>
<li>
<p>平均视频质量：在所有块中每个块平均的质量，计算为：
$$
\frac{1}{K} \sum^K_{k=1} q(B_k)
$$</p>
</li>
<li>
<p>平均质量变化：相邻块之间质量变化的平均值，计算为：
$$
\frac{1}{K-1} \sum^{K-1}_{k=1} | q(R_{k+1}) - q(R_k) |
$$</p>
</li>
<li>
<p>重缓冲总计时间：对每个块而言，当轮到其被消耗时但下载块的过程还没完成即出现了重缓冲，总时间计算为：
$$
\sum^K_{k=1} (\frac{d_k(R_k)}{C_k} - B_k)_+
$$</p>
</li>
<li>
<p>启动延迟 $T_s$ ，假设 $T_s \ll B_{max}$ 。</p>
</li>
</ol>
<p>对不同用户而言，上述4种因素的重要程度不同。使用上述分量的加权，定义视频块 $1$ 到 $K$ 的QoE：
$$
QoE^K_1 = \sum^K_{k=1} q(R_k) - \lambda \sum^K_{k=1} | q(R_{k+1}) - q(R_k) | - \mu \sum^K_{k=1} (\frac{d_k(R_k)}{C_k} - B_k)_+ - \mu_s T_s,\
\lambda, \mu, \mu_s \nless 0
$$
相对较小的 $\lambda$ 表示用户不太关心视频质量变化； $\lambda$ 越大表明越需要使视频质量变得光滑。</p>
<p>相对较大的 $\mu$ 表示用户很在意重缓冲；</p>
<p>在这里文章倾向于启动延迟很低，所以采用大 $\mu_s$ ；</p>
<p>QoE的最大化：</p>
<p>输入：吞吐量迹 ${C_t, t \in [t_1, t_{K+1}]}$</p>
<p>输出：码率选择 $R_1, &hellip;, R_K$；启动时间 $T_s$ ；</p>
<p>需要注意：当最大化的决策发生在播放过程中时，启动时间便不再存在；</p>
<p></p>
<h3 id="算法">算法</h3>
<p>上图中的QoE最大化问题是一种随机优化控制问题，随机性源自可获得的吞吐量 $C_t$ 。</p>
<p>$t_k$ 时刻播放器选择码率 $R_k$ ，只有过去的吞吐量 $\lbrace C_t, t \le t_k \rbrace$ 可知，未来的值 ${C_t, t &gt; t_k}$ 未知。</p>
<p>但是，<em>吞吐量预测器</em>可以用于获取对吞吐量的预测，定义其为 $\lbrace \hat{C_t}, t &gt; t_k \rbrace$ 。</p>
<p>基于这样的预测和缓冲区的信息（精确可知），<em>码率选择器</em>对下个块 $k$ 的码率选择可以表示为：
$$
R_k = f \big( B_k, \lbrace \hat{C_t}, t &gt; t_k \rbrace, \lbrace R_i, i &lt; k \rbrace \big)
$$
文章只关注码率自适应算法，假设已经得到了预测值，并根据预期预测误差对其进行了表征，即：</p>
<p>我们着重于 $f(\cdot)$ 的设计以及预测误差对比较控制算法性能的影响。</p>
<p>现有的两类自适应算法：基于速率和基于缓冲区，分别可以表示为：
$$
R_k = f \big( \lbrace \hat{C_t}, t &gt; t_k \rbrace, \lbrace R_i, i &lt; k \rbrace \big)
$$</p>
<p>$$
R_k = f(B_k, \lbrace R_i, i &lt; k \rbrace)
$$</p>
<p>前者只基于吞吐量的预测结果而不管缓冲区状况；后者只基于缓冲区而不管未来的吞吐量可能状况；</p>
<p>这两种方法在原则上都只是次优的，理想情况下我们想要同时考虑缓冲区占用和吞吐量预测结果。</p>
<p></p>
<h2 id="mpc-for-optimal-bitrate-adaptation">MPC for Optimal Bitrate Adaptation</h2>
<h3 id="why-mpc">Why MPC</h3>
<p>MPC天然适合码率自适应问题。</p>
<ul>
<li>
<p><strong>Strawman solutions</strong></p>
<p>码率自适应问题本质是<em>随机控制优化</em>问题，就这一点而言，有两个知名控制算法：</p>
<ol>
<li>Proportional-integral-derivation(PID) control.</li>
<li>Markov Decision Process(MDP) based control.</li>
</ol>
<p>PID相较MDP而言计算起来更加简单，只能用于使系统稳定，不能显式地优化QoE目标；此外PID被设计用于有连续的时间和连续的状态空间的问题中，用于当前这种高度离散化的问题中会导致性能亏损和不稳定。</p>
<p>应用MDP的话可以将吞吐量和缓冲区状态形式化为马氏过程，然后使用诸如值迭代和策略迭代等标准算法求出最优解。</p>
<p>（然而，这有一个很强的假设，即吞吐量动态遵循马尔可夫过程，不清楚这在实践中是否成立。我们将MDP的潜在用途和吞吐量动态分析作为未来的工作。）</p>
</li>
<li>
<p><strong>Case for MPC</strong></p>
<p>理想情况下，如果给出未来吞吐量的完美数据，那么启动时间 $T_s$ 和最优码率选择 $R_1, &hellip; R_K$ 可以一下子就计算出来；</p>
<p>实际情况中，虽然不能得到未来吞吐量的完美预测，但是我们可以假设吞吐量在较短的时间段 $[t_k, t_{k+N}]$ 内不会剧烈变化。</p>
<p>基于此，可以使用当前视界中的预测来应用第1个码率 $R_k$ ，之后将视界向前移动到 $[t_{k+1}, t_{k+N+1}]$ 。</p>
<p>而这种方案就称为MPC。MPC的一般好处在于，MPC可以利用预测在约束条件下在线优化动态系统中的复杂控制目标。</p>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for TBRA</title>
    <link>https://ayamir.github.io/posts/note-for-tbra/</link>
    <pubDate>Tue, 21 Dec 2021 10:11:23 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-tbra/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3474085.3475590" target="_blank" rel="noopener noreffer">TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming</a></p>
<p>Level：ACM MM 21</p>
<p>Keywords：Adaptive tiling and bitrate，Mobile streaming</p>
<h2 id="创新点">创新点</h2>
<h3 id="背景">背景</h3>
<p>现有的固定的tile划分方式严重依赖viewport预测的精度，然而viewport预测的准确率往往变化极大，这导致基于tile的策略实际效果并不一定能实现其设计初衷：保证QoE的同时减少带宽浪费。</p>
<p>考虑同样的viewport预测结果与不同的tile划分方式组合的结果：</p>
<p></p>
<p>从上图可以看到：</p>
<ul>
<li>如果采用$6 \times 6$的分块方式，就会浪费26，32两个tile的带宽，同时15，16，17作为本应在实际viewport中的tile并没有分配最高的优先级去请求。</li>
<li>如果采用$5 \times 5$的分块方式，即使预测的结果与实际的viewport有所出入，但是得益于tile分块较大，所有应该被请求的tile都得到了最高的优先级，用户的QoE得到了保证。</li>
</ul>
<p>另一方面，基于tile的方式带来了额外的编解码开销（可以看这一篇论文：<a href="https://ayamir.github.io/2021/12/note-for-optile/" target="_blank" rel="noopener noreffer">note-for-optile</a>），而这样的性能需求对于移动设备而言是不可忽略的。</p>
<h3 id="创新">创新</h3>
<p>除了考虑常见的因素如带宽波动和缓冲区占用之外，提出同时自适应分块策略和码率分配以应对变化的viewport预测性能和受限的移动设备的解码能力。</p>
<h3 id="论文组织">论文组织</h3>
<ol>
<li>首先使用现实世界的轨迹分析了典型的viewport预测算法并确定了其性能的不确定性。</li>
<li>接着讨论了不同的分块策略在tile选择和解码效率上的影响。</li>
<li>自适应的分块策略可以适应viewport预测的错误，并能保证tile选择的质量。</li>
<li>为解码时间建构了分析模型，可以在给定受限的计算资源时用于选择恰当的分块策略和码率。</li>
<li>形式化了优化模型，讨论了自适应算法的细节。</li>
<li>评估证明了方案的优越性。</li>
</ol>
<h2 id="motivation">Motivation</h2>
<h3 id="分块策略对tile选择的影响">分块策略对tile选择的影响</h3>
<p>实现4种轻量的viewport预测算法：线性回归LR、岭回归RR、支持向量回归、长短期记忆LSTM。</p>
<p>设置历史窗口大小为2s，预测窗口大小为1s；viewport的宽度和高度分别为100°和90°。</p>
<p>默认的分块策略为$6 \times 6$；头部移动数据集来自<a href="https://dl.acm.org/doi/10.1145/3204949.3208139" target="_blank" rel="noopener noreffer">公开数据集</a>。</p>
<h4 id="viewport预测的不准确性">viewport预测的不准确性</h4>
<p>研究表明，用户的头部运动主要发生在水平方向而较少发生在垂直方向，所以只分析水平方向的预测。</p>
<p>实际的商业移动终端只有有限的传感和处理能力，并不能支持高频的viewport预测采样。</p>
<p>视频内容的不同类型会显著影响预测的精度，基于录像环境（室内或户外）和相机的运动状态分类。</p>
<ul>
<li>
<p>改变采样频率会直接影响viewport预测的精度，频率越低，精度越低。</p>
</li>
<li>
<p>相机运动的viewport预测错误率比相机静止的明显更高。</p>
</li>
</ul>
<h4 id="通过分块容忍预测错误">通过分块容忍预测错误</h4>
<p>因为不管tile的哪个部分被包含在预测的viewport中，只要包含一部分就会请求整个tile，所以增大每个tile的尺寸能吸收预测错误。</p>
<p>实验验证：</p>
<p>设定从$4 \times 4$到$10 \times 10$的分块方式，使用不同的预测误差来检查分块设定可以容纳的最大预测误差，同时保持tile选择结果的相同质量。</p>
<p>用$F_1$分数来表示tile选择的质量：$F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall}$。</p>
<p>实验结果表明更大的tile尺寸更能容忍预测错误。</p>
<h3 id="分块策略对解码复杂性的影响">分块策略对解码复杂性的影响</h3>
<p>虽然当前的移动设备硬件性能发展迅速，但是实时的高码率高分辨率全景视频的解码任务还是充满挑战。</p>
<p>分块对于编码的影响：</p>
<ul>
<li>tile越小，帧内和帧间内容的相关区域就越小，编码效率越低。</li>
</ul>
<p>直接影响解码复杂性的因素：</p>
<ul>
<li>tile的数量。</li>
<li>视频的分辨率。</li>
<li>用于解码的资源。</li>
</ul>
<p>固定其中1个因素改变另外2个因素来检查其对解码的影响：</p>
<p></p>
<p>根据对图的观察可以得出这3个因素在经验上是相互独立的，因为这三幅图之中的图像几乎相同。</p>
<p>分别用$F_n(x), F_r(x), F_c(x)$表示tile数量、分辨率、线程数量为$x$时，解码时间与基线时间的比值。</p>
<p>将这3个比值作为3个乘子建立分析模型：
$$
D = D_0 \cdot F_n(x_1) \cdot F_r(x_2) \cdot F_c(x_3)
$$
上式表示计算整体的解码时间，其中tile数量为$x_1$、分辨率为$x_2$、线程数量为$x_3$；$D_0$时解码的基线时间。</p>
<p>这个模型将用于帮助做出分块和码率适应的决策。</p>
<p>注意在实际情况中，可供使用的计算资源（线程数）是受限的，需要根据设备当前可用的计算资源来分配。</p>
<h2 id="tbra的设计">TBRA的设计</h2>
<ul>
<li>$S = \lbrace s_1, s_2, &hellip; \rbrace$ 表示360°视频分块方式的集合；</li>
<li>对于分块方式$s_i$，$|s_i|$ 表示这种方案中tile的数量；</li>
<li>当 $i &lt; j$ 时，假设 $|s_i| &lt; |s_j|$；</li>
<li>对于分块方式$s$， $b_{i, j}$ 表示第 $i$ 块的tile $j$，$i \le 块的数量, j \le |s|$；</li>
<li>目标是确定分块方式$s$，并为每个tile确定其码率$b_{i, j}$；</li>
</ul>
<h3 id="分块自适应">分块自适应</h3>
<h4 id="自适应的概念">自适应的概念</h4>
<p>分块尺寸大小会导致viewport容错率和传输效率的变化。</p>
<ul>
<li>分块尺寸小，极端情况下每个像素点作为一个tile，viewport容错率最小，但是传输效率达到100%；</li>
<li>分块尺寸大，极端情况下整个视频帧作为一个tile，viewport容错率最大，但是传输效率最小；</li>
</ul>
<p>优化的目标就是在这两种极端条件中找到折中的最优解。</p>
<h4 id="分块选择">分块选择</h4>
<p>以$\overline{r_d}, d \in \lbrace left, right, up, down \rbrace$为半径扩大预测区域；$e_d$表示过去n秒中方向 $d$ 的预测错误平均值；
$$
\overline{r_d} = (1-\alpha) \cdot \overline{r_d} + \alpha \cdot e_d
$$
预测区域的扩展被进一步用于tile选择，受过去预测精度的动态影响。</p>
<p>下一步检查不同分块方式，进而找到QoE和传输效率之间的折中。</p>
<p>对于每个分块方式，比较基于扩展的预测区域的tile选择的质量。使用2个比值作为QoE和传输效率的度量：
$$
Miss\ Ratio = \frac{of\ missed\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p>$$
Waste\ ratio = \frac{of\ unnecessary\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p></p>
<p>这2个比值的tradeoff可以在上图中清晰地看出。</p>
<p>使用分块方式对应的惩罚$Tiling\ i_{penalty}$来评估其性能：
$$
Tiling\ i_{penalty} = \beta \cdot Miss\ Ratio + |1/cos(\phi_i)| \cdot Waste\ Ratio
$$
$\phi_i$ 是viewport $i$ 的中心纬度坐标，它表明随着viewport的垂直移动，浪费率的权重会发生变化。（因为投影方式是ERP）</p>
<p>检查完所有的方式之后，最终选择惩罚最小的分块方式。</p>
<h3 id="码率自适应">码率自适应</h3>
<h4 id="视频质量">视频质量</h4>
<p>$w_{i, j}$表示在第 $i$ 个视频块播放时，tile $j$ 的权重；在当前方案中 $w_{i, j} = 0\ or\ 1$ 取决于tile是否在预测的viewport中。</p>
<p>$q(b_{i, j})$ 是tile比特率选择 $b_{i, j}$ 与用户实际感知到的质量之间的非递减映射函数。</p>
<p>第 $i$ 个视频块的质量等级可以定义为：</p>
<p>$$
Q^{(1)}_i = \sum^n_{j=1} w_{i, j} q(b_{i, j})
$$</p>
<p>使用最新研究的<a href="https://ieeexplore.ieee.org/document/8979422/citations?tabFilter=papers" target="_blank" rel="noopener noreffer">主观视频质量模型</a>：
$$
subjective\ PSNR:\ q_i = PSNR_i \cdot [M(v_i)]^{\gamma} [R(v_i)]^{\delta}
$$
$M(v_i)$ 是检测阈值；$R(v_i)$ 是视网膜滑移率；$v_i$ 是第播放 $i$ 个视频块时viewport的移动速度；$\gamma = 0.172, \delta = -0.267$</p>
<h4 id="质量变化">质量变化</h4>
<p>连续视频块之间的强烈质量变化会损害QoE，定义质量变化作为响铃两个视频块之间质量的变化：
$$
Q^{(2)}_i = |Q^{(1)}_1 - Q^{(1)}_{i-1}|,\ i \in [2, m]
$$</p>
<h4 id="重缓冲时间">重缓冲时间</h4>
<p>参数设置：</p>
<ul>
<li>$C_i$ 表示下载视频块 $i$ 的预计吞吐量；</li>
<li>$B_i$ 表示客户端开始下载视频块 $i$ 时缓冲区的占用率；</li>
<li>$B_{default}$ 表示在启动阶段默认的缓冲区填充等级，记 $B_{default} = B_1$；</li>
<li>下载第 $i$ 个视频块需要时间 $\sum^n_{j=1} b_{i, j} / C_i$ ；</li>
<li>每个视频块的长度为 $L$ ；</li>
</ul>
<p>缓冲区的状态应该在每次视频块被下载的时候都得到更新，则下一个视频块 $i+1$ 的缓冲区占用情况可以计算为：
$$
B_{i+1} = max\lbrace B_1 - \sum^n_{j=1} b_{i, j} / C_i,\ 0\rbrace + L
$$
下载第 $i$ 个视频块时的重缓冲时间可以计算为：</p>
<p>$$
Q^{(3)}_i = max \lbrace \sum^n_{j=1} b_{i, j} / C_i - B_i,\ 0 \rbrace + t_{miss}
$$</p>
<p>第一部分是下载时间过长且缓冲区耗尽，视频无法播放情况下的重新缓冲时间；</p>
<p>第二部分 $t_{miss}$ 表示下载缺失的tile所花费的时间（在视频块播放过程中被看到但是之前没有分配码率的tile）。</p>
<h4 id="优化目标">优化目标</h4>
<p>第 $i$ 个视频块的整体优化目标可以定义为前述3个指标的加权和：
$$
Q_i = pQ^{(1)}_i - qQ^{(2)}_i - rQ^{(3)}_i
$$
各个系数的符号分配表示：最大化视频质量、最小化块间质量变化、最小化重缓冲时间。</p>
<p>传统意义上使用所有视频块的平均QoE作为优化对象，但实际上很难获得从块 $1$ 到块 $m$ 的整个视界的完美的未来信息。</p>
<p>为了处理预测长期吞吐量和用户行为的难度，采用<a href="https://dl.acm.org/doi/10.1145/2785956.2787486" target="_blank" rel="noopener noreffer">基于MPC的框架</a>，在有限的范围内优化多个视频块的QoE，最终的目标函数可以形式化为：
$$
\underset{b_{i, j}, i \in [t, t+k-1], j \in [1, n]}{max} \sum^{t+k-1}_{i=t} Q_i
$$
因为短期内的viewport预测性能和网络状况可以很容易得到，QoE优化可以通过使用窗口 $[t, t+k-1]$ 内的预测信息；</p>
<p>接着将视界向前移动到 $[t+1, t+k]$ ，更新新的优化窗口的信息，为下一个视频块执行QoE优化，直到最后一个窗口。</p>
<p>使用基于MPC的公式的优点：由于受限的问题规模，每个优化问题的实例都是实际可解的。</p>
<h4 id="高效求解">高效求解</h4>
<p>提出的公式天然适合在线求解，得益于短窗口的实例问题规模很小，QoE优化可以通过详尽搜索定期解决。</p>
<p>但是因为优化过程需要高频调用，所以对于大的搜索空间还是充满挑战。</p>
<p>为了支持实时优化，需要对搜索空间进行高效剪枝，确定几点约束：</p>
<ul>
<li>
<p>解码时间需要被约束；</p>
<p>解码时间应该短于回放长度。</p>
<p>给定移动设备上可用的计算资源，可以得到支持的最大解码线程数。</p>
<p>基于解码时间的分析模型，由于解码复杂度和分辨率的单调性，可以找到设备能够限定时间内解码的最大质量水平，这会将码率选择限制在有界搜索空间内。</p>
</li>
<li>
<p>码率选择应该考虑吞吐量的限制：$\sum^n_{j=1} b_{i, j} \le LC_i$ ；</p>
<p>不会主动耗尽缓冲区，无需让其处理吞吐量的波动。</p>
</li>
<li>
<p>码率选择应该考虑tile的分类；</p>
<p>tile的码率不应该低于同一个视频块中更低权重tile的码率： $b_{i, j} \ge b_{i, j'}, \forall w_{i, j} &gt; w_{i, j'}$ 。</p>
</li>
<li>
<p>属于相同类别的tile比特率选择应该是同一个等级；</p>
<p>这使码率自适应在tile类的级别上执行而非单个tile的级别，大大减小了搜索空间的规模。</p>
</li>
<li>
<p>当优化窗口中的吞吐量和用户行为保持稳定时，同一个窗口中的tile应该有相同的结果。</p>
</li>
</ul>
<h3 id="tbra-workflow">TBRA workflow</h3>
<p></p>
<p>这样的方式需要在服务端存储大量的按照不同分块方式划分的不同码率版本的视频块，这一点可以进一步研究。</p>
<p>但是对于移动终端设备而言，这样的解决方案只引入了可以忽略不计的开销。</p>
<p>观察到tile自适应问题具有全局最优通常就是局部最优的特点，因此可以大大减少计算量。</p>
<p>基于MPC的优化workflow还可以有效地解决码率自适应问题。</p>
]]></description>
</item>
<item>
    <title>Note for Content Motion Viewport Prediction</title>
    <link>https://ayamir.github.io/posts/note-for-content-motion-viewport-prediction/</link>
    <pubDate>Mon, 20 Dec 2021 10:47:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-content-motion-viewport-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/abs/10.1145/3328914" target="_blank" rel="noopener noreffer">Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking</a></p>
<p>Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019</p>
<p>Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model</p>
<h2 id="workflow">Workflow</h2>
<ul>
<li>Tracking：VR motion追踪算法：应用了高斯混合模型来检测物体的运动。</li>
<li>Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户viewport来自动更正潜在的预测错误。</li>
<li>Update：viewport动态更新算法：动态调整预测的viewport大小去覆盖感兴趣的潜在viewport，同时尽可能保证最低的带宽消耗。</li>
<li>Evaluation：经验用户/视频评估：构建VR viewport预测方法原型，使用经验360°视频和代表性的头部移动数据集评估。</li>
</ul>
<h2 id="全景直播推流的预备知识">全景直播推流的预备知识</h2>
<h3 id="vr推流直播">VR推流直播</h3>
<p></p>
<p>相比于传统的2D视频推流的特别之处：</p>
<ul>
<li>VR系统是交互式的，viewport的选择权在客户端；</li>
<li>呈现给用户的最终视图是整个视频的一部分；</li>
</ul>
<h3 id="用户头部移动的模式">用户头部移动的模式</h3>
<p>在大量的360°视频观看过程中，用户主要的头部移动模式有4种，使用$i-j\ move$来表示；</p>
<p>其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。</p>
<ul>
<li>$1-1\ move$：单个物体以单一方向移动；</li>
<li>$1-n\ move$：单个物体以多个方向移动；</li>
<li>$m-n\ move$：多个物体以多个方向移动；</li>
<li>$Arbitrary\ move$：用户不跟随任何感兴趣的物体而移动，viewport切换随机；</li>
</ul>
<p></p>
<p>现有的直播VR推流中的viewport预测方法是基于速度的方式，这种方式只对$1-1\ move$这一种模式有效。</p>
<p>本方案的目标是提出对4种模式都有效的预测策略。</p>
<h2 id="系统架构">系统架构</h2>
<p></p>
<h3 id="理论创新">理论创新</h3>
<ul>
<li>
<p>核心功能模块：</p>
<ol>
<li>
<p>motion detection：区分运动物体与静止的背景。</p>
</li>
<li>
<p>feature selection：选择代表性的特征并对运动物体做追踪。</p>
<p>这两个模块使系统能识别用户可能感兴趣的viewport。</p>
</li>
</ol>
</li>
<li>
<p>使用贝叶斯方法分析用户观看行为并形式化用户的兴趣模型。</p>
<ol>
<li>
<p>使用错误恢复机制来使当预测错误被检测到时的预测viewport去适应实际的viewport，尽管不能消除预测错误但是能避免在此基础上进一步的预测错误。</p>
</li>
<li>
<p>使用动态viewport更新算法来产生大小可变的viewport，通过同时考虑跟踪到的viewport轨迹和用户当前的速度（矢量）。</p>
<p>这样，即使用户的运动模式很复杂也能有更高的概率去覆盖潜在的视图。</p>
</li>
</ol>
</li>
</ul>
<h3 id="具体实施">具体实施</h3>
<ul>
<li>
<p>虽然提出的运动追踪和错误处理机制是计算密集型的任务，但是这些组件都部署在video packager中，运行在服务端。</p>
</li>
<li>
<p>将生成VR视图的工作负载移动到服务端，进一步减少了客户端的计算开销以及网络开销。</p>
</li>
</ul>
<h2 id="形式化">形式化</h2>
<h3 id="基于运动轨迹的viewport预测">基于运动轨迹的viewport预测</h3>
<p>使用<a href="https://ieeexplore.ieee.org/document/1333992" target="_blank" rel="noopener noreffer">GMM</a>完成运动检测，使用<a href="https://ieeexplore.ieee.org/document/323794" target="_blank" rel="noopener noreffer">Shi-Tomasi algorithm</a>解决运动轨迹跟踪问题。</p>
<p></p>
<ol>
<li>
<p>运动检测</p>
<p>GMM前景提取</p>
</li>
<li>
<p>特征选取与过滤</p>
<p>采用 Shi-Tomasi algorithm 从视频中检测代表性的特征，直接检测得到的代表性特征数量较多而难以追踪。</p>
<p>采用两种过滤的方法来减少要追踪的特征数量。</p>
<ul>
<li>
<p>比较当前帧和前一帧的特征，只保留其共有的部分。</p>
</li>
<li>
<p>采用第1步中运动检测的方式，只保留运动的部分。</p>
</li>
</ul>
</li>
<li>
<p>viewport生成</p>
<p>经过选择和过滤之后的特征通常分布在不能被单一用户视图所覆盖的广阔区域中。</p>
<p>在整个360°视频中可能存在多个运动的物体，即$m-n\ move$。</p>
<p>提出一种系统的方式来产生用户最可能跟随观看的viewport。</p>
<p>直觉是用户更可能将大部分注意力放在两种类型的物体上：</p>
<ul>
<li>离用户更近的物体。</li>
<li>就物理形状而言更“重要”的物体。</li>
</ul>
<p>这两种类型的物体大多包含最密集和最大量的特征，因此通过所有特征的重心来计算预测用户视图的中心。</p>
<p>对于剩余的特征列表：$\vec{F} = [f_1, f_2, f_3, &hellip;, f_k]$，其中$f_i(i = 1 &hellip; k)$表示特征$f_i = &lt;f^{(x)}_i, f^{(y)}_i&gt;$的像素点坐标，则预测出的viewport中心坐标可以计算出来：
$$
l_x = \frac{1}{k} \sum^k_{i=1} f^{(x)}_i;\ l_y = \frac{1}{k} \sum^k_{i=1} f^{(y)}_i.
$$
考虑到即使预测的viewport中包含用户观看的物体，预测得到的viewport也可能会与实际的viewport存在差异。</p>
<p>所以预测的viewport可能比实际的viewport要大，所以使用缩放因子$S_c$来产生预测的viewport。</p>
<p>给出用户viewport的大小$S_{user}$，预测的viewport可以通过$S_{pre} = S_c \cdot S_{user}$计算出来。</p>
</li>
</ol>
<h3 id="基于用户反馈的错误恢复">基于用户反馈的错误恢复</h3>
<p>video packager可以通过HMD和web服务器通过反向路径从用户处检索用户实际视图的反馈信息。</p>
<p>基于反馈的错误恢复机制在以下两种场景中表现良好：</p>
<ol>
<li>
<p>没有运动的物体</p>
<p>如果没有检测到运动的物体，则用户很可能是在观看静止的物体，这会导致基于运动目标的viewport预测失败。</p>
<p>在这种场景中，可以认为视频内容已经不再是决定用户viewport的因素，而只取决于用户自身的行为。</p>
<p>因此采用基于速度的方式来预测viewport。（这样的决策可以在运动检测模块没有检测到运动物体时就做出）</p>
<p>一旦从反馈路径上得到用户信息，可以产生用户viewport位置向量：$\vec{L} = [l_1, l_2, l_3, &hellip;, l_M]$，其中$l_i$表示第$i$个帧中用户viewport的位置，$M$表示视频播放缓冲区中的帧数。那么可以计算viewport速度：
$$
\vec{V} = \frac{\vec{(l_2 - l_1)} + \vec{(l_3 - l_2)} &hellip;.(l_M - l_{M-1})}{M-1} = \frac{(\vec{l_M - l_1})}{M-1}
$$
下一帧的预测位置$L_{M=1}$也可以计算出来：
$$
l_{M+1} = l_M + \vec{V}
$$</p>
</li>
<li>
<p>预测视图与实际视图的不匹配</p>
<p>一旦运动追踪策略检测到用户实际的视图和预测的视图不同，就会触发恢复机制去追踪用户实际在看着的物体。</p>
<p>可以使用运动追踪方式确定用户实际观察的物体的速度。</p>
<p>给出前一帧匹配的特征$\vec{FA} = [fA_1, fA_2, fA_3, &hellip;, fA_p]$和当前帧的特征$\vec{FB} = [fB_1, fB_2, fB_3, &hellip;, fB_p]$，可以计算出速度：
$$
V_x = \frac{1}{p} (\sum^p_{i=1} fB^{(x)}_i - \sum^p_{i=1}fA^{(x)}_i),\
V_y = \frac{1}{p} (\sum^p_{i=1} fB^{(y)}_i - \sum^p_{i=1}fA^{(y)}_i),
$$
假设预测的viewpoint是$(l_x, l_y)$，修改之后的viewpoint是$(l_x + V_x,\ l_y + V_y)$。</p>
</li>
</ol>
<h3 id="动态viewport更新">动态viewport更新</h3>
<p>前述的错误恢复机制发生在viewport预测错误出现之后，任务是避免未来更多的错误。</p>
<p>动态的viewport更新则努力避免viewport预测错误。</p>
<p>关键思想是扩大预测的viewport大小，以高概率去覆盖$m-n\ move$和$arbitrary\ move$下所有潜在的运动目标；更重要的是动态调整视图的大小去获得更高效的带宽利用率。</p>
<ul>
<li>
<p>对于一个360°全景视频，将360°的帧均分为$N = n \times n$个网格，每个网格看作是一个tile，预测的viewport即为$N$个tile的子集。</p>
</li>
<li>
<p>使用贝叶斯方法分析用户的观看行为，每个tile分配一个独立的贝叶斯模型，所以每个tile可以独立更新。</p>
</li>
<li>
<p>设$X$表示用户viewport，$Y$表示静态内容，$Z$表示运动物体。</p>
</li>
<li>
<p>未来的用户viewport可以以条件概率计算为$P(X|Y,\ Z)$，$Y$与$Z$相互独立。</p>
</li>
<li>
<p>用户的viewport可以通过反馈信息得出$P(X)$；用户观看静态特征可以表示为$P(X|Y)$；用户观看动态特征可以表示为$P(X|Z)$。</p>
</li>
<li>
<p>$P(X|Y, Z)$可以计算为：
$$
P(X|Y, Z) = \frac{P(Y|X) \cdot P(Z|X) \cdot P(X)}{P(Y, Z)}
$$</p>
</li>
<li>
<p>只要用户开始观看，对于tile $T_i$，就能得到其先验概率$P(Y_i|X_i)$和$P(Z_i|X_i)$，进而根据贝叶斯模型计算出$P(X|Y, Z)$。</p>
</li>
</ul>
<p>为每个tile定义两种属性：</p>
<ol>
<li>当前状态：表示此tile是否属于预测的viewport（属于标记为$PREDICTED$，不属于标记为$NONPREDICTED$）。</li>
<li>生存期：表示此tile会在view port中存在多长时间（例如定义3种等级：$ZERO$，$MEDIUM$，$HIGH$，实际的定义划分可以根据具体的用户和视频设定）。</li>
</ol>
<h2 id="预测步骤">预测步骤</h2>
<p>按照形式化中提出的3步，分为系统初始化、帧级别的更新、缓冲区级别的更新。</p>
<ol>
<li>
<p>系统初始化</p>
<p>初始化阶段中，view更新算法将所有的$N$个tile标注为$PREDICTED$，并将生存期设置为$MEDIUM$，即系统向用户发送完整的一帧作为自举。</p>
<p>这样设定的原因在于：当用户第一次启动视频会话时，允许“环视”类型的移动，这可能会覆盖360°帧的任意viewport。</p>
</li>
<li>
<p>帧级别的更新</p>
<p>给定一帧，应用修改后的motion追踪算法在运动区域中选择特征，而不使用特征的密度做进一步的过滤。</p>
<p>使用有多个tile的多个视图来覆盖一个放大的区域，该区域包含作为预测viewport的移动对象上的所有特征，这样就能适应$m-n\ move$中的用户行为。</p>
<p>设计帧级别的算法标记选择的tile作为$PREDICTED$并设置其生存期为$HIGH$（直觉上讲运动中的物体或用户所感兴趣的静态特征会更以长时间保留在viewport之中）。</p>
</li>
<li>
<p>缓冲区级别的更新</p>
<p>以缓冲区长度为间隔检索用户的实际视图，基于此可以对tile的两种属性做出调整。</p>
<ol>
<li>对于与用户实际视图重叠的tile，设置为$PREDICTED$和$HIGH$。</li>
<li>对于用户实际视图没有出现但出现在预测的视图中的tile，生存期减1，如果生存期减为$ZERO$，就重设其状态为$NONPREDICTED$，将其从预测的viewport中移除。</li>
</ol>
<p></p>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for RnnQoE</title>
    <link>https://ayamir.github.io/posts/note-for-rnnqoe/</link>
    <pubDate>Thu, 16 Dec 2021 19:53:10 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-rnnqoe/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9580281" target="_blank" rel="noopener noreffer">QoE-driven Mobile 360 Video Streaming: Predictive
View Generation and Dynamic Tile Selection</a></p>
<p>Level：ICCC 2021</p>
<p>Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles</p>
<h2 id="系统建模与形式化">系统建模与形式化</h2>
<h3 id="视频划分">视频划分</h3>
<p>先将视频划分成片段：$\Iota = {1, 2, &hellip;, I}$表示片段数为$I$的片段集合。</p>
<p>接着将片段在空间上均匀划分成$M \times N$个tile，FOV由被用户看到的tile所确定。</p>
<p>使用ERP投影，$(\phi_i, \theta_i),\ \phi_i \in (-180\degree, 180\degree], \theta_i \in (-90\degree, 90\degree]$来表示用户在第$i$个片段中的视点坐标。</p>
<p>播放过程中记录用户头部运动的轨迹，积累的数据可以用于FOV预测。</p>
<p>跨用户之间的FOV轨迹可以用于提高预测精度。</p>
<h3 id="qoe模型">QoE模型</h3>
<ul>
<li>
<p>前提</p>
<p>视频编解码器预先确定，无法调整每个tile的码率。</p>
</li>
<li>
<p>实现</p>
<ol>
<li>每个tile都以不同的码率编码成不同的版本。</li>
<li>每个tile都有两种分辨率的版本。</li>
</ol>
</li>
<li>
<p>QoE内容</p>
<p>客户端收到的视频质量和观看时的卡顿时间。</p>
</li>
<li>
<p>质量形式化</p>
<p>对于每个片段$i \in \Iota$，$S_i = {\tau_{i, j}}_{j=1}^{M \times N}$是用来表示用户实际看到的tile的集合的向量。</p>
<p>$\tau_{i, j} = 1$表示第$i$个段中的第$j$个tile被看到；$\tau_{i, j} = 0$表示未被看到。</p>
<p>同样的， $\tilde{S}_i = {\tilde{\tau}_{i, j}}_{j = 1}^{M \times N}$ 表示经过FOV预测和tile选择之后成功被传送到用户头戴设备上的tile集合的向量。</p>
<p>$\tilde{\tau}_{i, j} = 1$表示第$i$个段中的第$j$个tile被用户接收；$\tilde{\tau}_{i, j} = 0$表示未被接收。</p>
<p>第$i$个段的可感知到的质量可以表示为：</p>
<p>$$
Q_i = \sum_{j = 1}^{M \times N} p_{i, j}b_{i, j}\tau_{i, j}\tilde{\tau}_{i, j}
$$</p>
<p>$b_{i, j}$表示第$i$个片段的第$j$个tile的码率；$p_{i, j}$表示对不同位置tile所分配的权重；</p>
</li>
<li>
<p>关于权重$p_{i, j}$</p>
<p>研究表明用户在全景视频FOV中的注意力分配并不是均等的，越靠近FOV中心的tile对用户的QoE贡献越大。</p>
<p>下面讨论单个片段的情况：用$(\phi_j, \theta_j)$表示tile中心点的坐标，并映射到笛卡尔坐标系上$(x_j, y_j, z_j)$：</p>
<p>$$
x_j = cos\theta_jcos\phi_j,\ y_j = sin\theta_j,\ z_j = -cos\theta_jsin\phi_j
$$</p>
<p>则两个tile之间的半径距离$d_{j, j'}$可以表示为：</p>
<p>$$
d_{j, j'} = arccos(x_j x_{j'} + y_j y_{j'} + z_j z_{j'})
$$</p>
<p>对于第$i$个片段，假设用户FOV中心的tile为$j^*$，那么第$j$个tile的权重可以计算出来：</p>
<p>$$
p_{i, j} = (1 - d_{j, j^*} / \pi) \tau_{i, j}
$$</p>
</li>
<li>
<p>卡顿时间形式化</p>
<p>当$\tilde{\tau}_{i, j}$与$\tau_{i, j}$出现分歧时，用户就不能成功收到请求的tile，头戴设备中显示的内容就会被冻结，由此导致卡顿。</p>
<p>对于任意的片段$i \in \Iota$，相应的卡顿时间$D_i$可以计算出来：</p>
<p>$$
D_i = \frac{\sum_{j = 1}^{M \times N} b_{i, j} \cdot [\tau_{i, j} - \tilde{\tau}_{i, j}]^+}{\xi}
$$</p>
<p>$[x]^+ = max \lbrace x, 0 \rbrace $；$\xi$表示可用的网络资源（已知，并且在推流过程中保持为常数）</p>
<p>卡顿发生于在播放时，用户FOV内的tile还没有被传输到用户头戴设备中的时刻，终止于所有FOV内tile被成功传送的时刻。</p>
</li>
<li>
<p>质量与卡顿时间的结合</p>
<p>$$
max\ QoE = \sum_{i = 1}^I (Q_i - wD_i)
$$</p>
<p>$w$表示卡顿事件的惩罚权重。例如，w＝1000意味着1秒视频暂停接收的QoE惩罚与将片段的比特率降低1000 bps相同。</p>
</li>
</ul>
<h2 id="联合viewport预测与tile选择">联合viewport预测与tile选择</h2>
<p>联合框架包括viewport预测和动态tile选择两个阶段。</p>
<p>viewport预测阶段集成带有注意力机制的RNN，接收用户的历史头部移动信息作为输入，输出每个tile出现在FOV中的可能性分布。</p>
<p>选择tile阶段为预测的输出建立的上下文空间，基于上下文赌博机学习算法来选择tile并确定所选tile的质量版本。</p>
<p></p>
<p></p>
<h3 id="viewport预测">Viewport预测</h3>
<p>FOV预测问题可以看作是序列预测问题。</p>
<p>不同用户观看相同视频时的头部移动轨迹有强相关性，所以跨用户的行为分析可以用于提高新用户的viewport预测精度。</p>
<p>被广泛使用的LSTM的变体——Bi-LSTM（Bi-directional LSTM）用于FOV预测。</p>
<ol>
<li>
<p>输入参数构造</p>
<p>为了构造Bi-LSTM学习网络，需要对不同用户的viewpoint特性作表征。</p>
<ul>
<li>
<p>在用户观看事先划分好的$I$个片段时，记录每个片段对应的viewpoint坐标：</p>
<p>$\Phi_{1:I} = {\phi_i}^I_{i = 1},\ \Theta_{1:I} = {\theta_i}^I_{i=1}$</p>
</li>
<li>
<p>预测时使用的历史信息的窗口大小记为$k$；</p>
</li>
<li>
<p>对于第$i, (i &gt; k)$个片段，相应的viewpoint特性由$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$所给出；</p>
</li>
<li>
<p>列索引$m_i$和行索引$n_i$作为viewpoint tile $(\phi_i, \theta_i)$的标签，由<a href="https://zh.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener noreffer">独热编码</a>表示；</p>
</li>
<li>
<p>通过滑动预测的窗口，所看到的视频片段的特性和标签可以被获取。</p>
</li>
</ul>
</li>
<li>
<p>LSTM网络构造</p>
<p>整个网络包含3层：</p>
<ul>
<li>遗忘门层决定丢弃哪些信息；</li>
<li>更新门层决定哪类信息需要存储；</li>
<li>输出门层过滤输出信息。</li>
</ul>
<p>为了预测用户在第$i$个段的viewpoint，LSTM网络接受$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$作为输入；输出行列索引；</p>
<p>$$
m_i = LSTM(\theta_{i-k}, &hellip;, \phi_{i-1}; \alpha)
$$</p>
<p>$$
n_i = LSTM(\theta_{i-k}, &hellip;, \theta_{i-1}; \beta)
$$</p>
<p>$\alpha, \beta$是学习网络的参数；分类交叉熵被用作网络训练的损失函数。</p>
</li>
<li>
<p>Bi-LSTM的特殊构造</p>
<ul>
<li>
<p>将公共单向的LSTM划分成2个方向。</p>
<p>当前片段的输出利用前向和反向信息，这为网络提供了额外的上下文，加速了学习过程。</p>
</li>
<li>
<p>双向的LSTM不直接连接，不共享参数。</p>
</li>
<li>
<p>每个时间槽的输入会被分别传输到前向和反向的LSTM中，并分别根据其状态产生输出。</p>
</li>
<li>
<p>两个输出直接连接到Bi-LSTM的输出节点。</p>
</li>
<li>
<p>引入注意力机制为每步时间自动分配权重，从大量信息中选择性地筛选出重要信息。</p>
</li>
<li>
<p>将Softmax层堆叠在网络顶部，以获取不同tile的viewpoint概率。</p>
</li>
</ul>
</li>
</ol>
<p></p>
<h3 id="动态tile选择">动态tile选择</h3>
<p>使用上下文赌博机学习算法来补偿viewport预测错误对QoE造成的影响。</p>
<ul>
<li>
<p>上下文赌博机学习算法概况</p>
<p>上下文赌博机学习算法是一个基于特征的exploration-exploitation技术。</p>
<p>通过在多条手臂上重复执行选择过程，可以获得在不同上下文中的每条手臂的回报。</p>
<p>通过exploration-exploitation，目标是最大化累积的回报。</p>
</li>
<li>
<p>组成部分形式化</p>
<ol>
<li>
<p>上下文</p>
<p>直觉上讲，当预测的viewpoint不够精确时，需要扩大FOV并选择更多的tile进行传输。</p>
<p>为了做出第$i$个片段上的预测viewpoint填充决策，定义串联的上下文向量：</p>
<p>$c_i = [f^s_i, f^c_i]$，$f^s_i$表示自预测的上下文，$f^c_i$表示跨用户之间的预测上下文。</p>
<p>预测输出的用户$u$的viewpoint tile索引用$[\tilde{m}^u_{i-1}, \tilde{n}^u_{i-1}]$表示；</p>
<p>实际的用户$u$的viewpoint tile索引用$[m_{i-1}^u, n_{i-1}^u]$表示；</p>
<p>那么对第$i$个片段而言，自预测的上下文可以计算出来：</p>
<p>$$
f_i^s = [|m_{i-1}^u - \tilde{m}^u_{i-1}|, |n_{i-1}^u - \tilde{n}^u_{i-1}|]
$$
跨用户的上下文信息获取：使用KNN准则选择一组用户，其在前$k$个片段中的轨迹最接近用户$u$的轨迹。</p>
<p>使用$\zeta$表示获得的用户集合，使用</p>
<p>$$E_{\zeta_u}(m_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u} |m_i^u - \tilde{m}_i^u|$$</p>
<p>$$E_{\zeta_u}(n_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u}|n_i^u - \tilde{n}_i^u|$$</p>
<p>表示预测误差，则：</p>
<p>$$
f_i^u = [E_{\zeta_u}(m_i), E_{\zeta_u}(n_i)]
$$</p>
</li>
<li>
<p>手臂</p>
<p>根据从第一个阶段得到的每个tile的可能性分布，所有的tile可以用倒序的方式排列。</p>
<p>最高可能性的tile被看作FOV的中心，高码率以此tile为中心分配。</p>
<p>剩余的带宽用于扩展FOV，低可能性的tile被顺序选择来扩展FOV直至带宽耗尽。</p>
<p>手臂的状态$a \in {0, 1}$表示tile选择的策略：</p>
<ul>
<li>
<p>$a = 0$表示viewpoint预测准确，填充tile分配了高质量；</p>
</li>
<li>
<p>$a = 1$表示viewpoint预测不准确，填充tile分配的质量较低，为了传送尽可能多的tile而减少卡顿；</p>
</li>
</ul>
</li>
<li>
<p>回报</p>
<p>给定上下文$c_i$，选择手臂$a$，预期的回报$r_{i, a}$建模为$c_i$和$a$组合的线性函数：</p>
<p>$$
\Epsilon[r_{i, a}|c_{i, a}] = c_{i, a}^T \theta_a^*
$$</p>
<p>未知参数$\theta_a$表示每个手臂的特性，目标是为第$i$个片段选择最优的手臂：</p>
<p>$$
a_i^* = \underset{a}{argmax}\ c_{i, a}^T \theta_a^*
$$</p>
<p>使用<a href="https://zhuanlan.zhihu.com/p/38875273" target="_blank" rel="noopener noreffer">LinUCB</a>算法做出特征向量的精确估计并获取$\theta_a^*$。</p>
<p></p>
</li>
</ol>
</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>评估准备</p>
<ul>
<li>使用现有的<a href="https://github.com/xuyanyu-shh/VR-EyeTracking" target="_blank" rel="noopener noreffer">viewpoint轨迹数据集</a>，所有视频被编码为至少每秒25帧，长度为20到60秒；</li>
<li>视频每个片段被划分为$6 \times 12$的tile，每个的角度是$30\degree \times 30\degree$；</li>
<li>初始FOV设定为$90\degree \times 90\degree$，在viewpoint周围是$3 \times 3$的tile；</li>
<li>每个片段的长度为500ms；</li>
<li>默认的预测滑动窗口大小$k = 5$；</li>
<li>HD和LD版本分别以按照HEVC的$QP={32, 22}$的参数编码而得到；</li>
<li>训练集和测试集的比例为$7:3$；</li>
<li>Bi-LSTM层配置有128个隐单元；</li>
<li>batch大小为64；</li>
<li>epoch次数为60；</li>
</ul>
</li>
<li>
<p>性能参数</p>
<ul>
<li>
<p>预测精度</p>
</li>
<li>
<p>视频质量</p>
<p>由传送给用户的有效码率决定：例如实际FOV中的tile码率总和</p>
</li>
<li>
<p>卡顿时间</p>
</li>
<li>
<p>规范化的QoE</p>
<p>实际取得的QoE与在viewpoint轨迹已知情况下的QoE的比值</p>
</li>
</ul>
</li>
<li>
<p>对比目标</p>
<ul>
<li>预测阶段——预测精度
<ol>
<li>LSTM</li>
<li>LR</li>
<li>KNN</li>
</ol>
</li>
<li>取tile的阶段——规范化的QoE
<ol>
<li>两个阶段都使用纯LR</li>
<li>只预测而不做动态选择</li>
</ol>
</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for OpTile</title>
    <link>https://ayamir.github.io/posts/note-for-optile/</link>
    <pubDate>Mon, 13 Dec 2021 16:19:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-optile/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3123266.3123339" target="_blank" rel="noopener noreffer">OpTile: Toward Optimal Tiling in 360-degree Video Streaming</a></p>
<p>Level：ACM MM 17</p>
<p>Keyword：Dynamic tile division, Optimize encoding efficiency, Optimize tile size</p>
<h2 id="背景知识">背景知识</h2>
<h3 id="编码过程概述">编码过程概述</h3>
<ol>
<li>
<p>对一帧图像中的每一个 block，编码算法在当前帧的已解码部分或由解码器缓冲的临近的帧中搜索类似的 block。</p>
<p>当编码器在邻近的帧中找到一个 block 与当前 block 紧密匹配时，它会将这个类似的 block 编码进一个动作向量中。</p>
</li>
<li>
<p>编码器计算当前 block 和引用 block 之间像素点的差异，通过应用变换（如离散余弦变换），量化变换系数以及对剩余稀疏矩阵系数集应用无损熵编码（如 Huffman 编码）对计算出的差异进行编码。</p>
</li>
</ol>
<h3 id="对编码过程的影响">对编码过程的影响</h3>
<ol>
<li>基于 tile 的方式会减少可用于拷贝的 block 数量，增大了可供匹配的 tile 之间的距离。</li>
<li>不同的投影方式会影响编码变换输出的系数稀疏性，而这会降低视频编码效率。</li>
</ol>
<h3 id="投影过程">投影过程</h3>
<p>因为直接对 360 度图像和视频的编码技术还没有成熟，所以 360 度推流系统目前还需要先将 3D 球面投影到 2D 平面上。</p>
<p>目前应用最广的投影技术主要是 ERP 和 CMP，分别被 YouTube 和 Meta 采用。</p>
<h4 id="erp-投影">ERP 投影</h4>
<p>基于球面上点的左右偏航角$\theta$与上下俯仰角$\phi$将其映射到宽高分别为$W$和$H$的矩形上。</p>
<p>对于平面坐标为$(x, y)$的点，其球面坐标分别为：</p>
<p>$$
\theta = (\frac{x}{W} - 0.5) * 360
$$</p>
<p>$$
\phi = (0.5 - \frac{y}{H}) * 180
$$</p>
<h4 id="cmp-投影">CMP 投影</h4>
<p>将球面置于一个立方体中，光线从球心向外发射，并分别与球面和立方体相交于两点，这两点之间便建立了映射关系。</p>
<p>之后将立方体 6 个平面拼接成矩形，就可以使用标准的视频编码方式进行压缩。</p>
<p>关于投影方式还可以参考这里的讲解：<a href="https://zhuanlan.zhihu.com/p/106922217" target="_blank" rel="noopener noreffer">谈谈全景视频投影方式</a></p>
<h3 id="tile-方式的缺点">tile 方式的缺点</h3>
<ul>
<li>
<p>降低编码效率</p>
<p>tile 划分越细，编码越低效</p>
</li>
<li>
<p>增加更大的整体存储需求</p>
</li>
<li>
<p>可能要求更多的带宽</p>
</li>
</ul>
<h2 id="optile-的设计">OpTile 的设计</h2>
<p>直觉上需要增大一些 tile 的大小来使与这些 tile 相关联的片段能捕获高效编码所需的类似块。</p>
<p>同时也需要 tile 来分割视频帧来减少传输过程中造成的带宽浪费。</p>
<ul>
<li>
<p>为了明白哪些片段的空间部分可以被高效独立编码，需要关于 tile 的存储大小的不同维度的信息。</p>
</li>
<li>
<p>为了找到切分视频的最好位置，需要在片段播放过程中用户 viewport 运动轨迹的偏好。</p>
</li>
</ul>
<p>将编码效率和浪费数据的竞争考虑到同一个问题之中，这个问题关注的是<strong>一个片段中所有可能的视图的分布</strong>。</p>
<p>片段的每个可能的视图可以被 tile 的不同组合所覆盖。</p>
<p>目标是为一个片段选择一个 tile 覆盖层，以<strong>最小化固定时间段内视图分布的总传输带宽</strong>。</p>
<ul>
<li>目标分离的部分考虑整个固定时间段的表示（representation）的存储开销。</li>
<li>目标的存储部分与下载的带宽部分相竞争。例如，如果一个不受欢迎的视频一年只观看一次，那么我们更喜欢一个紧凑的表示，我们可以期望向用户发送更多未观看的像素。</li>
</ul>
<h2 id="问题形式化">问题形式化</h2>
<table>
<thead>
<tr>
<th style="text-align:center">segment/片段</th>
<th style="text-align:center">推流过程中可以被下载的连续播放的视频单元</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">basic sub-rectangle/基本子矩形</td>
<td style="text-align:center">推流过程中可以被下载的片段中最小的空间划分块</td>
</tr>
<tr>
<td style="text-align:center">solution sub-rectangle/解子矩形</td>
<td style="text-align:center">片段中由若干基本子矩形组成的任何矩形部分</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">$x$</th>
<th style="text-align:center">用于表示子矩形在解中的存在的二元向量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$c^{(stor)}$</td>
<td style="text-align:center">每个子矩形存储开销相关的向量</td>
</tr>
<tr>
<td style="text-align:center">$c^{(view)}$</td>
<td style="text-align:center">给定一个 segment 中用户 viewport 的分布，$c^{(view)}$指相关子矩形的预期下载字节</td>
</tr>
<tr>
<td style="text-align:center">$\alpha$</td>
<td style="text-align:center">分配到$c^{(view)}$的权重，以此来控制相对于传输一个片段的存储开销</td>
</tr>
</tbody>
</table>
<p>考虑将 1 个矩形片段划分成 4 个基本子矩形，其对应的坐标如下：</p>
<p></p>
<p>4 个基本子矩形可以有 9 种分配方式，成为解子矩形，如下（因为需要保持对应的空间关系，所以只有 9 种）：</p>
<p></p>
<ul>
<li>
<p>$x$的形式化</p>
<p>可以用向量$x$来分别表示上图中子矩形在解中的存在：</p>
<p>$$
[1 \times 1\ at\ (0, 0),\ 1 \times 1\ at\ (0, 1),\ 1 \times 1\ at\ (1, 0),
\
1 \times 1\ at\ (1, 1),\ 1 \times 2\ at\ (0, 0),\ 1 \times 2\ at\ (1, 0),
\
2 \times 1\ at\ (0, 0),\ 2 \times 1\ at\ (0,1),\ 2 \times 2\ at\ (0,0).]
$$</p>
<p>（$x$中每个二元变量的的组成：$1 \times 1$表示子矩形的形状，$(0,0)$表示所处的位置）</p>
<p>要使$x$有效，<strong>每个基本子矩形必须被$x$中编码的子矩形精确覆盖一次</strong>。例如：</p>
<ul>
<li>
<p>$[0, 0, 0, 0, 1, 1, 0, 0, 0]$=&gt;有效（第 5 和第 6 次序的位置分别对应$e$和$f$子矩形，恰好覆盖了所有基本子矩形 1 次）</p>
</li>
<li>
<p>$[0,0,0,1,1,0,0,0,0]$=&gt;无效（第 4 和第 5 次序的位置分别对应$d$和$e$子矩形，没有覆盖到$(1,0)$基本子矩形）</p>
</li>
<li>
<p>$[0,0,0,1,1,1,0,0,0]$=&gt;无效（第 4、第 5 和第 6 次序的位置分别对应$d$、$e$和$f$子矩形，$(1,1)$基本子矩形被覆盖了两次）</p>
</li>
</ul>
</li>
<li>
<p>$c^{(stor)}$的形式化</p>
<p>与每个$x$相对应的向量$c^{(stor)}$长度与$x$相等，其中每个元素是$x$中对应位置的子矩形的存储开销的估计值。</p>
</li>
<li>
<p>$c^{(view)}$的形式化</p>
<p>考虑分发子矩形的网络带宽开销时，需要考虑所有可能被分发的 360 度表面的视图。</p>
<p>为了简化问题，将片段所有可能的视图离散化到一个大小为$V$的集合中。</p>
<p>集合中每个元素表示一个<strong>事件</strong>，即向观看 360 度视频片段的用户显示基本子矩形的唯一子集。</p>
<p>注意到片段中被看到的视频区域可以包含来自多个视角的区域。</p>
<p>将之前离散化好的大小为$V$的集合中每个元素与可能性相关联：$[p_1, p_2, &hellip;, p_V]$。</p>
<p>考虑为给定的解下载视图$V$的开销，作为需要为该视图下载的数据量：</p>
<p>$$
quantity = x^{\top}diag(d_v)c^{(stor)}
$$</p>
<p>$d_v$是一个二元向量，其内容是按照$x$所描述的表示方案，对所有覆盖视图的子矩形的选择。</p>
<p>例如对于 ERP 投影中位置坐标为$yaw = 0, pitch = 90$即处于等矩形顶部的图像，对应的$d_{view-(0, 90)} = [1, 1, 0, 0, 1, 0, 1, 1, 1]$</p>
<p>（即上面图中$a, b, e, g, h, i$位置的子矩形包含此视图所需的基本子矩形）。</p>
<p>给出一个片段中的用户 viewport 分布，$c^{(view)}$的元素是相关联的子矩形预期的下载字节。</p>
<p>$$
c^{(view)} = \sum_v p_v diag(d_v) c^{(stor)}
$$</p>
<p>最后，将优化问题的基本子矩形覆盖约束编码为矩阵$A$。</p>
<p>$A$是一个列中包含给定子矩形解所覆盖的基本子矩形信息的二元矩阵。</p>
<p>对于$2 \times 2$的矩形片段，其$A$有 4 行 9 列，例子如下：</p>
<p></p>
<p>因此最终的问题可以形式化为一个整数线性程序：</p>
<p></p>
<ul>
<li>
<p>$c^{(stor)}$</p>
<p>可以理解为存储一段$\Delta t$时间长的片段的子矩形的存储开销；</p>
</li>
<li>
<p>$c^{(view)}$</p>
<p>可以理解为传输一个视图所需要的所有的子矩形的传输开销。</p>
</li>
<li>
<p>$\alpha$</p>
<p>控制相比于传输一个片段的相对存储开销，同时应该考虑片段的流行度。</p>
<p>即$\alpha$应该与所期望的片段在$\Delta t$的时间间隔内的下载次数成比例，$\alpha$应该可以通过经验测量以合理的精度进行估计。</p>
<p>可以通过将$x$的二元离散限制放松到$0 \le x_i \le 1\ \forall i$构成一个线性程序，其解为整数。</p>
<p>（对于有 33516 个变量的$x$，其解可以在单核 CPU 上用 7~10 秒求出）</p>
</li>
</ul>
</li>
</ul>
<h2 id="开销向量建构">开销向量建构</h2>
<p>首先需要建构出存储开销向量$c^{(stor)}$，但是对于有$n$个基本子矩形的子矩形，其建构复杂度为$O(n^2)$。</p>
<p>因此对每个子矩形进行编码来获得存储开销并不可行，所以利用视频压缩与运动估计之间的强相关性来预测$c^{(stor)}$的值。</p>
<ol>
<li>
<p>给定一个视频，首先暂时将其分成长度为 1 秒的片段，每个片段被限定为只拥有 1 个 GOP，片段的大小表示为$S_{orig}$。</p>
</li>
<li>
<p>接着抽取出每个片段中的动作序列用于之后的分析。</p>
</li>
<li>
<p>将片段从空间上划分成基本子矩形，每个基本子矩形包含$4 \times 4 = 16$个宏块（例如：$64 \times 64$个像素点）。</p>
</li>
<li>
<p>独立编码每个基本子矩形，其大小表示为$S_i$。</p>
</li>
<li>
<p>通过分析动作向量信息，可以推断出如果对基本子矩形$i$进行独立编码，指向基本子矩形$i$的原始运动向量应该重新定位多少。</p>
<p>将其表示为$r_i$。</p>
</li>
<li>
<p>每个运动向量的存储开销可以计算为：</p>
<p>$$
o = \frac{\sum_i S_i - S_{orig}}{\sum_i r_i}
$$</p>
<p>即：存储开销的整体增长除以被基本子矩形边界所分割的运动向量数。</p>
</li>
<li>
<p>如果基本子矩形被融合进更大的子矩形$t$，使用$m_t$来表示由于融合操作而无须再进行重定位的运动向量的数量：</p>
<p>$$
m_t = \sum_{i \in t} r_i - r_t
$$</p>
<p>$i \in t$表示基本子矩形位于子矩形$t$中。</p>
</li>
<li>
<p>为了估计任意子矩形$t$的大小，使用下面 5 个参数：
$$
\sum_{i \in t} S_i,\ \sum_{i \in t} r_i,\ m_t,\ o,\ n
$$
$n$表示$t$中基本子矩形的数量。</p>
</li>
</ol>
<p>实际操作：</p>
<ol>
<li>
<p>创建了来自 4 个单视角 360 度视频的 6082 个 tile 数据集。4 个视频都以两种分辨率进行编码：$1920 \times 960$和$3980 \times 1920$。</p>
</li>
<li>
<p>为了产生 tile，从视频中随机选取片段，随机选取 tile 的位置，宽度和高度。</p>
<p>设置 tile 的 size 最大为$12 \times 12$个基本子矩形。</p>
<p>对于每个选择的 tile，为其建构一个数据集元素：</p>
<ol>
<li>计算上面提到的 5 参数的特性向量。</li>
<li>使用 FFmpeg 编码 tile 的视频段来得到存储该段需要的空间。</li>
</ol>
</li>
<li>
<p>使用多层感知机 MLP 来估计 tile 的大小。</p>
<p>MLP 中包含 50 个节点的单隐层，激活函数为 ReLU 函数，300 次迭代的训练过程使用<a href="https://zhuanlan.zhihu.com/p/29672873" target="_blank" rel="noopener noreffer">L-BFGS 算法</a>。</p>
<p>为了评估 MLP 的预测效果，使用 4 折的交叉验证法。</p>
<p>每次折叠时先从 3 个视频训练 MLP，接着使用训练好的模型去预测第 4 个视频的 tile 大小。</p>
</li>
</ol>
<h2 id="实现">实现</h2>
<p></p>
<p>将视频划分成1秒长的片段，之后为每个片段解决整数线性问题来确定最优的tile划分策略。</p>
<ol>
<li>使用MLP模型估计每个tile的存储开销。</li>
<li>根据视图的集合$d$及其对应的可能性分布$p$，来估计视图的下载开销$c^{(view)}$。</li>
<li>构造矩阵$A$时，限制最大的tile大小为$12 \times 12$的基本子矩形（如果设置每个基本子矩形包含$64 \times 64$的像素，tile的最大尺寸即为$768 \times 768$的像素）。</li>
<li>使用<a href="https://www.gnu.org/software/glpk/" target="_blank" rel="noopener noreffer">GNU Linear Programming Kit</a>来解决问题。</li>
<li>将所有可能的解子矩形编码进一个二元向量$x$中来表示解。</li>
<li>GLPK的解表明一个可能的解子矩形是否应该被放入解中。</li>
<li>基于最终得到的解，划分片段并使用ffmepg以同样参数的x264格式进行编码。</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>度量指标</p>
<ol>
<li>服务端存储需求。</li>
<li>客户端需要下载的字节数。</li>
</ol>
</li>
<li>
<p>数据来源</p>
<p>数据集：<a href="http://dash.ipv6.enstb.fr/headMovements/" target="_blank" rel="noopener noreffer">dash.ipv6.enstb.fr</a></p>
</li>
<li>
<p>评估准备</p>
<p>下载5个使用ERP投影的视频，抽取出测试中用户看到的对应部分。</p>
<p>每个视频都有$1920 \times 960$和$3840 \times 1920$的两种分辨率的版本。</p>
<p>$1920 \times 960$视频的基本子矩形尺寸为$64 \times 64$的像素。</p>
<p>$3840 \times 1920$视频的基本子矩形尺寸为$128 \times 128$的像素。</p>
<p>将视频划分成1秒长的片段，对每个片段都产生出MLP所需的5元组特性。</p>
<p>之后使用训练好的MLP模型来预测所有可能的tile的大小。</p>
</li>
<li>
<p>数据选择</p>
<ol>
<li>
<p>从数据集中随机选择出40个用户的集合。</p>
</li>
<li>
<p>假设100°的水平和垂直FOV，并使用40个用户的头部方向来为每个片段产生$p_v$和$d_v$。</p>
<p>即：分块的决策基于每个片段的内容特征信息与用户的经验视图模式。</p>
</li>
</ol>
</li>
<li>
<p>参数设定：$\alpha = 0,1,1000$.</p>
</li>
<li>
<p>对比实验：</p>
<p>一组使用由ILP得出的结构进行分块；</p>
<p>另外一组：</p>
<ul>
<li>
<p>$1920 \times 960$的视频片段分别使用$64 \times 64$，$128 \times 128$，$256 \times 256$，$512 \times 512$的方案固定大小分块。</p>
</li>
<li>
<p>$3840 \times 1920$的视频片段分别使用$128 \times 128$，$256 \times 256$，$512 \times 512$，$1024 \times 1024$的方案固定大小分块。</p>
</li>
</ul>
</li>
<li>
<p>划分结果对比</p>
<p></p>
</li>
</ul>
<h3 id="服务端的存储大小">服务端的存储大小</h3>
<p></p>
<p>按照$\alpha = 0$方案分块之后的视频大小几乎与未分块之前的视频大小持平，有时甚至略微小于未分块前的视频大小。</p>
<p>因为所有分块方案都使用相同的编码参数，所以重新编码带来的有损压缩并不会影响竞争的公平性。</p>
<p>如果将$\alpha$的值调大，存储的大小会略微增大；固定分块大小的方案所得到的存储大小也会随tile变小而变大。</p>
<h3 id="客户端的下载大小">客户端的下载大小</h3>
<ul>
<li>
<p>预测完美的情况——下载的tile没有任何浪费</p>
<p></p>
<p>$\alpha= 1000$的情况下，OpTile的表现总是最好的。</p>
</li>
<li>
<p>正常预测的情况</p>
<p>预测的方法：假设用户的头部方向不会改变，预测的位置即为按照当前方向几秒之后的位置。</p>
<p></p>
<p>相比于完美假设的预测，所有分块方案的下载大小都增大了。</p>
<p>$\alpha = 1000$的方案在两个视频的情况下都取得了最小的下载大小。在剩下的3个视频中，OpTile方案的下载大小比起最优的固定分块大小方案不超过25%。</p>
<p>尽管固定分块大小的方案可能表现更好，但是这种表现随视频的改变而变化显著。</p>
<p><strong>因为固定分块的方案没有考虑视频内容的特性与用户的观看行为。</strong></p>
</li>
</ul>
]]></description>
</item>
</channel>
</rss>

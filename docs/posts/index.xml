<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - Ayamir&#39;s Blog</title>
        <link>https://ayamir.github.io/posts/</link>
        <description>所有文章 | Ayamir&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miracle_l@bupt.edu.cn (Ayamir)</managingEditor>
            <webMaster>miracle_l@bupt.edu.cn (Ayamir)</webMaster><lastBuildDate>Tue, 21 Dec 2021 10:11:23 &#43;0800</lastBuildDate><atom:link href="https://ayamir.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>Note for TBRA</title>
    <link>https://ayamir.github.io/2021/12/note-for-tbra/</link>
    <pubDate>Tue, 21 Dec 2021 10:11:23 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-tbra/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3474085.3475590" target="_blank" rel="noopener noreffer">TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming</a></p>
<p>Level：ACM MM 21</p>
<p>Keywords：Adaptive tiling and bitrate，Mobile streaming</p>
<h2 id="创新点">创新点</h2>
<h3 id="背景">背景</h3>
<p>现有的固定的tile划分方式严重依赖viewport预测的精度，然而viewport预测的准确率往往变化极大，这导致基于tile的策略实际效果并不一定能实现其设计初衷：保证QoE的同时减少带宽浪费。</p>
<p>考虑同样的viewport预测结果与不同的tile划分方式组合的结果：</p>
<p></p>
<p>从上图可以看到：</p>
<ul>
<li>如果采用$6 \times 6$的分块方式，就会浪费26，32两个tile的带宽，同时15，16，17作为本应在实际viewport中的tile并没有分配最高的优先级去请求。</li>
<li>如果采用$5 \times 5$的分块方式，即使预测的结果与实际的viewport有所出入，但是得益于tile分块较大，所有应该被请求的tile都得到了最高的优先级，用户的QoE得到了保证。</li>
</ul>
<p>另一方面，基于tile的方式带来了额外的编解码开销（可以看这一篇论文：<a href="https://ayamir.github.io/2021/12/note-for-optile/" target="_blank" rel="noopener noreffer">note-for-optile</a>），而这样的性能需求对于移动设备而言是不可忽略的。</p>
<h3 id="创新">创新</h3>
<p>除了考虑常见的因素如带宽波动和缓冲区占用之外，提出同时自适应分块策略和码率分配以应对变化的viewport预测性能和受限的移动设备的解码能力。</p>
<h3 id="论文组织">论文组织</h3>
<ol>
<li>首先使用现实世界的轨迹分析了典型的viewport预测算法并确定了其性能的不确定性。</li>
<li>接着讨论了不同的分块策略在tile选择和解码效率上的影响。</li>
<li>自适应的分块策略可以适应viewport预测的错误，并能保证tile选择的质量。</li>
<li>为解码时间建构了分析模型，可以在给定受限的计算资源时用于选择恰当的分块策略和码率。</li>
<li>形式化了优化模型，讨论了自适应算法的细节。</li>
<li>评估证明了方案的优越性。</li>
</ol>
<h2 id="motivation">Motivation</h2>
<h3 id="分块策略对tile选择的影响">分块策略对tile选择的影响</h3>
<p>实现4种轻量的viewport预测算法：线性回归LR、岭回归RR、支持向量回归、长短期记忆LSTM。</p>
<p>设置历史窗口大小为2s，预测窗口大小为1s；viewport的宽度和高度分别为100°和90°。</p>
<p>默认的分块策略为$6 \times 6$；头部移动数据集来自<a href="https://dl.acm.org/doi/10.1145/3204949.3208139" target="_blank" rel="noopener noreffer">公开数据集</a>。</p>
<h4 id="viewport预测的不准确性">viewport预测的不准确性</h4>
<p>研究表明，用户的头部运动主要发生在水平方向而较少发生在垂直方向，所以只分析水平方向的预测。</p>
<p>实际的商业移动终端只有有限的传感和处理能力，并不能支持高频的viewport预测采样。</p>
<p>视频内容的不同类型会显著影响预测的精度，基于录像环境（室内或户外）和相机的运动状态分类。</p>
<ul>
<li>
<p>改变采样频率会直接影响viewport预测的精度，频率越低，精度越低。</p>
</li>
<li>
<p>相机运动的viewport预测错误率比相机静止的明显更高。</p>
</li>
</ul>
<h4 id="通过分块容忍预测错误">通过分块容忍预测错误</h4>
<p>因为不管tile的哪个部分被包含在预测的viewport中，只要包含一部分就会请求整个tile，所以增大每个tile的尺寸能吸收预测错误。</p>
<p>实验验证：</p>
<p>设定从$4 \times 4$到$10 \times 10$的分块方式，使用不同的预测误差来检查分块设定可以容纳的最大预测误差，同时保持tile选择结果的相同质量。</p>
<p>用$F_1$分数来表示tile选择的质量：$F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall}$。</p>
<p>实验结果表明更大的tile尺寸更能容忍预测错误。</p>
<h3 id="分块策略对解码复杂性的影响">分块策略对解码复杂性的影响</h3>
<p>虽然当前的移动设备硬件性能发展迅速，但是实时的高码率高分辨率全景视频的解码任务还是充满挑战。</p>
<p>分块对于编码的影响：</p>
<ul>
<li>tile越小，帧内和帧间内容的相关区域就越小，编码效率越低。</li>
</ul>
<p>直接影响解码复杂性的因素：</p>
<ul>
<li>tile的数量。</li>
<li>视频的分辨率。</li>
<li>用于解码的资源。</li>
</ul>
<p>固定其中1个因素改变另外2个因素来检查其对解码的影响：</p>
<p></p>
<p>根据对图的观察可以得出这3个因素在经验上是相互独立的，因为这三幅图之中的图像几乎相同。</p>
<p>分别用$F_n(x), F_r(x), F_c(x)$表示tile数量、分辨率、线程数量为$x$时，解码时间与基线时间的比值。</p>
<p>将这3个比值作为3个乘子建立分析模型：
$$
D = D_0 \cdot F_n(x_1) \cdot F_r(x_2) \cdot F_c(x_3)
$$
上式表示计算整体的解码时间，其中tile数量为$x_1$、分辨率为$x_2$、线程数量为$x_3$；$D_0$时解码的基线时间。</p>
<p>这个模型将用于帮助做出分块和码率适应的决策。</p>
<p>注意在实际情况中，可供使用的计算资源（线程数）是受限的，需要根据设备当前可用的计算资源来分配。</p>
<h2 id="tbra的设计">TBRA的设计</h2>
<ul>
<li>$S = \lbrace s_1, s_2, &hellip; \rbrace$ 表示360°视频分块方式的集合；</li>
<li>对于分块方式$s_i$，$|s_i|$ 表示这种方案中tile的数量；</li>
<li>当 $i &lt; j$ 时，假设 $|s_i| &lt; |s_j|$；</li>
<li>对于分块方式$s$， $b_{i, j}$ 表示第 $i$ 块的tile $j$，$i \le 块的数量, j \le |s|$；</li>
<li>目标是确定分块方式$s$，并为每个tile确定其码率$b_{i, j}$；</li>
</ul>
<h3 id="分块自适应">分块自适应</h3>
<h4 id="自适应的概念">自适应的概念</h4>
<p>分块尺寸大小会导致viewport容错率和传输效率的变化。</p>
<ul>
<li>分块尺寸小，极端情况下每个像素点作为一个tile，viewport容错率最小，但是传输效率达到100%；</li>
<li>分块尺寸大，极端情况下整个视频帧作为一个tile，viewport容错率最大，但是传输效率最小；</li>
</ul>
<p>优化的目标就是在这两种极端条件中找到折中的最优解。</p>
<h4 id="分块选择">分块选择</h4>
<p>以$\overline{r_d}, d \in \lbrace left, right, up, down \rbrace$为半径扩大预测区域；$e_d$表示过去n秒中方向 $d$ 的预测错误平均值；
$$
\overline{r_d} = (1-\alpha) \cdot \overline{r_d} + \alpha \cdot e_d
$$
预测区域的扩展被进一步用于tile选择，受过去预测精度的动态影响。</p>
<p>下一步检查不同分块方式，进而找到QoE和传输效率之间的折中。</p>
<p>对于每个分块方式，比较基于扩展的预测区域的tile选择的质量。使用2个比值作为QoE和传输效率的度量：
$$
Miss\ Ratio = \frac{of\ missed\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p>$$
Waste\ ratio = \frac{of\ unnecessary\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p></p>
<p>这2个比值的tradeoff可以在上图中清晰地看出。</p>
<p>使用分块方式对应的惩罚$Tiling\ i_{penalty}$来评估其性能：
$$
Tiling\ i_{penalty} = \beta \cdot Miss\ Ratio + |1/cos(\phi_i)| \cdot Waste\ Ratio
$$
$\phi_i$ 是viewport $i$ 的中心纬度坐标，它表明随着viewport的垂直移动，浪费率的权重会发生变化。（因为投影方式是ERP）</p>
<p>检查完所有的方式之后，最终选择惩罚最小的分块方式。</p>
<h3 id="码率自适应">码率自适应</h3>
<h4 id="视频质量">视频质量</h4>
<p>$w_{i, j}$表示在第 $i$ 个视频块播放时，tile $j$ 的权重；在当前方案中 $w_{i, j} = 0\ or\ 1$ 取决于tile是否在预测的viewport中。</p>
<p>$q(b_{i, j})$ 是tile比特率选择 $b_{i, j}$ 与用户实际感知到的质量之间的非递减映射函数。</p>
<p>第 $i$ 个视频块的质量等级可以定义为：</p>
<p>$$
Q^{(1)}_i = \sum^n_{j=1} w_{i, j} q(b_{i, j})
$$</p>
<p>使用最新研究的<a href="https://ieeexplore.ieee.org/document/8979422/citations?tabFilter=papers" target="_blank" rel="noopener noreffer">主观视频质量模型</a>：
$$
subjective\ PSNR:\ q_i = PSNR_i \cdot [M(v_i)]^{\gamma} [R(v_i)]^{\delta}
$$
$M(v_i)$ 是检测阈值；$R(v_i)$ 是视网膜滑移率；$v_i$ 是第播放 $i$ 个视频块时viewport的移动速度；$\gamma = 0.172, \delta = -0.267$</p>
<h4 id="质量变化">质量变化</h4>
<p>连续视频块之间的强烈质量变化会损害QoE，定义质量变化作为响铃两个视频块之间质量的变化：
$$
Q^{(2)}_i = |Q^{(1)}_1 - Q^{(1)}_{i-1}|,\ i \in [2, m]
$$</p>
<h4 id="重缓冲时间">重缓冲时间</h4>
<p>参数设置：</p>
<ul>
<li>$C_i$ 表示下载视频块 $i$ 的预计吞吐量；</li>
<li>$B_i$ 表示客户端开始下载视频块 $i$ 时缓冲区的占用率；</li>
<li>$B_{default}$ 表示在启动阶段默认的缓冲区填充等级，记 $B_{default} = B_1$；</li>
<li>下载第 $i$ 个视频块需要时间 $\sum^n_{j=1} b_{i, j} / C_i$ ；</li>
<li>每个视频块的长度为 $L$ ；</li>
</ul>
<p>缓冲区的状态应该在每次视频块被下载的时候都得到更新，则下一个视频块 $i+1$ 的缓冲区占用情况可以计算为：
$$
B_{i+1} = max\lbrace B_1 - \sum^n_{j=1} b_{i, j} / C_i,\ 0\rbrace + L
$$
下载第 $i$ 个视频块时的重缓冲时间可以计算为：</p>
<p>$$
Q^{(3)}_i = max \lbrace \sum^n_{j=1} b_{i, j} / C_i - B_i,\ 0 \rbrace + t_{miss}
$$</p>
<p>第一部分是下载时间过长且缓冲区耗尽，视频无法播放情况下的重新缓冲时间；</p>
<p>第二部分 $t_{miss}$ 表示下载缺失的tile所花费的时间（在视频块播放过程中被看到但是之前没有分配码率的tile）。</p>
<h4 id="优化目标">优化目标</h4>
<p>第 $i$ 个视频块的整体优化目标可以定义为前述3个指标的加权和：
$$
Q_i = pQ^{(1)}_i - qQ^{(2)}_i - rQ^{(3)}_i
$$
各个系数的符号分配表示：最大化视频质量、最小化块间质量变化、最小化重缓冲时间。</p>
<p>传统意义上使用所有视频块的平均QoE作为优化对象，但实际上很难获得从块 $1$ 到块 $m$ 的整个视界的完美的未来信息。</p>
<p>为了处理预测长期吞吐量和用户行为的难度，采用<a href="https://dl.acm.org/doi/10.1145/2785956.2787486" target="_blank" rel="noopener noreffer">基于MPC的框架</a>，在有限的范围内优化多个视频块的QoE，最终的目标函数可以形式化为：
$$
\underset{b_{i, j}, i \in [t, t+k-1], j \in [1, n]}{max} \sum^{t+k-1}_{i=t} Q_i
$$
因为短期内的viewport预测性能和网络状况可以很容易得到，QoE优化可以通过使用窗口 $[t, t+k-1]$ 内的预测信息；</p>
<p>接着将视界向前移动到 $[t+1, t+k]$ ，更新新的优化窗口的信息，为下一个视频块执行QoE优化，直到最后一个窗口。</p>
<p>使用基于MPC的公式的优点：由于受限的问题规模，每个优化问题的实例都是实际可解的。</p>
<h4 id="高效求解">高效求解</h4>
<p>提出的公式天然适合在线求解，得益于短窗口的实例问题规模很小，QoE优化可以通过详尽搜索定期解决。</p>
<p>但是因为优化过程需要高频调用，所以对于大的搜索空间还是充满挑战。</p>
<p>为了支持实时优化，需要对搜索空间进行高效剪枝，确定几点约束：</p>
<ul>
<li>
<p>解码时间需要被约束；</p>
<p>解码时间应该短于回放长度。</p>
<p>给定移动设备上可用的计算资源，可以得到支持的最大解码线程数。</p>
<p>基于解码时间的分析模型，由于解码复杂度和分辨率的单调性，可以找到设备能够限定时间内解码的最大质量水平，这会将码率选择限制在有界搜索空间内。</p>
</li>
<li>
<p>码率选择应该考虑吞吐量的限制：$\sum^n_{j=1} b_{i, j} \le LC_i$ ；</p>
<p>不会主动耗尽缓冲区，无需让其处理吞吐量的波动。</p>
</li>
<li>
<p>码率选择应该考虑tile的分类；</p>
<p>tile的码率不应该低于同一个视频块中更低权重tile的码率： $b_{i, j} \ge b_{i, j'}, \forall w_{i, j} &gt; w_{i, j'}$ 。</p>
</li>
<li>
<p>属于相同类别的tile比特率选择应该是同一个等级；</p>
<p>这使码率自适应在tile类的级别上执行而非单个tile的级别，大大减小了搜索空间的规模。</p>
</li>
<li>
<p>当优化窗口中的吞吐量和用户行为保持稳定时，同一个窗口中的tile应该有相同的结果。</p>
</li>
</ul>
<h3 id="tbra-workflow">TBRA workflow</h3>
<p></p>
<p>这样的方式需要在服务端存储大量的按照不同分块方式划分的不同码率版本的视频块，这一点可以进一步研究。</p>
<p>但是对于移动终端设备而言，这样的解决方案只引入了可以忽略不计的开销。</p>
<p>观察到tile自适应问题具有全局最优通常就是局部最优的特点，因此可以大大减少计算量。</p>
<p>基于MPC的优化workflow还可以有效地解决码率自适应问题。</p>
]]></description>
</item>
<item>
    <title>Note for Content Motion Viewport Prediction</title>
    <link>https://ayamir.github.io/2021/12/note-for-content-motion-viewport-prediction/</link>
    <pubDate>Mon, 20 Dec 2021 10:47:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-content-motion-viewport-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/abs/10.1145/3328914" target="_blank" rel="noopener noreffer">Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking</a></p>
<p>Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019</p>
<p>Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model</p>
<h2 id="workflow">Workflow</h2>
<ul>
<li>Tracking：VR motion追踪算法：应用了高斯混合模型来检测物体的运动。</li>
<li>Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户viewport来自动更正潜在的预测错误。</li>
<li>Update：viewport动态更新算法：动态调整预测的viewport大小去覆盖感兴趣的潜在viewport，同时尽可能保证最低的带宽消耗。</li>
<li>Evaluation：经验用户/视频评估：构建VR viewport预测方法原型，使用经验360°视频和代表性的头部移动数据集评估。</li>
</ul>
<h2 id="全景直播推流的预备知识">全景直播推流的预备知识</h2>
<h3 id="vr推流直播">VR推流直播</h3>
<p></p>
<p>相比于传统的2D视频推流的特别之处：</p>
<ul>
<li>VR系统是交互式的，viewport的选择权在客户端；</li>
<li>呈现给用户的最终视图是整个视频的一部分；</li>
</ul>
<h3 id="用户头部移动的模式">用户头部移动的模式</h3>
<p>在大量的360°视频观看过程中，用户主要的头部移动模式有4种，使用$i-j\ move$来表示；</p>
<p>其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。</p>
<ul>
<li>$1-1\ move$：单个物体以单一方向移动；</li>
<li>$1-n\ move$：单个物体以多个方向移动；</li>
<li>$m-n\ move$：多个物体以多个方向移动；</li>
<li>$Arbitrary\ move$：用户不跟随任何感兴趣的物体而移动，viewport切换随机；</li>
</ul>
<p></p>
<p>现有的直播VR推流中的viewport预测方法是基于速度的方式，这种方式只对$1-1\ move$这一种模式有效。</p>
<p>本方案的目标是提出对4种模式都有效的预测策略。</p>
<h2 id="系统架构">系统架构</h2>
<p></p>
<h3 id="理论创新">理论创新</h3>
<ul>
<li>
<p>核心功能模块：</p>
<ol>
<li>
<p>motion detection：区分运动物体与静止的背景。</p>
</li>
<li>
<p>feature selection：选择代表性的特征并对运动物体做追踪。</p>
<p>这两个模块使系统能识别用户可能感兴趣的viewport。</p>
</li>
</ol>
</li>
<li>
<p>使用贝叶斯方法分析用户观看行为并形式化用户的兴趣模型。</p>
<ol>
<li>
<p>使用错误恢复机制来使当预测错误被检测到时的预测viewport去适应实际的viewport，尽管不能消除预测错误但是能避免在此基础上进一步的预测错误。</p>
</li>
<li>
<p>使用动态viewport更新算法来产生大小可变的viewport，通过同时考虑跟踪到的viewport轨迹和用户当前的速度（矢量）。</p>
<p>这样，即使用户的运动模式很复杂也能有更高的概率去覆盖潜在的视图。</p>
</li>
</ol>
</li>
</ul>
<h3 id="具体实施">具体实施</h3>
<ul>
<li>
<p>虽然提出的运动追踪和错误处理机制是计算密集型的任务，但是这些组件都部署在video packager中，运行在服务端。</p>
</li>
<li>
<p>将生成VR视图的工作负载移动到服务端，进一步减少了客户端的计算开销以及网络开销。</p>
</li>
</ul>
<h2 id="形式化">形式化</h2>
<h3 id="基于运动轨迹的viewport预测">基于运动轨迹的viewport预测</h3>
<p>使用<a href="https://ieeexplore.ieee.org/document/1333992" target="_blank" rel="noopener noreffer">GMM</a>完成运动检测，使用<a href="https://ieeexplore.ieee.org/document/323794" target="_blank" rel="noopener noreffer">Shi-Tomasi algorithm</a>解决运动轨迹跟踪问题。</p>
<p></p>
<ol>
<li>
<p>运动检测</p>
<p>GMM前景提取</p>
</li>
<li>
<p>特征选取与过滤</p>
<p>采用 Shi-Tomasi algorithm 从视频中检测代表性的特征，直接检测得到的代表性特征数量较多而难以追踪。</p>
<p>采用两种过滤的方法来减少要追踪的特征数量。</p>
<ul>
<li>
<p>比较当前帧和前一帧的特征，只保留其共有的部分。</p>
</li>
<li>
<p>采用第1步中运动检测的方式，只保留运动的部分。</p>
</li>
</ul>
</li>
<li>
<p>viewport生成</p>
<p>经过选择和过滤之后的特征通常分布在不能被单一用户视图所覆盖的广阔区域中。</p>
<p>在整个360°视频中可能存在多个运动的物体，即$m-n\ move$。</p>
<p>提出一种系统的方式来产生用户最可能跟随观看的viewport。</p>
<p>直觉是用户更可能将大部分注意力放在两种类型的物体上：</p>
<ul>
<li>离用户更近的物体。</li>
<li>就物理形状而言更“重要”的物体。</li>
</ul>
<p>这两种类型的物体大多包含最密集和最大量的特征，因此通过所有特征的重心来计算预测用户视图的中心。</p>
<p>对于剩余的特征列表：$\vec{F} = [f_1, f_2, f_3, &hellip;, f_k]$，其中$f_i(i = 1 &hellip; k)$表示特征$f_i = &lt;f^{(x)}_i, f^{(y)}_i&gt;$的像素点坐标，则预测出的viewport中心坐标可以计算出来：
$$
l_x = \frac{1}{k} \sum^k_{i=1} f^{(x)}_i;\ l_y = \frac{1}{k} \sum^k_{i=1} f^{(y)}_i.
$$
考虑到即使预测的viewport中包含用户观看的物体，预测得到的viewport也可能会与实际的viewport存在差异。</p>
<p>所以预测的viewport可能比实际的viewport要大，所以使用缩放因子$S_c$来产生预测的viewport。</p>
<p>给出用户viewport的大小$S_{user}$，预测的viewport可以通过$S_{pre} = S_c \cdot S_{user}$计算出来。</p>
</li>
</ol>
<h3 id="基于用户反馈的错误恢复">基于用户反馈的错误恢复</h3>
<p>video packager可以通过HMD和web服务器通过反向路径从用户处检索用户实际视图的反馈信息。</p>
<p>基于反馈的错误恢复机制在以下两种场景中表现良好：</p>
<ol>
<li>
<p>没有运动的物体</p>
<p>如果没有检测到运动的物体，则用户很可能是在观看静止的物体，这会导致基于运动目标的viewport预测失败。</p>
<p>在这种场景中，可以认为视频内容已经不再是决定用户viewport的因素，而只取决于用户自身的行为。</p>
<p>因此采用基于速度的方式来预测viewport。（这样的决策可以在运动检测模块没有检测到运动物体时就做出）</p>
<p>一旦从反馈路径上得到用户信息，可以产生用户viewport位置向量：$\vec{L} = [l_1, l_2, l_3, &hellip;, l_M]$，其中$l_i$表示第$i$个帧中用户viewport的位置，$M$表示视频播放缓冲区中的帧数。那么可以计算viewport速度：
$$
\vec{V} = \frac{\vec{(l_2 - l_1)} + \vec{(l_3 - l_2)} &hellip;.(l_M - l_{M-1})}{M-1} = \frac{(\vec{l_M - l_1})}{M-1}
$$
下一帧的预测位置$L_{M=1}$也可以计算出来：
$$
l_{M+1} = l_M + \vec{V}
$$</p>
</li>
<li>
<p>预测视图与实际视图的不匹配</p>
<p>一旦运动追踪策略检测到用户实际的视图和预测的视图不同，就会触发恢复机制去追踪用户实际在看着的物体。</p>
<p>可以使用运动追踪方式确定用户实际观察的物体的速度。</p>
<p>给出前一帧匹配的特征$\vec{FA} = [fA_1, fA_2, fA_3, &hellip;, fA_p]$和当前帧的特征$\vec{FB} = [fB_1, fB_2, fB_3, &hellip;, fB_p]$，可以计算出速度：
$$
V_x = \frac{1}{p} (\sum^p_{i=1} fB^{(x)}_i - \sum^p_{i=1}fA^{(x)}_i),\
V_y = \frac{1}{p} (\sum^p_{i=1} fB^{(y)}_i - \sum^p_{i=1}fA^{(y)}_i),
$$
假设预测的viewpoint是$(l_x, l_y)$，修改之后的viewpoint是$(l_x + V_x,\ l_y + V_y)$。</p>
</li>
</ol>
<h3 id="动态viewport更新">动态viewport更新</h3>
<p>前述的错误恢复机制发生在viewport预测错误出现之后，任务是避免未来更多的错误。</p>
<p>动态的viewport更新则努力避免viewport预测错误。</p>
<p>关键思想是扩大预测的viewport大小，以高概率去覆盖$m-n\ move$和$arbitrary\ move$下所有潜在的运动目标；更重要的是动态调整视图的大小去获得更高效的带宽利用率。</p>
<ul>
<li>
<p>对于一个360°全景视频，将360°的帧均分为$N = n \times n$个网格，每个网格看作是一个tile，预测的viewport即为$N$个tile的子集。</p>
</li>
<li>
<p>使用贝叶斯方法分析用户的观看行为，每个tile分配一个独立的贝叶斯模型，所以每个tile可以独立更新。</p>
</li>
<li>
<p>设$X$表示用户viewport，$Y$表示静态内容，$Z$表示运动物体。</p>
</li>
<li>
<p>未来的用户viewport可以以条件概率计算为$P(X|Y,\ Z)$，$Y$与$Z$相互独立。</p>
</li>
<li>
<p>用户的viewport可以通过反馈信息得出$P(X)$；用户观看静态特征可以表示为$P(X|Y)$；用户观看动态特征可以表示为$P(X|Z)$。</p>
</li>
<li>
<p>$P(X|Y, Z)$可以计算为：
$$
P(X|Y, Z) = \frac{P(Y|X) \cdot P(Z|X) \cdot P(X)}{P(Y, Z)}
$$</p>
</li>
<li>
<p>只要用户开始观看，对于tile $T_i$，就能得到其先验概率$P(Y_i|X_i)$和$P(Z_i|X_i)$，进而根据贝叶斯模型计算出$P(X|Y, Z)$。</p>
</li>
</ul>
<p>为每个tile定义两种属性：</p>
<ol>
<li>当前状态：表示此tile是否属于预测的viewport（属于标记为$PREDICTED$，不属于标记为$NONPREDICTED$）。</li>
<li>生存期：表示此tile会在view port中存在多长时间（例如定义3种等级：$ZERO$，$MEDIUM$，$HIGH$，实际的定义划分可以根据具体的用户和视频设定）。</li>
</ol>
<h2 id="预测步骤">预测步骤</h2>
<p>按照形式化中提出的3步，分为系统初始化、帧级别的更新、缓冲区级别的更新。</p>
<ol>
<li>
<p>系统初始化</p>
<p>初始化阶段中，view更新算法将所有的$N$个tile标注为$PREDICTED$，并将生存期设置为$MEDIUM$，即系统向用户发送完整的一帧作为自举。</p>
<p>这样设定的原因在于：当用户第一次启动视频会话时，允许“环视”类型的移动，这可能会覆盖360°帧的任意viewport。</p>
</li>
<li>
<p>帧级别的更新</p>
<p>给定一帧，应用修改后的motion追踪算法在运动区域中选择特征，而不使用特征的密度做进一步的过滤。</p>
<p>使用有多个tile的多个视图来覆盖一个放大的区域，该区域包含作为预测viewport的移动对象上的所有特征，这样就能适应$m-n\ move$中的用户行为。</p>
<p>设计帧级别的算法标记选择的tile作为$PREDICTED$并设置其生存期为$HIGH$（直觉上讲运动中的物体或用户所感兴趣的静态特征会更以长时间保留在viewport之中）。</p>
</li>
<li>
<p>缓冲区级别的更新</p>
<p>以缓冲区长度为间隔检索用户的实际视图，基于此可以对tile的两种属性做出调整。</p>
<ol>
<li>对于与用户实际视图重叠的tile，设置为$PREDICTED$和$HIGH$。</li>
<li>对于用户实际视图没有出现但出现在预测的视图中的tile，生存期减1，如果生存期减为$ZERO$，就重设其状态为$NONPREDICTED$，将其从预测的viewport中移除。</li>
</ol>
<p></p>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for RnnQoE</title>
    <link>https://ayamir.github.io/2021/12/note-for-rnnqoe/</link>
    <pubDate>Thu, 16 Dec 2021 19:53:10 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-rnnqoe/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9580281" target="_blank" rel="noopener noreffer">QoE-driven Mobile 360 Video Streaming: Predictive
View Generation and Dynamic Tile Selection</a></p>
<p>Level：ICCC 2021</p>
<p>Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles</p>
<h2 id="系统建模与形式化">系统建模与形式化</h2>
<h3 id="视频划分">视频划分</h3>
<p>先将视频划分成片段：$\Iota = {1, 2, &hellip;, I}$表示片段数为$I$的片段集合。</p>
<p>接着将片段在空间上均匀划分成$M \times N$个tile，FOV由被用户看到的tile所确定。</p>
<p>使用ERP投影，$(\phi_i, \theta_i),\ \phi_i \in (-180\degree, 180\degree], \theta_i \in (-90\degree, 90\degree]$来表示用户在第$i$个片段中的视点坐标。</p>
<p>播放过程中记录用户头部运动的轨迹，积累的数据可以用于FOV预测。</p>
<p>跨用户之间的FOV轨迹可以用于提高预测精度。</p>
<h3 id="qoe模型">QoE模型</h3>
<ul>
<li>
<p>前提</p>
<p>视频编解码器预先确定，无法调整每个tile的码率。</p>
</li>
<li>
<p>实现</p>
<ol>
<li>每个tile都以不同的码率编码成不同的版本。</li>
<li>每个tile都有两种分辨率的版本。</li>
</ol>
</li>
<li>
<p>QoE内容</p>
<p>客户端收到的视频质量和观看时的卡顿时间。</p>
</li>
<li>
<p>质量形式化</p>
<p>对于每个片段$i \in \Iota$，$S_i = {\tau_{i, j}}_{j=1}^{M \times N}$是用来表示用户实际看到的tile的集合的向量。</p>
<p>$\tau_{i, j} = 1$表示第$i$个段中的第$j$个tile被看到；$\tau_{i, j} = 0$表示未被看到。</p>
<p>同样的， $\tilde{S}_i = {\tilde{\tau}_{i, j}}_{j = 1}^{M \times N}$ 表示经过FOV预测和tile选择之后成功被传送到用户头戴设备上的tile集合的向量。</p>
<p>$\tilde{\tau}_{i, j} = 1$表示第$i$个段中的第$j$个tile被用户接收；$\tilde{\tau}_{i, j} = 0$表示未被接收。</p>
<p>第$i$个段的可感知到的质量可以表示为：</p>
<p>$$
Q_i = \sum_{j = 1}^{M \times N} p_{i, j}b_{i, j}\tau_{i, j}\tilde{\tau}_{i, j}
$$</p>
<p>$b_{i, j}$表示第$i$个片段的第$j$个tile的码率；$p_{i, j}$表示对不同位置tile所分配的权重；</p>
</li>
<li>
<p>关于权重$p_{i, j}$</p>
<p>研究表明用户在全景视频FOV中的注意力分配并不是均等的，越靠近FOV中心的tile对用户的QoE贡献越大。</p>
<p>下面讨论单个片段的情况：用$(\phi_j, \theta_j)$表示tile中心点的坐标，并映射到笛卡尔坐标系上$(x_j, y_j, z_j)$：</p>
<p>$$
x_j = cos\theta_jcos\phi_j,\ y_j = sin\theta_j,\ z_j = -cos\theta_jsin\phi_j
$$</p>
<p>则两个tile之间的半径距离$d_{j, j'}$可以表示为：</p>
<p>$$
d_{j, j'} = arccos(x_j x_{j'} + y_j y_{j'} + z_j z_{j'})
$$</p>
<p>对于第$i$个片段，假设用户FOV中心的tile为$j^*$，那么第$j$个tile的权重可以计算出来：</p>
<p>$$
p_{i, j} = (1 - d_{j, j^*} / \pi) \tau_{i, j}
$$</p>
</li>
<li>
<p>卡顿时间形式化</p>
<p>当$\tilde{\tau}_{i, j}$与$\tau_{i, j}$出现分歧时，用户就不能成功收到请求的tile，头戴设备中显示的内容就会被冻结，由此导致卡顿。</p>
<p>对于任意的片段$i \in \Iota$，相应的卡顿时间$D_i$可以计算出来：</p>
<p>$$
D_i = \frac{\sum_{j = 1}^{M \times N} b_{i, j} \cdot [\tau_{i, j} - \tilde{\tau}_{i, j}]^+}{\xi}
$$</p>
<p>$[x]^+ = max \lbrace x, 0 \rbrace $；$\xi$表示可用的网络资源（已知，并且在推流过程中保持为常数）</p>
<p>卡顿发生于在播放时，用户FOV内的tile还没有被传输到用户头戴设备中的时刻，终止于所有FOV内tile被成功传送的时刻。</p>
</li>
<li>
<p>质量与卡顿时间的结合</p>
<p>$$
max\ QoE = \sum_{i = 1}^I (Q_i - wD_i)
$$</p>
<p>$w$表示卡顿事件的惩罚权重。例如，w＝1000意味着1秒视频暂停接收的QoE惩罚与将片段的比特率降低1000 bps相同。</p>
</li>
</ul>
<h2 id="联合viewport预测与tile选择">联合viewport预测与tile选择</h2>
<p>联合框架包括viewport预测和动态tile选择两个阶段。</p>
<p>viewport预测阶段集成带有注意力机制的RNN，接收用户的历史头部移动信息作为输入，输出每个tile出现在FOV中的可能性分布。</p>
<p>选择tile阶段为预测的输出建立的上下文空间，基于上下文赌博机学习算法来选择tile并确定所选tile的质量版本。</p>
<p></p>
<p></p>
<h3 id="viewport预测">Viewport预测</h3>
<p>FOV预测问题可以看作是序列预测问题。</p>
<p>不同用户观看相同视频时的头部移动轨迹有强相关性，所以跨用户的行为分析可以用于提高新用户的viewport预测精度。</p>
<p>被广泛使用的LSTM的变体——Bi-LSTM（Bi-directional LSTM）用于FOV预测。</p>
<ol>
<li>
<p>输入参数构造</p>
<p>为了构造Bi-LSTM学习网络，需要对不同用户的viewpoint特性作表征。</p>
<ul>
<li>
<p>在用户观看事先划分好的$I$个片段时，记录每个片段对应的viewpoint坐标：</p>
<p>$\Phi_{1:I} = {\phi_i}^I_{i = 1},\ \Theta_{1:I} = {\theta_i}^I_{i=1}$</p>
</li>
<li>
<p>预测时使用的历史信息的窗口大小记为$k$；</p>
</li>
<li>
<p>对于第$i, (i &gt; k)$个片段，相应的viewpoint特性由$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$所给出；</p>
</li>
<li>
<p>列索引$m_i$和行索引$n_i$作为viewpoint tile $(\phi_i, \theta_i)$的标签，由<a href="https://zh.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener noreffer">独热编码</a>表示；</p>
</li>
<li>
<p>通过滑动预测的窗口，所看到的视频片段的特性和标签可以被获取。</p>
</li>
</ul>
</li>
<li>
<p>LSTM网络构造</p>
<p>整个网络包含3层：</p>
<ul>
<li>遗忘门层决定丢弃哪些信息；</li>
<li>更新门层决定哪类信息需要存储；</li>
<li>输出门层过滤输出信息。</li>
</ul>
<p>为了预测用户在第$i$个段的viewpoint，LSTM网络接受$\Phi_{i-1:i-k}$和$\Theta_{i-1:i-k}$作为输入；输出行列索引；</p>
<p>$$
m_i = LSTM(\theta_{i-k}, &hellip;, \phi_{i-1}; \alpha)
$$</p>
<p>$$
n_i = LSTM(\theta_{i-k}, &hellip;, \theta_{i-1}; \beta)
$$</p>
<p>$\alpha, \beta$是学习网络的参数；分类交叉熵被用作网络训练的损失函数。</p>
</li>
<li>
<p>Bi-LSTM的特殊构造</p>
<ul>
<li>
<p>将公共单向的LSTM划分成2个方向。</p>
<p>当前片段的输出利用前向和反向信息，这为网络提供了额外的上下文，加速了学习过程。</p>
</li>
<li>
<p>双向的LSTM不直接连接，不共享参数。</p>
</li>
<li>
<p>每个时间槽的输入会被分别传输到前向和反向的LSTM中，并分别根据其状态产生输出。</p>
</li>
<li>
<p>两个输出直接连接到Bi-LSTM的输出节点。</p>
</li>
<li>
<p>引入注意力机制为每步时间自动分配权重，从大量信息中选择性地筛选出重要信息。</p>
</li>
<li>
<p>将Softmax层堆叠在网络顶部，以获取不同tile的viewpoint概率。</p>
</li>
</ul>
</li>
</ol>
<p></p>
<h3 id="动态tile选择">动态tile选择</h3>
<p>使用上下文赌博机学习算法来补偿viewport预测错误对QoE造成的影响。</p>
<ul>
<li>
<p>上下文赌博机学习算法概况</p>
<p>上下文赌博机学习算法是一个基于特征的exploration-exploitation技术。</p>
<p>通过在多条手臂上重复执行选择过程，可以获得在不同上下文中的每条手臂的回报。</p>
<p>通过exploration-exploitation，目标是最大化累积的回报。</p>
</li>
<li>
<p>组成部分形式化</p>
<ol>
<li>
<p>上下文</p>
<p>直觉上讲，当预测的viewpoint不够精确时，需要扩大FOV并选择更多的tile进行传输。</p>
<p>为了做出第$i$个片段上的预测viewpoint填充决策，定义串联的上下文向量：</p>
<p>$c_i = [f^s_i, f^c_i]$，$f^s_i$表示自预测的上下文，$f^c_i$表示跨用户之间的预测上下文。</p>
<p>预测输出的用户$u$的viewpoint tile索引用$[\tilde{m}^u_{i-1}, \tilde{n}^u_{i-1}]$表示；</p>
<p>实际的用户$u$的viewpoint tile索引用$[m_{i-1}^u, n_{i-1}^u]$表示；</p>
<p>那么对第$i$个片段而言，自预测的上下文可以计算出来：</p>
<p>$$
f_i^s = [|m_{i-1}^u - \tilde{m}^u_{i-1}|, |n_{i-1}^u - \tilde{n}^u_{i-1}|]
$$
跨用户的上下文信息获取：使用KNN准则选择一组用户，其在前$k$个片段中的轨迹最接近用户$u$的轨迹。</p>
<p>使用$\zeta$表示获得的用户集合，使用</p>
<p>$$E_{\zeta_u}(m_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u} |m_i^u - \tilde{m}_i^u|$$</p>
<p>$$E_{\zeta_u}(n_i) = \frac{1}{|\zeta_u|}\sum_{u \in \zeta_u}|n_i^u - \tilde{n}_i^u|$$</p>
<p>表示预测误差，则：</p>
<p>$$
f_i^u = [E_{\zeta_u}(m_i), E_{\zeta_u}(n_i)]
$$</p>
</li>
<li>
<p>手臂</p>
<p>根据从第一个阶段得到的每个tile的可能性分布，所有的tile可以用倒序的方式排列。</p>
<p>最高可能性的tile被看作FOV的中心，高码率以此tile为中心分配。</p>
<p>剩余的带宽用于扩展FOV，低可能性的tile被顺序选择来扩展FOV直至带宽耗尽。</p>
<p>手臂的状态$a \in {0, 1}$表示tile选择的策略：</p>
<ul>
<li>
<p>$a = 0$表示viewpoint预测准确，填充tile分配了高质量；</p>
</li>
<li>
<p>$a = 1$表示viewpoint预测不准确，填充tile分配的质量较低，为了传送尽可能多的tile而减少卡顿；</p>
</li>
</ul>
</li>
<li>
<p>回报</p>
<p>给定上下文$c_i$，选择手臂$a$，预期的回报$r_{i, a}$建模为$c_i$和$a$组合的线性函数：</p>
<p>$$
\Epsilon[r_{i, a}|c_{i, a}] = c_{i, a}^T \theta_a^*
$$</p>
<p>未知参数$\theta_a$表示每个手臂的特性，目标是为第$i$个片段选择最优的手臂：</p>
<p>$$
a_i^* = \underset{a}{argmax}\ c_{i, a}^T \theta_a^*
$$</p>
<p>使用<a href="https://zhuanlan.zhihu.com/p/38875273" target="_blank" rel="noopener noreffer">LinUCB</a>算法做出特征向量的精确估计并获取$\theta_a^*$。</p>
<p></p>
</li>
</ol>
</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>评估准备</p>
<ul>
<li>使用现有的<a href="https://github.com/xuyanyu-shh/VR-EyeTracking" target="_blank" rel="noopener noreffer">viewpoint轨迹数据集</a>，所有视频被编码为至少每秒25帧，长度为20到60秒；</li>
<li>视频每个片段被划分为$6 \times 12$的tile，每个的角度是$30\degree \times 30\degree$；</li>
<li>初始FOV设定为$90\degree \times 90\degree$，在viewpoint周围是$3 \times 3$的tile；</li>
<li>每个片段的长度为500ms；</li>
<li>默认的预测滑动窗口大小$k = 5$；</li>
<li>HD和LD版本分别以按照HEVC的$QP={32, 22}$的参数编码而得到；</li>
<li>训练集和测试集的比例为$7:3$；</li>
<li>Bi-LSTM层配置有128个隐单元；</li>
<li>batch大小为64；</li>
<li>epoch次数为60；</li>
</ul>
</li>
<li>
<p>性能参数</p>
<ul>
<li>
<p>预测精度</p>
</li>
<li>
<p>视频质量</p>
<p>由传送给用户的有效码率决定：例如实际FOV中的tile码率总和</p>
</li>
<li>
<p>卡顿时间</p>
</li>
<li>
<p>规范化的QoE</p>
<p>实际取得的QoE与在viewpoint轨迹已知情况下的QoE的比值</p>
</li>
</ul>
</li>
<li>
<p>对比目标</p>
<ul>
<li>预测阶段——预测精度
<ol>
<li>LSTM</li>
<li>LR</li>
<li>KNN</li>
</ol>
</li>
<li>取tile的阶段——规范化的QoE
<ol>
<li>两个阶段都使用纯LR</li>
<li>只预测而不做动态选择</li>
</ol>
</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for OpTile</title>
    <link>https://ayamir.github.io/2021/12/note-for-optile/</link>
    <pubDate>Mon, 13 Dec 2021 16:19:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-optile/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3123266.3123339" target="_blank" rel="noopener noreffer">OpTile: Toward Optimal Tiling in 360-degree Video Streaming</a></p>
<p>Level：ACM MM 17</p>
<p>Keyword：Dynamic tile division, Optimize encoding efficiency, Optimize tile size</p>
<h2 id="背景知识">背景知识</h2>
<h3 id="编码过程概述">编码过程概述</h3>
<ol>
<li>
<p>对一帧图像中的每一个 block，编码算法在当前帧的已解码部分或由解码器缓冲的临近的帧中搜索类似的 block。</p>
<p>当编码器在邻近的帧中找到一个 block 与当前 block 紧密匹配时，它会将这个类似的 block 编码进一个动作向量中。</p>
</li>
<li>
<p>编码器计算当前 block 和引用 block 之间像素点的差异，通过应用变换（如离散余弦变换），量化变换系数以及对剩余稀疏矩阵系数集应用无损熵编码（如 Huffman 编码）对计算出的差异进行编码。</p>
</li>
</ol>
<h3 id="对编码过程的影响">对编码过程的影响</h3>
<ol>
<li>基于 tile 的方式会减少可用于拷贝的 block 数量，增大了可供匹配的 tile 之间的距离。</li>
<li>不同的投影方式会影响编码变换输出的系数稀疏性，而这会降低视频编码效率。</li>
</ol>
<h3 id="投影过程">投影过程</h3>
<p>因为直接对 360 度图像和视频的编码技术还没有成熟，所以 360 度推流系统目前还需要先将 3D 球面投影到 2D 平面上。</p>
<p>目前应用最广的投影技术主要是 ERP 和 CMP，分别被 YouTube 和 Meta 采用。</p>
<h4 id="erp-投影">ERP 投影</h4>
<p>基于球面上点的左右偏航角$\theta$与上下俯仰角$\phi$将其映射到宽高分别为$W$和$H$的矩形上。</p>
<p>对于平面坐标为$(x, y)$的点，其球面坐标分别为：</p>
<p>$$
\theta = (\frac{x}{W} - 0.5) * 360
$$</p>
<p>$$
\phi = (0.5 - \frac{y}{H}) * 180
$$</p>
<h4 id="cmp-投影">CMP 投影</h4>
<p>将球面置于一个立方体中，光线从球心向外发射，并分别与球面和立方体相交于两点，这两点之间便建立了映射关系。</p>
<p>之后将立方体 6 个平面拼接成矩形，就可以使用标准的视频编码方式进行压缩。</p>
<p>关于投影方式还可以参考这里的讲解：<a href="https://zhuanlan.zhihu.com/p/106922217" target="_blank" rel="noopener noreffer">谈谈全景视频投影方式</a></p>
<h3 id="tile-方式的缺点">tile 方式的缺点</h3>
<ul>
<li>
<p>降低编码效率</p>
<p>tile 划分越细，编码越低效</p>
</li>
<li>
<p>增加更大的整体存储需求</p>
</li>
<li>
<p>可能要求更多的带宽</p>
</li>
</ul>
<h2 id="optile-的设计">OpTile 的设计</h2>
<p>直觉上需要增大一些 tile 的大小来使与这些 tile 相关联的片段能捕获高效编码所需的类似块。</p>
<p>同时也需要 tile 来分割视频帧来减少传输过程中造成的带宽浪费。</p>
<ul>
<li>
<p>为了明白哪些片段的空间部分可以被高效独立编码，需要关于 tile 的存储大小的不同维度的信息。</p>
</li>
<li>
<p>为了找到切分视频的最好位置，需要在片段播放过程中用户 viewport 运动轨迹的偏好。</p>
</li>
</ul>
<p>将编码效率和浪费数据的竞争考虑到同一个问题之中，这个问题关注的是<strong>一个片段中所有可能的视图的分布</strong>。</p>
<p>片段的每个可能的视图可以被 tile 的不同组合所覆盖。</p>
<p>目标是为一个片段选择一个 tile 覆盖层，以<strong>最小化固定时间段内视图分布的总传输带宽</strong>。</p>
<ul>
<li>目标分离的部分考虑整个固定时间段的表示（representation）的存储开销。</li>
<li>目标的存储部分与下载的带宽部分相竞争。例如，如果一个不受欢迎的视频一年只观看一次，那么我们更喜欢一个紧凑的表示，我们可以期望向用户发送更多未观看的像素。</li>
</ul>
<h2 id="问题形式化">问题形式化</h2>
<table>
<thead>
<tr>
<th style="text-align:center">segment/片段</th>
<th style="text-align:center">推流过程中可以被下载的连续播放的视频单元</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">basic sub-rectangle/基本子矩形</td>
<td style="text-align:center">推流过程中可以被下载的片段中最小的空间划分块</td>
</tr>
<tr>
<td style="text-align:center">solution sub-rectangle/解子矩形</td>
<td style="text-align:center">片段中由若干基本子矩形组成的任何矩形部分</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">$x$</th>
<th style="text-align:center">用于表示子矩形在解中的存在的二元向量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$c^{(stor)}$</td>
<td style="text-align:center">每个子矩形存储开销相关的向量</td>
</tr>
<tr>
<td style="text-align:center">$c^{(view)}$</td>
<td style="text-align:center">给定一个 segment 中用户 viewport 的分布，$c^{(view)}$指相关子矩形的预期下载字节</td>
</tr>
<tr>
<td style="text-align:center">$\alpha$</td>
<td style="text-align:center">分配到$c^{(view)}$的权重，以此来控制相对于传输一个片段的存储开销</td>
</tr>
</tbody>
</table>
<p>考虑将 1 个矩形片段划分成 4 个基本子矩形，其对应的坐标如下：</p>
<p></p>
<p>4 个基本子矩形可以有 9 种分配方式，成为解子矩形，如下（因为需要保持对应的空间关系，所以只有 9 种）：</p>
<p></p>
<ul>
<li>
<p>$x$的形式化</p>
<p>可以用向量$x$来分别表示上图中子矩形在解中的存在：</p>
<p>$$
[1 \times 1\ at\ (0, 0),\ 1 \times 1\ at\ (0, 1),\ 1 \times 1\ at\ (1, 0),
\
1 \times 1\ at\ (1, 1),\ 1 \times 2\ at\ (0, 0),\ 1 \times 2\ at\ (1, 0),
\
2 \times 1\ at\ (0, 0),\ 2 \times 1\ at\ (0,1),\ 2 \times 2\ at\ (0,0).]
$$</p>
<p>（$x$中每个二元变量的的组成：$1 \times 1$表示子矩形的形状，$(0,0)$表示所处的位置）</p>
<p>要使$x$有效，<strong>每个基本子矩形必须被$x$中编码的子矩形精确覆盖一次</strong>。例如：</p>
<ul>
<li>
<p>$[0, 0, 0, 0, 1, 1, 0, 0, 0]$=&gt;有效（第 5 和第 6 次序的位置分别对应$e$和$f$子矩形，恰好覆盖了所有基本子矩形 1 次）</p>
</li>
<li>
<p>$[0,0,0,1,1,0,0,0,0]$=&gt;无效（第 4 和第 5 次序的位置分别对应$d$和$e$子矩形，没有覆盖到$(1,0)$基本子矩形）</p>
</li>
<li>
<p>$[0,0,0,1,1,1,0,0,0]$=&gt;无效（第 4、第 5 和第 6 次序的位置分别对应$d$、$e$和$f$子矩形，$(1,1)$基本子矩形被覆盖了两次）</p>
</li>
</ul>
</li>
<li>
<p>$c^{(stor)}$的形式化</p>
<p>与每个$x$相对应的向量$c^{(stor)}$长度与$x$相等，其中每个元素是$x$中对应位置的子矩形的存储开销的估计值。</p>
</li>
<li>
<p>$c^{(view)}$的形式化</p>
<p>考虑分发子矩形的网络带宽开销时，需要考虑所有可能被分发的 360 度表面的视图。</p>
<p>为了简化问题，将片段所有可能的视图离散化到一个大小为$V$的集合中。</p>
<p>集合中每个元素表示一个<strong>事件</strong>，即向观看 360 度视频片段的用户显示基本子矩形的唯一子集。</p>
<p>注意到片段中被看到的视频区域可以包含来自多个视角的区域。</p>
<p>将之前离散化好的大小为$V$的集合中每个元素与可能性相关联：$[p_1, p_2, &hellip;, p_V]$。</p>
<p>考虑为给定的解下载视图$V$的开销，作为需要为该视图下载的数据量：</p>
<p>$$
quantity = x^{\top}diag(d_v)c^{(stor)}
$$</p>
<p>$d_v$是一个二元向量，其内容是按照$x$所描述的表示方案，对所有覆盖视图的子矩形的选择。</p>
<p>例如对于 ERP 投影中位置坐标为$yaw = 0, pitch = 90$即处于等矩形顶部的图像，对应的$d_{view-(0, 90)} = [1, 1, 0, 0, 1, 0, 1, 1, 1]$</p>
<p>（即上面图中$a, b, e, g, h, i$位置的子矩形包含此视图所需的基本子矩形）。</p>
<p>给出一个片段中的用户 viewport 分布，$c^{(view)}$的元素是相关联的子矩形预期的下载字节。</p>
<p>$$
c^{(view)} = \sum_v p_v diag(d_v) c^{(stor)}
$$</p>
<p>最后，将优化问题的基本子矩形覆盖约束编码为矩阵$A$。</p>
<p>$A$是一个列中包含给定子矩形解所覆盖的基本子矩形信息的二元矩阵。</p>
<p>对于$2 \times 2$的矩形片段，其$A$有 4 行 9 列，例子如下：</p>
<p></p>
<p>因此最终的问题可以形式化为一个整数线性程序：</p>
<p></p>
<ul>
<li>
<p>$c^{(stor)}$</p>
<p>可以理解为存储一段$\Delta t$时间长的片段的子矩形的存储开销；</p>
</li>
<li>
<p>$c^{(view)}$</p>
<p>可以理解为传输一个视图所需要的所有的子矩形的传输开销。</p>
</li>
<li>
<p>$\alpha$</p>
<p>控制相比于传输一个片段的相对存储开销，同时应该考虑片段的流行度。</p>
<p>即$\alpha$应该与所期望的片段在$\Delta t$的时间间隔内的下载次数成比例，$\alpha$应该可以通过经验测量以合理的精度进行估计。</p>
<p>可以通过将$x$的二元离散限制放松到$0 \le x_i \le 1\ \forall i$构成一个线性程序，其解为整数。</p>
<p>（对于有 33516 个变量的$x$，其解可以在单核 CPU 上用 7~10 秒求出）</p>
</li>
</ul>
</li>
</ul>
<h2 id="开销向量建构">开销向量建构</h2>
<p>首先需要建构出存储开销向量$c^{(stor)}$，但是对于有$n$个基本子矩形的子矩形，其建构复杂度为$O(n^2)$。</p>
<p>因此对每个子矩形进行编码来获得存储开销并不可行，所以利用视频压缩与运动估计之间的强相关性来预测$c^{(stor)}$的值。</p>
<ol>
<li>
<p>给定一个视频，首先暂时将其分成长度为 1 秒的片段，每个片段被限定为只拥有 1 个 GOP，片段的大小表示为$S_{orig}$。</p>
</li>
<li>
<p>接着抽取出每个片段中的动作序列用于之后的分析。</p>
</li>
<li>
<p>将片段从空间上划分成基本子矩形，每个基本子矩形包含$4 \times 4 = 16$个宏块（例如：$64 \times 64$个像素点）。</p>
</li>
<li>
<p>独立编码每个基本子矩形，其大小表示为$S_i$。</p>
</li>
<li>
<p>通过分析动作向量信息，可以推断出如果对基本子矩形$i$进行独立编码，指向基本子矩形$i$的原始运动向量应该重新定位多少。</p>
<p>将其表示为$r_i$。</p>
</li>
<li>
<p>每个运动向量的存储开销可以计算为：</p>
<p>$$
o = \frac{\sum_i S_i - S_{orig}}{\sum_i r_i}
$$</p>
<p>即：存储开销的整体增长除以被基本子矩形边界所分割的运动向量数。</p>
</li>
<li>
<p>如果基本子矩形被融合进更大的子矩形$t$，使用$m_t$来表示由于融合操作而无须再进行重定位的运动向量的数量：</p>
<p>$$
m_t = \sum_{i \in t} r_i - r_t
$$</p>
<p>$i \in t$表示基本子矩形位于子矩形$t$中。</p>
</li>
<li>
<p>为了估计任意子矩形$t$的大小，使用下面 5 个参数：
$$
\sum_{i \in t} S_i,\ \sum_{i \in t} r_i,\ m_t,\ o,\ n
$$
$n$表示$t$中基本子矩形的数量。</p>
</li>
</ol>
<p>实际操作：</p>
<ol>
<li>
<p>创建了来自 4 个单视角 360 度视频的 6082 个 tile 数据集。4 个视频都以两种分辨率进行编码：$1920 \times 960$和$3980 \times 1920$。</p>
</li>
<li>
<p>为了产生 tile，从视频中随机选取片段，随机选取 tile 的位置，宽度和高度。</p>
<p>设置 tile 的 size 最大为$12 \times 12$个基本子矩形。</p>
<p>对于每个选择的 tile，为其建构一个数据集元素：</p>
<ol>
<li>计算上面提到的 5 参数的特性向量。</li>
<li>使用 FFmpeg 编码 tile 的视频段来得到存储该段需要的空间。</li>
</ol>
</li>
<li>
<p>使用多层感知机 MLP 来估计 tile 的大小。</p>
<p>MLP 中包含 50 个节点的单隐层，激活函数为 ReLU 函数，300 次迭代的训练过程使用<a href="https://zhuanlan.zhihu.com/p/29672873" target="_blank" rel="noopener noreffer">L-BFGS 算法</a>。</p>
<p>为了评估 MLP 的预测效果，使用 4 折的交叉验证法。</p>
<p>每次折叠时先从 3 个视频训练 MLP，接着使用训练好的模型去预测第 4 个视频的 tile 大小。</p>
</li>
</ol>
<h2 id="实现">实现</h2>
<p></p>
<p>将视频划分成1秒长的片段，之后为每个片段解决整数线性问题来确定最优的tile划分策略。</p>
<ol>
<li>使用MLP模型估计每个tile的存储开销。</li>
<li>根据视图的集合$d$及其对应的可能性分布$p$，来估计视图的下载开销$c^{(view)}$。</li>
<li>构造矩阵$A$时，限制最大的tile大小为$12 \times 12$的基本子矩形（如果设置每个基本子矩形包含$64 \times 64$的像素，tile的最大尺寸即为$768 \times 768$的像素）。</li>
<li>使用<a href="https://www.gnu.org/software/glpk/" target="_blank" rel="noopener noreffer">GNU Linear Programming Kit</a>来解决问题。</li>
<li>将所有可能的解子矩形编码进一个二元向量$x$中来表示解。</li>
<li>GLPK的解表明一个可能的解子矩形是否应该被放入解中。</li>
<li>基于最终得到的解，划分片段并使用ffmepg以同样参数的x264格式进行编码。</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>度量指标</p>
<ol>
<li>服务端存储需求。</li>
<li>客户端需要下载的字节数。</li>
</ol>
</li>
<li>
<p>数据来源</p>
<p>数据集：<a href="http://dash.ipv6.enstb.fr/headMovements/" target="_blank" rel="noopener noreffer">dash.ipv6.enstb.fr</a></p>
</li>
<li>
<p>评估准备</p>
<p>下载5个使用ERP投影的视频，抽取出测试中用户看到的对应部分。</p>
<p>每个视频都有$1920 \times 960$和$3840 \times 1920$的两种分辨率的版本。</p>
<p>$1920 \times 960$视频的基本子矩形尺寸为$64 \times 64$的像素。</p>
<p>$3840 \times 1920$视频的基本子矩形尺寸为$128 \times 128$的像素。</p>
<p>将视频划分成1秒长的片段，对每个片段都产生出MLP所需的5元组特性。</p>
<p>之后使用训练好的MLP模型来预测所有可能的tile的大小。</p>
</li>
<li>
<p>数据选择</p>
<ol>
<li>
<p>从数据集中随机选择出40个用户的集合。</p>
</li>
<li>
<p>假设100°的水平和垂直FOV，并使用40个用户的头部方向来为每个片段产生$p_v$和$d_v$。</p>
<p>即：分块的决策基于每个片段的内容特征信息与用户的经验视图模式。</p>
</li>
</ol>
</li>
<li>
<p>参数设定：$\alpha = 0,1,1000$.</p>
</li>
<li>
<p>对比实验：</p>
<p>一组使用由ILP得出的结构进行分块；</p>
<p>另外一组：</p>
<ul>
<li>
<p>$1920 \times 960$的视频片段分别使用$64 \times 64$，$128 \times 128$，$256 \times 256$，$512 \times 512$的方案固定大小分块。</p>
</li>
<li>
<p>$3840 \times 1920$的视频片段分别使用$128 \times 128$，$256 \times 256$，$512 \times 512$，$1024 \times 1024$的方案固定大小分块。</p>
</li>
</ul>
</li>
<li>
<p>划分结果对比</p>
<p></p>
</li>
</ul>
<h3 id="服务端的存储大小">服务端的存储大小</h3>
<p></p>
<p>按照$\alpha = 0$方案分块之后的视频大小几乎与未分块之前的视频大小持平，有时甚至略微小于未分块前的视频大小。</p>
<p>因为所有分块方案都使用相同的编码参数，所以重新编码带来的有损压缩并不会影响竞争的公平性。</p>
<p>如果将$\alpha$的值调大，存储的大小会略微增大；固定分块大小的方案所得到的存储大小也会随tile变小而变大。</p>
<h3 id="客户端的下载大小">客户端的下载大小</h3>
<ul>
<li>
<p>预测完美的情况——下载的tile没有任何浪费</p>
<p></p>
<p>$\alpha= 1000$的情况下，OpTile的表现总是最好的。</p>
</li>
<li>
<p>正常预测的情况</p>
<p>预测的方法：假设用户的头部方向不会改变，预测的位置即为按照当前方向几秒之后的位置。</p>
<p></p>
<p>相比于完美假设的预测，所有分块方案的下载大小都增大了。</p>
<p>$\alpha = 1000$的方案在两个视频的情况下都取得了最小的下载大小。在剩下的3个视频中，OpTile方案的下载大小比起最优的固定分块大小方案不超过25%。</p>
<p>尽管固定分块大小的方案可能表现更好，但是这种表现随视频的改变而变化显著。</p>
<p><strong>因为固定分块的方案没有考虑视频内容的特性与用户的观看行为。</strong></p>
</li>
</ul>
]]></description>
</item>
<item>
    <title>多媒体基础知识</title>
    <link>https://ayamir.github.io/2021/12/mm-base/</link>
    <pubDate>Mon, 13 Dec 2021 10:03:17 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/mm-base/</guid>
    <description><![CDATA[<h1 id="媒体处理过程">媒体处理过程</h1>
<p></p>
<h2 id="解协议">解协议</h2>
<p>将流媒体传输方案中要求的数据解析为标准的相应封装格式数据。</p>
<p>音视频在网络中传播时需要遵守对应的传输方案所要求的格式，如DASH、HLS将媒体内容分解成一系列小片段，每个片段有不同的备用码率版本。</p>
<p>同时应用层的协议会要求在媒体文件本身之外，传输信令数据（如对播放的控制或网络状态的描述）</p>
<p>解协议的过程会去除信令数据并保留音视频内容，需要的话还要对视频段进行拼接，最终将其还原成传输之前的媒体格式如MP4，FLV等。</p>
<h2 id="封装格式">封装格式</h2>
<p>封装格式如AVI、MPEG、Real Video将音频和视频组合打包成一个完整的文件.</p>
<p>封装格式不会影响视频的画质，影响画质的是视频的编码格式。</p>
<p>解封装过程就是将打包好的封装格式分离成某种编码的音频压缩文件和视频压缩文件，有时也包含字幕和脚本。</p>
<p>比如FLV或TS格式数据，解封装之后得到H.264-AVC编码的视频码流和AAC编码的音频码流。</p>
<h2 id="编码">编码</h2>
<p>视频的本质是一帧又一帧的图片。</p>
<p>所以对于一部每秒30帧，90分钟，分辨率为1920x1080，24位的真彩色的视频，在压缩之前的大小$S$满足：
$$
一帧大小s = 1920 * 1080 * 24 = 49766400(bit) = 6220800(Byte)
\
总帧数n = 90 * 60 * 30 = 162000
\
总大小S = s * n = 6220800 * 162000 = 1.0077696*10^{12}(Byte) \approx 939(GB)
$$
因为未经压缩的视频体积过于庞大，所以需要对其进行压缩，而压缩就是通常所说的编码。</p>
<p>视频编码方式：H.264-AVC，H.265-HEVC，H.266-VVC</p>
<p>音频编码方式：MP3，AAC</p>
<p>压缩比越大，解压还原之后播放的视频越失真，因为压缩过程中不可避免地丢失了视频中原有图像的数据信息。</p>
<h2 id="解码">解码</h2>
<p>解码就是解压缩过程。</p>
<p>解码之后能够得到系统音频驱动和视频驱动能识别的音频采样数据（如PCM数据）和视频像素数据（如YUV420，RGB数据）。</p>
<h2 id="音视频同步">音视频同步</h2>
<p>根据时间，帧率和采样率采用一定的算法，同步解码出来的音频和视频数据，将其分别送至声卡和显卡播放。</p>
<h1 id="视频质量指标">视频质量指标</h1>
<h2 id="分辨率">分辨率</h2>
<ul>
<li>
<p>分辨率指的是视频图像在一个单位尺寸内的精密度。</p>
</li>
<li>
<p>将视频放大足够大的倍数之后就能看到组成影像的基本单元：像素。</p>
</li>
<li>
<p>视频的分辨率从数值上描述了像素点的个数，如1920x1080：视频在水平方向有1920个像素，垂直方向有1080个像素。</p>
</li>
<li>
<p>常见的描述方式：</p>
<ul>
<li>1080P：指视频有<strong>1080行</strong>像素，P=&gt;Progressive（逐行扫描）</li>
<li>2K：指视频有<strong>2000列</strong>像素</li>
<li>MP：像素总数，指像素的行数P与列数K乘积的结果（百万像素）</li>
<li>1080P的分辨率为1920x1080=2073600，所以1080P通常也称为200万像素分辨率</li>
</ul>
</li>
<li>
<p>通常视频在同样大小的情况下，分辨率越高，所包含的像素点越多，画面就越细腻清晰</p>
</li>
<li>
<p>参考链接：</p>
<ul>
<li><a href="https://www.reneelab.com.cn/m/2k-4k-video-resolution.html" target="_blank" rel="noopener noreffer">科普：视频分辨率是什么？</a></li>
<li><a href="https://www.zhihu.com/question/24205632/answer/648608086" target="_blank" rel="noopener noreffer">「1080p」和「2k、4k」的关系与差别在哪里？</a></li>
</ul>
</li>
</ul>
<h2 id="视频帧率">视频帧率</h2>
<ul>
<li>
<p>帧率的单位FPS(Frame Per Second)或Hz，即每秒多少帧，决定视频画面的流畅程度。</p>
</li>
<li>
<p>低帧率会导致播放卡顿，镜头移动不顺畅，并伴随画面模糊的主观体验；</p>
<p>帧率过高则会造成眩晕的感觉。</p>
</li>
<li>
<p>不同帧率的视频在支持不同帧率的设备上播放：</p>
<ol>
<li>
<p>若设备最高支持60fps，则播放120fps视频的时候，设备会每隔一帧删除一帧，被删除的帧即成为无效帧。</p>
<p>所以高帧率的视频在低帧率设备上播放时会导致播放卡顿。</p>
</li>
<li>
<p>若设备最高支持120fps，则播放60fps视频的时候，设备会每隔一帧复制一帧，来填补空缺的帧位置。</p>
<p>但是效果和在60fps上的设备播放一样，不能提升播放流畅度。</p>
</li>
</ol>
</li>
<li>
<p>关于显卡对帧率的影像：</p>
<ol>
<li>显示器帧率低而显卡输出帧率高时，会导致<a href="https://zh.wikipedia.org/wiki/%E7%95%AB%E9%9D%A2%E6%92%95%E8%A3%82" target="_blank" rel="noopener noreffer">画面撕裂</a>：显示器同时将两帧或几帧显示在同一个画面上</li>
<li>显示器帧率高而显卡输出帧率低时，同视频帧率高显示器帧率低的情况。</li>
</ol>
</li>
</ul>
<h2 id="视频码率">视频码率</h2>
<ul>
<li>
<p>码率的概念出现在视频编码之后，因为压缩之后的视频已经成为二进制数据，所以使用码率的称呼。</p>
</li>
<li>
<p>码率的单位是bps(bit per second)，即每秒多少比特。</p>
</li>
<li>
<p>与视频质量的关系：</p>
<ol>
<li>
<p>分辨率不变的情况下，码率越大，压缩比越好，画面质量越清晰。</p>
<p>码率越高，精度越高，处理出的文件就越接近压缩前的原始状态，每一帧的图像质量越高，画质越清晰，当然对播放设备的解码能力要求也越高。</p>
<p>压缩比越小，视频体积越大，越接近源文件。</p>
</li>
</ol>
</li>
</ul>
<p></p>
]]></description>
</item>
<item>
    <title>Note for RainbowDQN and Multitype Tiles</title>
    <link>https://ayamir.github.io/2021/12/note-for-rainbowdqn-tiles/</link>
    <pubDate>Sat, 11 Dec 2021 16:14:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-rainbowdqn-tiles/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Level：IEEE Transaction on multimedia 21</p>
<p>Keyword：Rainbow-DQN, Multi-type tiles, Full streaming system</p>
<h2 id="问题形式化">问题形式化</h2>
<h3 id="模型">模型</h3>
<ol>
<li>
<p>原始视频用网格划分成$N$块tile，每个tile都被转码成$M$个不同的质量等级$q_i$。</p>
</li>
<li>
<p>基于传输控制模块得出的结果，播放器请求$t_i$个tile的$q_i$质量的版本并将其存储在缓冲区中，对应的缓冲区大小为$l_i$。</p>
</li>
<li>
<p>用户Viewport的信息用$V$表示，可以确定FOV的中心。</p>
</li>
<li>
<p>根据$V$可以将tile划分成3种类型：FOV、OOS、Base。</p>
</li>
<li>
<p>FOV中的tile被分配更高的码率；</p>
<p>OOS按照与$V$的距离逐步降低质量等级$q_i$；</p>
<p>Base总是使用低质量等级$q_{Base}$但使用完整的分辨率。</p>
</li>
<li>
<p>传输的tile在同步完成之后交给渲染器渲染。</p>
</li>
<li>
<p>播放器根据各项指标计算可以评估播放性能：</p>
<p>$&lt;V, B, Q, F, E&gt;$：viewport信息$V$，网络带宽$B$，FOV质量$Q$，重缓冲频率$F$，传输效率$E$。</p>
</li>
<li>
<p>传输控制模块用于确定每个tile的质量等级$q_i$和缓冲区大小$l_i$。</p>
</li>
<li>
<p>传输控制模块优化的最终目标是获取最大的性能：
$$
performance = E_{max},\ QoE \in accept\ range
$$</p>
</li>
</ol>
<h3 id="带宽评估">带宽评估</h3>
<ol>
<li>
<p>收集每个tile的下载日志来评估带宽。</p>
</li>
<li>
<p>使用<a href="https://zhuanlan.zhihu.com/p/32335746" target="_blank" rel="noopener noreffer">指数加权移动平均算法EWMA</a>使评估结果光滑，来应对网络波动。</p>
</li>
<li>
<p>第$t$次评估结果使用$B_t$表示，用下式计算：
$$
B_t = \beta B_{t-1} + (1-\beta)b_t
$$
$b_t$是B的第$t$次测量值；$\beta$是EWMA的加权系数。</p>
</li>
<li>
<p>$t=0$时，$B_0$被初始化为0；所以在初始的$t$比较小的时候，$B_t$与理想值相比就很小。</p>
<p>这种影响会随着$t$增大而减少。</p>
</li>
<li>
<p>为了优化启动过程，对公式做出修改：
$$
B_t = \frac{\beta B_{t-1} + (1-\beta)b_t}{1 - \beta^t}
$$
$t$较小的时候，分母会放大$B_t$；$t$较大时，分母趋近于1，影响随之消失。</p>
</li>
</ol>
<h3 id="fov表示和预测">FOV表示和预测</h3>
<ol>
<li>
<p>3D虚拟相机用于渲染视频，处于全景视频球面上的某条轨道，其坐标可以表示为$(\theta, \phi)$，可以直接从系统中获取。</p>
<p>相机始终朝向球的中心，所以用户的FOV中心坐标$(\theta^{'}, \phi^{'})$可以用$(\theta, \phi)$表示：
$$
\begin{cases}
\theta^{'} = (\theta + \pi)\ mod\ 2\pi,\ 0 \le \theta \le 2\pi
\
\phi^{'} = \pi - \phi,\ 0 \le \phi \le \pi
\end{cases}
$$</p>
</li>
<li>
<p>2D网格中tile坐标$(u, v)$可以通过球面坐标使用ERP投影获得
$$
\begin{cases}
u = \frac{\theta^{'}}{2\pi} \cdot W, 0 \le u \le W.
\
v = \frac{\phi^{'}}{\pi} \cdot H, 0 \le v \le H.
\end{cases}
$$
$W$和$H$分别表示使用ERP投影得到的矩形宽度和高度</p>
</li>
<li>
<p>短期的FOV预测基于目前和历史的FOV信息。</p>
<p>使用$(U_t, V_t)$表示$t$时刻的FOV中心位置；$U_{t1:t2}$和$V_{t1:t2}$分别表示从$t1$到$t2$过程中$U$和$V$的序列；
$$
\begin{cases}
\hat{U}<em>{t+T_f} = f_U (U</em>{t-T_p:t}).
\
\hat{V}<em>{t+T_f} = f_V (V</em>{t-T_p:t}).
\end{cases}
$$
$T_p$是过去记录的滑动窗口；$T_f$是短期的预测窗口；$f_U$和$f_V$分别对应$U$和$V$方向上的映射函数；</p>
<p>因为是时间序列回归模型，所以映射函数使用LSTM。</p>
</li>
</ol>
<h3 id="qoe评估">QoE评估</h3>
<p>QoE由3个部分组成：平均FOV质量$Q$、重缓冲频率$F$与FOV内tile的质量变化（因为平均分配所以不考虑）。</p>
<ol>
<li>
<p>FOV质量$Q$</p>
<p>第$t$次的FOV质量评估表示为$Q_t$：
$$
Q_t = \frac{\beta Q_{t-1} + (1-\beta) \frac{1}{k} \cdot \sum_{j=1}^{k} max{q_j, q_b}}{1 - \beta^t}
$$
$q_j$表示第$j$条FOV tile流的质量；$k$表示FOV内tile的数量；</p>
<p>为了避免评估结果的大幅波动，使用了EWMA来光滑结果。</p>
<p>当第$j$条tile流因为缓冲区不足不能成功播放时，$q_j = q_{Base}$（这表明了Base tile在提高QoE中的作用）。</p>
</li>
<li>
<p>重缓冲频率$F$</p>
<p>在基于tile的传输中，每条流都属于一个缓冲区。所以当FOV中tile的缓冲区处于饥饿状态时，重缓冲就会发生。</p>
<p>重缓冲频率描述了FOV内的tile流在一段时间内的重新缓冲频率。</p>
<p>第$t$次重缓冲频率的评估表示为$F_t$：
$$
F_t = \frac{\beta F_{t-\tau} + (1-\beta) \frac{f_t}{\tau}}{1 - \beta^{\tau}}
$$
$f_t$表示播放失败的次数；$\tau$表示一段时间；</p>
</li>
</ol>
<h3 id="传输效率评估">传输效率评估</h3>
<p>第$t$次传输效率评估表示为$E_t$，$E_t$通过传输的FOV内tile占总tile的比率来计算：
$$
E_t = \frac{\beta E_{t-1} + (1-\beta) \frac{total^{FOV}}{total^{ALL}}}{1 - \beta^t}
$$
$total^{FOV}$表示FOV内tile的数据量；$total^{ALL}$表示tile的总共数据量；</p>
<p>效率计算并不在传输过程中完成，因为需要获取哪些tile在FOV中的信息，效率评估滞后于播放过程。</p>
<h3 id="问题形式化-1">问题形式化</h3>
<p>传输控制的任务：确定所有tile流的质量等级$\chi$和缓冲区大小$\psi$。
$$
\chi = &lt;q_1, q_2, &hellip;, q_N&gt;
\
\psi = &lt;l_1, l_2, &hellip;, l_N&gt;
\
&lt;Q, F, E&gt; = \xi (B, V, \chi, \psi)
$$
$\chi$和$\psi$与带宽$B$和Viewport轨迹$V$一起作用于系统$\xi$，最终影响FOV质量$Q$，重缓冲频率$F$和传输效率$E$。</p>
<p>进一步，将目标形式化为获得每条tile流的$q_i$和$l_i$通过限制QoE满足可接受的范围、在此基础上最大化传输效率：
$$
\underset{\chi, \psi}{argmax} \sum_{t=0}^{+\infty} E_t,
$$</p>
<p>$$
s.t.:\ 0 \le q_i \le M,
$$</p>
<p>$$
0 \le l_i \le L,
$$</p>
<p>$$
Q^{min} \le Q_t \le M,
$$</p>
<p>$$
0 \le F_t \le F^{max}.
$$</p>
<p>$q_i$和$l_i$分别受限于质量版本数$M$和最大缓冲区大小$L$；</p>
<p>$Q_t$受限于最低QoE标准$Q^{min}$；</p>
<p>$F_t$受限于最大能忍受的重缓冲频率$F^{max}$。</p>
<h2 id="系统架构">系统架构</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>将原始视频转码为有不同比特率的多个版本。</li>
<li>转码后的视频被划分成多个tile。</li>
<li>传输协议使用MPEG-DASH。</li>
</ol>
<h3 id="客户端">客户端</h3>
<h4 id="评估器">评估器</h4>
<ul>
<li>任务：获取 QoE、FOV预测、传输效率、网络带宽</li>
<li>组成：
<ul>
<li>QoE评估器：评估当前FOV质量=&gt;Q和重缓冲频率=&gt;F（近似为Q+F=QoE）</li>
<li>FOV预测器：基于历史FOV信息预测短期未来的FOV=&gt;P</li>
<li>根据下载和播放日志：计算传输效率=&gt;E并估计带宽=&gt;B</li>
</ul>
</li>
</ul>
<h4 id="控制器">控制器</h4>
<ul>
<li>任务：控制传输过程中的推流</li>
<li>目标：保证QoE在可接受的范围之内、最大化传输效率</li>
<li>详细：基于FOV预测将tile划分成3种类型：FOV、OOS、Base</li>
<li>输入：Q、F、E、B（QoE+传输效率和带宽）</li>
<li>过程：Rainbow-DQN</li>
<li>输出：决定每个tile流的码率和缓冲区大小（作为下载器的输入）</li>
</ul>
<h4 id="下载器">下载器</h4>
<ul>
<li>输入：tile码率和缓冲区大小</li>
<li>过程：基于HTTP/2进行并行下载</li>
<li>输出：下载好的tile</li>
</ul>
<h4 id="视频缓冲区">视频缓冲区</h4>
<ul>
<li>任务：解码、同步、存储下载好的tile等待渲染器消耗，大小供控制器调节</li>
<li>随着FOV的切换缓冲区内容可能被循环利用</li>
</ul>
<h4 id="全景渲染器">全景渲染器</h4>
<ul>
<li>任务：将同步好的tile拼接，tile质量：FOV&gt;OOS&gt;Base</li>
<li>投影方式：ERP</li>
</ul>
<h2 id="控制器-1">控制器</h2>
<h3 id="控制过程">控制过程</h3>
<ol>
<li>
<p>设定QoE的可接受范围。</p>
</li>
<li>
<p>将网络带宽和用户FOV设定为外部因素而非环境</p>
<p>为什么：因为这两个因素变化太快，在面对不同传输条件时，直接作为环境会导致决策过程的不稳定性并且难以收敛。</p>
</li>
<li>
<p>最优化的对象只是最大化累积的传输效率。</p>
<p>为什么：简单</p>
</li>
</ol>
<h3 id="tile聚合和决策">tile聚合和决策</h3>
<ol>
<li>
<p>tile分类原则：</p>
<ul>
<li>
<p>控制器无需为每个tile独立决定码率Q和缓冲区大小L</p>
</li>
<li>
<p>FOV内的tile应该被分配相近的码率，FOV内的tile应该聚集成一组，OSS和Base同理</p>
<p>为什么：避免相邻tile的锐利边界，只考虑3组而非所有tile降低了计算复杂性和决策延迟</p>
<p>（能否实现独立的tile码率计算或更细粒度的划分值得调研？与内容感知的方案结合？）</p>
</li>
</ul>
</li>
<li>
<p>基于距离的tile分类实现方式：</p>
<ul>
<li>
<p>使用评估器预测出的FOV坐标来分类FOV和OOS的tile</p>
</li>
<li>
<p>tile出现在未来FOV的可能性由距离计算</p>
<p>tile中心点坐标$(\omega_i, \mu_i)$、FOV坐标$(\hat{U}, \hat{V})$</p>
<p>距离的变化区间内存在一个临界点，临界点之内的划分为FOV，之外的划分为OOS</p>
<ul>
<li>
<p>度量距离的方式：
$$
\Delta Dis_U = min{|\omega_i - \hat{U}|, |1+\omega_i - \hat{U}|}
$$</p>
<p>（这里为何不直接使用$|\omega_i - \hat{U}|$？）
$$
Dis_i =
\begin{cases}
{\sqrt{({\Delta Dis_{U}})^2 + {(\mu_i - \hat{V})}^2},\  \frac{R}{H} \le \hat{V} \le 1 - \frac{R}{H}}
\
{\Delta Dis_U + |\mu_i - \hat{V}|,\ Others}
\end{cases}
$$</p>
</li>
<li>
<p>因为ERP的投影方式会在两级需要更多的tile，因此使用一个矩形来代表两极的FOV</p>
<p>（可以深入调研ERP在两极处的处理方式）</p>
</li>
<li>
<p>$Dis_i$使用曼哈顿距离来测量。临界点初始化为$2\cdot R$，并随着FOV中心和两极的垂直距离增长。</p>
</li>
<li>
<p>FOV看作是半径为R的圆，使用欧式距离测量。临界点初始化为$R$</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>聚合tile的决策</p>
<ul>
<li>使用2个变量：$K$作为FOV和非FOV的tile的带宽分配比率；$Len$作为tile缓冲区的大小。
<ul>
<li>
<p>$K$确定之后，分配给FOV内tile的带宽被均匀分配（可否非均匀分配）</p>
<p>$K$不直接与网络状况相关因此可以保持控制的稳定性</p>
</li>
<li>
<p>$Len$：所有传输的tile的缓冲区长度$l_i$都被设为$Len$  （文中并没有这样做的原因解释）</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="基于drl的传输控制算法">基于DRL的传输控制算法</h3>
<p>相关术语解释：<a href="https://www.jianshu.com/p/1dfd84cd2e69" target="_blank" rel="noopener noreffer">Rainbow DQN</a>、<a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e" target="_blank" rel="noopener noreffer">RL Dictionary</a>、<a href="https://zhuanlan.zhihu.com/p/38358183" target="_blank" rel="noopener noreffer">PER</a>、<a href="https://zhuanlan.zhihu.com/p/34747205" target="_blank" rel="noopener noreffer">TD-Error</a></p>
<ol>
<li>
<p>控制过程</p>
<ol>
<li>
<p>首先调整buffer长度Len，并划分FOV与非FOV的带宽分配。</p>
</li>
<li>
<p>等viewport预测完成之后，tile被分类为属于FOV和OOS的tile。</p>
</li>
<li>
<p>FOV的带宽被平均分给其中每一个tile并决定FOV内tile的质量等级$q_i$。</p>
<p>非FOV的带宽按照与FOV的距离分配，每超过一个距离单位$Dis_i$就降低一级质量$q_i$。</p>
</li>
<li>
<p>最终的输出是请求序列，每个请求序列中包括质量等级$q_i$和预期的缓冲区大小$l_i$。</p>
</li>
<li>
<p>根据输出做出调整之后，接收奖励反馈并不断完成自身更新。</p>
</li>
</ol>
</li>
<li>
<p>状态设计</p>
<p>状态设计为5元组：$&lt;K, Len, Q, F, E&gt;$（传输控制参数$K$，$Len$、QoE指标：FOV质量Q和重缓冲频率$F$、传输效率$E$）</p>
<p>没有直接使用带宽$B$和viewport轨迹$V$，因为：</p>
<ol>
<li>随机性强与变化幅度较大带来的不稳定性（如何定义随机性强弱和变化幅度大小？）</li>
<li>希望设计的模型有一定的通用性，可以与不同的网络情况和用户轨迹相兼容</li>
</ol>
</li>
<li>
<p>动作设计</p>
<p>两种动作：调整$K$和$Len$（两者的连续变化区间被离散化，调整的每一步分别用$\Delta k$和$\Delta l$表示）</p>
<p>调整的方式被形式化为二元组：$&lt;n_1, n_2&gt;$，$n_1$和$n_2$分别用于表示$K$和$Len$的调整</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">-n</th>
<th style="text-align:center">0</th>
<th style="text-align:center">n</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">K</td>
<td style="text-align:center">减少n$\Delta k$</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">增加n$\Delta k$</td>
</tr>
<tr>
<td style="text-align:center">Len</td>
<td style="text-align:center">减少n$\Delta l$</td>
<td style="text-align:center">不变</td>
<td style="text-align:center">增加n$\Delta l$</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>奖励函数</p>
<p>因为QoE的各项指标权重难以确定，没有使用传统的基于加权的方式。</p>
<p>设定了<strong>能接受的QoE范围</strong>和<strong>在此基础上最大化的传输效率</strong>作为最后的<strong>性能</strong>指标，形式化之后如下：
$$
Reward =
\begin{cases}
-INF,\ F \ge F^{max}
\
-INF,\ E \le E^{min}
\
E,\ Others
\end{cases}
$$</p>
<p>$-INF$意味着终止当前episode；动作越能使系统满足高QoE的同时高效运行，得分越高；</p>
<p>为了最大化传输效率，使用$E$作为奖励回报。</p>
<p>FOV质量$Q$并没有参与到奖励函数中，因为：<strong>高Q意味着高性能，但是低Q不一定意味着低性能</strong>，详细解释如下：</p>
<ul>
<li>在带宽不足的情况下，低Q可能已经是这种条件下的满足性能的最好选择。</li>
<li>高传输效率意味着传输了更多的FOV数据，也能满足高FOV质量的目标。</li>
</ul>
</li>
<li>
<p>模型设计
基于Rainbow-DQN模型：</p>
<ul>
<li>
<p>输入是5元组$&lt;K, Len, Q, F, E&gt;$。</p>
</li>
<li>
<p>神经网络使用64维的3隐层模型。</p>
</li>
<li>
<p>为了提高鲁棒性，神经网络的第3层使用Dueling DQN的方式，将Q值$Q(s, a)$分解为状态价值$V(s)$和优势函数$A(s,a)$：
$$
Q(s, a) = V(s) + A(s, a)
$$</p>
<p>$V(s)$表示系统处于状态$s$时的性能；$A(s,a)$表示系统处于状态$s$时动作$a$带来的性能；</p>
</li>
<li>
<p>为了避免价值过高估计，使用Double DQN的方式，设计了两个独立的神经网络：评估网络和目标网络。</p>
<p>评估网络用于动作选择；目标网络是评估网络从最后一个episode的拷贝用于动作评估。</p>
</li>
<li>
<p>为了缓解神经网络的不稳定性（更快收敛），使用大小为$v$的回放池来按照时间序列保存客户端的经验。</p>
<p>因为网络带宽和FOV轨迹在短期内存在特定的规律性，回放池中有相似状态和相似采样时间的样本更加重要，出现了优先级</p>
<p>所以使用优先经验回放PER，而优先级使用时间查分误差TD-error定义
$$
\delta_i = r_{i+1} + \gamma Q(s_{i+1}, arg\underset{a}{max}Q(s_{i+1}, a; \theta_i); \theta_i^{'}) - Q(s_i, a_i; \theta_i)
$$</p>
<p>$r_i$是奖励；$\gamma$是折扣因子</p>
</li>
<li>
<p>损失函数使用均方误差定义
$$
J = \frac{1}{v} \sum_{i=1}^{v} \omega_i(\delta_i)^2
$$</p>
<p>$\omega_i$是回放缓冲中第i个样本的重要性采样权重</p>
</li>
</ul>
</li>
</ol>
<h2 id="实验验证">实验验证</h2>
<ol>
<li>
<p>环境设定</p>
<ul>
<li>
<p>传输控制模块：基于<a href="https://tensorforce.readthedocs.io/en/latest/" target="_blank" rel="noopener noreffer">TensorForce</a>（配置教程：<a href="https://zhuanlan.zhihu.com/p/60241809" target="_blank" rel="noopener noreffer">用TensorForce快速搭建深度强化学习模型</a>）；</p>
<p>开发工具集：<a href="https://gym.openai.com/" target="_blank" rel="noopener noreffer">OpenAI Gym</a></p>
</li>
<li>
<p>数据来源：使用全景视频播放设备收集，加入高斯噪声来产生更多数据。</p>
</li>
</ul>
</li>
<li>
<p>结果分析</p>
<ul>
<li>
<p>与其他DQN算法的对比——DQN、Double DQN、Dueling DQN</p>
<ul>
<li>
<p>对比训练过程中每个episode中的最大累计奖励：$MAX_{reward}$</p>
</li>
<li>
<p>对比模型收敛所需要的最少episode：$MIN_{episode}$</p>
<p>相同的带宽和FOV轨迹</p>
</li>
</ul>
</li>
<li>
<p>与其他策略对比性能——高QoE和高传输效率</p>
<ul>
<li>
<p>随机控制策略：随机确定K和Len</p>
</li>
<li>
<p>固定分配策略：固定K和Len的值</p>
</li>
<li>
<p>只预测Viewport策略：使用LSTM做预测，不存在OSS与Base，所有带宽都用于FOV</p>
<p>带宽和FOV轨迹的均值和方差相等</p>
</li>
</ul>
</li>
<li>
<p>与其他全景视频推流系统的对比</p>
<ul>
<li>
<p>DashEntire360：使用Dash直接传送完整的360度视频，使用线性回归来估计带宽并动态调整视频比特率</p>
</li>
<li>
<p>360ProbDash：在DashEntire360的基础上划分tile基于Dash传输，使用可能性模型为tile分配比特率</p>
</li>
<li>
<p>DRL360：使用DRL来优化多项QoE指标</p>
<p>实现三种系统、使用随机网络带宽和FOV轨迹。</p>
<p>使用DRL360中提出的方式测量QoE：
$$
V_{QoE} = \eta_1 Q - \eta_2 F - \eta_3 A
$$</p>
<p>$A$是viewport随时间的平均变化，反映FOV质量Q的变化；</p>
<p>$\eta_1, \eta_2, \eta_3$分别是3种QoE指标的非负加权，使用4种加权方式来训练模型并对比：</p>
<p>$&lt;1, 1, 1&gt;$，$&lt;1, 0.25, 0.25&gt;$，$&lt;1, 4, 1&gt;$，$&lt;1,1,4&gt;$</p>
</li>
</ul>
</li>
<li>
<p>在不同环境下的性能评估——带宽是否充足、FOV轨迹是否活跃（4种环境）</p>
</li>
</ul>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for 360ProbDASH</title>
    <link>https://ayamir.github.io/2021/12/note-for-360probdash/</link>
    <pubDate>Thu, 09 Dec 2021 10:20:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-360probdash/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3123266.3123291" target="_blank" rel="noopener noreffer">360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming</a></p>
<p>Level: ACM MM 17</p>
<p>Keyword:</p>
<p>Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation</p>
<h2 id="工作范围与目标">工作范围与目标</h2>
<p>应用层-&gt;基于tile-&gt;viewport预测的可能性模型+预期质量的最大化</p>
<ul>
<li>
<p>针对小buffer提出了<code>target-buffer-based rate control</code>算法来避免重缓冲事件（避免卡顿）</p>
</li>
<li>
<p>提出viewport预测的可能性模型计算tile被看到的可能性（避免边缘效应）</p>
</li>
<li>
<p>形式化QoE-driven优化问题：</p>
<p>在传输率受限的情况下最小化viewport内的质量失真和空间质量变化（获取受限状态下最好的视频质量）</p>
</li>
</ul>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>形式化参数</p>
<p>$M*N$个tile，M指tile序列的序号，N指不同的码率等级</p>
<p>$r_{i, j}$指比特率，$d_{i, j}$指失真，$p_{i}$指被看到的可能性（$\sum_{i=1}^{N}p_{i} = 1$）</p>
<p>$\Phi(X)$指质量失真，$\Psi(X)$指质量变化</p>
</li>
<li>
<p>目标</p>
<p>找到推流段的集合：$X = {x_{i, j}}$，其中${x_{i, j}} = 1$指被第$&lt;i, j&gt;$个tile被选中；$x_{i, j} = 0$则是未选中。
$$
\underset{X}{min}\ \Phi(X) + \eta \cdot \Psi(X) \
s.t. \sum_{i=1}^{N}\sum_{j=1}^{M}x_{i, j}\cdot r_{i, j} \le R, \
\sum_{j=1}^{M}x_{i, j} \le 1, x_{i, j} \in {0, 1}, \forall i.
$$
整个公式即为前所述的问题的形式化表达的公式化结果。</p>
</li>
<li>
<p>模型细节</p>
<ol>
<li>
<p>$\Phi(X)$和$\Psi(X)$的计算=&gt;通过考虑球面到平面的映射</p>
<p>通过计算球面上点的Mean Squared Error来得到S-PSNR进而评估质量：$d_{i, j}$来表示第${&lt;i, j&gt;}$个段的MSE</p>
<p>
$$
\phi_i = \frac{\pi}{2} - h_i \cdot \frac{\pi}{H}, \Delta\phi = \Delta h \cdot \frac{\pi}{H}, \
\theta_i = w_i \cdot \frac{2\pi}{W}, \ \Delta\theta = \Delta w \cdot \frac{2\pi}{W},
$$
$H$和$W$分别指按照ERP格式投影之后的视频高度和宽度</p>
<p>第$i$个tile的空间面积用$s_i$表示：
$$
s_i\ =\ \iint_{\Omega_i}Rd\phi Rcos\phi d\theta \
=\Delta\theta R^2[sin(\phi_i + \Delta\phi) - sin\phi_i],
$$
$R$指球的半径（$R = W/2\pi$），所以整体的球面质量失真$D_{i, j}$可以计算出来：
$$
D_{i, j} = d_{i, j} \cdot s_i,
$$
结合每个tile被看到的概率$p_i$可以得出$\Phi(X)$和$\Psi(X)$
$$
\Phi(X)=\frac{\sum_{i=1}^N\sum_{j=1}^MD_{i, j}\cdot x_{i,j}\cdot p_i}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i},\
\Psi(X) = \frac{\sum_{i=1}^N\sum_{j=1}^Mx_{i, j}\cdot p_i \cdot\ (D_{i,j}-s_{i} \cdot \Phi(X))^2}{\sum_{i=1}^N\sum_{j=1}^Mx_{i,j}\cdot s_i}.
$$</p>
</li>
<li>
<p>Viewport的可能性模型</p>
<ol>
<li>
<p>方向预测=&gt;<strong>线性回归模型</strong></p>
<p>将用户的欧拉角看作是$yaw(\alpha)$，$pitch(\beta)$和$rool(\gamma)$，应用线性回归做预测
$$
\begin{cases}
\hat{\alpha}(t_0 + \delta) = m_{\alpha}\delta+\alpha(t_0),\
\hat{\beta}(t_0 + \delta) = m_{\beta}\delta+\beta(t_0),\
\hat{\gamma}(t_0 + \delta) = m_{\gamma}\delta+\gamma(t_0).
\end{cases}
$$</p>
</li>
<li>
<p>预测错误的分布=&gt;<strong>高斯分布</strong>，根据公式均值和标准差都能从统计信息中计算出来</p>
<p>收集5名志愿者的头部移动轨迹并投影到3个方向上绘制成图，实验结果为预测错误呈现高斯分布（样本数可能不够？）</p>
<p>
$$
\begin{cases}
P_{yaw}(\alpha) = \frac{1}{\sigma_{\alpha}\sqrt{2\pi}}exp{-\frac{[\alpha-(\hat{\alpha}+\mu_{\alpha})]^2}{2\sigma_{\alpha}^2}},\
P_{pitch}(\beta) = \frac{1}{\sigma_{\beta}\sqrt{2\pi}}exp{-\frac{[\beta-(\hat{\beta}+\mu_{\beta})]^2}{2\sigma_{\beta}^2}},\
P_{roll}(\gamma) = \frac{1}{\sigma_{\gamma}\sqrt{2\pi}}exp{-\frac{[\gamma-(\hat{\gamma}+\mu_{\gamma})]^2}{2\sigma_{\gamma}^2}}.
\end{cases}
$$
3个方向各自<strong>独立</strong>，因此最终的预测错误$P_E(\alpha,\beta,\gamma)$可以表示为：
$$
P_E(\alpha, \beta, \gamma) = P_{yaw}(\alpha)P_{pitch}(\beta)P_{roll}(\gamma).
$$</p>
</li>
<li>
<p>球面上点被看到的可能性</p>
<p>球面坐标为$(\phi, \theta)$点的可能性表示为$P_s(\phi, \theta)$</p>
<p>因为一个点可能在多个不同的viewport里面，所以定义按照用户方向从点$(\phi, \theta)$出发能看到的点集$L(\phi, theta)$</p>
<p>因此空间点$s$被看到的可能性可以表示为：
$$
P_s(\phi, \theta) = \frac{1}{|L(\phi, \theta)|}\sum_{(\alpha, \beta, \gamma) \in L(\phi, \theta)}P_E(\alpha, \beta, \gamma),
$$</p>
</li>
<li>
<p>球面上tile被看到的可能性</p>
<p>tile内各个点被看到的可能性的<strong>均值</strong>即为tile被看到的可能性（可否使用其他方式？）
$$
p_i = \frac{1}{|U_i|} \sum_{(\phi, \theta) \in U_i} P_s(\phi, \theta).
$$
$U_i$表示tile内的空间点集</p>
</li>
</ol>
</li>
<li>
<p><code>Target-Buffer-based</code> Rate Control</p>
<p>因为长期的头部移动预测会产生较高的预测错误，所以不能采用大缓冲区（没有cite来证明这一点）</p>
<p></p>
<p>将处于相同时刻的段集合成一个块存储在缓冲区中。</p>
<p>在自适应的第k步，定义$d_k$作为此时的buffer占用情况（等到第k个块被下载完毕）
$$
b_k = b_{k-1} - \frac{R_k \cdot T}{C_k} + T
$$
$C_k$表示平均带宽，$R_k$表示总计的码率</p>
<p>为了避免重新缓冲设定目标buffer占用$B_{target}$，并使buffer占用保持在$B_{target}$（$b_k = B_{target}$）</p>
<p>因此总计的码率需要满足：
$$
R_k = \frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T),
$$
这里的$C_k$表示可以从历史的段下载信息中估计出来的带宽</p>
<p>设定$R$的下界$R_{min}$之后（没有说明为何需要设定下界），公式12可以修正为如下：
$$
R_k = max{\frac{C_k}{T} \cdot (b_{k-1} - B_{target} + T), R_{min}}.
$$</p>
</li>
</ol>
</li>
</ol>
<h2 id="实现">实现</h2>
<h3 id="服务端">服务端</h3>
<ol>
<li>
<p>视频裁剪器</p>
<p>将视频帧切割成tile</p>
</li>
<li>
<p>编码器</p>
<p>对tile进行划分并将其编码成多种码率的段</p>
</li>
<li>
<p>MPD产生器</p>
<p>添加<strong>SRD特性</strong>来表示段之间的空间关系</p>
<p>添加经度和<strong>纬度</strong>属性来表示</p>
<p>添加<strong>质量失真</strong>和<strong>尺寸</strong>属性</p>
</li>
<li>
<p>Apache HTTP服务器</p>
<p>存储视频段和mpd文件，向客户端推流</p>
</li>
</ol>
<h3 id="客户端">客户端</h3>
<ol>
<li>
<p>基础：dash.js</p>
</li>
<li>
<p>额外的模块</p>
<ul>
<li>
<p><code>QoE-driver Optimizer</code>
$$
Output = HTTP\ GET请求中的最优段
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Target\ buffer\ based\ Rate\ Controller\
Viewport\ Probabilistic\ Model\
QR\ Map
\end{cases}
$$</p>
</li>
<li>
<p><code>Target-buffer-based Rate Controller</code>
$$
Output = 总计的传输码率，按照公式13计算而来
$$</p>
<p>$$
Input = Output\ of\ {Bandwidth\ Estimation\ module
$$</p>
</li>
<li>
<p><code>Viewport Probabilistic Model</code>
$$
Output = 每个tile被看到的可能性，按照公式10计算而来
$$</p>
<p>$$
Input = Output\ of\
\begin{cases}
Orientation\ Prediction\ module\
SRD\ information
\end{cases}
$$</p>
</li>
<li>
<p><code>QR Map</code>QR=&gt;Quality-Rate
$$
Output = 所有段的QR映射
$$</p>
<p>$$
Input = MPD中的属性
$$</p>
</li>
<li>
<p><code>Bandwidth Estimation</code>（没有展开研究，因为不是关键？）
$$
Output = 前3秒带宽估计的平均值
$$</p>
<p>$$
Input = 下载段过程中的吞吐量变化
$$</p>
<p>可以通过<code>onProgess()</code>的回调函数<code>XMLHttpRequest API</code>获取</p>
</li>
<li>
<p><code>Orientation Prediction</code>
$$
Output = 用户方向信息的预测结果（yaw, pitch, roll）
$$</p>
<p>$$
Input = Web\ API中获取的DeviceOrientation信息，使用线性回归做预测
$$</p>
</li>
</ul>
</li>
</ol>
<h2 id="评估">评估</h2>
<ul>
<li>
<p>整体设定</p>
<ol>
<li>将用户头部移动轨迹编码进播放器来模拟用户头部移动</li>
<li>积极操控网络状况来观察不同方案对网络波动的反应</li>
</ol>
</li>
<li>
<p>详细设定</p>
<ul>
<li>
<p>服务端</p>
<ol>
<li>
<p>视频选择</p>
<p>2880x1440分辨率、时长3分钟、投影格式ERP</p>
</li>
<li>
<p>切分设置</p>
<p>每个块长1s（$T=1$）、每个块被分成6x12个tile（$N=72$）</p>
<p>每个段的码率设置为${20, 50, 100, 200, 300}$，单位kpbs</p>
</li>
<li>
<p>视频编码</p>
<p><a href="http://www.videolan.org/developers/x264.html" target="_blank" rel="noopener noreffer">开源编码器x264</a></p>
</li>
<li>
<p>视频分包</p>
<p><a href="https://gpac.wp.mines-telecom.fr/mp4box/" target="_blank" rel="noopener noreffer">MP4Box</a></p>
</li>
<li>
<p>注意事项</p>
<p>每个段的确切尺寸可能与其码率不同，尤其对于长度较短的块。</p>
<p>为了避免这影响到码率自适应，将段的确切尺寸也写入MPD文件中</p>
</li>
</ol>
</li>
<li>
<p>客户端</p>
<ol>
<li>
<p>缓冲区设定（经过实验得出的参数）</p>
<p>$B_{max}=3s$，$B_{target}=2.5s$，$R_{min}=200kbps$，$权重\eta=0.0015$</p>
</li>
</ol>
</li>
<li>
<p>高斯分布设定</p>
<table>
<thead>
<tr>
<th style="text-align:center">Yaw</th>
<th style="text-align:center">Pitch</th>
<th style="text-align:center">Roll</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\mu_{\alpha}=-0.54,\ \sigma_{\alpha}=7.03$</td>
<td style="text-align:center">$\mu_{\beta}=0.18,\ \sigma_{\beta}=2.55$</td>
<td style="text-align:center">$\mu_{\gamma}=2.16,\ \sigma_{\gamma}=0.15$</td>
</tr>
</tbody>
</table>
</li>
</ul>
</li>
<li>
<p>比较对象</p>
<ul>
<li>ERP：原始视频格式</li>
<li>Tile：只请求用户当前viewport的tile，不使用viewport预测，作为baseline</li>
<li>Tile-LR：使用线性回归做预测，每个tile的码率被平均分配</li>
</ul>
</li>
<li>
<p>性能指标</p>
<ul>
<li>卡顿率：卡顿时间占播放总时长的比例</li>
<li>Viewport PSNR：直接反应Viewport内的视频质量</li>
<li>空间质量差异：Viewport内质量的协方差</li>
<li>Viewport偏差：空白区域在Viewport中的比例</li>
</ul>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for Dante</title>
    <link>https://ayamir.github.io/2021/12/note-for-dante/</link>
    <pubDate>Wed, 08 Dec 2021 22:14:15 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/12/note-for-dante/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link: <a href="https://dl.acm.org/doi/10.1145/3232565.3234686" target="_blank" rel="noopener noreffer">https://dl.acm.org/doi/10.1145/3232565.3234686</a></p>
<p>Level: SIGCOMM 18</p>
<p>Keyword: UDP+FOV-aware+FEC</p>
<h2 id="工作范围">工作范围</h2>
<p></p>
<h2 id="目标">目标</h2>
<p>在给定序列的帧中，为<strong>每个tile</strong>设定FEC冗余，根据其被看到的可能性的加权最小化平均质量降低。</p>
<h2 id="问题建模">问题建模</h2>
<ol>
<li>
<p>输入
估计的丢包率$p$、发送速率$f$、有$n$个tile的$m$个帧($&lt;i, j&gt;$来表示第$i$个帧的第$j$个tile</p>
<p>第$&lt;i, j&gt;$个tile的大小$v_{i, j}$、第$&lt;i, j&gt;$个tile被看到的可能性$\gamma_{i, j}$、</p>
<p>如果第$&lt;i, j&gt;$ 个tile没有被恢复的质量降低率、最大延迟$T$</p>
</li>
<li>
<p>输出</p>
<p>第$&lt;i, j&gt;$个tile的FEC冗余率$r_{i, j} = \frac{冗余包数量}{原始包数量}$</p>
</li>
<li>
<p>最优化问题的形式化
$$
minimize\  \sum_{0&lt;i\le m}\sum_{0&lt;j\le n} \gamma_{i, j}d_{i, j}(p, r_{i, j})
$$</p>
<p>$$
subject\ \ to\ \  \frac{1}{f}\sum_{0&lt;i\le m}\sum_{0&lt;j\le n}v_{i, j}(1+r_{i, j}) \le T
$$</p>
<p>$$
r_{i, j} \le 0
$$</p>
<p>（1）：最小化最终被看到的tile的质量衰减的加权和，权重按照被看到的可能性分配。</p>
<p>（2）：经过重新编码的包和原始的包需要在T时刻之前发出。</p>
<p>​      Dante将1个GOP(Group of Pictures)中的所有帧当作一批处理，$T$作为GOP的持续时间</p>
<p>​      $f$：使用TCP Friendly Rate Control algorithm，基于估计的丢包率和网络延迟来计算得出</p>
<p>（3）：确保冗余率总是非负的。</p>
</li>
<li>
<p>关键变量是$d_{i, j}(p, r)$：丢包率是p情况下，采用r作为冗余率的第$&lt;i, j&gt;$个tile的质量衰减
$$
d_{i, j}(p, r) = \delta_{i, j},\ if\ r &lt; \frac{1}{1-p}; 0, otherwise.
$$</p>
<p>假设帧中有k个原始包，质量衰减发生在丢失的包不能被恢复的情况下。</p>
<p>FEC可以容忍 $r \cdot k$ 个丢包=&gt;即当 $p(r<em>k+k)$ 大于  $r</em>k$  时会发生质量衰减。</p>
</li>
<li>
<p>过多的丢包会导致依赖链上所有帧的质量衰减，因此考虑帧之间的依赖关系之后，可以重新计算质量衰减：</p>
<p>$$
d^{*}<em>{i, j}(p, r) = \sum</em>{0&lt;c\le i}w_{c, i}d_{c, j}(p, r)
$$</p>
<p>$w_{c, i}$ 编码帧i对帧c的依赖作为单独的第c个帧的质量衰减的权重；</p>
<p>最终第i个帧的第j个tile的最终质量衰减就是所有依赖的质量衰减的和。</p>
</li>
</ol>
<h2 id="fec冗余的自适应逻辑">FEC冗余的自适应逻辑</h2>
<ol>
<li>
<p>关于$d_{i, j}(p, r)$ ：因为是分段函数，所以其值会因为r和p的大小关系而急剧改变。</p>
<p>利用背包问题的思想可以将其规约成NP完全问题：</p>
<p>将每个tile看作是一个物品，共有m*n个。</p>
<p><strong>如果$r_{i, j} &lt; \frac{1}{1-p}$ ，则表示不把第&lt;i,j&gt;和物品放入背包；否则就是将其放入背包。</strong></p>
<p>公式1可以转化为：最大化所有物品二元变量的线性组合；</p>
<p>公式2可以转化为：二元变量的另一个线性组合必须低于阈值约束。</p>
<p>因此整个问题就能被完全转化为<strong>0-1背包</strong>问题</p>
</li>
<li>
<p>算法</p>
<p></p>
<p>整体上是背包问题的标准解法，能以线性复杂度（因为变量只是B)解决问题。</p>
</li>
</ol>
<h2 id="原型设计">原型设计</h2>
<p></p>
<ul>
<li>使用基于TCP和UDP的两条连接来分别传输控制信息（双向：到客户端的播放会话的起至点和到服务端的网络信息反馈）和视频数据包</li>
<li>服务端根据反馈的网络信息，在每个GOP的边界时刻运行算法1来确定下一个GOP的帧和tile的FEC冗余。
确定之后服务端使用RS码来插入冗余包，和原始视频数据包一起重新编码，并使用基于TFRC的发送率发送数据。</li>
<li>Dante的实现是对应用程序级比特率适配策略的补充，并且可以通过对视频播放器进行最小更改来替换现有的底层传输协议来部署。</li>
</ul>
<h2 id="实验评估">实验评估</h2>
<ul>
<li>
<p>环境：使用Gilbert模型来模拟实现丢包事件（而非使用统一随机丢包）</p>
<p>创造了两种网络条件good（丢包率0.5%）和bad（丢包率2%）</p>
</li>
</ul>
<h2 id="局限性">局限性</h2>
<ul>
<li>效果主要依赖于Viewport预测的结果是否准确</li>
</ul>
]]></description>
</item>
<item>
    <title>沉浸式流媒体传输的实际度量</title>
    <link>https://ayamir.github.io/2021/11/note11/</link>
    <pubDate>Mon, 22 Nov 2021 15:21:59 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note11/</guid>
    <description><![CDATA[<h2 id="度量指标">度量指标</h2>
<ol>
<li>viewport预测精度。
<ul>
<li>使用预测的viewport坐标和实际用户的viewport坐标的大圈距离来量化。</li>
</ul>
</li>
<li>视频质量。
<ul>
<li>viewport内部的tile质量（1～5）。</li>
<li>tile在最高质量层之上花费的时间。</li>
<li>根据用户视线的分布而提出的加权质量度量。</li>
</ul>
</li>
</ol>
<h2 id="度量参数">度量参数</h2>
<ol>
<li>分块策略</li>
<li>带宽</li>
<li>延迟</li>
<li>viewport预测</li>
<li>HTTP版本</li>
<li>持久化的连接数量</li>
</ol>
]]></description>
</item>
<item>
    <title>沉浸式推流中应用层的优化</title>
    <link>https://ayamir.github.io/2021/11/note10/</link>
    <pubDate>Mon, 15 Nov 2021 10:13:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/2021/11/note10/</guid>
    <description><![CDATA[<h2 id="背景">背景</h2>
<p>大多数的HAS方案使用HTTP/1.1协议进行请求-回应的事务来取得需要的资源、缓冲取到的视频段并以线性的顺序播放。传统的HAS中，只需要1个GET请求来取得下一个视频的暂时的部分。只要视频段的持续时间比网络内的时延高，这种方法就可行。</p>
<p>在基于VR的HAS方案中，播放1条视频片段就需要取得多种资源：1次GET请求需要同时请求基础的tile层和每个空间视频tile。使用4x4的tile方案时，客户端需要发起不少于17次GET请求。使用 1 s 数量级的分段持续时间，即使是 20 ms 的微小网络延迟也会显着阻碍客户端和服务器之间的整体吞吐量，因此会导致较低的视频质量。</p>
<h2 id="解决方案">解决方案</h2>
<h3 id="使用多条持久的tcp连接">使用多条持久的TCP连接</h3>
<p>大多数的现代浏览器都支持同时建立并维持多达6条TCP连接来减少页面加载时间，并行地获取请求的资源。这允许增加整体吞吐量，并部分消除网络延迟引入的空闲 RTT 周期。</p>
<p>类似地，基于 VR 的 HAS 客户端可以使用多个 TCP 连接并行下载不同的tile。</p>
<h3 id="使用http2协议的服务端push特性">使用HTTP/2协议的服务端push特性</h3>
<p>HTTP/2协议引入了请求和相应的多路复用、头部压缩和请求优先级的特性，这可以减少页面加载时间。</p>
<p>服务端直接push短视频片段可以减少视频的启动时间和端到端延迟。</p>
<p>并且，服务端push特性可以应用在基于tile的VR视频推流中，客户端可以向服务器同时请求一条视频片段的所有tile。</p>
<p>服务端可以使用特制的请求处理器，允许客户端为每个tile定义一系列质量等级。</p>
<p>因此可以将应用的启发式自适应的速率的决定传达给服务器，这允许客户端以期望的质量级别取得所有图块。</p>
]]></description>
</item>
</channel>
</rss>

<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - Ayamir&#39;s Blog</title>
        <link>https://ayamir.github.io/posts/</link>
        <description>所有文章 | Ayamir&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miracle_l@bupt.edu.cn (Ayamir)</managingEditor>
            <webMaster>miracle_l@bupt.edu.cn (Ayamir)</webMaster><lastBuildDate>Tue, 18 Jan 2022 16:07:02 &#43;0800</lastBuildDate><atom:link href="https://ayamir.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>Note for Popularity Aware 360-Degree Video Streaming</title>
    <link>https://ayamir.github.io/posts/note-for-popularity-aware-360-degree-video-streaming/</link>
    <pubDate>Tue, 18 Jan 2022 16:07:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-popularity-aware-360-degree-video-streaming/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/abstract/document/9488856/" target="_blank" rel="noopener noreffer">Popularity-Aware 360-Degree Video Streaming</a></p>
<p>Level：IEEE INFOCOM 2021</p>
<p>Keywords：Dynamic tiling, Cross-user division, Heuristic QoE optimization</p>
<h2 id="motivation">Motivation</h2>
<p>将视频划分成分块进行编码之后，会降低编码效率，并增大服务端的存储压力。（细节可以参见<a href="https://ayamir.github.io/posts/note-for-optile/" target="_blank" rel="noopener noreffer">Optile</a>）</p>
<p>而分块时根据用户的ROI来确定不同的大小，并在客户端预取，这可以节省带宽。</p>
<p>用户的ROI推断利用跨用户的偏好来确定，即所谓的<code>Popularity-Aware</code>。</p>
<h2 id="model-and-formulation">Model and Formulation</h2>
<h3 id="video-model">Video Model</h3>
<p>视频从时间上被分成固定长度的片段，接着每个片段被从空间上划分成 $C$ 个分块。</p>
<p>除了常规的分块之外， $M$ 个宏块也被建构出来。</p>
<p>每个常规分块和宏块都被编码成 $V$ 个不同的码率质量等级并存储在服务端。</p>
<p>整个推流过程可以看作是一系列连续的下载任务。</p>
<p>客户端在每次下载任务中的目标是：选择恰当分块（宏块或者常规分块的集合）的恰当质量。</p>
<p>用 $L$ 表示客户端请求分块时，缓冲区中已经下载但还没有查看的视频的视频长度，为了避免缓冲事件，分块需要在缓冲区被清空即 $L = 0$ 之前被下载完毕。</p>
<h3 id="qoe-model">QoE Model</h3>
<p>$$
Q(V_k) = Q_{0}(V_k) - {\omega}_v I_v (V_k) - {\omega}_r I_r (V_k)
$$</p>
<p>$V_k$ 表示下载的第 $k$ 段视频质量； $Q_0$ 表示平均质量； $I_v$ 表示由质量变化导致的质量损害； $I_r$ 表示由缓冲事件导致的质量损害； ${\omega}_v$ 和 ${\omega}_r$ 分别表示质量变化和缓冲的加权因子；</p>
<ul>
<li>
<p>平均质量：
$$
Q_0(V_k) = q(\overline{V_k})
$$
$\overline{V_k}$ 表示<code>FoV</code>内的平均视频质量； $q(\cdot)$ 表示视频质量和用户实际感知质量之间的映射函数；</p>
</li>
<li>
<p>质量变化：两个连续段之间的质量差异和<code>FoV</code>内不同空间位置tile的质量差异会导致用户不适。
$$
I_v(V_k) = |Q_0(V_k) - Q_0(V_{k-1})| + \widehat{V_k}
$$
$|Q_0(V_k) - Q_0(V_{k-1})|$ 表示连续段间的<code>FoV</code>内时间质量差异； $\widehat{V_k}$ 表示一个视频段的<code>FoV</code>内空间质量差异；</p>
</li>
<li>
<p>缓冲：
$$
L_r(V_k) = {(\frac{S(V_k)}{R} - L, 0)}_+
$$
$S(V_k)$ 表示段数据量大小； $R$ 表示下载吞吐量； ${(x)}_+ = max \lbrace x, 0 \rbrace$ ；</p>
</li>
</ul>
<h3 id="formulation">Formulation</h3>
<p>用 ${\beta}^v_m ({\beta}^v_c)$ 表示对应的宏块或常规块是否被下载：</p>
<p>${\beta}^v_m = 1$ 表示下载编码的质量等级为 $v$ 的宏块，消耗的带宽为 $B^v_m$ ，反之 $ {\beta}^v_m = 0$ 表示不下载；</p>
<p>${\beta}^v_c = 1$ 表示下载编码的质量等级为 $v$ 的常规块，消耗的带宽为 $B^v_c$，反之 ${\beta}^v_m = 0$ 表示不下载；</p>
<p>客户端应该优先下载覆盖用户<code>FoV</code> 的宏块，如果没有这样的宏块则去下载对应的常规块的集合。</p>
<p>优化目标：
$$
max\ Q(\lbrace v | {\forall}_{m, v} {\beta}^v_m = 1 \rbrace) + Q(\lbrace v | {\forall}_{c, v} {\beta}^v_c = 1 \rbrace)
$$
同时需要满足以下3个约束：
$$
\sum^{M}_{m=1} \sum^{V}_{v=1} {\beta}^v_m + 1(\sum^{C}_{c=1} \sum^{V}_{v=1} {\beta}^v_c) = 1
$$</p>
<p>$$
\sum^{V}_{v=1} {\beta}^v_c \le 1,\ for\ c = 1, &hellip;, C
$$</p>
<p>$$
\sum^{M}_{m=1} \sum^{V}_{v=1} {\beta}^v_m B^v_m + \sum^{C}_{c=1} \sum^{V}_{v=1} {\beta}^v_c B^v_c \le R \cdot L
$$</p>
<p>$Q(\cdot)$ 是公式1中定义的质量； $R$ 是网络带宽； $1(x) = 1 \iff x &gt; 0$ ；$1(x) = 0 \iff x \le 0$ ；</p>
<p>约束1强制为观看区域下载宏块或常规块的集合，只下载宏块的一个质量版本；</p>
<p>约束2规定只下载常规块的一个质量版本；</p>
<p>约束3保证视频数据可以在开始播放之前被完全下载；</p>
<p>给出用户的观看区域之后，候选的宏块或对应的常规块集合也可以求出。</p>
<p>将<code>QoE</code>最大化的问题分解成两个子问题：</p>
<ul>
<li>确定宏块的质量等级；</li>
<li>确定常规块的质量等级；</li>
</ul>
<p>最后的解取这两种方案能取得更大<code>QoE</code>的那种。</p>
<p>如果<code>QoE</code>模型不考虑常规块之间的质量差异，则整体的<code>QoE</code>等价于下载的常规块的平均质量等级。</p>
<p>确定常规块质量等级的问题则可以简化为：
$$
max\ \sum_{c \in C} \sum^{V}_{v=1} Q({\beta}^v_c v)
$$
需要满足以下2个约束：
$$
\sum^{V}_{v=1} {\beta}^v_c = 1,\ for\ c \in C
$$</p>
<p>$$
\sum_{c \in C} \sum^{V}_{v=1} {\beta}^v_c B^v_c \le R \cdot L
$$</p>
<p>$C$ 表示覆盖观看区域的常规块集合。</p>
<p>简化之后的子问题可以通过对多项选择背包问题的简化，证明为是<code>NP-hard</code>问题，基于此提出启发式算法。</p>
<h2 id="基于宏块的流行性感知推流">基于宏块的流行性感知推流</h2>
<h3 id="基于观看区域确定宏块">基于观看区域确定宏块</h3>
<p>不同用户对相同视频的观看有着相似的ROI，其视野中心是相近的，因此首先确定其视野中心并聚类到一起。</p>
<p>不能直接应用的知名聚类算法：</p>
<ul>
<li>需要事先确定簇（即宏块）数量的算法（事先并不能确定需要多少宏块）：<code>K-means</code></li>
<li>簇会越聚越大的算法（这样会失去节约带宽的优点）：<code>DBSCAN</code></li>
</ul>
<p>提出的算法用2个参数 $\lambda$ 和 $\gamma$ 来保证彼此相近的两个视野中心被归入同一簇，同时基于簇的宏块不至于太大。</p>
<ul>
<li>被归入同一簇的视野中心之间的距离应该小于等于 $\lambda$；</li>
<li>同一个簇的任意两个视野中心之间的距离应该小于等于 $\gamma$；</li>
</ul>
<p>为了确定这两个参数，还需要考虑常规块的大小带来的影响。</p>
<p>算法描述：</p>
<p>给出用 $P$ 表示的点集，其中每个点表示一个用户的视野中心位置；</p>
<p>用 $N_p = \lbrace q | q \in P \land q \neq p \land dist(p, q) \le \lambda \rbrace$ 来表示与点 $p$ 之间欧式距离小于 $\lambda$ 的点集（即为临近点集）；</p>
<ol>
<li>初始化拥有最多临近点的点所在的簇，例如： $p = {argmax}_{p \in P} |N_p|$；</li>
<li>添加临近簇内任何点的点到簇中，扩张过程直到找不到符合条件的点位置；</li>
<li>检查簇中任意两个点之间的距离是否大于 $\gamma$ ，如果存在这种情况就使用<code>K-means</code>算法将这个簇分成两个子簇；</li>
<li>从 $P$ 中移除簇中的点；</li>
<li>重复1-4的过程直到 $P = \empty$；</li>
</ol>
<p></p>
<h3 id="宏块优化">宏块优化</h3>
<p>通过简单地覆盖簇中用户的所有观看区域来为每个簇建构宏块可能会导致建构出不必要的大宏块，因此需要确定恰当的宏块大小。</p>
<ul>
<li>
<p>首先需要确定哪些用户的观看区域应该被用于构建宏块，这样用户下载宏块时的带宽使用率小于下载一组常规块时的带宽使用率：$B_m$ 和 $B_c$ 分别表示覆盖相同观看区域的宏块和常规块的数据量大小。</p>
</li>
<li>
<p>为了解决用户头部运动的随机性，宏块应该在覆盖用户观看区域之外加上一些边界区域。边界区域可以基于用户观看中心的变化来确定，变化通过在推流观看过程中以固定采样率记录。</p>
<p>一个视频片段中 $x(y)$ 坐标的变化定义为 $x(y)$ 坐标的标准差。</p>
<p></p>
<p>实验发现：在一个视频片段中，用户的 $x(y)$ 坐标的变化很小。</p>
<p>分别用 $A_x$ 和 $A_y$ 表示 $x$ 和 $y$ 方向上的变化，构建的宏块应该覆盖用户的观看区域，并为 $x(y)$ 方向加上 $\frac{A_x}{2}(\frac{A_y}{2})$ 的边缘区域。</p>
</li>
</ul>
<p>宏块构造问题的形式化：</p>
<p>为每个用户 $i$ 引入二元变量 ${\alpha}_i$ ，${\alpha}_i = 1$ 表示此用户的观看区域用于构建宏块，反之则没有；</p>
<p>实际应用中即为：如果 ${\alpha}_i = 1$ ，则用户 $i$ 可以下载宏块；否则用户只能下载对应的常规块集合。</p>
<p>问题的目标是：在下载宏块或相同质量等级的常规块集合时，最小化所有用户的总带宽使用量。
$$
\underset{\lbrace {\alpha}_i \rbrace}{min}\ \sum^{N_j}_{i=1} {\alpha}_i B_m + (1-{\alpha}_i) B_c
$$
$N_j$ 表示在 $j^{th}$ 簇中的用户数量；解决问题之后，可以用所有 ${\alpha}_i = 1$ 的用户观看区域构建宏块；</p>
<p>尽管暴力枚举法可以完成最优求解，但是其时间复杂度为 $O(2^{N_j})$ ，为了减少实际建构宏块的时间，提出一种类似于<a href="https://en.wikipedia.org/wiki/Random_sample_consensus" target="_blank" rel="noopener noreffer">随机采样一致性算法</a>的迭代算法，每次迭代中，所做工作如下：</p>
<ol>
<li>随机选取用户观察区域的子集。</li>
<li>编码宏块，用 $B_m$ 表示构建的宏块的带宽使用量。</li>
<li>检查建构的宏块是否覆盖用户 $i \in \lbrace 1, &hellip;N_j \rbrace$ ，是则${\alpha}_i = 1$；否则 ${\alpha}_i = 0$。</li>
<li>检查总共的带宽使用量是否比之前迭代的更小，是则用当前迭代建构的宏块更新最终的宏块；否则继续迭代。</li>
</ol>
<p>为了避免预测失败时用户看到空白区域，在下载观看区域的高质量宏块或常规块集合之外，也以最低质量下载其余的常规块。</p>
<h3 id="流行性感知推流">流行性感知推流</h3>
<p>服务端基于多个用户的历史观看信息建构宏块，同时也使用常规块的划分方案编码视频。</p>
<p>客户端在推流过程中选择恰当块（宏块或常规块集）的恰当的质量等级来最大化用户的<code>QoE</code>。</p>
<p>流行性感知的推流算法首先为每个视频段预测用户的观看区域，之后预取相应的宏块或常规块集。</p>
<ul>
<li>使用<a href="https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db" target="_blank" rel="noopener noreffer">岭回归</a>做VP，输入用户在一系列历史帧中的观看区域中心坐标，输出未来帧中用户的观看区域位置。</li>
<li>基于预测的观看区域，算法确定是否存在覆盖预测区域及其边缘区域的宏块，是则搜索并下载满足条件的最高质量的宏块；否则下载相应区域的常规块集。</li>
</ul>
<p></p>
<p>选择常规块集时首先为所有要选择的块确定满足贷款限制的最高质量等级，分配完之后如果还有剩余的带宽，算法会根据常规块与视野中心距离的远近程度提高一个质量等级，越近越优先提高。同时考虑到空间质量差异会降低<code>QoE</code>，所以提高质量的行为只有在超过半数的常规块满足条件时才会执行。</p>
]]></description>
</item>
<item>
    <title>Summary for VR and Panoramic Video</title>
    <link>https://ayamir.github.io/posts/summary-for-vr-and-panoramic-video/</link>
    <pubDate>Mon, 17 Jan 2022 17:02:51 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/summary-for-vr-and-panoramic-video/</guid>
    <description><![CDATA[<p>VR和360度全景视频都是获得沉浸式体验的重要途径，除此之外，AR（Argmented Reality）和MR（Mixed Reality）也是比较火的概念，可以用来对比学习。</p>
<h2 id="全景视频">全景视频</h2>
<ol>
<li>全景视频实际上事先通过特殊的全景摄像机录制好视频，之后可以在<code>HMD</code>中观看。虽然看到的图像相对于用户当前环境而言是虚拟的，但是终归是从实际环境中录制而来的，本质上更贴近普通视频的全景推广。</li>
<li>在全景视频的观看过程中，用户只有3DoF的自由度，即只能完成头部的3个角度的运动，同时手柄实际上并不能和视频中的内容进行交互。</li>
<li>全景视频的主要应用在于实景导览，通过事先由拍摄者带着全景录像设备行走拍摄，用户观看时实际是将自己带入到全景设备的位置上，同时移动头部来观察不同角度的视频。</li>
</ol>
<h2 id="vr">VR</h2>
<ol>
<li>VR主要做的工作是创造出一个完全虚拟的环境，用户戴上<code>HMD</code>之后可以通过其看到虚拟环境中的事物，同时也可以使用<code>HMD</code>配套的手柄等设备进行操作，完成与虚拟环境之间的交互；</li>
<li>VR支持的是6DoF的自由度，即除了头部的运动之外也支持身体的前后、左右、上下的移动，手柄；</li>
<li>VR的主要应用在于游戏，比如广受好评的<code>Beat Saber</code>（又称<a href="https://zh.wikipedia.org/zh-cn/%E8%8A%82%E5%A5%8F%E5%85%89%E5%89%91" target="_blank" rel="noopener noreffer"><code>节奏光剑</code></a>），用户根据音乐节奏通过挥动手柄（在虚拟环境中被建模成光剑）来准确地按照提示的方向去砍击方块；</li>
</ol>
<h2 id="ar和mr">AR和MR</h2>
<ol>
<li>
<p>AR主要做的工作是将虚拟世界中的事物投影到现实世界中，主体是现实世界，虚拟事物用于增强现实世界。</p>
<p>MR主要做的工作是将现实世界中的事物虚拟化进入虚拟世界中，主体是虚拟世界，现实事物混合进虚拟世界中。</p>
</li>
<li>
<p>AR实现起来比较简单，只需要将计算机产生的图像投影显示在现实中即可，目前的应用比如游戏<code>Pokémon GO</code>里面的<code>AR-mode</code>，启用之后游戏中遇到的<code>Pokémon</code>就可以投影在现实中。</p>
<p>MR实现起来比较复杂，首先需要用摄像头扫描物体，得到的2D图像再交给计算机采用算法进行3D重建，最后将虚拟化建模好的物体展示到虚拟世界中，目前的应用比如<code>Meta</code>推出的<code>Workrooms</code>，线上的远距离视频会议在虚拟世界中可以变成虚拟人物之间面对面的交流。</p>
</li>
</ol>
<h2 id="总结">总结</h2>
<ol>
<li>
<p>全景视频侧重于对虚拟环境的观察，而VR侧重于对虚拟环境的交互。</p>
</li>
<li>
<p>全景视频实际上是将用户带入到全景摄像机的位置上，让用户产生自己身临拍摄的环境中的感觉，本质上是对传统视频的推广；</p>
<p>VR实际上是将用户完全带入到虚拟的环境中，用户可以和虚拟环境中的事物进行交互，而虚拟环境中发生的一切都和现实无关，本质上是对传统游戏的推广；</p>
</li>
<li>
<p>全景视频实际上和VR、AR、MR这种概念距离比较远，实际上只是因为全景摄像机相较于普通摄像机的360度视角的特殊性，这能让用户产生沉浸感。</p>
</li>
<li>
<p>VR相比于AR、MR而言，是纯粹的虚拟环境，并不涉及到现实事物（除了<code>HMD</code>配套的手柄等设备），而纯粹的虚拟环境将人带入到了一个完全不同的世界，也是VR沉浸式体验的来源。</p>
</li>
<li>
<p>AR和MR是虚拟和现实交融的技术，前者主体是现实，后者主体是虚拟环境。</p>
</li>
</ol>
<p></p>
]]></description>
</item>
<item>
    <title>Note for srlABR Cross User</title>
    <link>https://ayamir.github.io/posts/note-for-srlabr-cross-user/</link>
    <pubDate>Sat, 15 Jan 2022 18:46:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-srlabr-cross-user/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9234071" target="_blank" rel="noopener noreffer">Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network</a></p>
<p>Level：IEEE Transactions on Broadcasting 2021</p>
<p>Keywords：Cross-user vp, Sequetial RL ABR</p>
<h2 id="主要工作">主要工作</h2>
<ul>
<li>使用跨用户注意力网络<code>CUAN</code>来做VP；</li>
<li>使用<code>360SRL</code>来做ABR</li>
<li>将上面两者集成到了推流框架中；</li>
</ul>
<h2 id="vp">VP</h2>
<h3 id="motivation">Motivation</h3>
<p>形式化VP问题如下：</p>
<p>给出 $p^{th}$ 用户的 $1-t$ 时间内的历史视点坐标 $L^{p}_{1:t} = \lbrace l^p_1, l^p_2, &hellip;, l^p_t \rbrace$ ，其中 $l^p_t = (x_t, y_t), x_t \in [-180, 180]; y_t \in [-90, 90]$ ；</p>
<p>同一视频的不同用户视点表示为 $L^{1:M}_{1:t+T}$ ， $M$ 表示其他用户的数量；</p>
<p>目标是预测未来的 $T$ 个时刻的视点位置 $L^p_i, i = t+1, &hellip;, t+T$ ；</p>
<p>最终可以用数学公式表达为：
$$
\underset{F}{min} \sum^{t+T}_{k = t+1} {\parallel l^p_k - \hat{l}^p_k \parallel}_1
$$</p>
<p>现有的用<code>KNN</code>做的跨用户预测基于LR的模型，而LR的模型很容易产生偏差，所以为了增强<code>KNN</code>的性能，同时考虑单用户的历史视点轨迹和跨用户的视点轨迹。</p>
<ul>
<li>提出一种注意力机制来自动提取来自其他用户视口的有用信息；</li>
<li>对于与当前用户有相似偏好的用户轨迹信息给与更多的注意；</li>
<li>相似性通过基于过去时间段内其他用户的轨迹计算出来；</li>
</ul>
<h3 id="design">Design</h3>
<p></p>
<ol>
<li>
<p>轨迹编码器模块从用户的历史视点位置提取时间特征；</p>
<p>使用<code>LSTM</code>来编码用户的观看路径；</p>
<p>为了预测 ${(t+1)}^{th}$ 帧的视点位置，首先向<code>LSTM</code>输入 $p^{th}$ 用户的历史视点坐标：
$$
f^{p}_{t+1} = h(l^p_1, l^p_2, &hellip;, l^p_t)
$$
$h(\cdot)$ 是<code>LSTM</code>的输入输出函数；</p>
<p>接着使用相同的<code>LTSM</code>编码其他用户的观看轨迹：
$$
f^{i}_{t+1} = h(l^i_1, l^i_2, &hellip;, l^i_{t+1}), i \in \lbrace 1, &hellip;, M \rbrace
$$</p>
</li>
<li>
<p>注意力模块从其他用户的视点轨迹中提取与 $p^{th}$ 用户相关的信息</p>
<p>首先推导出 $p^{th}$ 用户和其他用户之间的相关系数：
$$
s^{pi}_{t+1} = z(f^{i}_{t+1}, l^{p}_{t+1}), i \in \lbrace 1, &hellip;, M \rbrace \cup \lbrace p \rbrace;
$$
$s^{th}_{t+1}$ 表示 $p^{th}$ 用户和 $i^{th}$ 用户之间的相似性；$z()$ 由内积运算建模（还可用其他方式建模比如多个FC层）；</p>
<p>接着将相关系数规范化：
$$
{\alpha}^{pi}_{t+1} = \frac{e^{s^{pi}_{t+1}}}{\sum_{i \in \lbrace 1,&hellip; M \rbrace \cup {\lbrace p \rbrace}^{e^{s^{pi}_{t+1}}}}}
$$
最后得到融合特征：
$$
g^{p}_{t+1} = \sum_{i \in {\lbrace 1,&hellip;M \rbrace \cup \lbrace p \rbrace}} {\alpha}^{pi}_{t+1} \cdot f^{i}_{t+1}
$$
融合特征被最后用于VP。</p>
</li>
<li>
<p>VP模块预测 ${(t+1)}^{th}$ 帧的视点位置</p>
<p>$$
\hat{l}^{p}_{t+1} = r(g^{p}_{t+1})
$$
函数 $r(\cdot)$ 由一层FC建模。值得注意的是，对应于未来 T 帧的视点是以滚动方式预测的。</p>
</li>
</ol>
<h3 id="loss">Loss</h3>
<p>损失函数定义为预测的视点位置和实际视点位置之间的所有绝对差异的总和：
$$
L = \sum^{t+T}_{i=t} {|\hat{l}^p_i - l^p_i|}_1
$$</p>
<h3 id="details">Details</h3>
<ul>
<li>使用<code>PyTorch</code>实现；</li>
<li>函数 $h(\cdot)$ 由两个堆叠的<code>LSTM</code>层组成，两者都有32个神经元；</li>
<li>函数 $r(\cdot)$ 包含一个带有32个神经元的FC层，接着是<code>Tanh</code>函数；</li>
<li>历史视点和未来视点的长度设定为1秒和5秒；</li>
<li>每次迭代从数据集中随机产生2048个样本；</li>
<li>所有训练变量的优化函数采用<code>Adam</code>；</li>
<li>$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$；</li>
<li>$learning\ rate = 10^{-3}, training\ epoch = 50$；</li>
</ul>
<h2 id="abr">ABR</h2>
<h3 id="formulation">Formulation</h3>
<p>全景视频被切分成 $m$ 个长度为 $T$ 秒的视频片段，每个视频片段空间上划分成 $N$ 个分块，分别以 $M$ 个不同的码率等级编码。因此对于每段有 $N \times M$ 个可选的编码块。</p>
<p>ABR的目标是为每个片段找到最优的码率集 $X = \lbrace x_{i, j} \rbrace \in Z^{N \times M}$ （ $x_{i, j} = 1$ 意味着为 $i^{th}$ 块选择 $j^{th}$ 的码率等级）：
$$
\underset{X}{max} \sum^{m}_{t=1} Q_t
$$
$Q_t$ 表示 $t^{th}$ 段的QoE分数，与以下几个方面有关：</p>
<ul>
<li>
<p>VIewport Quality：
$$
Q^1_t = \sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot p_i \cdot r_{i,j}
$$
$p_i$ 表示 $i^{th}$ 分块的规范化观看概率； $r_{i,j}$ 记录块 $(i, j)$ 的码率；</p>
</li>
<li>
<p>Viewport Temporal Variation：
$$
Q^2_t = |Q^1_t - Q^{1}_{t-1}|
$$</p>
</li>
<li>
<p>Viewport Spatial Variation：
$$
Q^3_t = \frac{1}{2} \sum^{N}_{i=1} \sum_{u \in U_i} p_i \cdot p_u \sum^{M}_{j=1} |x_{i,j} \cdot r_{i,j} - x_{u,j} \cdot r_{u,j}|
$$
$U_i$ 表示 $i^{th}$ 个分块的1跳邻居中的tile索引<a href="https://ieeexplore.ieee.org/document/8486606" target="_blank" rel="noopener noreffer">[1]</a>；</p>
</li>
<li>
<p>Rebuffering：
$$
Q^4_t = max(\frac{\sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot r_{i,j} \cdot T}{\xi_t} - b_{t-1}, 0)
$$
$\xi_t$ 表示网络吞吐量； $b_{t-1}$ 表示播放器的缓冲区占用率；</p>
<p>最终的QoE可以由上面的指标定义：
$$
Q_t = Q^1_t - \eta_1 \cdot Q^2_t - \eta_2 \cdot Q^3_t - \eta_3 \cdot Q^4_t
$$
$\eta_*$ 是可调节的参数，与不同的用户偏好对应。</p>
</li>
</ul>
<h3 id="sequential-rl-based-abr">Sequential RL-Based ABR</h3>
<p>假设基于tile的全景推流ABR过程也是MDP。</p>
<p></p>
<p>细节在<a href="https://ayamir.github.io/posts/note-for-360srl/" target="_blank" rel="noopener noreffer">360SRL</a>中已经说明清楚。</p>
]]></description>
</item>
<item>
    <title>Note for 360SRL</title>
    <link>https://ayamir.github.io/posts/note-for-360srl/</link>
    <pubDate>Thu, 13 Jan 2022 12:08:36 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-360srl/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/8784927" target="_blank" rel="noopener noreffer">360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming</a></p>
<p>Level：ICME 2019</p>
<p>Keywords：ABR、RL、Sequential decision</p>
<h2 id="创新点">创新点</h2>
<ul>
<li>在MDP中，将N维决策空间内的一次决策转换为1维空间内的N次级联顺序决策处理来降低复杂度。</li>
</ul>
<h2 id="问题定义">问题定义</h2>
<p>原始的全景视频被划分成每段固定长度为 $T$ 的片段，</p>
<p>每个片段包含 $N$ 个分块，并以 $M$ 的码率等级独立编码，</p>
<p>因此对每个片段，有 $N \times M$ 种可选的编码块。</p>
<p>为了保证播放时的流畅性，需要确定最优的预取集合：</p>
<p>${a_0, &hellip;, a_i, &hellip;, a_{N-1}}, i \in \lbrace 0, &hellip;, N-1 \rbrace, a_i \in \lbrace 0, &hellip;, M-1 \rbrace $</p>
<p>分别用 $q_{i, a_i}$ 和 $w_{i, a_i}$ 表示码率选择为 $a^{th}_i$ 的 $i^{th}$ 分块的质量和相应的分块片段大小。</p>
<p>用 $p_i \in [0, 1]$ 表示 $i^{th}$ 块的被看到的可能性。</p>
<h2 id="顺序abr决策">顺序ABR决策</h2>
<p></p>
<h2 id="代理设计">代理设计</h2>
<h3 id="状态">状态</h3>
<p>对于 $i^{th}$ 维，输入状态包括原始的环境状态 $s_t$ ；</p>
<p>与之前维度的动作集合相关的信号： $u^{i}_{s_t} = \lbrace Th, C_i, p_{0:i-1}, q_{0:i-1}, b_t, p_i, S_i, Q_{t-1} \rbrace$</p>
<p>$Th$ ：表示过去 m 次下载一个段的平均吞吐量；</p>
<p>$C_i \in R^M$ ：表示 $i^{th}$ 个分块的可用块大小向量；</p>
<p>$p_{0:i-1}$ 和 $q_{0:i-1, a^{0:i-1}_{t}}$ 分别表示选中的码率集合和看到之前 $i-1$ 个分块的概率集；</p>
<p>$b_t$ 是缓冲区大小；</p>
<p>$p_i$ 是 $i^{th}$ 个分块被看到的可能性；</p>
<p>$S_i$ 是之前选择的 $i-1$ 个分块的块大小之和： $S_i = \sum^{i-1}_{h=0} C_{h, a^h_t}$ ；</p>
<p>$Q_{t-1}$ 记录了最后一个段中 $N$ 个分块的平均视频质量；</p>
<h3 id="动作">动作</h3>
<p>动作空间离散，代理输出定义为价值函数：$f(u^i_{s_t}, a^i_t)$</p>
<p>表示所选状态的价值 $a^i_t \in \lbrace 0, &hellip;, M-1 \rbrace$ 处于状态 $u_{s_t}^i$ .</p>
<h3 id="回报">回报</h3>
<p>回报定义为下列因素的加权和：</p>
<p>平均视频质量 $q^{avg}_t$，空间视频质量方差 $q^{s_v}_t$，时间视频质量方差 $q^{t_v}_t$ ，重缓冲时间 $T^r_t$</p>
<p>$$
q^{avg}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot q_{i, a_i}
$$</p>
<p>$$
q^{s_v}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot |q_{i, a_i} - q^{avg}_t|
$$</p>
<p>$$
q^{t_v}_t = |q^{avg}_{t-1} - q^{avg}_t|
$$</p>
<p>$$
T^r_t = max \lbrace T_t - b_{t-1}, 0 \rbrace
$$</p>
<p>$$
R_t = w_1 \cdot q^{avg}_t - w_2 \cdot q^{s_v}_t - w_3 \cdot q^{t_v}_t - w_4 \cdot T^r_t
$$</p>
<h2 id="训练方法">训练方法</h2>
<p>使用<code>DQN</code>作为基本的算法来学习动作-价值函数 $Q(s_t, a_t; \theta)$ ，其中 $\theta$ 作为参数，对应的贪心策略为 $\pi(s_t; \theta) = \underset{\theta}{argmax} Q(s_t, a_t; \theta)$ 。</p>
<p><code>DQN</code>网络的关键想法是更新最小化损失函数的方向上的参数：
$$
L(\theta) = E[y_t - Q(s_t, a_t; \theta)]
$$</p>
<p>$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \pi(s_{t+1}; {\theta}'); {\theta}')
$$
${\theta}'$ 表示固定且分离的目标网络的参数；</p>
<p>$r(\cdot)$ 是即时奖励函数，即上面公式5中的 $R_t$ ；</p>
<p>$\gamma \in [0, 1]$ 是折扣因子；</p>
<p>为了缓解过拟合，引入 <code>double-DQN</code> 的结构，所以公式7被重写为：
$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, {\pi}(s_{t+1}; \theta); {\theta}')
$$
利用公式6和公式8可以得出 $i^{th}$ 维的暂时损失函数：
$$
l^i_t = Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$
其中 $Q_{target}$ 满足：</p>
<p>$$
Q_{target} = r_t + {\gamma}_u \cdot Q(u^0_{s_{t+1}}, \pi(u^0_{s_{t+1}}; 0); {\theta}')
$$</p>
<p>${\gamma}_u$ 和 ${\gamma}_b$ 分别代表”Top MDP“和”Bottom MDP“的折扣因子，训练中设定 ${\gamma}_b = 1$ 。</p>
<p>观察公式9和公式10可以看出每维都有相同的目标函数，意味着无法区别每个独立维度的动作 $a^i_t$ 对 $r_t$ 的贡献。</p>
<p>为了克服限制，根据某个分块的动作 $a^i_t$ 与其观看概率成正比的先验知识，向 $l^i_t$ 添加一个额外的 $r^i_{extra}$ ：
$$
l^i_t = r^i_{extra} + Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$</p>
<p>$$
r^i_{extra} =
\begin{cases}
0, p_i &gt; P ;
\
-a^i_t, p_i \le P
\end{cases}
$$</p>
<p>通过设定一个观看概率的阈值 $P$ ，对观看概率低于 $P$ 但选择了高码率的分块施加 $-a^i_t$ 的奖励。</p>
<p>因此最终的平均损失可以形式化为：
$$
l^{avg}_t = \frac{1}{N} \sum^{N-1}_{i=0} l^i_t
$$
接着使用梯度下降法来更新模型，学习率设定为 $\alpha$：
$$
\theta \larr \theta + \alpha \triangledown l^{avg}_t
$$
同时，在训练阶段利用经验回放法来提高<code>360SRL</code>的泛化性。</p>
<p></p>
<p></p>
<h2 id="实现细节">实现细节</h2>
<p></p>
<p>特征从输入状态中通过特征提取网络提取出来。</p>
<p>初始的4个输入通过带有128个过滤器的1维卷积层被传递，4个输入核心大小分别为 $1 \times m$ 、 $1 \times M$ 、 $1 \times N$ 、 $1 \times M$ ，后续这4个输入被喂给有128个神经元的全连接层；</p>
<p>随后特征映射被连接成一个张量，接着是具有1024个神经元和256个神经元的前向网络；</p>
<p>整个动作-价值网络的输出是M维的向量。</p>
<p>特征提取层和前向网络层都使用 <code>Leaky-ReLU</code>作为激活函数，最后是层归一化层。</p>
]]></description>
</item>
<item>
    <title>Summary for VP</title>
    <link>https://ayamir.github.io/posts/summary-for-vp/</link>
    <pubDate>Fri, 07 Jan 2022 23:08:36 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/summary-for-vp/</guid>
    <description><![CDATA[<h2 id="vp-在传输中所处的作用">VP 在传输中所处的作用</h2>
<p>基于 tile 的全景视频传输方式之所以热门，就是因其可以通过只传输用户 FoV 内的分块而大幅减少观看过程中消耗的带宽。</p>
<p>所以对用户 FoV 的预测是首先要处理的因素，如果 VP 精度很高，那么所有的带宽都可以用很高的码率去传输 FoV 内的分块。</p>
<h2 id="两种方式的基本假设">两种方式的基本假设</h2>
<ul>
<li>
<p>基于轨迹的方法的基本假设</p>
<p>相对于当前时刻，前 $hw$ (history window)内用户的 FoV 位置对未来可预测的 $pw$ (predict window)内用户的 FoV 位置有影响，比如用户只有很小可能性会在很短的一段单位时间内做 180 度的转弯，而更小角度的调整则更可能发生。</p>
</li>
<li>
<p>基于内容的方法的基本假设</p>
<p>用户的FoV变化是因为对视频内容感兴趣，即ROI与FoV之间有相关关系，比如在观看篮球比赛这样的全景视频时，用户的FoV更可能专注于篮球。</p>
<p>按照提取ROI的来源不同可以分为两种类型：</p>
<ol>
<li>从视频内容本身出发，使用CV方法去猜测ROI；</li>
<li>从用户观看视频的热图出发，相当于得到了经过统计之后的平均FoV分布，以此推测其他用户的ROI；</li>
</ol>
</li>
</ul>
<p>基于轨迹的方式是要在最表层的历史和预测的轨迹之间学习，即假设两者之间只有时空关系。</p>
<p>跨用户的方式则假设由用户群体所得出的热图可以用来预测单个用户的FoV，即利用共性来推断个性。</p>
<p>基于内容的方式直接提取视频显著图来推断FoV，即进一步假设共性与视频内容本身有关系。</p>
<h2 id="跨用户预测的概念">跨用户预测的概念</h2>
<ul>
<li>
<p>基本假设</p>
<p>就单个用户而言，在观看视频过程中其FoV的变化看似随机，但是其行为可能从用户群体的角度去看是跨用户相通的，即多个用户在观看视频时可能会表现出相似的，可以学习的行为模式，这种行为模式可以帮助提高VP的精度。</p>
</li>
<li>
<p>实际应用</p>
<p>基于轨迹的跨用户：如果训练的模型是基于轨迹的离线模型如LSTM，那么实际上训练好的模型已经学习到了这种跨用户的行为模式；而如果采用的是边训练边预测的模型如LR（输入历史窗口的经纬度数据，输出预测窗口的经纬度数据），那么这样的模型就是纯粹的单用户模型。</p>
<p>基于内容的跨用户：将用户在观看视频帧时的注意点作为研究对象，找到用户群体在面对同一帧视频时共同关注的空间区域，而这就是用户间相似的行为模式。这种与内容相结合的跨用户方式即为实际研究中所指的跨用户的研究方式。（实际上就是基于内容的研究方法，只不过出发点不是视频本身，而是用户在观看视频时的FoV）</p>
</li>
</ul>
<h2 id="实际应用">实际应用</h2>
<p></p>
<ul>
<li>
<p>图中3个黄色矩形表示3种方法：</p>
<ol>
<li>
<p>ROI extract：基于内容的预测</p>
</li>
<li>
<p>Multiple watchers' FoV：跨用户的预测</p>
</li>
<li>
<p>Multiple watchers' trajectories：基于轨迹的预测</p>
</li>
</ol>
</li>
<li>
<p>绿色渐变矩形表示直接使用用户当前的历史轨迹数据去训练模型，接着做出预测。</p>
</li>
</ul>
<h2 id="研究方法">研究方法</h2>
<ul>
<li>
<p>基于轨迹的方法</p>
<p>在线训练：输入历史窗口的位置信息，不断迭代修正模型，输出预测窗口的位置信息。</p>
<p>离线训练：输入任何采样条件下的多对hw和pw信息来拟合模型。</p>
</li>
<li>
<p>跨用户的方法</p>
<p>求出多个用户在同一帧上的热图，以此作为FoV预测的依据。</p>
</li>
<li>
<p>基于内容的方法</p>
<p>提取视频帧中的显著图，以此作为FoV预测的依据。</p>
</li>
</ul>
<h2 id="优点">优点</h2>
<ol>
<li>使用回归实现的在线训练模型实现简单，反应迅速，有优秀的短期预测精度。</li>
<li>跨用户的热图可以帮助长期的预测，可以提供合理的离线全视频FOV预测，并具有一致的性能。（因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入）</li>
<li>显著图对于ROI集中突出的预测效果较好。</li>
</ol>
<h2 id="缺点">缺点</h2>
<ol>
<li>使用回归实现的在线训练模型在预测窗口增大时，性能会显著下降。</li>
<li>提取显著图的方式一方面训练开销比较大，另一方面对于ROI不够集中突出的视频效果并不好。</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for Content Assisted Prediction</title>
    <link>https://ayamir.github.io/posts/note-for-content-assisted-prediction/</link>
    <pubDate>Thu, 06 Jan 2022 15:17:33 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-content-assisted-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://www.researchgate.net/publication/333971523_Content_Assisted_Viewport_Prediction_for_Panoramic_Video_Streaming" target="_blank" rel="noopener noreffer">Content Assisted Viewport Prediction for Panoramic Video Streaming</a></p>
<p>Level：IEEE CVPR 2019 CV4ARVR</p>
<p>Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion</p>
<h2 id="主要工作">主要工作</h2>
<h3 id="基于轨迹预测">基于轨迹预测</h3>
<p>输入：历史窗口轨迹</p>
<p>模型：64个神经元的单层LSTM，在输入层后面加上一个额外的减法层进行点归一化，以及一个加法层来恢复输出之前的值；用ADAM进行优化，MAE作为损失函数。</p>
<h3 id="跨用户热图">跨用户热图</h3>
<p>除了观看者自己的历史FOV轨迹之外，其他观看者对同一视频帧的观看方向也有启发性。</p>
<p></p>
<p>对视频的每一帧，首先收集用户的观看方向（坐标使用原始的来自三个方向的欧拉角表示，而非经纬度）。</p>
<p>接着将坐标投影到用经纬度表示的180x360像素的平面图上，对于图中的每个像素点，可以数出其被看到的次数；并对周围像素应用二维高斯光滑。</p>
<p>上面的过程可以为视频生成热图：</p>
<p></p>
<h3 id="视频帧的显著图">视频帧的显著图</h3>
<p>鉴于观看相同的全景视频时跨用户行为的共性，进一步假设是内容促使多个观众观看公共区域，因此提取出每个帧的显著图可能会表明用户的RoI。</p>
<p>对特定的视频帧，应用经典的特征密集型方法——Ittykoch，它首先根据强度、边缘、颜色和方向将图像分解为多个特征通道，然后将它们组合成识别显著区域。</p>
<p>除了在静态视频帧上检测显著性之外，进一步进行背景减法来减少不太可能感兴趣的区域：应用基于高斯混合的背景/前景分割算法，高级思想是在连续帧之间临时过滤变化的像素点。</p>
<p>结合上面这两个过程可以为视频帧提取时间显著图。</p>
<p></p>
<h3 id="多模态融合">多模态融合</h3>
<p></p>
<p>使用包含3个LSTM分支的深度学习模型来融合上述的几种预测方式的结果。</p>
<p>基于轨迹的LSTM（图中绿色分支）从历史窗口 $hw$ 中接受 $n$ 个坐标的输入，接着预测未来窗口 $pw$ 中的 $m$ 个坐标，用 $trj_y_{i}$ 表示；</p>
<p>基于热图的LSTM（图中蓝色分支）将每个预测步骤对应的视频帧的热图作为输入，并在 $pw$ 中输出第2组 $m$ 个坐标的预测，用 $ht_y_{i}$ 表示：</p>
<p>对于每个热图，让其通过3个卷积层，每个卷积层后面都有一个最大池化层。然后，在此图像特征提取之后，应用展平步骤和1个密集层来回归坐标（经纬度表示）。</p>
<p>基于显著图的LSTM采用与热图相似的架构，将显著图作为输入，在 $pw$ 中输出第3组 $m$ 个坐标的预测，用 $sal_y_{i}$ 表示。</p>
<p>对热图和显著图的分支，应用 <code>TimeDistributed</code>层，以便其参数在预测步骤中保持一致。</p>
<p>最终在每个预测步骤连接 $trj_y$ ， $ht_y$，和 $sal_y$ ，并产生一个最终输出 $y$ 。</p>
<p>每个模型的损失函数采用MAE，优化函数采用ADAM。</p>
<p>为每个分支的输出以及最终的输出都检查损失，单独和联合地去调整其参数。</p>
<h2 id="评估">评估</h2>
<p>使用2折的交叉验证。</p>
<h3 id="超参数">超参数</h3>
<ol>
<li>$pw$ 的大小：0.1s，1.0s，2.0s；</li>
<li>$hw$ 的大小：0.05s，0.6s，1.0s；（分别与上面的 $pw$ 对应）</li>
<li>用于训练的用户数：[3, 10, 30]</li>
</ol>
<p></p>
<h3 id="结果与分析">结果与分析</h3>
<ol>
<li>所有模型的预测精度随着 $pw$ 的增长而下降，表明长期预测问题更难解决；</li>
<li>所有模型的精度预测误差几乎是纬度预测误差的二倍，可能由于运动区域在水平方向的翻倍；</li>
<li>线性回归模型只有在 $pw$ 很短的时候预测精确，随着 $pw$ 的增长，其预测精度会迅速下降；</li>
<li>基于 LSTM 的轨迹模型始终优于所有 $pw$ 的基线模型，但更多的训练观众无助于显着提高准确性。</li>
<li>跨用户的热图和显著图可以帮助长期的预测，可以提供合理的离线全视频FOV预测，并具有一致的性能（因为独立于 $pw$ ，并且不需要历史窗口 $hw$ 的轨迹输入），当 $pw$ 增长时，其预测精度超过了基于历史轨迹的模型；</li>
<li>结合3种模型之后，可以平衡来自历史轨迹、跨用户兴趣和内容显著性的输入，不论 $pw$ 长或短都能产生优化的预测结果；</li>
</ol>
<h3 id="例外情况">例外情况</h3>
<p></p>
<p>M3在经度上的表现并不适用于上面图中标示的两个视频（Mega.Coaster和GTR.Drives.First.Ever）</p>
<p>原因分析：</p>
<p>这两个视频的共同特点是在驾驶路径的一侧具有高运动内容的驾驶内容，因此用户在观看这些视频时，大多数FOV始终以行驶轨迹为中心。因此用户不太可能改变其观看方向，这导致即使 $pw = 2.0s$ 时，单一基于轨迹的模型的预测精度也更高。相比之下，从对内容角度出发的分析无济于事，但可能会引入观众可能会忽略的变道，进而造成预测误差。</p>
]]></description>
</item>
<item>
    <title>Note for GPAC</title>
    <link>https://ayamir.github.io/posts/note-for-gpac/</link>
    <pubDate>Thu, 30 Dec 2021 10:23:26 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-gpac/</guid>
    <description><![CDATA[<h2 id="dash客户端自适应逻辑">Dash客户端自适应逻辑</h2>
<ol>
<li><em>tile priority setup</em>：根据定义的规则对tile进行优先级排名。</li>
<li><em>rate allocation</em>：收集网络吞吐量信息和tile码率信息，使用确定的tile优先级排名为其分配码率，努力最大化视频质量。</li>
<li><em>rate adaption</em>：在播放过程中，执行码率自适应算法，基于播放速度、质量切换的次数、缓冲区占用情况等。</li>
</ol>
<h3 id="tile-priority-setup">tile priority setup</h3>
<ol>
<li>
<p>Dash客户端加载带有SRD信息的MPD文件时，首先确定使用SRD描述的tile集合。</p>
</li>
<li>
<p>确定tile之间的编码依赖（尤其是使用HEVC编码的tile时）</p>
</li>
<li>
<p>为每个独立的tile向媒体渲染器请求一个视频对象，并向其通知tile的SRD信息。</p>
</li>
<li>
<p>渲染器根据需要的显示大小调整SRD信息之后，执行视频对象的最终布局。</p>
</li>
<li>
<p>一旦tile集合被确定，客户端向每个tile分配优先级。（每次码率自适应执行的时候都需要分配tile优先级）</p>
<p></p>
</li>
</ol>
<h3 id="rate-allocation">Rate allocation</h3>
<ol>
<li>首先需要估计可用带宽（tile场景和非tile场景的估计不同）</li>
<li>在一个视频段播放过程中，客户端需要去下载多个段（并行-HTTP/2）</li>
<li>带宽可以在下载单个段或多个段的平均指标中估计出来。</li>
<li>一旦带宽估计完成，码率分配将tile根据其优先级进行分类。</li>
<li>一开始所有的tile都分配成最低的优先级对应的码率，然后从高到低依次增长优先级高的tile的码率。</li>
<li>一旦每个tile的码率分配完成，将为目标带宽等于所选比特率的每个tile调用常规速率自适应算法</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for MPC</title>
    <link>https://ayamir.github.io/posts/note-for-mpc/</link>
    <pubDate>Thu, 23 Dec 2021 10:39:32 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-mpc/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/2785956.2787486" target="_blank" rel="noopener noreffer">A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP</a></p>
<p>Level：ACM SIGCOMM 15</p>
<p>Keywords：Model Predictive Control，ABR，DASH</p>
<h2 id="motivation">Motivation</h2>
<p>关于码率自适应的逻辑，现有的解决方案还没有形成清晰的、一致的意见。不同类型的方案之间优化的出发点并不相同，比如基于速率和基于缓冲区，而且没有广泛考虑各方面的因素并形成折中。</p>
<p>文章引入了控制论中的方法，将各方面的影响因素形式化为<em>随机优化控制</em>问题，利用<strong>模型预测控制MPC</strong>将两种不同出发点的解决方案结合到一起，进而解决其最优化的问题。而仿真结果也证明，如果能运行一个最优化的MPC算法，并且预测误差很低，那么MPC方案可以优于传统的基于速率和基于缓冲区的策略。</p>
<h2 id="背景">背景</h2>
<ul>
<li>播放器端为QoE需要考虑的问题：
<ol>
<li>最小化冲缓冲事件发生的次数；</li>
<li>在吞吐量限制下尽可能传输码率较高的视频；</li>
<li>最小化播放器开始播放花费的时间（启动时间）；</li>
<li>保持播放过程平滑，尽可能避免大幅度的码率变化；</li>
</ol>
</li>
<li>这些目标相互冲突的原因：
<ol>
<li>最小化重缓冲次数和启动时间会导致只选择最低码率的视频；</li>
<li>尽可能选择高码率的视频会导致很多的重缓冲事件；</li>
<li>保持播放过程平滑可能会与最小的重缓冲次数与最大化的平均码率相冲突；</li>
</ol>
</li>
</ul>
<h2 id="控制论模型">控制论模型</h2>
<h3 id="视频推流模型">视频推流模型</h3>
<ol>
<li>
<p>参数形式化</p>
<ul>
<li>
<p>将视频建模成连续片段的集合，即：$V = \lbrace 1, 2, &hellip;, K \rbrace$，每个片段长为$L$秒；</p>
</li>
<li>
<p>每个片段以不同码率编码，$R$ 作为所有可用码率的集合；</p>
</li>
<li>
<p>播放器可以选择以码率$R_k \in R$ 下载第$k$块片段，$d_k(R_k)$ 表示以码率$R_k$编码的视频大小；</p>
<ul>
<li>对于恒定码率CBR的情况，$d_k(R_k) = L \times R_k$；</li>
<li>对于变化码率VBR的情况，$d_k \sim R_k$；</li>
</ul>
</li>
<li>
<p>选择的码率越高，用户感知到的质量越高：</p>
<p>$q(\cdot):R \rightarrow \R_+$ 是一个不减函数，是选择的码率 $R_k$ 到用户感知到的视频质量 $q(R_k)$ 的映射；</p>
</li>
<li>
<p>片段被下载到<em>回访缓冲</em>中，其中包含下载了的但还没看过的片段。</p>
</li>
<li>
<p>$B(t) \in [0, B_{max}]$ 表示 $t$ 时刻缓冲区的占用， $B_{max}$ 表示内容提供商的策略和播放器的存储限制；</p>
</li>
</ul>
</li>
<li>
<p>播放过程形式化</p>
<p>在 $t_k$ 时刻，视频播放器开始下载第 $k$ 个块，这个块的下载时间可以计算为： $d_k(R_k) / C_k$； $C_k$ 表示下载过程中经历的平均下载速度；</p>
<p>一旦第 $k$ 个块下载完毕，播放器等待 $\Delta t_k$ 时间并在 $t_{k+1}$ 时刻下载下一个块 $k+1$ ；</p>
<p>假设等待时间 $\Delta t_k$ 很短并且不会导致重缓冲事件，用 $C_t$ 表示 $t$ 时刻的网络吞吐量：
$$
t_{k+1} = t_k + \frac{d_k(R_k)}{C_k} + \Delta t_k
$$</p>
<p>$$
C_k = \frac{1}{t_{k+1} - t_k - \Delta t_k} \int_{t_k}^{t_{k+1} - \Delta t_k} C_t dt
$$</p>
<p>$B(t)$ 的变化取决于下载的块和播放的块的数量：</p>
<p>在第 $k$ 个块下载完毕之后缓冲区占用增长 $L$ 秒；用户观看一个块之后缓冲区占用减少 $L$ 秒；</p>
<p>$B_k = B(t_k)$ 表示播放器开始下载第 $k$ 个块时的缓冲区占用；</p>
<p>缓冲区占用的动态变化可以表示为：
$$
B_{k+1} = \big( (B_k - \frac{d_k(R_k)}{C_k})_+ + L - \Delta t_k \big)_+
$$
其中 $(x)_+ = max\lbrace x, 0 \rbrace $ 确保其非负；</p>
<p>如果 $B_k &lt; d_k(R_k) / C_k$ ，表示缓冲区在播放器还在下载第 $k$ 个块时变空，而这会导致重缓冲事件；</p>
<p></p>
<p>等待时间 $\Delta t_k$ 的确定也称为<em>块调度</em>问题，本文中假设播放器在第 $k$ 个块下载完毕之后尽可能快地去下载第 $k+1$ 个块（除了缓冲区满了的情况，播放器等待缓冲区中的块被消耗之后再下载新的块）：
$$
\Delta t_k = \Big( \big( B_k - \frac{d_k(R_k)}{C_k} \big)_+ + L - B_max \Big)_+
$$</p>
</li>
</ol>
<h3 id="qoe最大化问题">QoE最大化问题</h3>
<p>QoE的组成部分：</p>
<ol>
<li>
<p>平均视频质量：在所有块中每个块平均的质量，计算为：
$$
\frac{1}{K} \sum^K_{k=1} q(B_k)
$$</p>
</li>
<li>
<p>平均质量变化：相邻块之间质量变化的平均值，计算为：
$$
\frac{1}{K-1} \sum^{K-1}_{k=1} | q(R_{k+1}) - q(R_k) |
$$</p>
</li>
<li>
<p>重缓冲总计时间：对每个块而言，当轮到其被消耗时但下载块的过程还没完成即出现了重缓冲，总时间计算为：
$$
\sum^K_{k=1} (\frac{d_k(R_k)}{C_k} - B_k)_+
$$</p>
</li>
<li>
<p>启动延迟 $T_s$ ，假设 $T_s \ll B_{max}$ 。</p>
</li>
</ol>
<p>对不同用户而言，上述4种因素的重要程度不同。使用上述分量的加权，定义视频块 $1$ 到 $K$ 的QoE：
$$
QoE^K_1 = \sum^K_{k=1} q(R_k) - \lambda \sum^K_{k=1} | q(R_{k+1}) - q(R_k) | - \mu \sum^K_{k=1} (\frac{d_k(R_k)}{C_k} - B_k)_+ - \mu_s T_s,\
\lambda, \mu, \mu_s \nless 0
$$
相对较小的 $\lambda$ 表示用户不太关心视频质量变化； $\lambda$ 越大表明越需要使视频质量变得光滑。</p>
<p>相对较大的 $\mu$ 表示用户很在意重缓冲；</p>
<p>在这里文章倾向于启动延迟很低，所以采用大 $\mu_s$ ；</p>
<p>QoE的最大化：</p>
<p>输入：吞吐量迹 ${C_t, t \in [t_1, t_{K+1}]}$</p>
<p>输出：码率选择 $R_1, &hellip;, R_K$；启动时间 $T_s$ ；</p>
<p>需要注意：当最大化的决策发生在播放过程中时，启动时间便不再存在；</p>
<p></p>
<h3 id="算法">算法</h3>
<p>上图中的QoE最大化问题是一种随机优化控制问题，随机性源自可获得的吞吐量 $C_t$ 。</p>
<p>$t_k$ 时刻播放器选择码率 $R_k$ ，只有过去的吞吐量 $\lbrace C_t, t \le t_k \rbrace$ 可知，未来的值 ${C_t, t &gt; t_k}$ 未知。</p>
<p>但是，<em>吞吐量预测器</em>可以用于获取对吞吐量的预测，定义其为 $\lbrace \hat{C_t}, t &gt; t_k \rbrace$ 。</p>
<p>基于这样的预测和缓冲区的信息（精确可知），<em>码率选择器</em>对下个块 $k$ 的码率选择可以表示为：
$$
R_k = f \big( B_k, \lbrace \hat{C_t}, t &gt; t_k \rbrace, \lbrace R_i, i &lt; k \rbrace \big)
$$
文章只关注码率自适应算法，假设已经得到了预测值，并根据预期预测误差对其进行了表征，即：</p>
<p>我们着重于 $f(\cdot)$ 的设计以及预测误差对比较控制算法性能的影响。</p>
<p>现有的两类自适应算法：基于速率和基于缓冲区，分别可以表示为：
$$
R_k = f \big( \lbrace \hat{C_t}, t &gt; t_k \rbrace, \lbrace R_i, i &lt; k \rbrace \big)
$$</p>
<p>$$
R_k = f(B_k, \lbrace R_i, i &lt; k \rbrace)
$$</p>
<p>前者只基于吞吐量的预测结果而不管缓冲区状况；后者只基于缓冲区而不管未来的吞吐量可能状况；</p>
<p>这两种方法在原则上都只是次优的，理想情况下我们想要同时考虑缓冲区占用和吞吐量预测结果。</p>
<p></p>
<h2 id="mpc-for-optimal-bitrate-adaptation">MPC for Optimal Bitrate Adaptation</h2>
<h3 id="why-mpc">Why MPC</h3>
<p>MPC天然适合码率自适应问题。</p>
<ul>
<li>
<p><strong>Strawman solutions</strong></p>
<p>码率自适应问题本质是<em>随机控制优化</em>问题，就这一点而言，有两个知名控制算法：</p>
<ol>
<li>Proportional-integral-derivation(PID) control.</li>
<li>Markov Decision Process(MDP) based control.</li>
</ol>
<p>PID相较MDP而言计算起来更加简单，只能用于使系统稳定，不能显式地优化QoE目标；此外PID被设计用于有连续的时间和连续的状态空间的问题中，用于当前这种高度离散化的问题中会导致性能亏损和不稳定。</p>
<p>应用MDP的话可以将吞吐量和缓冲区状态形式化为马氏过程，然后使用诸如值迭代和策略迭代等标准算法求出最优解。</p>
<p>（然而，这有一个很强的假设，即吞吐量动态遵循马尔可夫过程，不清楚这在实践中是否成立。我们将MDP的潜在用途和吞吐量动态分析作为未来的工作。）</p>
</li>
<li>
<p><strong>Case for MPC</strong></p>
<p>理想情况下，如果给出未来吞吐量的完美数据，那么启动时间 $T_s$ 和最优码率选择 $R_1, &hellip; R_K$ 可以一下子就计算出来；</p>
<p>实际情况中，虽然不能得到未来吞吐量的完美预测，但是我们可以假设吞吐量在较短的时间段 $[t_k, t_{k+N}]$ 内不会剧烈变化。</p>
<p>基于此，可以使用当前视界中的预测来应用第1个码率 $R_k$ ，之后将视界向前移动到 $[t_{k+1}, t_{k+N+1}]$ 。</p>
<p>而这种方案就称为MPC。MPC的一般好处在于，MPC可以利用预测在约束条件下在线优化动态系统中的复杂控制目标。</p>
</li>
</ul>
]]></description>
</item>
<item>
    <title>Note for TBRA</title>
    <link>https://ayamir.github.io/posts/note-for-tbra/</link>
    <pubDate>Tue, 21 Dec 2021 10:11:23 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-tbra/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/10.1145/3474085.3475590" target="_blank" rel="noopener noreffer">TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming</a></p>
<p>Level：ACM MM 21</p>
<p>Keywords：Adaptive tiling and bitrate，Mobile streaming</p>
<h2 id="创新点">创新点</h2>
<h3 id="背景">背景</h3>
<p>现有的固定的tile划分方式严重依赖viewport预测的精度，然而viewport预测的准确率往往变化极大，这导致基于tile的策略实际效果并不一定能实现其设计初衷：保证QoE的同时减少带宽浪费。</p>
<p>考虑同样的viewport预测结果与不同的tile划分方式组合的结果：</p>
<p></p>
<p>从上图可以看到：</p>
<ul>
<li>如果采用$6 \times 6$的分块方式，就会浪费26，32两个tile的带宽，同时15，16，17作为本应在实际viewport中的tile并没有分配最高的优先级去请求。</li>
<li>如果采用$5 \times 5$的分块方式，即使预测的结果与实际的viewport有所出入，但是得益于tile分块较大，所有应该被请求的tile都得到了最高的优先级，用户的QoE得到了保证。</li>
</ul>
<p>另一方面，基于tile的方式带来了额外的编解码开销（可以看这一篇论文：<a href="https://ayamir.github.io/2021/12/note-for-optile/" target="_blank" rel="noopener noreffer">note-for-optile</a>），而这样的性能需求对于移动设备而言是不可忽略的。</p>
<h3 id="创新">创新</h3>
<p>除了考虑常见的因素如带宽波动和缓冲区占用之外，提出同时自适应分块策略和码率分配以应对变化的viewport预测性能和受限的移动设备的解码能力。</p>
<h3 id="论文组织">论文组织</h3>
<ol>
<li>首先使用现实世界的轨迹分析了典型的viewport预测算法并确定了其性能的不确定性。</li>
<li>接着讨论了不同的分块策略在tile选择和解码效率上的影响。</li>
<li>自适应的分块策略可以适应viewport预测的错误，并能保证tile选择的质量。</li>
<li>为解码时间建构了分析模型，可以在给定受限的计算资源时用于选择恰当的分块策略和码率。</li>
<li>形式化了优化模型，讨论了自适应算法的细节。</li>
<li>评估证明了方案的优越性。</li>
</ol>
<h2 id="motivation">Motivation</h2>
<h3 id="分块策略对tile选择的影响">分块策略对tile选择的影响</h3>
<p>实现4种轻量的viewport预测算法：线性回归LR、岭回归RR、支持向量回归、长短期记忆LSTM。</p>
<p>设置历史窗口大小为2s，预测窗口大小为1s；viewport的宽度和高度分别为100°和90°。</p>
<p>默认的分块策略为$6 \times 6$；头部移动数据集来自<a href="https://dl.acm.org/doi/10.1145/3204949.3208139" target="_blank" rel="noopener noreffer">公开数据集</a>。</p>
<h4 id="viewport预测的不准确性">viewport预测的不准确性</h4>
<p>研究表明，用户的头部运动主要发生在水平方向而较少发生在垂直方向，所以只分析水平方向的预测。</p>
<p>实际的商业移动终端只有有限的传感和处理能力，并不能支持高频的viewport预测采样。</p>
<p>视频内容的不同类型会显著影响预测的精度，基于录像环境（室内或户外）和相机的运动状态分类。</p>
<ul>
<li>
<p>改变采样频率会直接影响viewport预测的精度，频率越低，精度越低。</p>
</li>
<li>
<p>相机运动的viewport预测错误率比相机静止的明显更高。</p>
</li>
</ul>
<h4 id="通过分块容忍预测错误">通过分块容忍预测错误</h4>
<p>因为不管tile的哪个部分被包含在预测的viewport中，只要包含一部分就会请求整个tile，所以增大每个tile的尺寸能吸收预测错误。</p>
<p>实验验证：</p>
<p>设定从$4 \times 4$到$10 \times 10$的分块方式，使用不同的预测误差来检查分块设定可以容纳的最大预测误差，同时保持tile选择结果的相同质量。</p>
<p>用$F_1$分数来表示tile选择的质量：$F_1 = \frac{2 \cdot precision \cdot recall}{precision + recall}$。</p>
<p>实验结果表明更大的tile尺寸更能容忍预测错误。</p>
<h3 id="分块策略对解码复杂性的影响">分块策略对解码复杂性的影响</h3>
<p>虽然当前的移动设备硬件性能发展迅速，但是实时的高码率高分辨率全景视频的解码任务还是充满挑战。</p>
<p>分块对于编码的影响：</p>
<ul>
<li>tile越小，帧内和帧间内容的相关区域就越小，编码效率越低。</li>
</ul>
<p>直接影响解码复杂性的因素：</p>
<ul>
<li>tile的数量。</li>
<li>视频的分辨率。</li>
<li>用于解码的资源。</li>
</ul>
<p>固定其中1个因素改变另外2个因素来检查其对解码的影响：</p>
<p></p>
<p>根据对图的观察可以得出这3个因素在经验上是相互独立的，因为这三幅图之中的图像几乎相同。</p>
<p>分别用$F_n(x), F_r(x), F_c(x)$表示tile数量、分辨率、线程数量为$x$时，解码时间与基线时间的比值。</p>
<p>将这3个比值作为3个乘子建立分析模型：
$$
D = D_0 \cdot F_n(x_1) \cdot F_r(x_2) \cdot F_c(x_3)
$$
上式表示计算整体的解码时间，其中tile数量为$x_1$、分辨率为$x_2$、线程数量为$x_3$；$D_0$时解码的基线时间。</p>
<p>这个模型将用于帮助做出分块和码率适应的决策。</p>
<p>注意在实际情况中，可供使用的计算资源（线程数）是受限的，需要根据设备当前可用的计算资源来分配。</p>
<h2 id="tbra的设计">TBRA的设计</h2>
<ul>
<li>$S = \lbrace s_1, s_2, &hellip; \rbrace$ 表示360°视频分块方式的集合；</li>
<li>对于分块方式$s_i$，$|s_i|$ 表示这种方案中tile的数量；</li>
<li>当 $i &lt; j$ 时，假设 $|s_i| &lt; |s_j|$；</li>
<li>对于分块方式$s$， $b_{i, j}$ 表示第 $i$ 块的tile $j$，$i \le 块的数量, j \le |s|$；</li>
<li>目标是确定分块方式$s$，并为每个tile确定其码率$b_{i, j}$；</li>
</ul>
<h3 id="分块自适应">分块自适应</h3>
<h4 id="自适应的概念">自适应的概念</h4>
<p>分块尺寸大小会导致viewport容错率和传输效率的变化。</p>
<ul>
<li>分块尺寸小，极端情况下每个像素点作为一个tile，viewport容错率最小，但是传输效率达到100%；</li>
<li>分块尺寸大，极端情况下整个视频帧作为一个tile，viewport容错率最大，但是传输效率最小；</li>
</ul>
<p>优化的目标就是在这两种极端条件中找到折中的最优解。</p>
<h4 id="分块选择">分块选择</h4>
<p>以$\overline{r_d}, d \in \lbrace left, right, up, down \rbrace$为半径扩大预测区域；$e_d$表示过去n秒中方向 $d$ 的预测错误平均值；
$$
\overline{r_d} = (1-\alpha) \cdot \overline{r_d} + \alpha \cdot e_d
$$
预测区域的扩展被进一步用于tile选择，受过去预测精度的动态影响。</p>
<p>下一步检查不同分块方式，进而找到QoE和传输效率之间的折中。</p>
<p>对于每个分块方式，比较基于扩展的预测区域的tile选择的质量。使用2个比值作为QoE和传输效率的度量：
$$
Miss\ Ratio = \frac{of\ missed\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p>$$
Waste\ ratio = \frac{of\ unnecessary\ pixels\ in\ expanded\ prediction}{of\ viewed\ pixels}
$$</p>
<p></p>
<p>这2个比值的tradeoff可以在上图中清晰地看出。</p>
<p>使用分块方式对应的惩罚$Tiling\ i_{penalty}$来评估其性能：
$$
Tiling\ i_{penalty} = \beta \cdot Miss\ Ratio + |1/cos(\phi_i)| \cdot Waste\ Ratio
$$
$\phi_i$ 是viewport $i$ 的中心纬度坐标，它表明随着viewport的垂直移动，浪费率的权重会发生变化。（因为投影方式是ERP）</p>
<p>检查完所有的方式之后，最终选择惩罚最小的分块方式。</p>
<h3 id="码率自适应">码率自适应</h3>
<h4 id="视频质量">视频质量</h4>
<p>$w_{i, j}$表示在第 $i$ 个视频块播放时，tile $j$ 的权重；在当前方案中 $w_{i, j} = 0\ or\ 1$ 取决于tile是否在预测的viewport中。</p>
<p>$q(b_{i, j})$ 是tile比特率选择 $b_{i, j}$ 与用户实际感知到的质量之间的非递减映射函数。</p>
<p>第 $i$ 个视频块的质量等级可以定义为：</p>
<p>$$
Q^{(1)}_i = \sum^n_{j=1} w_{i, j} q(b_{i, j})
$$</p>
<p>使用最新研究的<a href="https://ieeexplore.ieee.org/document/8979422/citations?tabFilter=papers" target="_blank" rel="noopener noreffer">主观视频质量模型</a>：
$$
subjective\ PSNR:\ q_i = PSNR_i \cdot [M(v_i)]^{\gamma} [R(v_i)]^{\delta}
$$
$M(v_i)$ 是检测阈值；$R(v_i)$ 是视网膜滑移率；$v_i$ 是第播放 $i$ 个视频块时viewport的移动速度；$\gamma = 0.172, \delta = -0.267$</p>
<h4 id="质量变化">质量变化</h4>
<p>连续视频块之间的强烈质量变化会损害QoE，定义质量变化作为响铃两个视频块之间质量的变化：
$$
Q^{(2)}_i = |Q^{(1)}_1 - Q^{(1)}_{i-1}|,\ i \in [2, m]
$$</p>
<h4 id="重缓冲时间">重缓冲时间</h4>
<p>参数设置：</p>
<ul>
<li>$C_i$ 表示下载视频块 $i$ 的预计吞吐量；</li>
<li>$B_i$ 表示客户端开始下载视频块 $i$ 时缓冲区的占用率；</li>
<li>$B_{default}$ 表示在启动阶段默认的缓冲区填充等级，记 $B_{default} = B_1$；</li>
<li>下载第 $i$ 个视频块需要时间 $\sum^n_{j=1} b_{i, j} / C_i$ ；</li>
<li>每个视频块的长度为 $L$ ；</li>
</ul>
<p>缓冲区的状态应该在每次视频块被下载的时候都得到更新，则下一个视频块 $i+1$ 的缓冲区占用情况可以计算为：
$$
B_{i+1} = max\lbrace B_1 - \sum^n_{j=1} b_{i, j} / C_i,\ 0\rbrace + L
$$
下载第 $i$ 个视频块时的重缓冲时间可以计算为：</p>
<p>$$
Q^{(3)}_i = max \lbrace \sum^n_{j=1} b_{i, j} / C_i - B_i,\ 0 \rbrace + t_{miss}
$$</p>
<p>第一部分是下载时间过长且缓冲区耗尽，视频无法播放情况下的重新缓冲时间；</p>
<p>第二部分 $t_{miss}$ 表示下载缺失的tile所花费的时间（在视频块播放过程中被看到但是之前没有分配码率的tile）。</p>
<h4 id="优化目标">优化目标</h4>
<p>第 $i$ 个视频块的整体优化目标可以定义为前述3个指标的加权和：
$$
Q_i = pQ^{(1)}_i - qQ^{(2)}_i - rQ^{(3)}_i
$$
各个系数的符号分配表示：最大化视频质量、最小化块间质量变化、最小化重缓冲时间。</p>
<p>传统意义上使用所有视频块的平均QoE作为优化对象，但实际上很难获得从块 $1$ 到块 $m$ 的整个视界的完美的未来信息。</p>
<p>为了处理预测长期吞吐量和用户行为的难度，采用<a href="https://dl.acm.org/doi/10.1145/2785956.2787486" target="_blank" rel="noopener noreffer">基于MPC的框架</a>，在有限的范围内优化多个视频块的QoE，最终的目标函数可以形式化为：
$$
\underset{b_{i, j}, i \in [t, t+k-1], j \in [1, n]}{max} \sum^{t+k-1}_{i=t} Q_i
$$
因为短期内的viewport预测性能和网络状况可以很容易得到，QoE优化可以通过使用窗口 $[t, t+k-1]$ 内的预测信息；</p>
<p>接着将视界向前移动到 $[t+1, t+k]$ ，更新新的优化窗口的信息，为下一个视频块执行QoE优化，直到最后一个窗口。</p>
<p>使用基于MPC的公式的优点：由于受限的问题规模，每个优化问题的实例都是实际可解的。</p>
<h4 id="高效求解">高效求解</h4>
<p>提出的公式天然适合在线求解，得益于短窗口的实例问题规模很小，QoE优化可以通过详尽搜索定期解决。</p>
<p>但是因为优化过程需要高频调用，所以对于大的搜索空间还是充满挑战。</p>
<p>为了支持实时优化，需要对搜索空间进行高效剪枝，确定几点约束：</p>
<ul>
<li>
<p>解码时间需要被约束；</p>
<p>解码时间应该短于回放长度。</p>
<p>给定移动设备上可用的计算资源，可以得到支持的最大解码线程数。</p>
<p>基于解码时间的分析模型，由于解码复杂度和分辨率的单调性，可以找到设备能够限定时间内解码的最大质量水平，这会将码率选择限制在有界搜索空间内。</p>
</li>
<li>
<p>码率选择应该考虑吞吐量的限制：$\sum^n_{j=1} b_{i, j} \le LC_i$ ；</p>
<p>不会主动耗尽缓冲区，无需让其处理吞吐量的波动。</p>
</li>
<li>
<p>码率选择应该考虑tile的分类；</p>
<p>tile的码率不应该低于同一个视频块中更低权重tile的码率： $b_{i, j} \ge b_{i, j'}, \forall w_{i, j} &gt; w_{i, j'}$ 。</p>
</li>
<li>
<p>属于相同类别的tile比特率选择应该是同一个等级；</p>
<p>这使码率自适应在tile类的级别上执行而非单个tile的级别，大大减小了搜索空间的规模。</p>
</li>
<li>
<p>当优化窗口中的吞吐量和用户行为保持稳定时，同一个窗口中的tile应该有相同的结果。</p>
</li>
</ul>
<h3 id="tbra-workflow">TBRA workflow</h3>
<p></p>
<p>这样的方式需要在服务端存储大量的按照不同分块方式划分的不同码率版本的视频块，这一点可以进一步研究。</p>
<p>但是对于移动终端设备而言，这样的解决方案只引入了可以忽略不计的开销。</p>
<p>观察到tile自适应问题具有全局最优通常就是局部最优的特点，因此可以大大减少计算量。</p>
<p>基于MPC的优化workflow还可以有效地解决码率自适应问题。</p>
]]></description>
</item>
<item>
    <title>Note for Content Motion Viewport Prediction</title>
    <link>https://ayamir.github.io/posts/note-for-content-motion-viewport-prediction/</link>
    <pubDate>Mon, 20 Dec 2021 10:47:18 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-content-motion-viewport-prediction/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://dl.acm.org/doi/abs/10.1145/3328914" target="_blank" rel="noopener noreffer">Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking</a></p>
<p>Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019</p>
<p>Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model</p>
<h2 id="workflow">Workflow</h2>
<ul>
<li>Tracking：VR motion追踪算法：应用了高斯混合模型来检测物体的运动。</li>
<li>Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户viewport来自动更正潜在的预测错误。</li>
<li>Update：viewport动态更新算法：动态调整预测的viewport大小去覆盖感兴趣的潜在viewport，同时尽可能保证最低的带宽消耗。</li>
<li>Evaluation：经验用户/视频评估：构建VR viewport预测方法原型，使用经验360°视频和代表性的头部移动数据集评估。</li>
</ul>
<h2 id="全景直播推流的预备知识">全景直播推流的预备知识</h2>
<h3 id="vr推流直播">VR推流直播</h3>
<p></p>
<p>相比于传统的2D视频推流的特别之处：</p>
<ul>
<li>VR系统是交互式的，viewport的选择权在客户端；</li>
<li>呈现给用户的最终视图是整个视频的一部分；</li>
</ul>
<h3 id="用户头部移动的模式">用户头部移动的模式</h3>
<p>在大量的360°视频观看过程中，用户主要的头部移动模式有4种，使用$i-j\ move$来表示；</p>
<p>其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。</p>
<ul>
<li>$1-1\ move$：单个物体以单一方向移动；</li>
<li>$1-n\ move$：单个物体以多个方向移动；</li>
<li>$m-n\ move$：多个物体以多个方向移动；</li>
<li>$Arbitrary\ move$：用户不跟随任何感兴趣的物体而移动，viewport切换随机；</li>
</ul>
<p></p>
<p>现有的直播VR推流中的viewport预测方法是基于速度的方式，这种方式只对$1-1\ move$这一种模式有效。</p>
<p>本方案的目标是提出对4种模式都有效的预测策略。</p>
<h2 id="系统架构">系统架构</h2>
<p></p>
<h3 id="理论创新">理论创新</h3>
<ul>
<li>
<p>核心功能模块：</p>
<ol>
<li>
<p>motion detection：区分运动物体与静止的背景。</p>
</li>
<li>
<p>feature selection：选择代表性的特征并对运动物体做追踪。</p>
<p>这两个模块使系统能识别用户可能感兴趣的viewport。</p>
</li>
</ol>
</li>
<li>
<p>使用贝叶斯方法分析用户观看行为并形式化用户的兴趣模型。</p>
<ol>
<li>
<p>使用错误恢复机制来使当预测错误被检测到时的预测viewport去适应实际的viewport，尽管不能消除预测错误但是能避免在此基础上进一步的预测错误。</p>
</li>
<li>
<p>使用动态viewport更新算法来产生大小可变的viewport，通过同时考虑跟踪到的viewport轨迹和用户当前的速度（矢量）。</p>
<p>这样，即使用户的运动模式很复杂也能有更高的概率去覆盖潜在的视图。</p>
</li>
</ol>
</li>
</ul>
<h3 id="具体实施">具体实施</h3>
<ul>
<li>
<p>虽然提出的运动追踪和错误处理机制是计算密集型的任务，但是这些组件都部署在video packager中，运行在服务端。</p>
</li>
<li>
<p>将生成VR视图的工作负载移动到服务端，进一步减少了客户端的计算开销以及网络开销。</p>
</li>
</ul>
<h2 id="形式化">形式化</h2>
<h3 id="基于运动轨迹的viewport预测">基于运动轨迹的viewport预测</h3>
<p>使用<a href="https://ieeexplore.ieee.org/document/1333992" target="_blank" rel="noopener noreffer">GMM</a>完成运动检测，使用<a href="https://ieeexplore.ieee.org/document/323794" target="_blank" rel="noopener noreffer">Shi-Tomasi algorithm</a>解决运动轨迹跟踪问题。</p>
<p></p>
<ol>
<li>
<p>运动检测</p>
<p>GMM前景提取</p>
</li>
<li>
<p>特征选取与过滤</p>
<p>采用 Shi-Tomasi algorithm 从视频中检测代表性的特征，直接检测得到的代表性特征数量较多而难以追踪。</p>
<p>采用两种过滤的方法来减少要追踪的特征数量。</p>
<ul>
<li>
<p>比较当前帧和前一帧的特征，只保留其共有的部分。</p>
</li>
<li>
<p>采用第1步中运动检测的方式，只保留运动的部分。</p>
</li>
</ul>
</li>
<li>
<p>viewport生成</p>
<p>经过选择和过滤之后的特征通常分布在不能被单一用户视图所覆盖的广阔区域中。</p>
<p>在整个360°视频中可能存在多个运动的物体，即$m-n\ move$。</p>
<p>提出一种系统的方式来产生用户最可能跟随观看的viewport。</p>
<p>直觉是用户更可能将大部分注意力放在两种类型的物体上：</p>
<ul>
<li>离用户更近的物体。</li>
<li>就物理形状而言更“重要”的物体。</li>
</ul>
<p>这两种类型的物体大多包含最密集和最大量的特征，因此通过所有特征的重心来计算预测用户视图的中心。</p>
<p>对于剩余的特征列表：$\vec{F} = [f_1, f_2, f_3, &hellip;, f_k]$，其中$f_i(i = 1 &hellip; k)$表示特征$f_i = &lt;f^{(x)}_i, f^{(y)}_i&gt;$的像素点坐标，则预测出的viewport中心坐标可以计算出来：
$$
l_x = \frac{1}{k} \sum^k_{i=1} f^{(x)}_i;\ l_y = \frac{1}{k} \sum^k_{i=1} f^{(y)}_i.
$$
考虑到即使预测的viewport中包含用户观看的物体，预测得到的viewport也可能会与实际的viewport存在差异。</p>
<p>所以预测的viewport可能比实际的viewport要大，所以使用缩放因子$S_c$来产生预测的viewport。</p>
<p>给出用户viewport的大小$S_{user}$，预测的viewport可以通过$S_{pre} = S_c \cdot S_{user}$计算出来。</p>
</li>
</ol>
<h3 id="基于用户反馈的错误恢复">基于用户反馈的错误恢复</h3>
<p>video packager可以通过HMD和web服务器通过反向路径从用户处检索用户实际视图的反馈信息。</p>
<p>基于反馈的错误恢复机制在以下两种场景中表现良好：</p>
<ol>
<li>
<p>没有运动的物体</p>
<p>如果没有检测到运动的物体，则用户很可能是在观看静止的物体，这会导致基于运动目标的viewport预测失败。</p>
<p>在这种场景中，可以认为视频内容已经不再是决定用户viewport的因素，而只取决于用户自身的行为。</p>
<p>因此采用基于速度的方式来预测viewport。（这样的决策可以在运动检测模块没有检测到运动物体时就做出）</p>
<p>一旦从反馈路径上得到用户信息，可以产生用户viewport位置向量：$\vec{L} = [l_1, l_2, l_3, &hellip;, l_M]$，其中$l_i$表示第$i$个帧中用户viewport的位置，$M$表示视频播放缓冲区中的帧数。那么可以计算viewport速度：
$$
\vec{V} = \frac{\vec{(l_2 - l_1)} + \vec{(l_3 - l_2)} &hellip;.(l_M - l_{M-1})}{M-1} = \frac{(\vec{l_M - l_1})}{M-1}
$$
下一帧的预测位置$L_{M=1}$也可以计算出来：
$$
l_{M+1} = l_M + \vec{V}
$$</p>
</li>
<li>
<p>预测视图与实际视图的不匹配</p>
<p>一旦运动追踪策略检测到用户实际的视图和预测的视图不同，就会触发恢复机制去追踪用户实际在看着的物体。</p>
<p>可以使用运动追踪方式确定用户实际观察的物体的速度。</p>
<p>给出前一帧匹配的特征$\vec{FA} = [fA_1, fA_2, fA_3, &hellip;, fA_p]$和当前帧的特征$\vec{FB} = [fB_1, fB_2, fB_3, &hellip;, fB_p]$，可以计算出速度：
$$
V_x = \frac{1}{p} (\sum^p_{i=1} fB^{(x)}_i - \sum^p_{i=1}fA^{(x)}_i),\
V_y = \frac{1}{p} (\sum^p_{i=1} fB^{(y)}_i - \sum^p_{i=1}fA^{(y)}_i),
$$
假设预测的viewpoint是$(l_x, l_y)$，修改之后的viewpoint是$(l_x + V_x,\ l_y + V_y)$。</p>
</li>
</ol>
<h3 id="动态viewport更新">动态viewport更新</h3>
<p>前述的错误恢复机制发生在viewport预测错误出现之后，任务是避免未来更多的错误。</p>
<p>动态的viewport更新则努力避免viewport预测错误。</p>
<p>关键思想是扩大预测的viewport大小，以高概率去覆盖$m-n\ move$和$arbitrary\ move$下所有潜在的运动目标；更重要的是动态调整视图的大小去获得更高效的带宽利用率。</p>
<ul>
<li>
<p>对于一个360°全景视频，将360°的帧均分为$N = n \times n$个网格，每个网格看作是一个tile，预测的viewport即为$N$个tile的子集。</p>
</li>
<li>
<p>使用贝叶斯方法分析用户的观看行为，每个tile分配一个独立的贝叶斯模型，所以每个tile可以独立更新。</p>
</li>
<li>
<p>设$X$表示用户viewport，$Y$表示静态内容，$Z$表示运动物体。</p>
</li>
<li>
<p>未来的用户viewport可以以条件概率计算为$P(X|Y,\ Z)$，$Y$与$Z$相互独立。</p>
</li>
<li>
<p>用户的viewport可以通过反馈信息得出$P(X)$；用户观看静态特征可以表示为$P(X|Y)$；用户观看动态特征可以表示为$P(X|Z)$。</p>
</li>
<li>
<p>$P(X|Y, Z)$可以计算为：
$$
P(X|Y, Z) = \frac{P(Y|X) \cdot P(Z|X) \cdot P(X)}{P(Y, Z)}
$$</p>
</li>
<li>
<p>只要用户开始观看，对于tile $T_i$，就能得到其先验概率$P(Y_i|X_i)$和$P(Z_i|X_i)$，进而根据贝叶斯模型计算出$P(X|Y, Z)$。</p>
</li>
</ul>
<p>为每个tile定义两种属性：</p>
<ol>
<li>当前状态：表示此tile是否属于预测的viewport（属于标记为$PREDICTED$，不属于标记为$NONPREDICTED$）。</li>
<li>生存期：表示此tile会在view port中存在多长时间（例如定义3种等级：$ZERO$，$MEDIUM$，$HIGH$，实际的定义划分可以根据具体的用户和视频设定）。</li>
</ol>
<h2 id="预测步骤">预测步骤</h2>
<p>按照形式化中提出的3步，分为系统初始化、帧级别的更新、缓冲区级别的更新。</p>
<ol>
<li>
<p>系统初始化</p>
<p>初始化阶段中，view更新算法将所有的$N$个tile标注为$PREDICTED$，并将生存期设置为$MEDIUM$，即系统向用户发送完整的一帧作为自举。</p>
<p>这样设定的原因在于：当用户第一次启动视频会话时，允许“环视”类型的移动，这可能会覆盖360°帧的任意viewport。</p>
</li>
<li>
<p>帧级别的更新</p>
<p>给定一帧，应用修改后的motion追踪算法在运动区域中选择特征，而不使用特征的密度做进一步的过滤。</p>
<p>使用有多个tile的多个视图来覆盖一个放大的区域，该区域包含作为预测viewport的移动对象上的所有特征，这样就能适应$m-n\ move$中的用户行为。</p>
<p>设计帧级别的算法标记选择的tile作为$PREDICTED$并设置其生存期为$HIGH$（直觉上讲运动中的物体或用户所感兴趣的静态特征会更以长时间保留在viewport之中）。</p>
</li>
<li>
<p>缓冲区级别的更新</p>
<p>以缓冲区长度为间隔检索用户的实际视图，基于此可以对tile的两种属性做出调整。</p>
<ol>
<li>对于与用户实际视图重叠的tile，设置为$PREDICTED$和$HIGH$。</li>
<li>对于用户实际视图没有出现但出现在预测的视图中的tile，生存期减1，如果生存期减为$ZERO$，就重设其状态为$NONPREDICTED$，将其从预测的viewport中移除。</li>
</ol>
<p></p>
</li>
</ol>
]]></description>
</item>
</channel>
</rss>

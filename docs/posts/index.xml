<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - Ayamir&#39;s Blog</title>
        <link>https://ayamir.github.io/posts/</link>
        <description>所有文章 | Ayamir&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>miracle_l@bupt.edu.cn (Ayamir)</managingEditor>
            <webMaster>miracle_l@bupt.edu.cn (Ayamir)</webMaster><lastBuildDate>Sun, 27 Feb 2022 10:39:45 &#43;0800</lastBuildDate><atom:link href="https://ayamir.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)</title>
    <link>https://ayamir.github.io/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/</link>
    <pubDate>Sun, 27 Feb 2022 10:39:45 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/</guid>
    <description><![CDATA[<h1 id="bitrate-adaptation-schemes">Bitrate Adaptation Schemes</h1>
<h2 id="client-based">Client-based</h2>
<p>Recently, most of the proposed bitrate adaptation schemes reside at the client side, according to the specifications in the DASH standard.</p>
<p></p>
<p>Purposes:</p>
<ol>
<li>Minimal rebuffering events when the playback buffer depletes.</li>
<li>Minimal startup delay especially in case of live video streaming.</li>
<li>A high overall playback bitrate level with respect to network resources.</li>
<li>Minimal video quality oscillations, which occur due to frequent switching.</li>
</ol>
<h3 id="available-bandwidth-based">Available bandwidth-based</h3>
<p>The client makes its representation decisions based on the measured available network bandwidth, which is usually calculated as the size of the fetched segment(s) divided by the transfer time.</p>
<p>This scheme suffers from poor QoE due to a lack of a reliable bandwidth estimation methods, which results in frequent buffer underruns.</p>
<h4 id="general-context">General context</h4>
<ul>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/1943552.1943575" target="_blank" rel="noopener noreffer">Based on segment fetch time(SFT)</a> measures the time starting from sending the HTTP GET request to receiving the last byte of the segment. Sequential and parallel segment fetching method in CDNs, by using metric that compares the expected segment fetch time(ESFT) with the measured SFT to determine if the selected segment bitrate matches the network capacity.</p>
<p><a href="https://ieeexplore.ieee.org/document/6333880/" target="_blank" rel="noopener noreffer">Based on the bitrate observed for the last segment downloaded</a> and the estimated throughput that was calculated during the previous estimation.</p>
</li>
<li>
<p><a href="https://ieeexplore.ieee.org/document/6774592" target="_blank" rel="noopener noreffer">Probe AND Adapt</a> tries to eliminate the ON-OFF steady state issue as well as reduce bitrate oscillations when multiple clients share the same bottleneck link.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/2789168.2790118" target="_blank" rel="noopener noreffer">piStream</a> enables clients to estimate bandwidth based on a resource monitor module that act as a physical-layer daemon.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/2155555.2155580" target="_blank" rel="noopener noreffer">SVC with DASH</a> prefetches base layers of future segments or downloads enhancement layers for existing segments using a bandwidth-sloping-based heuristic.</p>
</li>
</ul>
<h4 id="mobile-context">Mobile context</h4>
<h5 id="static">Static</h5>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/2964284.2964313" target="_blank" rel="noopener noreffer">DASH2M</a> uses HTTP/2 server push and stream terminate properties to reduce the battery consumption of the mobile device. Adaptive k-push scheme propose to increase/decrease k according to a bandwidth increase/decrease while keeping in mind the overall power consumption in a push cycle.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2990505" target="_blank" rel="noopener noreffer">LOw-LatenceY Prediction-based adaPtation(LOLYPOP)</a> leverages TCP throughput predictions on multiple times scales to achieve low latency and improve QoE.</li>
</ul>
<h5 id="motive">Motive</h5>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/2964284.2964333" target="_blank" rel="noopener noreffer">GeoStream</a>: introduce the use of geostatistics to estimate future bandwidth in unknown locations.</li>
</ul>
<h3 id="playback-buffer-based">Playback Buffer-Based</h3>
<p>The client uses the playout buffer occupancy as a criterion to select the next segment bitrate during video playback.</p>
<p>This scheme suffers from many limitations including low overall QoE and instability issues, especially in the case of long-term bandwidth fluctuations. SVC-based approaches have limitations related to the complexity of SVC.</p>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/7177435" target="_blank" rel="noopener noreffer">Base</a> combines the buffer size with a tool-set of client metrics for accurate rate selection and smooth switching.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2619239.2626296" target="_blank" rel="noopener noreffer">BBA</a> aims to maximize the average video quality and avoid unnecessary rebuffering events, but suffers from QoE degradation during long-term bandwidth fluctuations.</li>
<li><a href="https://ieeexplore.ieee.org/document/7524428" target="_blank" rel="noopener noreffer">BOLA</a> uses online control algorithm that treats bitrate adaptation as a utility maximization problem. Provide strong theorectical proof that it is near optimal, design a QoE model that incorporates both the average playback quality and the rebuffering time. It is implemented and available in the <code>dash.js</code> player.</li>
<li><a href="https://ieeexplore.ieee.org/document/6573184" target="_blank" rel="noopener noreffer">BIEB</a> maximizes video quality based on SVC priority while reducing the number of quality oscillations and avoiding stalls and frequent bitrate switching. it maintains a stable buffer occupancy before increasing the quality.</li>
<li><a href="https://dl.acm.org/doi/10.1145/3123266.3123390" target="_blank" rel="noopener noreffer">QUEuing Theory approach to DASH Rate Adaptation(QUETRA)</a> allows to calculate the expected buffer occupancy given a bitrate choice, network throughput, and buffer capacity.</li>
</ul>
<h3 id="mixed-adaptation">Mixed Adaptation</h3>
<p>The client makes its bitrate selection based on a combination of metrics including available bandwidth, buffer occupancy, segment size and/or duration.</p>
<h4 id="simple-client">Simple client</h4>
<ul>
<li>Control-theoretic based: <a href="https://dl.acm.org/doi/10.1145/2829988.2787486" target="_blank" rel="noopener noreffer">FastMPC</a>, <a href="https://ieeexplore.ieee.org/document/6410740" target="_blank" rel="noopener noreffer">Another</a></li>
<li>Optimization problem: <a href="https://dl.acm.org/doi/10.1145/2557642.2557658" target="_blank" rel="noopener noreffer">Streaming video over HTTP with Consistent Quality</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/2413176.2413190" target="_blank" rel="noopener noreffer">Towards agile and smooth video adaptation in dynamic HTTP streaming</a> aims to balance bandwidth utilization and smoothness in DASH in both single and multiple CDN(s) scenarois.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2910017.2910593" target="_blank" rel="noopener noreffer">SQUAD</a> is a lightweight bitrate adaptation algorithm that uses the available bandwidth and buffer information to increase the average bitrate while minimizing the number of quality switches.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2155555.2155582" target="_blank" rel="noopener noreffer">Multi-path solution for abr in wireless networks</a> avoids the problems of TCP congestion control by using parallel TCP streams.</li>
<li><a href="https://ieeexplore.ieee.org/document/7247436" target="_blank" rel="noopener noreffer">SARA</a> is Segment-Aware Rate Adaptation algorithm based on the segment size variation, the available bandwidth estimate and the buffer occupancy. It extends MPD file to include the size of every segment.</li>
<li><a href="https://dl.acm.org/doi/10.1145/2910017.2910596" target="_blank" rel="noopener noreffer">ABMA+</a> selects the highest segment representation based on the estimated <em>probability of video rebuffering</em>. It makes use of buffer maps, which define the playout buffer capacity that is required under certain conditions to satisfy a rebuffering threshold and to avoid heavy online calculations.</li>
<li><a href="https://dl.acm.org/doi/10.1145/3204949.3204961" target="_blank" rel="noopener noreffer">GTA</a> uses a cooperative game in coalition formation then formulates the bitrate selection problem as a bargaining process and consensus mechanism. GTA improves QoE and video stability without increasing the stall rate or startup delay.</li>
</ul>
<h4 id="multiple-clients">Multiple clients</h4>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/6691442" target="_blank" rel="noopener noreffer">ELASTIC</a> generates a long-lived TCP flow and avoids the ON-OFF steady state behavior which leads to bandwidth overestimations. Ensure bandwidth fairness between competing clients based on network feedback assistance, but without taking the QoE into consideration. In addition, it ignores quality oscillations in its bitrate decisions.</li>
<li><a href="https://ieeexplore.ieee.org/document/6229732" target="_blank" rel="noopener noreffer">Adaptation algorithm for HAS</a> uses current buffer occupancy level to estimate available bandwidth and average bitrate of the different bitarte levels from MPD as metrics in its bitrate selection.</li>
<li><a href="https://ieeexplore.ieee.org/document/6704839" target="_blank" rel="noopener noreffer">FESTIVE</a> contains:
<ul>
<li>a bandwidth estimator module</li>
<li>a bitrate selection and update method with stateful player</li>
<li>a randomized scheduler which incorporates the buffer size to schedule the download of the next segment.</li>
</ul>
</li>
<li><a href="https://ieeexplore.ieee.org/document/8101529" target="_blank" rel="noopener noreffer">TSDASH</a> uses a logarithmic-increase-multiplicative-decrease (LIMD) based bandwidth probing algorithm to estimate the available bandwidth and a dual-threshold buffer for the bitrate adaptation.</li>
</ul>
<h3 id="mdp-based">MDP-Based</h3>
<p>The video streaming process is formulated as a finite MDP to be able to make adaptation decisions under fluctuating network conditions.</p>
<p>This scheme may suffer from instability, unfairness and underutilization when the number of clients increases, probably because such factors are not taken into account in the MDP models and due to clients' decentralized ON-OFF patterns.</p>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/6774598" target="_blank" rel="noopener noreffer">Real-time best-action search algorithm over multiple access networks</a> uses both Bluetooth and WiFi links to simultaneously download video segments. However, this scheme shows limitations during user mobility which negatively affect QoE.</li>
<li><a href="https://ieeexplore.ieee.org/document/7305810" target="_blank" rel="noopener noreffer">Optimizing in Vehicular environment</a> introduces a three-variant of RL-based algorithms which take advantage of the historical bandwidth samples to build an accurate bandwidth estimation model.</li>
<li><a href="https://ieeexplore.ieee.org/document/6838245" target="_blank" rel="noopener noreffer">Multi-agent Q-Learning-based for fairness</a> uses a central manager in charge of collecting QoE statistics and coordination between the competing clients. The algorithm ensures a fair QoE distribution and improves QoE while avoiding suboptimal decisions.(without considering stalls and quality switches)</li>
<li><a href="https://dl.acm.org/doi/10.1145/2910017.2910603" target="_blank" rel="noopener noreffer">Online learning adaptation</a> aims to select the optimal representation and maximize the long-term expected QoE. The reward function is calculated from a combination of quality oscillations, segment quality and stalls experienced by the client. It exploits a parallel learning technique to avoid slow convergence and suboptimal solutions.</li>
<li><a href="https://ieeexplore.ieee.org/document/7393865" target="_blank" rel="noopener noreffer">mDASH</a> aims to improve QoE during long-term bandwidth variations. It takes buffer size, bandwidth conditions and bitrate stability as Markov state variables.</li>
<li><a href="https://dl.acm.org/doi/10.1145/3098822.3098843" target="_blank" rel="noopener noreffer">Pensive</a> does not rely on pre-programmed models or assumptions about the environment, but gradually learns the best policy for bitrate decisions through observation and experience.</li>
<li><a href="https://ieeexplore.ieee.org/document/8048013" target="_blank" rel="noopener noreffer">D-DASH</a> combines DL and RL to improve QoE, achieves a good trade-off between policy optimality and convergence speed during the decision process.</li>
</ul>
<h2 id="server-based">Server-Based</h2>
<p>Server-based schemes use a bitrate shaping method at the server side and do not require any cooperation from the client. The switching between the bitrates is implicitly controlled by the bitrate shaper. The client still makes its own decisions, but the decisions are more or less determined by the shaping method on the server.</p>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/2155555.2155557" target="_blank" rel="noopener noreffer">Traffic shaping</a> analyzes instability and unfairness issues in the presence of multiple HAS players competing for the available bandwidth. This method can be deployed at a home gateway to improve fairness, stability and convergence delay, and to eliminate the OFF periods during the steady states.</li>
<li><a href="https://ieeexplore.ieee.org/document/7524620" target="_blank" rel="noopener noreffer">Tracker-assisted adaptation</a> uses a architecture which consists of clients communicating with a server through a shared proxy and a server having a tracker functionality that manages the clients' statuses and helps them share knowledge about their statues.</li>
<li><a href="https://dl.acm.org/doi/10.1145/1943552.1943573" target="_blank" rel="noopener noreffer">Quality Adaptation Controller</a> aims to control the size of the server sending buffer in order to adjust and select the most appropriate bitrate level for each DASH player. It maintains the playback buffer occupancy of each player as stable as possible and to match bitrate level decisions with the available bandwidth.</li>
<li><a href="https://ieeexplore.ieee.org/document/7983147/" target="_blank" rel="noopener noreffer">Multi-Source Stream system</a>: the client fetches the segments from multi-source stream servers.</li>
</ul>
<p></p>
<p>Cons:</p>
<ol>
<li>Produce high overhead on the server side with a high complexity</li>
<li>These schemes also need modifications to the MPD or a custom server software to implement the bitrate adaptation logic.(a violation of the DASH-standard design principles)</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)</title>
    <link>https://ayamir.github.io/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/</link>
    <pubDate>Sat, 26 Feb 2022 11:26:06 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/</guid>
    <description><![CDATA[<h1 id="paper-overview">Paper Overview</h1>
<p>Link: <a href="https://ieeexplore.ieee.org/document/8424813" target="_blank" rel="noopener noreffer">https://ieeexplore.ieee.org/document/8424813</a></p>
<p>Level: IEEE Communications Surveys &amp; Tutorials 2019</p>
<h1 id="background">Background</h1>
<h2 id="traditional-non-has-ip-based-streaming">Traditional non-HAS IP-based streaming</h2>
<ol>
<li>
<p>The client receives media that is typically <em>pushed</em> by a media server using <strong>connection-oriented</strong> protocol such as Real-time Messaging Protocol(RTMP/TCP) or <strong>connectionless</strong> protocol such as Real-time Transport Protocol(RTP/UDP).</p>
</li>
<li>
<p>Real-time Streaming Protocol(RTSP) is a common protocol to control the media servers, which is responsible for setting up a streaming session and keeping the state information during this session, but is not responsible for actual media delivery(task for protocol like RTP).</p>
</li>
<li>
<p>The media server performs rate adaption and data delivery scheduling based on the RTP Control Protocol(RTCP) reports sent by the client.</p>
</li>
<li>
<p>When it comes to NAT and firewall, additional protocols or configurations are needed during the session establishment.</p>
</li>
</ol>
<p>The characteristics result in complex and expensive servers. These scalability and vendor dependency issues as well as high maintenance costs have resulted in deployment challenges for protocols like RTSP.</p>
<h2 id="has">HAS</h2>
<p>Around 2005, HTTP adaptive streaming(HAS) became popular and dominant, which treated the media content like regular Web content and delivered it in small pieces over HTTP protocol.</p>
<ol>
<li>HTTP as application and TCP as the transport-layer protocol.</li>
<li>Client <em>pull</em> the data from a standard HTTP server, which simply hosts the media content.</li>
<li>HAS solutions employ dynamic adaptation with respect to varying network conditions to provide a seamless streaming experience.</li>
<li>The original file/stream is partitioned into <em>segments</em> (also called <em>chunks</em>) of equi-length playback time. Multiple versions(also called representations) of each segment are generated that vary in bitrate/resolution/quality using an encoder or a transcoder.</li>
<li>The server generates an index file, which is a manifest that lists the available representations including HTTP urls to identify the segments along with their availability times.</li>
<li>The client first receives the manifest that contains the metadata for video, audio, subtitles and other features, then constantly measures certain parameters: available network bandwidth, buffer status, battery and CPU levels, etc. According to these parameters, the HAS client repeatedly fetches the most suitable next segment among the available representations from the server.</li>
</ol>
<p>Advantages:</p>
<ol>
<li>It use HTTP to deliver video segments, which simplifies the traversal through NATs and firewalls.</li>
<li>At the server side, it use conventional Web servers or caches available within the networks of ISPs and CDNs.</li>
<li>At the client side, it requests and fetches each segment independently from others and maintains the playback session state, whereas the server is not required to maintain any state.</li>
<li>It doesn&rsquo;t require a persistent connection between the client and server, which improves system scalability and reduces implementation and deployment costs.</li>
</ol>
<h2 id="comparison-summary">Comparison Summary</h2>
<p></p>
<p></p>
<h1 id="challenges">Challenges</h1>
<h2 id="multi-client-competitionstability-issues">Multi-Client Competition/Stability Issues</h2>
<p>A centralized management controller can enhance the overall video quality, while improve QoE.</p>
<p>A robust HAS scheme should achieve 3 main objectives:</p>
<ol>
<li><em>Stability</em>: HAS clients should avoid frequent bitrate switching.</li>
<li><em>Fairness</em>: Multiple HAS clients competing for available bandwidth should equally share network resources based on viewer, content and device characteristics.</li>
<li><em>High Utilization</em>: While the clients attempt to be stable and fair, network resources should be used as efficiently as possible.</li>
</ol>
<p>A streaming session consists of 2 states: buffer-filling state and steady state.</p>
<ul>
<li>
<p>The buffer-filling state aims to fill the playback buffer and reach a certain threshold where the playback can be initiated or resumed.</p>
</li>
<li>
<p>The steady state is to keep the buffer level above a minimum threshold despite bandwidth fluctuation or interruptions. The steady state consists of 2 activity periods referred to as ON and OFF.</p>
<p>The client requests a segment every $T_s$ time units, where $T_s$ represents the content time duration of each segment, and sum of ON and OFF period durations equals $T_s$.</p>
<ul>
<li>ON period: client downloads the current segment and notes the achieved throughput value that will be later used in selecting the appropriate bitrate for future segments.</li>
<li>OFF period: client becomes idle temporarily.</li>
</ul>
</li>
</ul>
<p></p>
<p>There are different cases during competition process.</p>
<ol>
<li>
<p>The ON periods of clients don&rsquo;t overlap during the current segment download, each client will overestimate the available bandwidth. So longer download time will cause the initially non-overlapping ON periods to eventually start overlapping.</p>
<p></p>
</li>
<li>
<p>As the amount of overlap increases, the clients will have lower bandwidth estimations and start selecting segments that have lower bitrate. These segment will take less time to download, causing the amount of overlap among the ON periods to precedurally shorten, until the process reverts to its initial situation.</p>
<p></p>
</li>
<li>
<p>The cycle repeats itself, causing periodic up and down shift in the selected bitrates, leading to unstable video quality, unfairness, and underutilization.</p>
<p></p>
</li>
</ol>
<h2 id="consistent-quality-streaming">Consistent-Quality Streaming</h2>
<p>The correlation between video bitrate and its perceptual quality is non-linear.</p>
<ul>
<li>Different video content types have unique characteristics.</li>
<li>Differences of inter-stream and intra-stream video scene complexity across content.</li>
</ul>
<p></p>
<p></p>
<h2 id="qoe-optimization-and-measurement">QoE Optimization and Measurement</h2>
<p>HAS scheme uses application control loop, which also interacts with a lower-layer control loop(such as TCP congestion control). It plays a key role in determining the viewer QoE.</p>
<p></p>
<p>Factors influencing QoE are categorized as:</p>
<ol>
<li>Perceptual, directly perceived by the viewer.</li>
<li>Technical, indirectly affecting the QoE.</li>
</ol>
<h3 id="perceptual">Perceptual</h3>
<p>Perceptual factors include the video image quality, initial delay, stalling duration and frequency.</p>
<p>The impact of these factors differs depending on the users subjectivity.</p>
<p>Most users consider initial delays less critical than stalling.</p>
<h3 id="technical">Technical</h3>
<p>Technical factors include the algorithms, parameters, and hardware/software used in streaming system.</p>
<p>Specifically, factors are:</p>
<ul>
<li>Server side: encoding parameters, video qualities and segment size.</li>
<li>Client side: adaptation parameters and environment that clients reside in.</li>
</ul>
<h3 id="qoe-measurement">QoE measurement</h3>
<ol>
<li>Objective matrics: Peak Signal-to-Noise Ratio(PSNR), Structural SIMilarity(SSIM and SSIMplus), Perceived Video Quality(PVQ) and Statistically Indifferent Quality Variation(SIQV).</li>
<li>Subjective matrics: Mean Opinion Score(MOS).</li>
<li>Quality-of-Service (QoS)-derived matrics: startup delay, average video bitrate, quality switches and rebuffering events.</li>
</ol>
<p>Try to optimize each metric is difficult because it may result in conflicts.</p>
<h2 id="inter-destination-multimedia-synchronization">Inter-Destination Multimedia Synchronization</h2>
<p>Online communities are drifting towards watching online videos together in a synchronized manner.</p>
<p>Having Multiple streaming clients distributed in different geographical locations poses challenges in delivering video content simultaneously, while keeping the playback state of each client the same.</p>
<p>Typically, IDMS solutions involve a master node to which clients synchronize their playout to.</p>
<p>Rainer et proposed an IDMS architecture for DASH by using a distribute control scheme where peers can communicate and negotiate a reference placback timestamp in each session.</p>
<p>In another work, Rainer et provided a crowdsourced subjective evaluation to find a asynchronism threshold at which QoE was not significantly affected.</p>
]]></description>
</item>
<item>
    <title>WebXR for Panoramic Video</title>
    <link>https://ayamir.github.io/posts/webxr-for-panoramic-video/</link>
    <pubDate>Fri, 25 Feb 2022 11:04:23 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/webxr-for-panoramic-video/</guid>
    <description><![CDATA[<p>最近几天一直在用<code>WebXR</code>的技术重构目前的基于分块的全景视频自适应码率播放客户端，下面简述一下过程。</p>
<p>首先结论是：分块播放+自适应码率+完全的沉浸式场景体验=Impossible（直接使用WebXR提供的API）</p>
<h2 id="分块播放">分块播放</h2>
<p>分块播放的本质是将一整块的全景视频从空间上划分成多个小块，各个小块在时间上与原视频的长度是相同的。</p>
<p>在实际播放的时候需要将各个小块按照原有的空间顺序排列好之后播放，为了避免各个分块播放进度不同的问题，播放时还需要经过统一的时间同步。</p>
<p>对应到web端的技术实现就是：</p>
<p>一个分块的视频&lt;-&gt;一个<code>&lt;video&gt;</code>h5元素&lt;-&gt;一个<code>&lt;canvas&gt;</code>h5元素</p>
<p>视频的播放过程就是各个分块对应的<code>&lt;canvas&gt;</code>元素不断重新渲染的过程</p>
<p>各个分块时间同步的实现需要一个基准视频进行对齐，大体上的原理如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="kd">let</span> <span class="nx">baseVideo</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>
<span class="kd">let</span> <span class="nx">videos</span> <span class="o">=</span> <span class="p">[];</span>

<span class="nx">initBaseVideo</span><span class="p">();</span>
<span class="nx">initVideos</span><span class="p">();</span>

<span class="k">for</span> <span class="p">(</span><span class="nx">video</span> <span class="k">in</span> <span class="nx">videos</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">video</span><span class="p">.</span><span class="nx">currentTime</span> <span class="o">=</span> <span class="nx">baseVideo</span><span class="p">.</span><span class="nx">currentTime</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="自适应码率">自适应码率</h2>
<p>自适应码率的方案使用<code>dashjs</code>库实现，即对每个分块<code>&lt;video&gt;</code>元素的播放都用<code>dashjs</code>的方案控制：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="kr">import</span> <span class="p">{</span><span class="nx">MediaPlayer</span><span class="p">}</span> <span class="nx">from</span> <span class="s1">&#39;dashjs&#39;</span><span class="p">;</span>

<span class="kd">let</span> <span class="nx">videos</span> <span class="o">=</span> <span class="p">[];</span>
<span class="kd">let</span> <span class="nx">dashs</span> <span class="o">=</span> <span class="p">[];</span>
<span class="kd">let</span> <span class="nx">mpdUrls</span> <span class="o">=</span> <span class="p">[];</span>

<span class="nx">initVideos</span><span class="p">();</span>
<span class="nx">initMpdUrls</span><span class="p">();</span>

<span class="k">for</span> <span class="p">(</span><span class="kd">let</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">tileNum</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nx">video</span> <span class="o">=</span> <span class="nx">videos</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span>
    <span class="kd">let</span> <span class="nx">dash</span> <span class="o">=</span> <span class="nx">MediaPlayer</span><span class="p">().</span><span class="nx">create</span><span class="p">();</span>
    <span class="nx">dash</span><span class="p">.</span><span class="nx">initialize</span><span class="p">(</span><span class="nx">video</span><span class="p">,</span> <span class="nx">mpdUrls</span><span class="p">[</span><span class="nx">i</span><span class="p">],</span> <span class="kc">true</span><span class="p">);</span>
    <span class="nx">dash</span><span class="p">.</span><span class="nx">updateSettings</span><span class="p">(</span><span class="nx">dashSettings</span><span class="p">);</span>
    <span class="nx">dashs</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">dash</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p>通过对<code>dashSettings</code>的调整的可以设置各种可用的dash参数如不同质量版本下的缓冲区长度，播放暂停时是否终止后台下载等。</p>
<h2 id="沉浸式场景体验">沉浸式场景体验</h2>
<p>全景视频的完全的沉浸式体验目前在<code>Oculus Browser</code>上有两种实现方式：</p>
<ol>
<li>直接使用浏览器默认的全屏功能之后选择视频为：普通视频或180度视频或360度视频。</li>
<li>使用最新的<code>WebXR session</code>的<code>layers</code>特性，手动代码实现。</li>
</ol>
<p>第1种方式因为并没有给出实际的<code>API</code>，所以不可能与分块传输的视频相结合，所以只能使用第2种方式手动实现。</p>
<p>其对应的草案标准地址：https://www.w3.org/TR/webxrlayers-1/</p>
<p></p>
<p>可以看到目前最新的开发标准刚在1个月前完成。</p>
<p><code>WebXR</code>中的开发流程如下：</p>
<ol>
<li>判断浏览器是否支持<code>immersive-vr</code>，如果支持就请求<code>xrSession</code>，所需的特性为<code>layers</code>：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="kr">import</span> <span class="p">{</span><span class="nx">WebXRButton</span><span class="p">}</span> <span class="nx">from</span> <span class="s1">&#39;webxr-button.js&#39;</span><span class="p">;</span>

<span class="kd">let</span> <span class="nx">xrButton</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">WebXRButton</span><span class="p">({</span>
    <span class="nx">onRequestSession</span><span class="o">:</span> <span class="nx">onRequestSession</span><span class="p">,</span>
    <span class="nx">onEndSession</span><span class="o">:</span> <span class="nx">onEndSession</span>
<span class="p">});</span>
<span class="kd">let</span> <span class="nx">xrSession</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>

<span class="kd">function</span> <span class="nx">onRequestSession</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nx">xrSession</span><span class="p">)</span> <span class="p">{</span>
        <span class="nx">navigator</span><span class="p">.</span><span class="nx">xr</span><span class="p">.</span><span class="nx">requestSession</span><span class="p">(</span><span class="s1">&#39;immersive-vr&#39;</span><span class="p">,</span> <span class="p">{</span>
            <span class="nx">requiredFeatures</span><span class="o">:</span> <span class="p">[</span><span class="s1">&#39;layers&#39;</span><span class="p">],</span>
        <span class="p">}).</span><span class="nx">then</span><span class="p">(</span><span class="nx">onSessionStarted</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="nx">onEndSession</span><span class="p">();</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kd">function</span> <span class="nx">onEndSession</span><span class="p">()</span> <span class="p">{</span>
    <span class="nx">xrSession</span><span class="p">.</span><span class="nx">end</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">if</span> <span class="p">(</span><span class="nx">navigator</span><span class="p">.</span><span class="nx">xr</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">navigator</span><span class="p">.</span><span class="nx">xr</span><span class="p">.</span><span class="nx">isSessionSupported</span><span class="p">(</span><span class="s1">&#39;immersive-vr&#39;</span><span class="p">).</span><span class="nx">then</span><span class="p">((</span><span class="nx">supported</span><span class="p">)</span> <span class="p">=&gt;</span> <span class="p">{</span>
		<span class="k">if</span> <span class="p">(</span><span class="nx">supported</span><span class="p">)</span> <span class="p">{</span>
            <span class="nx">xrButton</span><span class="p">.</span><span class="nx">enabled</span> <span class="o">=</span> <span class="nx">supported</span><span class="p">;</span>
        <span class="p">}</span>
	<span class="p">})</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>获取到需要的<code>xrSession</code>之后请求<code>ReferenceSpace</code>，并创建会话中需要的对象，之后用创建的图层更新会话的渲染器状态，并设置<code>requestAnimationFrame</code>需要的回调函数：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="kd">let</span> <span class="nx">xrRefSpace</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>
<span class="kd">let</span> <span class="nx">xrMediaFactory</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>

<span class="kd">function</span> <span class="nx">onSessionStarted</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">xrSession</span> <span class="o">=</span> <span class="nx">session</span><span class="p">;</span>
    <span class="nx">xrButton</span><span class="p">.</span><span class="nx">textContent</span> <span class="o">=</span> <span class="s2">&#34;Exit XR&#34;</span><span class="p">;</span>
    
    <span class="nx">xrMediaFactory</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">XRMediaBinding</span><span class="p">(</span><span class="nx">session</span><span class="p">);</span>
    
    <span class="nx">session</span><span class="p">.</span><span class="nx">requestReferenceSpace</span><span class="p">(</span><span class="s1">&#39;local&#39;</span><span class="p">).</span><span class="nx">then</span><span class="p">((</span><span class="nx">refSpace</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span>
        <span class="nx">xrRefSpace</span> <span class="o">=</span> <span class="nx">refSpace</span><span class="p">;</span>
        
        <span class="kd">let</span> <span class="nx">baseLayer</span> <span class="o">=</span> <span class="nx">xrMediaFactory</span><span class="p">.</span><span class="nx">createEquirectLayer</span><span class="p">(</span><span class="nx">baseVideo</span><span class="p">,</span> <span class="p">{</span>
        	<span class="nx">space</span><span class="o">:</span> <span class="nx">refSpace</span><span class="p">,</span>
        	<span class="nx">centralHorizontalAngle</span><span class="o">:</span> <span class="nb">Math</span><span class="p">.</span><span class="nx">PI</span> <span class="o">*</span> <span class="mi">2</span>
    	<span class="p">});</span>
    	<span class="nx">session</span><span class="p">.</span><span class="nx">updateRenderState</span><span class="p">({</span><span class="nx">layers</span><span class="o">:</span> <span class="p">[</span><span class="nx">baseLayer</span><span class="p">]});</span>
    	<span class="nx">session</span><span class="p">.</span><span class="nx">requestAnimationFrame</span><span class="p">(</span><span class="nx">onXRFrame</span><span class="p">);</span>
    <span class="p">});</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>最后设置每次<code>xrSession</code>要求渲染新帧的函数，并设定渲染循环：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-javascript" data-lang="javascript"><span class="kd">let</span> <span class="nx">xrViewerPose</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>

<span class="kd">function</span> <span class="nx">onXRFrame</span><span class="p">(</span><span class="nx">time</span><span class="p">,</span> <span class="nx">frame</span><span class="p">)</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nx">session</span> <span class="o">=</span> <span class="nx">frame</span><span class="p">.</span><span class="nx">session</span><span class="p">;</span>
    <span class="nx">session</span><span class="p">.</span><span class="nx">requestAnimationFrame</span><span class="p">(</span><span class="nx">onXRFrame</span><span class="p">);</span>
    
    <span class="nx">xrViewerPose</span> <span class="o">=</span> <span class="nx">frame</span><span class="p">.</span><span class="nx">getViewerPose</span><span class="p">(</span><span class="nx">xrRefSpace</span><span class="p">);</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">xrViewerPose</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><p><code>onXRFrame</code>函数在每次渲染新帧时调用，其中每帧对应的观看者的相对位置以及头戴设备的线速度和角速度等变量可以从<code>xrViewerPose</code>中取得。</p>
<p>这么看<code>WebXR</code>的完全沉浸式体验是可行的，但是问题出在需要与分块结合。</p>
<p><code>xrMediaFactory</code>作为<code>XRMediaBinding</code>绑定到当前<code>xrSession</code>的实例对象，可以用来创建采用等距长方形投影的方式的图层<code>XREquirectLayer</code>：</p>
<p></p>
<p>虽然这里出现了可以创建采用<code>Equirectangular</code>方式投影的图层，并可以通过指定其初始化参数完成不同大小的偏移创建，但是这里的处理方式还是将一个完整视频从映射到球面上的方式，即不管怎么改变参数，创建出来的总是有4条曲边的球面块：</p>
<p></p>
<p>并不能实现每个分块以特定的映射逻辑将其不重不漏的铺到球面上的功能。</p>
<p>不过就算可以实现这样的功能，因为1个图层与1个视频块相绑定，在实际创建中发现：</p>
<ul>
<li>
<p>在一个<code>xrSession</code>中最多只能创建16个图层，并不能与<code>MxN</code>的分块逻辑相对应；</p>
</li>
<li>
<p>创建16个图层之后整个<code>xrSession</code>会变得异常卡顿，视频已无法正常播放；</p>
</li>
</ul>
<p>那么是否可以先将多个分块的视频从空间上拼接好，将最终拼接好的视频进行等距长方投影？</p>
<p>首先从实际的实现上没法完成，因为每个视频在h5中本质是<code>&lt;video&gt;</code>元素，多个<code>&lt;video&gt;</code>元素并不能在<code>DOM</code>的基础上实现空间的复原，就算有办法做到，最后在与<code>layer</code>绑定时也必须是1个<code>&lt;video&gt;</code>元素而这1个<code>&lt;video&gt;</code>元素还需要实现各个部分的自适应码率变化，这完全是不可行的。</p>
<p>测试的代码地址：<a href="https://github.com/ayamir/tiled-vr-dash-platform/blob/main/client/eqrt-media-demo/media-layer-sample.html" target="_blank" rel="noopener noreffer">media-layer-sample</a></p>
<p>进一步的解决办法是存在的：</p>
<p>因为目前的<code>WebXR</code>不能够满足需求，所以需要深入<code>WebGL</code>的层面，手动设计一套将各个分块以等距长方投影的方式映射到球面上的逻辑，同时还要与<code>WebXR</code>上层的处理API相对应，任务工作量和难度还需要进一步评估。</p>
]]></description>
</item>
<item>
    <title>Use Jupyter Notebook in Conda Env</title>
    <link>https://ayamir.github.io/posts/use-jupyter-notebook-in-conda-env/</link>
    <pubDate>Tue, 15 Feb 2022 17:19:26 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/use-jupyter-notebook-in-conda-env/</guid>
    <description><![CDATA[<ol>
<li>
<p>激活预先配置好的<code>conda</code>环境，这里假设环境名为<code>keras-tf-2.1.0</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">conda activate keras-tf-2.1.0
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>安装<code>ipykernel</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell">pip3 install ipykernel --user
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>为<code>ipykernel</code>安装环境：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh">python3 -m ipykernel install --user --name<span class="o">=</span>keras-tf-2.1.0
</code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>打开<code>notebook</code>更改服务之后刷新即可：</p>
<p></p>
</li>
</ol>
]]></description>
</item>
<item>
    <title>Note for Content Based Vp for Live Streaming (2)</title>
    <link>https://ayamir.github.io/posts/note-for-content-based-vp-for-live-streaming-2/</link>
    <pubDate>Tue, 25 Jan 2022 11:59:24 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-content-based-vp-for-live-streaming-2/</guid>
    <description><![CDATA[<h1 id="liveobj">LiveObj</h1>
<p><code>LiveDeep</code>方法利用卷积层从视频内容中提取深层特征，不受动态背景的影响。然而在整个推流会话中需要更新一个带有大量权重的巨大的神经网络模型。同时因为没有历史视频和用户的轨迹的数据，模型需要在运行时从随机权重开始训练。而这会导致两个问题：</p>
<ol>
<li>模型需要花很长时间从一次预测错误中恢复；</li>
<li>在初始化的阶段预测率成功率很低；</li>
</ol>
<p>为了解决这两个问题，提出预训练的模型来分析视频内容，对视频的语义进行层次化。</p>
<p><strong>基于对内容的分析，进一步设计了一个轻量级用户模型，将用户偏好映射到不同的视频内容。</strong></p>
<h2 id="用户观看行为分析">用户观看行为分析</h2>
<p>在直播推流中，不能通过分析其他用户的行为模式来得到特定用户的<code>ROI</code>，因此只能直接从视频内容本身入手。</p>
<p>通过对视频内容从空间和时间两个维度的分析得出结论：用户的<code>ROI</code>与物体的大小无关，而是很大程度上依赖于物体在视频中的语义，即用户倾向于观看有意义的事物。</p>
<p>这一结论可以给出推断<code>FoV</code>的直觉：基于检测视频中有意义的物体。</p>
<h2 id="methods">Methods</h2>
<p>首先提出两种直观的通过分析视频内容的视点预测方法，进一步总结这些方法的局限性，并逐步切换到对<code>LiveObj</code>的讨论。</p>
<h3 id="basic-method">Basic method</h3>
<p><em>Basic</em>方法检测视频中所有的对象并使用其中心作为预测的中心。</p>
<p>给出每个帧中的 $k$ 个物体， $\vec{O} = [o_1, o_2, o_3, &hellip;, o_k]$ ，其中每个 $o_i(i = 1, &hellip;, k)$ 表示物体的中心坐标： $o_i = &lt;o^{(x)}_i, o^{(y)}_i&gt;$ 。</p>
<p>最终的预测中心点坐标可以计算出来：
$$
C_x = \frac{1}{k} \sum^{k}_{i=1} o^{(x)}_i;\ C_y = \frac{1}{k} \sum^{k}_{i=1} o^{(y)}_i
$$</p>
<h3 id="over-cover-method">Over-Cover method</h3>
<p>受<code>LiveMotion</code>方法的启发，其创建了不规则的预测<code>FoV</code>来覆盖更多的潜在的区域，<em>Over-Cover</em>的方式预测的<code>FoV</code>会覆盖所有包含物体的区域。</p>
<p>采用<code>YOLOv3</code>来处理帧并检测物体，接着每个检测到的对象生成与该对象共享相同中心的预测子视图，所有子视图的聚合形成最终的预测视口。</p>
<h3 id="summary-for-intuitive-methods">Summary for intuitive methods</h3>
<p><em>Basic</em>方式可能会在多个物体的场景中无法正确选择目标；</p>
<p><em>Over-Cover</em>方式覆盖所有可能的目标来满足较高的精度，但会导致更高的带宽使用量；</p>
<p><em>Velocity</em>方式能很快的适应用户偏好的变化，但是预测精度在长期预测的情况下会显著下降；</p>
<h2 id="liveobj-method">LiveObj Method</h2>
<p><em>Over-Cover</em>方法将所有检测到的目标合并到预测的<code>FoV</code>中而导致冗余问题，而用户一次只能观看其中的几个。</p>
<p>为了解决这个问题，提出基于用户的反馈选择最吸引人的目标，例如用户当前的<code>FoV</code>来形成预测的<code>FoV</code>。</p>
<p>基于这种想法而提出<code>LiveObj</code>，一种基于轨迹的VP方式，通过从<em>Over-Cover</em>方法的结果中过滤掉用户更小可能性看到的目标来缩小最终的<code>FoV</code>。</p>
<p></p>
<ul>
<li><em>Object Detection</em>：处理视频帧并检测目标；</li>
<li><em>User View Estimation</em>：分析用户反馈并用<em>Velocity</em>的方式估计<code>FoV</code>；</li>
<li><em>Object tracking</em>：追踪用户观看的目标；</li>
<li><em>RL-based modeling</em>：接受估计出的<code>FoV</code>和被追踪的目标，最终更新每个分块的状态（选中或未选中）</li>
</ul>
<h3 id="object-detection-and-tracking">Object Detection and Tracking</h3>
<ol>
<li>
<p>Detection：<code>YOLOv3</code>；</p>
</li>
<li>
<p>Tracking：追踪的基本假设是用户会在接下来的一段时间内接着观看当前看着的目标。追踪任务在直播推流的运行时完成。因此每隔几秒收集用户反馈，并进一步推断用户之前正在观看的目标，然后据此更新追踪目标。</p>
<p>追踪算法：</p>
<p></p>
</li>
</ol>
<h3 id="user-view-estimation">User View Estimation</h3>
<p>分析用户的反馈处于两个目的：</p>
<ol>
<li>估计未来的用户的<code>FoV</code>；</li>
<li>校准当前用户<code>FoV</code>以及要跟踪的对象；</li>
</ol>
<p>给出用户反馈（即过去片段中实际的<code>FoV</code>），首先更新用户<code>FoV</code>并分析用户的行为模式，并根据此模式计算出下一帧中的预期用户速度。然后识别更新后的<code>FoV</code>中的对象，这些对象确定为<code>ROI</code>，对象追踪步骤将这些更新用于未来的片段来提高预测精度。</p>
<h3 id="rl-based-modeling">RL-based Modeling</h3>
<p>因为预测的误差和用户实际<code>FoV</code>的变化，可能会导致追踪的目标从<code>FoV</code>中消失，而这会使整个预测算法完全失效。所以提出一个基于RL的模型来为每个分块建立用户行为模型，旨在最小化预测误差。</p>
<p>出发点是<strong>不同的分块有不同的概率包含有意义的目标，并且更可能包含有意义目标的分块通常对目标检测错误更敏感。</strong></p>
<p>将上面的观察形式化为一个策略学习过程 $M$：
$$
M = &lt;S, A, P_{s, a, s'}, R&gt;
$$
其中 $S$ 和 $A$ 表示状态和动作， $P_{s, a, s'}$ 是给定状态 $s$ 的情况下选择动作 $a$ 的概率，转移之后的状态为 $s'$ ，$R$ 表示奖励函数。</p>
<p>系统的目标是通过设定不同的 $P_{s, a, s'}$ 的值，来学习每个分块对目标检测误差的不同的敏感性。</p>
<p>状态-价值函数用于估计在为所有可能的状态 $s \in S$ 选择动作 $a$ 时的价值，形式化为：
$$
v = E[Q_{s, a} | S_t = s]
$$</p>
<p>$$
Q_{s, a} = R^a_s + \gamma \sum_{s' \in S} P_{s, a, s'} v
$$</p>
<p>其中：$\gamma$ 是奖励参数。</p>
<p>最终的目标是通过计算每个 $P_{s, a, s'}$ 找到最大的 $max(Q_{s, a})$。</p>
<p>而这一过程很耗费时间，因此使用修改之后的<code>Q-learning</code>过程，用贪心的方式来解决最优化问题。</p>
<p><code>Q-learning</code>过程在直播推流中有别于传统点播中的应用：</p>
<ol>
<li>预测同时基于当前的输入（目标追踪和<code>FoV</code>估计的结果）和历史状态（分块是否被选择）；</li>
<li>奖励基于用户的反馈在线生成，并且会在整个推流会话中变化，而不是预先设定好的奖励矩阵 $R$ ；</li>
<li>由于直播推流中内容的不可提前获取性， $Q$ 表必须在每次预测中更新；</li>
</ol>
<p>特别的，为每个分块都创建一个 $Q$ 表，对于每个 $Q$ 表有4种类型：</p>
<ul>
<li><em>object only</em>;</li>
<li><em>object and viewport</em>;</li>
<li><em>viewport only</em>;</li>
<li><em>no objects or viewport</em>;</li>
</ul>
<p>将这4种类型和2种中历史状态（选中或未选中）组合之后，得到每个表中状态 $s$ 的8个选项组合；</p>
<p>对每个状态而言，有2种动作（选中或不选中），因此每个表有8个状态和2个动作。</p>
<p>对每个表的奖励基于用户是否看到了分块而更新。</p>
<p>基于状态 $s$ 的对动作 $a$ 的选择转化成了：在相同输入的情况下找到 $max(Q(s, s'))$；</p>
<p></p>
<h1 id="liveroi">LiveROI</h1>
<p><code>LiveObj</code>的基础是对象检测算法，用于分析视频内容的敏感性。但是其检测性能可能会受到算法、对象的缩放程度和全景视频导致的扭曲失真的影响，进而引起预测误差。类似于<code>LiveObj</code>的出发点，<code>LiveROI</code>的目标是通过使用动作识别来对视频内容进行分析，这会降低预测性能与前面所提因素的敏感性。</p>
<p>使用<code>3D-CNN</code>等预先训练的模型来分析每个分块上的视频内容，以完成动作识别。同时基于<code>NLP</code>技术，使用轻量级用户模型将用户偏好映射到不同的视频内容。</p>
<h2 id="用户对视频内容的偏好">用户对视频内容的偏好</h2>
<p>最基本的研究问题是：找到直播视频内容中的有效特征和信号或用户的行为，这些与用户的未来的<code>FoV</code>有强相关关系，因此可以将其作为预测因子。</p>
<p>通过对两个固定主题的视频的实验可以得出：</p>
<ol>
<li>用户花绝大多数的时间在视频中有意义的部分；</li>
<li><code>ROI</code>在空间上只占整个帧很小的部分；</li>
</ol>
<h2 id="liveroi-method">LiveROI Method</h2>
<p>融合视频内容感知和用户偏好反馈（即以用户头部运动轨迹的形式）来预测实时VR视频流中的<code>FoV</code>。</p>
<p>主要想法是使用CV算法去理解每个分块的内容，除此之外，采用实时的用户反馈方便分块的选择。</p>
<p>需要满足的条件是：所有分块上的视频处理开销应该保持较小，以避免视频冻结和累计的实时延迟。</p>
<p>使用<code>3D-CNN</code>进行视频理解，重点是识别视频中隐含的有意义的动作，动作识别结果用于以自然语言的格式描述视频内容。这种3D-CNN模型可以在公共数据集上进行训练，因此具有通用性，以适应各种类型的动作和视频，这使得它可以用于实时VR流传输，因为在流传输会话之前没有关于视频内容的先验知识。</p>
<p>但是具有有意义动作的区域可能不是用户最后会确定的<code>FoV</code>，尤其是在目标视频中存在多个有意义动作的情况下。</p>
<p>为了解决这一问题，通过收集用户关于偏好视频内容的实时描述，进一步设计了基于“词/短语”的用户偏好模型。</p>
<p>采用<strong>词语嵌入</strong>的方法，通过比较两个来源短语的语义相似度，确定最佳匹配区域作为预测<code>FoV</code>，以此来桥接动作识别结果和用户偏好模型。</p>
<h3 id="workflow">Workflow</h3>
<p></p>
<p><code>3D-CNN</code>的输入数据包含一批 $T$ 张图像，因此统一在一个视频片段中子采样 $T$ 帧。</p>
<p>每个子采样的帧都划分成 $M \times N$ 个分块，VP问题定义为确定要包含在<code>FoV</code>中的分块。</p>
<p>为了避免由于分块带来的潜在的信息损失（有意义的动作被划分成多个分块），每个用于动作识别的输入图像是从比原始分块边界更大的区域中所提取出来的，但是将共享与原始分块相同的中心。</p>
<p><code>3D-CNN</code>模块的输出是动作识别结果，即结果矩阵。</p>
<p>面对 $M \times N$ 个分块，为了满足性能要求，将每个分块的动作识别过程视为相互独立的过程，创建 $m \times n$ 个线程来实现并行识别，每个线程向结果矩阵输出对应分块的结果向量。</p>
<p>在预测的最后一步，生成包含所有分块的预测分数的得分向量。进一步对所有的分数向量进行排序，并定位第 $M$ 个值，该值设定为选择分块进入预测<code>FoV</code>中的阈值。通过控制 $M$ 的大小可以控制预测的<code>FoV</code>的大小，分数向量中的分数表示用户对分块内容的感兴趣程度。</p>
<p>为了计算分数向量，进一步设计用户向量，其中包含描述用户偏好的词或短语。考虑到推流过程中用户可能会改变兴趣，用户向量会基于用户实时轨迹更新。</p>
<p>在给定用户向量和结果矩阵中的词或短语的情况下，考虑到非自然语言中的两个不同的词可能具有相近的含义，不直接进行词比较，而是使用词分析来计算其相关性。</p>
<h3 id="cnn-model">CNN Model</h3>
<p>采用<code>ECO lite</code>模型完成VR直播推流中的动作识别。所有来自同一视频片段的图像都被储存在一个缓冲帧集合中。</p>
<p><code>ECO lite</code>模型为2D CNN提取特征图的任务收集工作帧集合（分别由前一视频片段和当前视频片段的缓冲帧集合的后半部分和前半部分组成），在下一个阶段，从每个片段获得的特征图被堆叠到更高的表示中，之后被送到之后的3D CNN中用于最终的动作预测。具体的识别过程中同样使用多线程并行处理，处理1帧图像是每次创建和分块数相同的线程，为每个分块都初始化一个<code>ECO lite</code>模型。</p>
<p>显然预训练的模型不能为直播推流提供正确的推理结果，但是它可以看作是对视频内容的验证，即：给定一种类型的视频内容，其实其本身被误分类了，但在同一个模型之下它总是会被分类进在整个推流过程中都有相近分数的簇中。</p>
<p>利用这个特性，基于动作识别模型提供的对视频内容的描述，进一步设计动态的用户模型来映射用户偏好到不同的视频内容上。</p>
<h3 id="nlp-model">NLP Model</h3>
<p>为了桥接动作识别和用户偏好向量，必须分析词/短语之间的相似性。</p>
<p>然而现有的ML算法不能直接处理生数据，因为输入必须是数值。为了解决这个问题，采用单词嵌入技术，使用多种语言模型以数值向量的形式来表示单词，以此来确保有相近意义的词有相近密度的表示。</p>
<p>具体处理时使用<a href="https://github.com/artetxem/phrase2vec" target="_blank" rel="noopener noreffer"><code>Phrase2Vec</code></a>作为NLP模块的模型（作为<code>Word2Vec</code>的扩展，能更好的分析两个短语之间的相似性）。</p>
<h2 id="用户模型与预测">用户模型与预测</h2>
<p></p>
<p>图5.3阐明了基于结果向量和用户向量的预测过程。由动作识别得出的结果向量，包括一个动作向量 $A$ 和一个权重向量 $W$ 。用户向量包括偏好向量 $P$ 和可能性向量 $L$ 。$A$ 和 $P$ 包含词和短语，描述了视频内容和用户偏好。 $W$ 和 $L$ 分别由表示神经网络对动作结果的置信度和用户对视频内容的参考可能性的值组成。</p>
<p>假设每帧25个分块，CNN模块的输出结果是25个 $A$ 向量和25个 $W$ 向量；对与用户偏好，只使用1个 $P$ 向量和1个 $L$ 向量。</p>
<p>最终的分数向量 $S$ 计算为每个 $A$ 和 唯一的 $P$ 之间的相关性。结果也受相应的 $W$ 和 $L$ 的影响而调整。</p>
<p>假设余弦相似性函数为 $\rho$ ，那么 $A$ 和 $P$ 中的每个 $a_i$ 和 $p_i$ 的计算可以表示为：
$$
{\rho}_i (a_i, p_i) = Phrase2Vec(a_i, p_i)
$$
设定每个向量中包含5个元素，分数向量 $S$ 计算为：
$$
S = L \cdot W \cdot \sum {\rho} (A, P)
$$
对应于25个分块，最终的分数向量中包含25个元素。 $s_k$ 表示 $k_{th}$ 分块的分数值，详细算法：</p>
<p></p>
<p>分数向量更新完毕之后就可以获得每个分块内容和用户偏好之间的相关性，用帧上每个分块的亮度来做可视化：</p>
<p></p>
<p>将分数向量中的元素从高到低排序，选定 $\frac{1}{3}$ 作为阈值，将前 $\frac{1}{3}$ 的分块看作相同的分数等级作为最后的预测区域。</p>
<p>为了应对推流过程中用户偏好的变化，为分数向量的计算设计动态加权的用户偏好向量。</p>
<p>设定用户偏好向量 $P$ 的大小与动作向量 $A$ 的大小相同，一旦系统获取到用户实际的<code>FoV</code>位置，就计算其视野中心并定位到相应的分块，使用前一视频片段中该选中分块的动作向量 $A'$ 来更新用户的偏好向量。</p>
]]></description>
</item>
<item>
    <title>Content Based VP for Live Streaming (1)</title>
    <link>https://ayamir.github.io/posts/note-for-content-based-vp-for-live-streaming-1/</link>
    <pubDate>Sat, 22 Jan 2022 18:03:09 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-content-based-vp-for-live-streaming-1/</guid>
    <description><![CDATA[<h1 id="livemotion">LiveMotion</h1>
<h2 id="motivation">Motivation</h2>
<p>基于视频中物体的运动模式来做对应的<code>FoV</code>预测。</p>
<p>将用户的<code>FoV</code>轨迹与视频内容中运动物体的轨迹结合到一起考虑：</p>
<p></p>
<p>细节可以参见：<a href="https://ayamir.github.io/posts/note-for-content-motion-viewport-prediction/" target="_blank" rel="noopener noreffer">note-for-content-motion-viewport-prediction</a>.</p>
<h1 id="livedeep">LiveDeep</h1>
<p>受限于<code>Motion</code>识别算法，前面提出的<code>LiveMotion</code>只能作用于有清晰并且容易分别的前景背景边界的视频，其健壮性并不能满足全景直播推流的场景。</p>
<h2 id="method">Method</h2>
<p><code>LiveDeep</code>处理问题的场景为：</p>
<ol>
<li>视频内容在线生成；</li>
<li>没有历史用户数据；</li>
<li>预测需要满足实时性的要求；</li>
</ol>
<p><code>LiveDeep</code>的设计原则：</p>
<ol>
<li><em>online</em>：在线训练在线预测；</li>
<li><em>lifelong</em>：模型在整个视频播放会话中更新；</li>
<li><em>real-time</em>：预测带来的处理延迟不能影响推流延迟；</li>
</ol>
<p><code>CNN</code>的设计：</p>
<ol>
<li>在推流会话的运行时收集并标注训练数据；</li>
<li>以交替迭代的方式进行基于当前视频片段的推理和基于之前视频片段的训练；</li>
<li>子采样少部分的代表帧来运行VP以满足实时性的要求；</li>
</ol>
<h2 id="framework">Framework</h2>
<p></p>
<h3 id="setup">Setup</h3>
<ol>
<li>分包器将视频按照DASH标准将视频分段，每个段作为训练模型和预测的单元；</li>
<li>考虑到不同的视频可能具有不同的帧速率，在每个单元中统一采样 $k$ 帧而非以固定的采样率采样；</li>
<li>将每帧图像划分成 $x \times y$ 个分块，最终每个单元中要处理的分块数为 $k \times x \times y$ ；</li>
<li>训练集来自于用户的实时反馈，根据实际<code>FoV</code>和预测<code>FoV</code>之间的差距来标注数据；</li>
<li>用户的轨迹数据来自于用户的实时头部轨迹，采样的帧与<code>CNN</code>模块采样的帧同步；</li>
</ol>
<h3 id="details">Details</h3>
<ol>
<li>在用于训练的图像还没有被标注之前并不能直接预测，所以CNN模块只能以随机的权重给出预测结果。用预测结果与实际结果计算出损失值之后以此来更新CNN模型；</li>
<li>LSTM模型只能以用户观看到视频之后的实际轨迹作为训练的输入输入数据；</li>
<li>对下一个片段而言，首先使用两个模块独立做出预测。每个模块的预测都基于子采样之后的 $k$ 个帧；</li>
<li>为了产生对整个片段的预测结果，假设相邻的帧之间共享相同的视野中心（时空局部性）；</li>
<li>取两个模块预测输出的共同的部分作为最终的预测结果；</li>
</ol>
<h2 id="cnn-module">CNN Module</h2>
<p></p>
<p>使用经典的CNN：VGG作为骨干网络，修改最后一层，只输出两类：感兴趣的和不感兴趣的。</p>
<h3 id="推理和视口生成">推理和视口生成</h3>
<p>直观上的想法是选择被分类为感兴趣的部分，并且这些所选部分在原始帧中的位置将指示其他帧中可能感兴趣的<code>FoV</code>。</p>
<p>实际上存在的问题是：几乎所有的部分都被分类为感兴趣的一类，最终结果是整个帧被选择作为预测的结果。</p>
<p>所以不直接使用CNN网络的输出，而是在被分类为感兴趣的部分中进一步细分。通过对输出的分数排序并选择前 $M$ 份比例的输出作为最终的结果，这样通过控制 $M$ 的大小可以调整精度和消耗的带宽。</p>
<h3 id="训练过程">训练过程</h3>
<p>在传统的监督训练中，训练时间取决于可接受的最低损失值和epoch的值。为了满足实时性，<code>LiveDeep</code>采用较高的最低损失值和较低的最大epoch值。</p>
<ul>
<li>
<p><em><strong>High acceptable loss value</strong></em>：因为直接对从被分类为感兴趣的部分中去获取最终结果，所以通过实验证明，损失值应该要比常规的CNN更高：设定为0.2。</p>
</li>
<li>
<p><em><strong>The number of epochs</strong></em>：因为直播推流的特殊性，重复的训练并不能持续降低损失，所以采用较小的值：10。</p>
</li>
<li>
<p><em><strong>The batch size</strong></em>：受限于训练的图像，将其设定为训练图像的个数即： $k \times x \times y$。</p>
</li>
<li>
<p><em><strong>Dynamic learning rate</strong></em>：</p>
<p></p>
</li>
</ul>
<h2 id="lstm-module">LSTM Module</h2>
<p>单纯的<code>CNN</code>模型可能会导致对视频内容有强记忆性，而这会使模型在面对新视频内容时需要花较长的时间去接受用户偏好，即对于用户偏好的快速切换不能做出即时响应。而<code>LSTM</code>的模块用于弥补这一缺陷；</p>
<p>采用与原始的<code>LSTM</code>模型相同的训练过程：先用收集的训练数据训练模型然后推断未来的数据。</p>
<p>收集用户在过去的视频片段中的用户轨迹，包括从 $k$ 个子采样帧中的 $k$ 个采样点，因此作为训练数据，同时将每个采样点中每个帧的索引指定为时间戳。最终模型的输出是预测出的分块的索引。</p>
<h2 id="混合模型">混合模型</h2>
<p>将<code>CNN</code>模块得到的输出作为主要的结果，接着结合<code>LSTM</code>模块的输出结果作为最终的预测结果。</p>
]]></description>
</item>
<item>
    <title>Note for Popularity Aware 360-Degree Video Streaming</title>
    <link>https://ayamir.github.io/posts/note-for-popularity-aware-360-degree-video-streaming/</link>
    <pubDate>Tue, 18 Jan 2022 16:07:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-popularity-aware-360-degree-video-streaming/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/abstract/document/9488856/" target="_blank" rel="noopener noreffer">Popularity-Aware 360-Degree Video Streaming</a></p>
<p>Level：IEEE INFOCOM 2021</p>
<p>Keywords：Dynamic tiling, Cross-user division, Heuristic QoE optimization</p>
<h2 id="motivation">Motivation</h2>
<p>将视频划分成分块进行编码之后，会降低编码效率，并增大服务端的存储压力。（细节可以参见<a href="https://ayamir.github.io/posts/note-for-optile/" target="_blank" rel="noopener noreffer">Optile</a>）</p>
<p>而分块时根据用户的ROI来确定不同的大小，并在客户端预取，这可以节省带宽。</p>
<p>用户的ROI推断利用跨用户的偏好来确定，即所谓的<code>Popularity-Aware</code>。</p>
<h2 id="model-and-formulation">Model and Formulation</h2>
<h3 id="video-model">Video Model</h3>
<p>视频从时间上被分成固定长度的片段，接着每个片段被从空间上划分成 $C$ 个分块。</p>
<p>除了常规的分块之外， $M$ 个宏块也被建构出来。</p>
<p>每个常规分块和宏块都被编码成 $V$ 个不同的码率质量等级并存储在服务端。</p>
<p>整个推流过程可以看作是一系列连续的下载任务。</p>
<p>客户端在每次下载任务中的目标是：选择恰当分块（宏块或者常规分块的集合）的恰当质量。</p>
<p>用 $L$ 表示客户端请求分块时，缓冲区中已经下载但还没有查看的视频的视频长度，为了避免缓冲事件，分块需要在缓冲区被清空即 $L = 0$ 之前被下载完毕。</p>
<h3 id="qoe-model">QoE Model</h3>
<p>$$
Q(V_k) = Q_{0}(V_k) - {\omega}_v I_v (V_k) - {\omega}_r I_r (V_k)
$$</p>
<p>$V_k$ 表示下载的第 $k$ 段视频质量； $Q_0$ 表示平均质量； $I_v$ 表示由质量变化导致的质量损害； $I_r$ 表示由缓冲事件导致的质量损害； ${\omega}_v$ 和 ${\omega}_r$ 分别表示质量变化和缓冲的加权因子；</p>
<ul>
<li>
<p>平均质量：
$$
Q_0(V_k) = q(\overline{V_k})
$$
$\overline{V_k}$ 表示<code>FoV</code>内的平均视频质量； $q(\cdot)$ 表示视频质量和用户实际感知质量之间的映射函数；</p>
</li>
<li>
<p>质量变化：两个连续段之间的质量差异和<code>FoV</code>内不同空间位置tile的质量差异会导致用户不适。
$$
I_v(V_k) = |Q_0(V_k) - Q_0(V_{k-1})| + \widehat{V_k}
$$
$|Q_0(V_k) - Q_0(V_{k-1})|$ 表示连续段间的<code>FoV</code>内时间质量差异； $\widehat{V_k}$ 表示一个视频段的<code>FoV</code>内空间质量差异；</p>
</li>
<li>
<p>缓冲：
$$
L_r(V_k) = {(\frac{S(V_k)}{R} - L, 0)}_+
$$
$S(V_k)$ 表示段数据量大小； $R$ 表示下载吞吐量； ${(x)}_+ = max \lbrace x, 0 \rbrace$ ；</p>
</li>
</ul>
<h3 id="formulation">Formulation</h3>
<p>用 ${\beta}^v_m ({\beta}^v_c)$ 表示对应的宏块或常规块是否被下载：</p>
<p>${\beta}^v_m = 1$ 表示下载编码的质量等级为 $v$ 的宏块，消耗的带宽为 $B^v_m$ ，反之 $ {\beta}^v_m = 0$ 表示不下载；</p>
<p>${\beta}^v_c = 1$ 表示下载编码的质量等级为 $v$ 的常规块，消耗的带宽为 $B^v_c$，反之 ${\beta}^v_m = 0$ 表示不下载；</p>
<p>客户端应该优先下载覆盖用户<code>FoV</code> 的宏块，如果没有这样的宏块则去下载对应的常规块的集合。</p>
<p>优化目标：
$$
max\ Q(\lbrace v | {\forall}_{m, v} {\beta}^v_m = 1 \rbrace) + Q(\lbrace v | {\forall}_{c, v} {\beta}^v_c = 1 \rbrace)
$$
同时需要满足以下3个约束：
$$
\sum^{M}_{m=1} \sum^{V}_{v=1} {\beta}^v_m + 1(\sum^{C}_{c=1} \sum^{V}_{v=1} {\beta}^v_c) = 1
$$</p>
<p>$$
\sum^{V}_{v=1} {\beta}^v_c \le 1,\ for\ c = 1, &hellip;, C
$$</p>
<p>$$
\sum^{M}_{m=1} \sum^{V}_{v=1} {\beta}^v_m B^v_m + \sum^{C}_{c=1} \sum^{V}_{v=1} {\beta}^v_c B^v_c \le R \cdot L
$$</p>
<p>$Q(\cdot)$ 是公式1中定义的质量； $R$ 是网络带宽； $1(x) = 1 \iff x &gt; 0$ ；$1(x) = 0 \iff x \le 0$ ；</p>
<p>约束1强制为观看区域下载宏块或常规块的集合，只下载宏块的一个质量版本；</p>
<p>约束2规定只下载常规块的一个质量版本；</p>
<p>约束3保证视频数据可以在开始播放之前被完全下载；</p>
<p>给出用户的观看区域之后，候选的宏块或对应的常规块集合也可以求出。</p>
<p>将<code>QoE</code>最大化的问题分解成两个子问题：</p>
<ul>
<li>确定宏块的质量等级；</li>
<li>确定常规块的质量等级；</li>
</ul>
<p>最后的解取这两种方案能取得更大<code>QoE</code>的那种。</p>
<p>如果<code>QoE</code>模型不考虑常规块之间的质量差异，则整体的<code>QoE</code>等价于下载的常规块的平均质量等级。</p>
<p>确定常规块质量等级的问题则可以简化为：
$$
max\ \sum_{c \in C} \sum^{V}_{v=1} Q({\beta}^v_c v)
$$
需要满足以下2个约束：
$$
\sum^{V}_{v=1} {\beta}^v_c = 1,\ for\ c \in C
$$</p>
<p>$$
\sum_{c \in C} \sum^{V}_{v=1} {\beta}^v_c B^v_c \le R \cdot L
$$</p>
<p>$C$ 表示覆盖观看区域的常规块集合。</p>
<p>简化之后的子问题可以通过对多项选择背包问题的简化，证明为是<code>NP-hard</code>问题，基于此提出启发式算法。</p>
<h2 id="基于宏块的流行性感知推流">基于宏块的流行性感知推流</h2>
<h3 id="基于观看区域确定宏块">基于观看区域确定宏块</h3>
<p>不同用户对相同视频的观看有着相似的ROI，其视野中心是相近的，因此首先确定其视野中心并聚类到一起。</p>
<p>不能直接应用的知名聚类算法：</p>
<ul>
<li>需要事先确定簇（即宏块）数量的算法（事先并不能确定需要多少宏块）：<code>K-means</code></li>
<li>簇会越聚越大的算法（这样会失去节约带宽的优点）：<code>DBSCAN</code></li>
</ul>
<p>提出的算法用2个参数 $\lambda$ 和 $\gamma$ 来保证彼此相近的两个视野中心被归入同一簇，同时基于簇的宏块不至于太大。</p>
<ul>
<li>被归入同一簇的视野中心之间的距离应该小于等于 $\lambda$；</li>
<li>同一个簇的任意两个视野中心之间的距离应该小于等于 $\gamma$；</li>
</ul>
<p>为了确定这两个参数，还需要考虑常规块的大小带来的影响。</p>
<p>算法描述：</p>
<p>给出用 $P$ 表示的点集，其中每个点表示一个用户的视野中心位置；</p>
<p>用 $N_p = \lbrace q | q \in P \land q \neq p \land dist(p, q) \le \lambda \rbrace$ 来表示与点 $p$ 之间欧式距离小于 $\lambda$ 的点集（即为临近点集）；</p>
<ol>
<li>初始化拥有最多临近点的点所在的簇，例如： $p = {argmax}_{p \in P} |N_p|$；</li>
<li>添加临近簇内任何点的点到簇中，扩张过程直到找不到符合条件的点位置；</li>
<li>检查簇中任意两个点之间的距离是否大于 $\gamma$ ，如果存在这种情况就使用<code>K-means</code>算法将这个簇分成两个子簇；</li>
<li>从 $P$ 中移除簇中的点；</li>
<li>重复1-4的过程直到 $P = \empty$；</li>
</ol>
<p></p>
<h3 id="宏块优化">宏块优化</h3>
<p>通过简单地覆盖簇中用户的所有观看区域来为每个簇建构宏块可能会导致建构出不必要的大宏块，因此需要确定恰当的宏块大小。</p>
<ul>
<li>
<p>首先需要确定哪些用户的观看区域应该被用于构建宏块，这样用户下载宏块时的带宽使用率小于下载一组常规块时的带宽使用率：$B_m$ 和 $B_c$ 分别表示覆盖相同观看区域的宏块和常规块的数据量大小。</p>
</li>
<li>
<p>为了解决用户头部运动的随机性，宏块应该在覆盖用户观看区域之外加上一些边界区域。边界区域可以基于用户观看中心的变化来确定，变化通过在推流观看过程中以固定采样率记录。</p>
<p>一个视频片段中 $x(y)$ 坐标的变化定义为 $x(y)$ 坐标的标准差。</p>
<p></p>
<p>实验发现：在一个视频片段中，用户的 $x(y)$ 坐标的变化很小。</p>
<p>分别用 $A_x$ 和 $A_y$ 表示 $x$ 和 $y$ 方向上的变化，构建的宏块应该覆盖用户的观看区域，并为 $x(y)$ 方向加上 $\frac{A_x}{2}(\frac{A_y}{2})$ 的边缘区域。</p>
</li>
</ul>
<p>宏块构造问题的形式化：</p>
<p>为每个用户 $i$ 引入二元变量 ${\alpha}_i$ ，${\alpha}_i = 1$ 表示此用户的观看区域用于构建宏块，反之则没有；</p>
<p>实际应用中即为：如果 ${\alpha}_i = 1$ ，则用户 $i$ 可以下载宏块；否则用户只能下载对应的常规块集合。</p>
<p>问题的目标是：在下载宏块或相同质量等级的常规块集合时，最小化所有用户的总带宽使用量。
$$
\underset{\lbrace {\alpha}_i \rbrace}{min}\ \sum^{N_j}_{i=1} {\alpha}_i B_m + (1-{\alpha}_i) B_c
$$
$N_j$ 表示在 $j^{th}$ 簇中的用户数量；解决问题之后，可以用所有 ${\alpha}_i = 1$ 的用户观看区域构建宏块；</p>
<p>尽管暴力枚举法可以完成最优求解，但是其时间复杂度为 $O(2^{N_j})$ ，为了减少实际建构宏块的时间，提出一种类似于<a href="https://en.wikipedia.org/wiki/Random_sample_consensus" target="_blank" rel="noopener noreffer">随机采样一致性算法</a>的迭代算法，每次迭代中，所做工作如下：</p>
<ol>
<li>随机选取用户观察区域的子集。</li>
<li>编码宏块，用 $B_m$ 表示构建的宏块的带宽使用量。</li>
<li>检查建构的宏块是否覆盖用户 $i \in \lbrace 1, &hellip;N_j \rbrace$ ，是则${\alpha}_i = 1$；否则 ${\alpha}_i = 0$。</li>
<li>检查总共的带宽使用量是否比之前迭代的更小，是则用当前迭代建构的宏块更新最终的宏块；否则继续迭代。</li>
</ol>
<p>为了避免预测失败时用户看到空白区域，在下载观看区域的高质量宏块或常规块集合之外，也以最低质量下载其余的常规块。</p>
<h3 id="流行性感知推流">流行性感知推流</h3>
<p>服务端基于多个用户的历史观看信息建构宏块，同时也使用常规块的划分方案编码视频。</p>
<p>客户端在推流过程中选择恰当块（宏块或常规块集）的恰当的质量等级来最大化用户的<code>QoE</code>。</p>
<p>流行性感知的推流算法首先为每个视频段预测用户的观看区域，之后预取相应的宏块或常规块集。</p>
<ul>
<li>使用<a href="https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db" target="_blank" rel="noopener noreffer">岭回归</a>做VP，输入用户在一系列历史帧中的观看区域中心坐标，输出未来帧中用户的观看区域位置。</li>
<li>基于预测的观看区域，算法确定是否存在覆盖预测区域及其边缘区域的宏块，是则搜索并下载满足条件的最高质量的宏块；否则下载相应区域的常规块集。</li>
</ul>
<p></p>
<p>选择常规块集时首先为所有要选择的块确定满足贷款限制的最高质量等级，分配完之后如果还有剩余的带宽，算法会根据常规块与视野中心距离的远近程度提高一个质量等级，越近越优先提高。同时考虑到空间质量差异会降低<code>QoE</code>，所以提高质量的行为只有在超过半数的常规块满足条件时才会执行。</p>
]]></description>
</item>
<item>
    <title>Summary for VR and Panoramic Video</title>
    <link>https://ayamir.github.io/posts/summary-for-vr-and-panoramic-video/</link>
    <pubDate>Mon, 17 Jan 2022 17:02:51 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/summary-for-vr-and-panoramic-video/</guid>
    <description><![CDATA[<p>VR和360度全景视频都是获得沉浸式体验的重要途径，除此之外，AR（Argmented Reality）和MR（Mixed Reality）也是比较火的概念，可以用来对比学习。</p>
<h2 id="全景视频">全景视频</h2>
<ol>
<li>全景视频实际上事先通过特殊的全景摄像机录制好视频，之后可以在<code>HMD</code>中观看。虽然看到的图像相对于用户当前环境而言是虚拟的，但是终归是从实际环境中录制而来的，本质上更贴近普通视频的全景推广。</li>
<li>在全景视频的观看过程中，用户只有3DoF的自由度，即只能完成头部的3个角度的运动，同时手柄实际上并不能和视频中的内容进行交互。</li>
<li>全景视频的主要应用在于实景导览，通过事先由拍摄者带着全景录像设备行走拍摄，用户观看时实际是将自己带入到全景设备的位置上，同时移动头部来观察不同角度的视频。</li>
</ol>
<h2 id="vr">VR</h2>
<ol>
<li>VR主要做的工作是创造出一个完全虚拟的环境，用户戴上<code>HMD</code>之后可以通过其看到虚拟环境中的事物，同时也可以使用<code>HMD</code>配套的手柄等设备进行操作，完成与虚拟环境之间的交互；</li>
<li>VR支持的是6DoF的自由度，即除了头部的运动之外也支持身体的前后、左右、上下的移动，手柄；</li>
<li>VR的主要应用在于游戏，比如广受好评的<code>Beat Saber</code>（又称<a href="https://zh.wikipedia.org/zh-cn/%E8%8A%82%E5%A5%8F%E5%85%89%E5%89%91" target="_blank" rel="noopener noreffer"><code>节奏光剑</code></a>），用户根据音乐节奏通过挥动手柄（在虚拟环境中被建模成光剑）来准确地按照提示的方向去砍击方块；</li>
</ol>
<h2 id="ar和mr">AR和MR</h2>
<ol>
<li>
<p>AR主要做的工作是将虚拟世界中的事物投影到现实世界中，主体是现实世界，虚拟事物用于增强现实世界。</p>
<p>MR主要做的工作是将现实世界中的事物虚拟化进入虚拟世界中，主体是虚拟世界，现实事物混合进虚拟世界中。</p>
</li>
<li>
<p>AR实现起来比较简单，只需要将计算机产生的图像投影显示在现实中即可，目前的应用比如游戏<code>Pokémon GO</code>里面的<code>AR-mode</code>，启用之后游戏中遇到的<code>Pokémon</code>就可以投影在现实中。</p>
<p>MR实现起来比较复杂，首先需要用摄像头扫描物体，得到的2D图像再交给计算机采用算法进行3D重建，最后将虚拟化建模好的物体展示到虚拟世界中，目前的应用比如<code>Meta</code>推出的<code>Workrooms</code>，线上的远距离视频会议在虚拟世界中可以变成虚拟人物之间面对面的交流。</p>
</li>
</ol>
<h2 id="总结">总结</h2>
<ol>
<li>
<p>全景视频侧重于对虚拟环境的观察，而VR侧重于对虚拟环境的交互。</p>
</li>
<li>
<p>全景视频实际上是将用户带入到全景摄像机的位置上，让用户产生自己身临拍摄的环境中的感觉，本质上是对传统视频的推广；</p>
<p>VR实际上是将用户完全带入到虚拟的环境中，用户可以和虚拟环境中的事物进行交互，而虚拟环境中发生的一切都和现实无关，本质上是对传统游戏的推广；</p>
</li>
<li>
<p>全景视频实际上和VR、AR、MR这种概念距离比较远，实际上只是因为全景摄像机相较于普通摄像机的360度视角的特殊性，这能让用户产生沉浸感。</p>
</li>
<li>
<p>VR相比于AR、MR而言，是纯粹的虚拟环境，并不涉及到现实事物（除了<code>HMD</code>配套的手柄等设备），而纯粹的虚拟环境将人带入到了一个完全不同的世界，也是VR沉浸式体验的来源。</p>
</li>
<li>
<p>AR和MR是虚拟和现实交融的技术，前者主体是现实，后者主体是虚拟环境。</p>
</li>
</ol>
<p></p>
]]></description>
</item>
<item>
    <title>Note for srlABR Cross User</title>
    <link>https://ayamir.github.io/posts/note-for-srlabr-cross-user/</link>
    <pubDate>Sat, 15 Jan 2022 18:46:02 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-srlabr-cross-user/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/9234071" target="_blank" rel="noopener noreffer">Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network</a></p>
<p>Level：IEEE Transactions on Broadcasting 2021</p>
<p>Keywords：Cross-user vp, Sequetial RL ABR</p>
<h2 id="主要工作">主要工作</h2>
<ul>
<li>使用跨用户注意力网络<code>CUAN</code>来做VP；</li>
<li>使用<code>360SRL</code>来做ABR</li>
<li>将上面两者集成到了推流框架中；</li>
</ul>
<h2 id="vp">VP</h2>
<h3 id="motivation">Motivation</h3>
<p>形式化VP问题如下：</p>
<p>给出 $p^{th}$ 用户的 $1-t$ 时间内的历史视点坐标 $L^{p}_{1:t} = \lbrace l^p_1, l^p_2, &hellip;, l^p_t \rbrace$ ，其中 $l^p_t = (x_t, y_t), x_t \in [-180, 180]; y_t \in [-90, 90]$ ；</p>
<p>同一视频的不同用户视点表示为 $L^{1:M}_{1:t+T}$ ， $M$ 表示其他用户的数量；</p>
<p>目标是预测未来的 $T$ 个时刻的视点位置 $L^p_i, i = t+1, &hellip;, t+T$ ；</p>
<p>最终可以用数学公式表达为：
$$
\underset{F}{min} \sum^{t+T}_{k = t+1} {\parallel l^p_k - \hat{l}^p_k \parallel}_1
$$</p>
<p>现有的用<code>KNN</code>做的跨用户预测基于LR的模型，而LR的模型很容易产生偏差，所以为了增强<code>KNN</code>的性能，同时考虑单用户的历史视点轨迹和跨用户的视点轨迹。</p>
<ul>
<li>提出一种注意力机制来自动提取来自其他用户视口的有用信息；</li>
<li>对于与当前用户有相似偏好的用户轨迹信息给与更多的注意；</li>
<li>相似性通过基于过去时间段内其他用户的轨迹计算出来；</li>
</ul>
<h3 id="design">Design</h3>
<p></p>
<ol>
<li>
<p>轨迹编码器模块从用户的历史视点位置提取时间特征；</p>
<p>使用<code>LSTM</code>来编码用户的观看路径；</p>
<p>为了预测 ${(t+1)}^{th}$ 帧的视点位置，首先向<code>LSTM</code>输入 $p^{th}$ 用户的历史视点坐标：
$$
f^{p}_{t+1} = h(l^p_1, l^p_2, &hellip;, l^p_t)
$$
$h(\cdot)$ 是<code>LSTM</code>的输入输出函数；</p>
<p>接着使用相同的<code>LTSM</code>编码其他用户的观看轨迹：
$$
f^{i}_{t+1} = h(l^i_1, l^i_2, &hellip;, l^i_{t+1}), i \in \lbrace 1, &hellip;, M \rbrace
$$</p>
</li>
<li>
<p>注意力模块从其他用户的视点轨迹中提取与 $p^{th}$ 用户相关的信息</p>
<p>首先推导出 $p^{th}$ 用户和其他用户之间的相关系数：
$$
s^{pi}_{t+1} = z(f^{i}_{t+1}, l^{p}_{t+1}), i \in \lbrace 1, &hellip;, M \rbrace \cup \lbrace p \rbrace;
$$
$s^{th}_{t+1}$ 表示 $p^{th}$ 用户和 $i^{th}$ 用户之间的相似性；$z()$ 由内积运算建模（还可用其他方式建模比如多个FC层）；</p>
<p>接着将相关系数规范化：
$$
{\alpha}^{pi}_{t+1} = \frac{e^{s^{pi}_{t+1}}}{\sum_{i \in \lbrace 1,&hellip; M \rbrace \cup {\lbrace p \rbrace}^{e^{s^{pi}_{t+1}}}}}
$$
最后得到融合特征：
$$
g^{p}_{t+1} = \sum_{i \in {\lbrace 1,&hellip;M \rbrace \cup \lbrace p \rbrace}} {\alpha}^{pi}_{t+1} \cdot f^{i}_{t+1}
$$
融合特征被最后用于VP。</p>
</li>
<li>
<p>VP模块预测 ${(t+1)}^{th}$ 帧的视点位置</p>
<p>$$
\hat{l}^{p}_{t+1} = r(g^{p}_{t+1})
$$
函数 $r(\cdot)$ 由一层FC建模。值得注意的是，对应于未来 T 帧的视点是以滚动方式预测的。</p>
</li>
</ol>
<h3 id="loss">Loss</h3>
<p>损失函数定义为预测的视点位置和实际视点位置之间的所有绝对差异的总和：
$$
L = \sum^{t+T}_{i=t} {|\hat{l}^p_i - l^p_i|}_1
$$</p>
<h3 id="details">Details</h3>
<ul>
<li>使用<code>PyTorch</code>实现；</li>
<li>函数 $h(\cdot)$ 由两个堆叠的<code>LSTM</code>层组成，两者都有32个神经元；</li>
<li>函数 $r(\cdot)$ 包含一个带有32个神经元的FC层，接着是<code>Tanh</code>函数；</li>
<li>历史视点和未来视点的长度设定为1秒和5秒；</li>
<li>每次迭代从数据集中随机产生2048个样本；</li>
<li>所有训练变量的优化函数采用<code>Adam</code>；</li>
<li>$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$；</li>
<li>$learning\ rate = 10^{-3}, training\ epoch = 50$；</li>
</ul>
<h2 id="abr">ABR</h2>
<h3 id="formulation">Formulation</h3>
<p>全景视频被切分成 $m$ 个长度为 $T$ 秒的视频片段，每个视频片段空间上划分成 $N$ 个分块，分别以 $M$ 个不同的码率等级编码。因此对于每段有 $N \times M$ 个可选的编码块。</p>
<p>ABR的目标是为每个片段找到最优的码率集 $X = \lbrace x_{i, j} \rbrace \in Z^{N \times M}$ （ $x_{i, j} = 1$ 意味着为 $i^{th}$ 块选择 $j^{th}$ 的码率等级）：
$$
\underset{X}{max} \sum^{m}_{t=1} Q_t
$$
$Q_t$ 表示 $t^{th}$ 段的QoE分数，与以下几个方面有关：</p>
<ul>
<li>
<p>VIewport Quality：
$$
Q^1_t = \sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot p_i \cdot r_{i,j}
$$
$p_i$ 表示 $i^{th}$ 分块的规范化观看概率； $r_{i,j}$ 记录块 $(i, j)$ 的码率；</p>
</li>
<li>
<p>Viewport Temporal Variation：
$$
Q^2_t = |Q^1_t - Q^{1}_{t-1}|
$$</p>
</li>
<li>
<p>Viewport Spatial Variation：
$$
Q^3_t = \frac{1}{2} \sum^{N}_{i=1} \sum_{u \in U_i} p_i \cdot p_u \sum^{M}_{j=1} |x_{i,j} \cdot r_{i,j} - x_{u,j} \cdot r_{u,j}|
$$
$U_i$ 表示 $i^{th}$ 个分块的1跳邻居中的tile索引<a href="https://ieeexplore.ieee.org/document/8486606" target="_blank" rel="noopener noreffer">[1]</a>；</p>
</li>
<li>
<p>Rebuffering：
$$
Q^4_t = max(\frac{\sum^{N}_{i=1} \sum^{M}_{j=1} x_{i,j} \cdot r_{i,j} \cdot T}{\xi_t} - b_{t-1}, 0)
$$
$\xi_t$ 表示网络吞吐量； $b_{t-1}$ 表示播放器的缓冲区占用率；</p>
<p>最终的QoE可以由上面的指标定义：
$$
Q_t = Q^1_t - \eta_1 \cdot Q^2_t - \eta_2 \cdot Q^3_t - \eta_3 \cdot Q^4_t
$$
$\eta_*$ 是可调节的参数，与不同的用户偏好对应。</p>
</li>
</ul>
<h3 id="sequential-rl-based-abr">Sequential RL-Based ABR</h3>
<p>假设基于tile的全景推流ABR过程也是MDP。</p>
<p></p>
<p>细节在<a href="https://ayamir.github.io/posts/note-for-360srl/" target="_blank" rel="noopener noreffer">360SRL</a>中已经说明清楚。</p>
]]></description>
</item>
<item>
    <title>Note for 360SRL</title>
    <link>https://ayamir.github.io/posts/note-for-360srl/</link>
    <pubDate>Thu, 13 Jan 2022 12:08:36 &#43;0800</pubDate><author>miracle_l@bupt.edu.cn (Ayamir)</author><guid>https://ayamir.github.io/posts/note-for-360srl/</guid>
    <description><![CDATA[<h2 id="论文概况">论文概况</h2>
<p>Link：<a href="https://ieeexplore.ieee.org/document/8784927" target="_blank" rel="noopener noreffer">360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming</a></p>
<p>Level：ICME 2019</p>
<p>Keywords：ABR、RL、Sequential decision</p>
<h2 id="创新点">创新点</h2>
<ul>
<li>在MDP中，将N维决策空间内的一次决策转换为1维空间内的N次级联顺序决策处理来降低复杂度。</li>
</ul>
<h2 id="问题定义">问题定义</h2>
<p>原始的全景视频被划分成每段固定长度为 $T$ 的片段，</p>
<p>每个片段包含 $N$ 个分块，并以 $M$ 的码率等级独立编码，</p>
<p>因此对每个片段，有 $N \times M$ 种可选的编码块。</p>
<p>为了保证播放时的流畅性，需要确定最优的预取集合：</p>
<p>${a_0, &hellip;, a_i, &hellip;, a_{N-1}}, i \in \lbrace 0, &hellip;, N-1 \rbrace, a_i \in \lbrace 0, &hellip;, M-1 \rbrace $</p>
<p>分别用 $q_{i, a_i}$ 和 $w_{i, a_i}$ 表示码率选择为 $a^{th}_i$ 的 $i^{th}$ 分块的质量和相应的分块片段大小。</p>
<p>用 $p_i \in [0, 1]$ 表示 $i^{th}$ 块的被看到的可能性。</p>
<h2 id="顺序abr决策">顺序ABR决策</h2>
<p></p>
<h2 id="代理设计">代理设计</h2>
<h3 id="状态">状态</h3>
<p>对于 $i^{th}$ 维，输入状态包括原始的环境状态 $s_t$ ；</p>
<p>与之前维度的动作集合相关的信号： $u^{i}_{s_t} = \lbrace Th, C_i, p_{0:i-1}, q_{0:i-1}, b_t, p_i, S_i, Q_{t-1} \rbrace$</p>
<p>$Th$ ：表示过去 m 次下载一个段的平均吞吐量；</p>
<p>$C_i \in R^M$ ：表示 $i^{th}$ 个分块的可用块大小向量；</p>
<p>$p_{0:i-1}$ 和 $q_{0:i-1, a^{0:i-1}_{t}}$ 分别表示选中的码率集合和看到之前 $i-1$ 个分块的概率集；</p>
<p>$b_t$ 是缓冲区大小；</p>
<p>$p_i$ 是 $i^{th}$ 个分块被看到的可能性；</p>
<p>$S_i$ 是之前选择的 $i-1$ 个分块的块大小之和： $S_i = \sum^{i-1}_{h=0} C_{h, a^h_t}$ ；</p>
<p>$Q_{t-1}$ 记录了最后一个段中 $N$ 个分块的平均视频质量；</p>
<h3 id="动作">动作</h3>
<p>动作空间离散，代理输出定义为价值函数：$f(u^i_{s_t}, a^i_t)$</p>
<p>表示所选状态的价值 $a^i_t \in \lbrace 0, &hellip;, M-1 \rbrace$ 处于状态 $u_{s_t}^i$ .</p>
<h3 id="回报">回报</h3>
<p>回报定义为下列因素的加权和：</p>
<p>平均视频质量 $q^{avg}_t$，空间视频质量方差 $q^{s_v}_t$，时间视频质量方差 $q^{t_v}_t$ ，重缓冲时间 $T^r_t$</p>
<p>$$
q^{avg}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot q_{i, a_i}
$$</p>
<p>$$
q^{s_v}_t = \frac{1}{\sum^{N-1}_{i=0} p_i} \cdot \sum^{N-1}_{i=0} p_i \cdot |q_{i, a_i} - q^{avg}_t|
$$</p>
<p>$$
q^{t_v}_t = |q^{avg}_{t-1} - q^{avg}_t|
$$</p>
<p>$$
T^r_t = max \lbrace T_t - b_{t-1}, 0 \rbrace
$$</p>
<p>$$
R_t = w_1 \cdot q^{avg}_t - w_2 \cdot q^{s_v}_t - w_3 \cdot q^{t_v}_t - w_4 \cdot T^r_t
$$</p>
<h2 id="训练方法">训练方法</h2>
<p>使用<code>DQN</code>作为基本的算法来学习动作-价值函数 $Q(s_t, a_t; \theta)$ ，其中 $\theta$ 作为参数，对应的贪心策略为 $\pi(s_t; \theta) = \underset{\theta}{argmax} Q(s_t, a_t; \theta)$ 。</p>
<p><code>DQN</code>网络的关键想法是更新最小化损失函数的方向上的参数：
$$
L(\theta) = E[y_t - Q(s_t, a_t; \theta)]
$$</p>
<p>$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \pi(s_{t+1}; {\theta}'); {\theta}')
$$
${\theta}'$ 表示固定且分离的目标网络的参数；</p>
<p>$r(\cdot)$ 是即时奖励函数，即上面公式5中的 $R_t$ ；</p>
<p>$\gamma \in [0, 1]$ 是折扣因子；</p>
<p>为了缓解过拟合，引入 <code>double-DQN</code> 的结构，所以公式7被重写为：
$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, {\pi}(s_{t+1}; \theta); {\theta}')
$$
利用公式6和公式8可以得出 $i^{th}$ 维的暂时损失函数：
$$
l^i_t = Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$
其中 $Q_{target}$ 满足：</p>
<p>$$
Q_{target} = r_t + {\gamma}_u \cdot Q(u^0_{s_{t+1}}, \pi(u^0_{s_{t+1}}; 0); {\theta}')
$$</p>
<p>${\gamma}_u$ 和 ${\gamma}_b$ 分别代表”Top MDP“和”Bottom MDP“的折扣因子，训练中设定 ${\gamma}_b = 1$ 。</p>
<p>观察公式9和公式10可以看出每维都有相同的目标函数，意味着无法区别每个独立维度的动作 $a^i_t$ 对 $r_t$ 的贡献。</p>
<p>为了克服限制，根据某个分块的动作 $a^i_t$ 与其观看概率成正比的先验知识，向 $l^i_t$ 添加一个额外的 $r^i_{extra}$ ：
$$
l^i_t = r^i_{extra} + Q_{target} - Q(u^i_{s_t}, a^i_t; \theta), \forall i \in [0, &hellip;N-1]
$$</p>
<p>$$
r^i_{extra} =
\begin{cases}
0, p_i &gt; P ;
\
-a^i_t, p_i \le P
\end{cases}
$$</p>
<p>通过设定一个观看概率的阈值 $P$ ，对观看概率低于 $P$ 但选择了高码率的分块施加 $-a^i_t$ 的奖励。</p>
<p>因此最终的平均损失可以形式化为：
$$
l^{avg}_t = \frac{1}{N} \sum^{N-1}_{i=0} l^i_t
$$
接着使用梯度下降法来更新模型，学习率设定为 $\alpha$：
$$
\theta \larr \theta + \alpha \triangledown l^{avg}_t
$$
同时，在训练阶段利用经验回放法来提高<code>360SRL</code>的泛化性。</p>
<p></p>
<p></p>
<h2 id="实现细节">实现细节</h2>
<p></p>
<p>特征从输入状态中通过特征提取网络提取出来。</p>
<p>初始的4个输入通过带有128个过滤器的1维卷积层被传递，4个输入核心大小分别为 $1 \times m$ 、 $1 \times M$ 、 $1 \times N$ 、 $1 \times M$ ，后续这4个输入被喂给有128个神经元的全连接层；</p>
<p>随后特征映射被连接成一个张量，接着是具有1024个神经元和256个神经元的前向网络；</p>
<p>整个动作-价值网络的输出是M维的向量。</p>
<p>特征提取层和前向网络层都使用 <code>Leaky-ReLU</code>作为激活函数，最后是层归一化层。</p>
]]></description>
</item>
</channel>
</rss>

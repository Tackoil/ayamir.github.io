<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper on Ayamir&#39;s blog</title>
    <link>http://localhost:1313/categories/paper/</link>
    <description>Recent content in Paper on Ayamir&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 25 Apr 2024 19:02:12 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Note for DQB</title>
      <link>http://localhost:1313/posts/papers/note-for-dqb/</link>
      <pubDate>Sun, 20 Mar 2022 22:09:11 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-dqb/</guid>
      <description>整体概况 Link：Modeling the Perceptual Quality for Viewport-Adaptive Omnidirectional Video Streaming Considering Dynamic Quality Boundary Artifact Level：IEEE TCSVT 2021&#xA;DQB: Dynamic Quality Boundary，指在基于分块的 FoV 自适应全景视频推流过程中低质量分块区域的暴露和质量切换现象。&#xA;DQB 现象实际上就是 FoV 内分块间的质量差异和随时间变化的分块质量变化。 这篇论文主要的贡献在于深入研究了这种现象，并且针对此提出了可以利用现存的 QoE 评估指标的模型，并且可以实际应用。&#xA;Model 的建立 执行一系列主观评估，由低质量分块的比例和质量导致的感知质量的降低可以基于主观实验结果完成建模。 结合剩下分块的感知质量可以完成单帧质量模型的建模。 最后将一段时间内的所有帧的感知质量池化，就完成了整个的模型。 主观实验的设定 获得 FoV 内帧的感知质量（低质量分块和高质量分块同时存在） 获取整个视频的感知质量（与上面的实验过程相近，只是过程中没有暂停） 获取整个视频的感知质量（没有引入 DQB，所有分块质量相同） 实验结果&#xA;帧质量感知模型 从上面的实验结果可以看出来高质量区域与低质量区域的质量差距 $d_n$ 越大，DQB 效应越显著（符合直觉）。将这部分影响因素看作是感知质量的主要影响因素：&#xA;$$ d_n = Q_{H, n} - Q_{L, n} $$&#xA;$Q_{H, n}$ 和 $Q_{L, n}$ 分别表示第 $n$个 帧高质量分块和低质量分块的感知质量。 这两个质量从主观实验 3 的主观质量获得，在之后的训练过程中可以被客观质量评估的结果所替换。&#xA;为了调查质量差异 $d_n$ 和感知质量降低 $D_n$ 之间的关系，通过使用实验 1 的帧质量分数计算得出第$n$个帧的感知质量降低：</description>
    </item>
    <item>
      <title>Note for Toward Immersive Experience</title>
      <link>http://localhost:1313/posts/papers/note-for-toward-immersive-experience/</link>
      <pubDate>Wed, 09 Mar 2022 11:20:37 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-toward-immersive-experience/</guid>
      <description>Overview Link: Toward Immersive Experience: Evaluation for Interactive Network Services&#xA;Level: IEEE Network 2022&#xA;Keywords: QoE Metrics&#xA;Background Compared with traditional QoE for regular video/audio services, the existing work on IE is still in its infancy. This work aims at providing systematic and comprehensive research on IE for interactive network services, mainly studying the following three fundamental and challenging issues.&#xA;What is the essential difference between IE and traditional QoE? Which categories of factors mainly influence IE?</description>
    </item>
    <item>
      <title>Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)</title>
      <link>http://localhost:1313/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/</link>
      <pubDate>Sun, 27 Feb 2022 10:39:45 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/</guid>
      <description>Bitrate Adaptation Schemes Client-based Recently, most of the proposed bitrate adaptation schemes reside at the client side, according to the specifications in the DASH standard.&#xA;Purposes:&#xA;Minimal rebuffering events when the playback buffer depletes. Minimal startup delay especially in case of live video streaming. A high overall playback bitrate level with respect to network resources. Minimal video quality oscillations, which occur due to frequent switching. Available bandwidth-based The client makes its representation decisions based on the measured available network bandwidth, which is usually calculated as the size of the fetched segment(s) divided by the transfer time.</description>
    </item>
    <item>
      <title>Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)</title>
      <link>http://localhost:1313/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/</link>
      <pubDate>Sat, 26 Feb 2022 11:26:06 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/</guid>
      <description>Paper Overview Link: https://ieeexplore.ieee.org/document/8424813&#xA;Level: IEEE Communications Surveys &amp;amp; Tutorials 2019&#xA;Background Traditional non-HAS IP-based streaming The client receives media that is typically pushed by a media server using connection-oriented protocol such as Real-time Messaging Protocol(RTMP/TCP) or connectionless protocol such as Real-time Transport Protocol(RTP/UDP).&#xA;Real-time Streaming Protocol(RTSP) is a common protocol to control the media servers, which is responsible for setting up a streaming session and keeping the state information during this session, but is not responsible for actual media delivery(task for protocol like RTP).</description>
    </item>
    <item>
      <title>Note for Content Based Vp for Live Streaming (2)</title>
      <link>http://localhost:1313/posts/papers/note-for-content-based-vp-for-live-streaming-2/</link>
      <pubDate>Tue, 25 Jan 2022 11:59:24 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-content-based-vp-for-live-streaming-2/</guid>
      <description>LiveObj LiveDeep方法利用卷积层从视频内容中提取深层特征，不受动态背景的影响。然而在整个推流会话中需要更新一个带有大量权重的巨大的神经网络模型。同时因为没有历史视频和用户的轨迹的数据，模型需要在运行时从随机权重开始训练。而这会导致两个问题：&#xA;模型需要花很长时间从一次预测错误中恢复； 在初始化的阶段预测率成功率很低； 为了解决这两个问题，提出预训练的模型来分析视频内容，对视频的语义进行层次化。&#xA;基于对内容的分析，进一步设计了一个轻量级用户模型，将用户偏好映射到不同的视频内容。&#xA;用户观看行为分析 在直播推流中，不能通过分析其他用户的行为模式来得到特定用户的ROI，因此只能直接从视频内容本身入手。&#xA;通过对视频内容从空间和时间两个维度的分析得出结论：用户的ROI与物体的大小无关，而是很大程度上依赖于物体在视频中的语义，即用户倾向于观看有意义的事物。&#xA;这一结论可以给出推断FoV的直觉：基于检测视频中有意义的物体。&#xA;Methods 首先提出两种直观的通过分析视频内容的视点预测方法，进一步总结这些方法的局限性，并逐步切换到对LiveObj的讨论。&#xA;Basic method Basic方法检测视频中所有的对象并使用其中心作为预测的中心。&#xA;给出每个帧中的 $k$ 个物体， $\vec{O} = [o_1, o_2, o_3, &amp;hellip;, o_k]$ ，其中每个 $o_i(i = 1, &amp;hellip;, k)$ 表示物体的中心坐标： $o_i = &amp;lt;o^{(x)}_i, o^{(y)}_i&amp;gt;$ 。&#xA;最终的预测中心点坐标可以计算出来： $$ C_x = \frac{1}{k} \sum^{k}_{i=1} o^{(x)}_i;\ C_y = \frac{1}{k} \sum^{k}_{i=1} o^{(y)}_i $$&#xA;Over-Cover method 受LiveMotion方法的启发，其创建了不规则的预测FoV来覆盖更多的潜在的区域，Over-Cover的方式预测的FoV会覆盖所有包含物体的区域。&#xA;采用YOLOv3来处理帧并检测物体，接着每个检测到的对象生成与该对象共享相同中心的预测子视图，所有子视图的聚合形成最终的预测视口。&#xA;Summary for intuitive methods Basic方式可能会在多个物体的场景中无法正确选择目标；&#xA;Over-Cover方式覆盖所有可能的目标来满足较高的精度，但会导致更高的带宽使用量；&#xA;Velocity方式能很快的适应用户偏好的变化，但是预测精度在长期预测的情况下会显著下降；&#xA;LiveObj Method Over-Cover方法将所有检测到的目标合并到预测的FoV中而导致冗余问题，而用户一次只能观看其中的几个。&#xA;为了解决这个问题，提出基于用户的反馈选择最吸引人的目标，例如用户当前的FoV来形成预测的FoV。&#xA;基于这种想法而提出LiveObj，一种基于轨迹的 VP 方式，通过从Over-Cover方法的结果中过滤掉用户更小可能性看到的目标来缩小最终的FoV。&#xA;Object Detection：处理视频帧并检测目标； User View Estimation：分析用户反馈并用Velocity的方式估计FoV； Object tracking：追踪用户观看的目标； RL-based modeling：接受估计出的FoV和被追踪的目标，最终更新每个分块的状态（选中或未选中） Object Detection and Tracking Detection：YOLOv3；</description>
    </item>
    <item>
      <title>Content Based VP for Live Streaming (1)</title>
      <link>http://localhost:1313/posts/papers/note-for-content-based-vp-for-live-streaming-1/</link>
      <pubDate>Sat, 22 Jan 2022 18:03:09 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-content-based-vp-for-live-streaming-1/</guid>
      <description>LiveMotion Motivation 基于视频中物体的运动模式来做对应的FoV预测。&#xA;将用户的FoV轨迹与视频内容中运动物体的轨迹结合到一起考虑：&#xA;细节可以参见：note-for-content-motion-viewport-prediction.&#xA;LiveDeep 受限于Motion识别算法，前面提出的LiveMotion只能作用于有清晰并且容易分别的前景背景边界的视频，其健壮性并不能满足全景直播推流的场景。&#xA;Method LiveDeep处理问题的场景为：&#xA;视频内容在线生成； 没有历史用户数据； 预测需要满足实时性的要求； LiveDeep的设计原则：&#xA;online：在线训练在线预测； lifelong：模型在整个视频播放会话中更新； real-time：预测带来的处理延迟不能影响推流延迟； CNN的设计：&#xA;在推流会话的运行时收集并标注训练数据； 以交替迭代的方式进行基于当前视频片段的推理和基于之前视频片段的训练； 子采样少部分的代表帧来运行 VP 以满足实时性的要求； Framework Setup 分包器将视频按照 DASH 标准将视频分段，每个段作为训练模型和预测的单元； 考虑到不同的视频可能具有不同的帧速率，在每个单元中统一采样 $k$ 帧而非以固定的采样率采样； 将每帧图像划分成 $x \times y$ 个分块，最终每个单元中要处理的分块数为 $k \times x \times y$ ； 训练集来自于用户的实时反馈，根据实际FoV和预测FoV之间的差距来标注数据； 用户的轨迹数据来自于用户的实时头部轨迹，采样的帧与CNN模块采样的帧同步； Details 在用于训练的图像还没有被标注之前并不能直接预测，所以 CNN 模块只能以随机的权重给出预测结果。用预测结果与实际结果计算出损失值之后以此来更新 CNN 模型； LSTM 模型只能以用户观看到视频之后的实际轨迹作为训练的输入输入数据； 对下一个片段而言，首先使用两个模块独立做出预测。每个模块的预测都基于子采样之后的 $k$ 个帧； 为了产生对整个片段的预测结果，假设相邻的帧之间共享相同的视野中心（时空局部性）； 取两个模块预测输出的共同的部分作为最终的预测结果； CNN Module 使用经典的 CNN：VGG 作为骨干网络，修改最后一层，只输出两类：感兴趣的和不感兴趣的。&#xA;推理和视口生成 直观上的想法是选择被分类为感兴趣的部分，并且这些所选部分在原始帧中的位置将指示其他帧中可能感兴趣的FoV。&#xA;实际上存在的问题是：几乎所有的部分都被分类为感兴趣的一类，最终结果是整个帧被选择作为预测的结果。&#xA;所以不直接使用 CNN 网络的输出，而是在被分类为感兴趣的部分中进一步细分。通过对输出的分数排序并选择前 $M$ 份比例的输出作为最终的结果，这样通过控制 $M$ 的大小可以调整精度和消耗的带宽。</description>
    </item>
    <item>
      <title>Note for Popularity Aware 360-Degree Video Streaming</title>
      <link>http://localhost:1313/posts/papers/note-for-popularity-aware-360-degree-video-streaming/</link>
      <pubDate>Tue, 18 Jan 2022 16:07:02 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-popularity-aware-360-degree-video-streaming/</guid>
      <description>论文概况 Link：Popularity-Aware 360-Degree Video Streaming&#xA;Level：IEEE INFOCOM 2021&#xA;Keywords：Dynamic tiling, Cross-user division, Heuristic QoE optimization&#xA;Motivation 将视频划分成分块进行编码之后，会降低编码效率，并增大服务端的存储压力。（细节可以参见Optile）&#xA;而分块时根据用户的 ROI 来确定不同的大小，并在客户端预取，这可以节省带宽。&#xA;用户的 ROI 推断利用跨用户的偏好来确定，即所谓的Popularity-Aware。&#xA;Model and Formulation Video Model 视频从时间上被分成固定长度的片段，接着每个片段被从空间上划分成 $C$ 个分块。&#xA;除了常规的分块之外， $M$ 个宏块也被建构出来。&#xA;每个常规分块和宏块都被编码成 $V$ 个不同的码率质量等级并存储在服务端。&#xA;整个推流过程可以看作是一系列连续的下载任务。&#xA;客户端在每次下载任务中的目标是：选择恰当分块（宏块或者常规分块的集合）的恰当质量。&#xA;用 $L$ 表示客户端请求分块时，缓冲区中已经下载但还没有查看的视频的视频长度，为了避免缓冲事件，分块需要在缓冲区被清空即 $L = 0$ 之前被下载完毕。&#xA;QoE Model $$ Q(V_k) = Q_{0}(V_k) - {\omega}_v I_v (V_k) - {\omega}_r I_r (V_k) $$&#xA;$V_k$ 表示下载的第 $k$ 段视频质量； $Q_0$ 表示平均质量； $I_v$ 表示由质量变化导致的质量损害； $I_r$ 表示由缓冲事件导致的质量损害； ${\omega}_v$ 和 ${\omega}_r$ 分别表示质量变化和缓冲的加权因子；</description>
    </item>
    <item>
      <title>Note for srlABR Cross User</title>
      <link>http://localhost:1313/posts/papers/note-for-srlABR-cross-user/</link>
      <pubDate>Sat, 15 Jan 2022 18:46:02 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-srlABR-cross-user/</guid>
      <description>论文概况 Link：Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network&#xA;Level：IEEE Transactions on Broadcasting 2021&#xA;Keywords：Cross-user vp, Sequetial RL ABR&#xA;主要工作 使用跨用户注意力网络CUAN来做 VP； 使用360SRL来做 ABR 将上面两者集成到了推流框架中； VP Motivation 形式化 VP 问题如下：&#xA;给出 $p^{th}$ 用户的 $1-t$ 时间内的历史视点坐标 $L^{p}_{1:t} = \lbrace l^p_1, l^p_2, &amp;hellip;, l^p_t \rbrace$ ，其中 $l^p_t = (x_t, y_t), x_t \in [-180, 180]; y_t \in [-90, 90]$ ；&#xA;同一视频的不同用户视点表示为 $L^{1:M}_{1:t+T}$ ， $M$ 表示其他用户的数量；&#xA;目标是预测未来的 $T$ 个时刻的视点位置 $L^p_i, i = t+1, &amp;hellip;, t+T$ ；</description>
    </item>
    <item>
      <title>Note for 360SRL</title>
      <link>http://localhost:1313/posts/papers/note-for-360srl/</link>
      <pubDate>Thu, 13 Jan 2022 12:08:36 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-360srl/</guid>
      <description>论文概况 Link：360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming&#xA;Level：ICME 2019&#xA;Keywords：ABR、RL、Sequential decision&#xA;创新点 在 MDP 中，将 N 维决策空间内的一次决策转换为 1 维空间内的 N 次级联顺序决策处理来降低复杂度。 问题定义 原始的全景视频被划分成每段固定长度为 $T$ 的片段，&#xA;每个片段包含 $N$ 个分块，并以 $M$ 的码率等级独立编码，&#xA;因此对每个片段，有 $N \times M$ 种可选的编码块。&#xA;为了保证播放时的流畅性，需要确定最优的预取集合：&#xA;${a_0, &amp;hellip;, a_i, &amp;hellip;, a_{N-1}}, i \in \lbrace 0, &amp;hellip;, N-1 \rbrace, a_i \in \lbrace 0, &amp;hellip;, M-1 \rbrace $&#xA;分别用 $q_{i, a_i}$ 和 $w_{i, a_i}$ 表示码率选择为 $a^{th}_i$ 的 $i^{th}$ 分块的质量和相应的分块片段大小。</description>
    </item>
    <item>
      <title>Note for Content Assisted Prediction</title>
      <link>http://localhost:1313/posts/papers/note-for-content-assisted-prediction/</link>
      <pubDate>Thu, 06 Jan 2022 15:17:33 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-content-assisted-prediction/</guid>
      <description>论文概况 Link：Content Assisted Viewport Prediction for Panoramic Video Streaming&#xA;Level：IEEE CVPR 2019 CV4ARVR&#xA;Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion&#xA;主要工作 基于轨迹预测 输入：历史窗口轨迹&#xA;模型：64 个神经元的单层 LSTM，在输入层后面加上一个额外的减法层进行点归一化，以及一个加法层来恢复输出之前的值；用 ADAM 进行优化，MAE 作为损失函数。&#xA;跨用户热图 除了观看者自己的历史 FOV 轨迹之外，其他观看者对同一视频帧的观看方向也有启发性。&#xA;对视频的每一帧，首先收集用户的观看方向（坐标使用原始的来自三个方向的欧拉角表示，而非经纬度）。&#xA;接着将坐标投影到用经纬度表示的 180x360 像素的平面图上，对于图中的每个像素点，可以数出其被看到的次数；并对周围像素应用二维高斯光滑。&#xA;上面的过程可以为视频生成热图：&#xA;视频帧的显著图 鉴于观看相同的全景视频时跨用户行为的共性，进一步假设是内容促使多个观众观看公共区域，因此提取出每个帧的显著图可能会表明用户的 RoI。&#xA;对特定的视频帧，应用经典的特征密集型方法——Ittykoch，它首先根据强度、边缘、颜色和方向将图像分解为多个特征通道，然后将它们组合成识别显著区域。&#xA;除了在静态视频帧上检测显著性之外，进一步进行背景减法来减少不太可能感兴趣的区域：应用基于高斯混合的背景/前景分割算法，高级思想是在连续帧之间临时过滤变化的像素点。&#xA;结合上面这两个过程可以为视频帧提取时间显著图。&#xA;多模态融合 使用包含 3 个 LSTM 分支的深度学习模型来融合上述的几种预测方式的结果。&#xA;基于轨迹的 LSTM（图中绿色分支）从历史窗口 $hw$ 中接受 $n$ 个坐标的输入，接着预测未来窗口 $pw$ 中的 $m$ 个坐标，用 $trj_y_{i}$ 表示；&#xA;基于热图的 LSTM（图中蓝色分支）将每个预测步骤对应的视频帧的热图作为输入，并在 $pw$ 中输出第 2 组 $m$ 个坐标的预测，用 $ht_y_{i}$ 表示：&#xA;对于每个热图，让其通过 3 个卷积层，每个卷积层后面都有一个最大池化层。然后，在此图像特征提取之后，应用展平步骤和 1 个密集层来回归坐标（经纬度表示）。</description>
    </item>
    <item>
      <title>Note for GPAC</title>
      <link>http://localhost:1313/posts/papers/note-for-gpac/</link>
      <pubDate>Thu, 30 Dec 2021 10:23:26 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-gpac/</guid>
      <description>Dash 客户端自适应逻辑 tile priority setup：根据定义的规则对 tile 进行优先级排名。 rate allocation：收集网络吞吐量信息和 tile 码率信息，使用确定的 tile 优先级排名为其分配码率，努力最大化视频质量。 rate adaption：在播放过程中，执行码率自适应算法，基于播放速度、质量切换的次数、缓冲区占用情况等。 tile priority setup Dash 客户端加载带有 SRD 信息的 MPD 文件时，首先确定使用 SRD 描述的 tile 集合。&#xA;确定 tile 之间的编码依赖（尤其是使用 HEVC 编码的 tile 时）&#xA;为每个独立的 tile 向媒体渲染器请求一个视频对象，并向其通知 tile 的 SRD 信息。&#xA;渲染器根据需要的显示大小调整 SRD 信息之后，执行视频对象的最终布局。&#xA;一旦 tile 集合被确定，客户端向每个 tile 分配优先级。（每次码率自适应执行的时候都需要分配 tile 优先级）&#xA;Rate allocation 首先需要估计可用带宽（tile 场景和非 tile 场景的估计不同） 在一个视频段播放过程中，客户端需要去下载多个段（并行-HTTP/2） 带宽可以在下载单个段或多个段的平均指标中估计出来。 一旦带宽估计完成，码率分配将 tile 根据其优先级进行分类。 一开始所有的 tile 都分配成最低的优先级对应的码率，然后从高到低依次增长优先级高的 tile 的码率。 一旦每个 tile 的码率分配完成，将为目标带宽等于所选比特率的每个 tile 调用常规速率自适应算法 </description>
    </item>
    <item>
      <title>Note for MPC</title>
      <link>http://localhost:1313/posts/papers/note-for-mpc/</link>
      <pubDate>Thu, 23 Dec 2021 10:39:32 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-mpc/</guid>
      <description>论文概况 Link：A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP&#xA;Level：ACM SIGCOMM 15&#xA;Keywords：Model Predictive Control，ABR，DASH&#xA;Motivation 关于码率自适应的逻辑，现有的解决方案还没有形成清晰的、一致的意见。不同类型的方案之间优化的出发点并不相同，比如基于速率和基于缓冲区，而且没有广泛考虑各方面的因素并形成折中。&#xA;文章引入了控制论中的方法，将各方面的影响因素形式化为随机优化控制问题，利用模型预测控制 MPC将两种不同出发点的解决方案结合到一起，进而解决其最优化的问题。而仿真结果也证明，如果能运行一个最优化的 MPC 算法，并且预测误差很低，那么 MPC 方案可以优于传统的基于速率和基于缓冲区的策略。&#xA;背景 播放器端为 QoE 需要考虑的问题： 最小化冲缓冲事件发生的次数； 在吞吐量限制下尽可能传输码率较高的视频； 最小化播放器开始播放花费的时间（启动时间）； 保持播放过程平滑，尽可能避免大幅度的码率变化； 这些目标相互冲突的原因： 最小化重缓冲次数和启动时间会导致只选择最低码率的视频； 尽可能选择高码率的视频会导致很多的重缓冲事件； 保持播放过程平滑可能会与最小的重缓冲次数与最大化的平均码率相冲突； 控制论模型 视频推流模型 参数形式化&#xA;将视频建模成连续片段的集合，即：$V = \lbrace 1, 2, &amp;hellip;, K \rbrace$，每个片段长为$L$秒；&#xA;每个片段以不同码率编码，$R$ 作为所有可用码率的集合；&#xA;播放器可以选择以码率$R_k \in R$ 下载第$k$块片段，$d_k(R_k)$ 表示以码率$R_k$编码的视频大小；&#xA;对于恒定码率 CBR 的情况，$d_k(R_k) = L \times R_k$； 对于变化码率 VBR 的情况，$d_k \sim R_k$； 选择的码率越高，用户感知到的质量越高：&#xA;$q(\cdot):R \rightarrow \R_+$ 是一个不减函数，是选择的码率 $R_k$ 到用户感知到的视频质量 $q(R_k)$ 的映射；</description>
    </item>
    <item>
      <title>Note for TBRA</title>
      <link>http://localhost:1313/posts/papers/note-for-tbra/</link>
      <pubDate>Tue, 21 Dec 2021 10:11:23 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-tbra/</guid>
      <description>论文概况 Link：TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming&#xA;Level：ACM MM 21&#xA;Keywords：Adaptive tiling and bitrate，Mobile streaming&#xA;创新点 背景 现有的固定的 tile 划分方式严重依赖 viewport 预测的精度，然而 viewport 预测的准确率往往变化极大，这导致基于 tile 的策略实际效果并不一定能实现其设计初衷：保证 QoE 的同时减少带宽浪费。&#xA;考虑同样的 viewport 预测结果与不同的 tile 划分方式组合的结果：&#xA;从上图可以看到：&#xA;如果采用$6 \times 6$的分块方式，就会浪费 26，32 两个 tile 的带宽，同时 15，16，17 作为本应在实际 viewport 中的 tile 并没有分配最高的优先级去请求。 如果采用$5 \times 5$的分块方式，即使预测的结果与实际的 viewport 有所出入，但是得益于 tile 分块较大，所有应该被请求的 tile 都得到了最高的优先级，用户的 QoE 得到了保证。 另一方面，基于 tile 的方式带来了额外的编解码开销（可以看这一篇论文：note-for-optile），而这样的性能需求对于移动设备而言是不可忽略的。&#xA;创新 除了考虑常见的因素如带宽波动和缓冲区占用之外，提出同时自适应分块策略和码率分配以应对变化的 viewport 预测性能和受限的移动设备的解码能力。&#xA;论文组织 首先使用现实世界的轨迹分析了典型的 viewport 预测算法并确定了其性能的不确定性。 接着讨论了不同的分块策略在 tile 选择和解码效率上的影响。 自适应的分块策略可以适应 viewport 预测的错误，并能保证 tile 选择的质量。 为解码时间建构了分析模型，可以在给定受限的计算资源时用于选择恰当的分块策略和码率。 形式化了优化模型，讨论了自适应算法的细节。 评估证明了方案的优越性。 Motivation 分块策略对 tile 选择的影响 实现 4 种轻量的 viewport 预测算法：线性回归 LR、岭回归 RR、支持向量回归、长短期记忆 LSTM。</description>
    </item>
    <item>
      <title>Note for Content Motion Viewport Prediction</title>
      <link>http://localhost:1313/posts/papers/note-for-content-motion-viewport-prediction/</link>
      <pubDate>Mon, 20 Dec 2021 10:47:18 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-content-motion-viewport-prediction/</guid>
      <description>论文概况 Link：Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking&#xA;Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019&#xA;Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model&#xA;Workflow Tracking：VR motion 追踪算法：应用了高斯混合模型来检测物体的运动。 Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户 viewport 来自动更正潜在的预测错误。 Update：viewport 动态更新算法：动态调整预测的 viewport 大小去覆盖感兴趣的潜在 viewport，同时尽可能保证最低的带宽消耗。 Evaluation：经验用户/视频评估：构建 VR viewport 预测方法原型，使用经验 360°视频和代表性的头部移动数据集评估。 全景直播推流的预备知识 VR 推流直播 相比于传统的 2D 视频推流的特别之处：&#xA;VR 系统是交互式的，viewport 的选择权在客户端； 呈现给用户的最终视图是整个视频的一部分； 用户头部移动的模式 在大量的 360°视频观看过程中，用户主要的头部移动模式有 4 种，使用$i-j\ move$来表示；&#xA;其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。&#xA;$1-1\ move$：单个物体以单一方向移动； $1-n\ move$：单个物体以多个方向移动； $m-n\ move$：多个物体以多个方向移动； $Arbitrary\ move$：用户不跟随任何感兴趣的物体而移动，viewport 切换随机； 现有的直播 VR 推流中的 viewport 预测方法是基于速度的方式，这种方式只对$1-1\ move$这一种模式有效。</description>
    </item>
    <item>
      <title>Note for RnnQoE</title>
      <link>http://localhost:1313/posts/papers/note-for-rnnQoE/</link>
      <pubDate>Thu, 16 Dec 2021 19:53:10 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-rnnQoE/</guid>
      <description>论文概况 Link：QoE-driven Mobile 360 Video Streaming: Predictive View Generation and Dynamic Tile Selection&#xA;Level：ICCC 2021&#xA;Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles&#xA;系统建模与形式化 视频划分 先将视频划分成片段：$\Iota = {1, 2, &amp;hellip;, I}$表示片段数为$I$的片段集合。&#xA;接着将片段在空间上均匀划分成$M \times N$个 tile，FOV 由被用户看到的 tile 所确定。&#xA;使用 ERP 投影，$(\phi_i, \theta_i),\ \phi_i \in (-180\degree, 180\degree], \theta_i \in (-90\degree, 90\degree]$来表示用户在第$i$个片段中的视点坐标。&#xA;播放过程中记录用户头部运动的轨迹，积累的数据可以用于 FOV 预测。&#xA;跨用户之间的 FOV 轨迹可以用于提高预测精度。&#xA;QoE 模型 前提&#xA;视频编解码器预先确定，无法调整每个 tile 的码率。&#xA;实现&#xA;每个 tile 都以不同的码率编码成不同的版本。 每个 tile 都有两种分辨率的版本。 QoE 内容</description>
    </item>
    <item>
      <title>Note for OpTile</title>
      <link>http://localhost:1313/posts/papers/note-for-optile/</link>
      <pubDate>Mon, 13 Dec 2021 16:19:02 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-optile/</guid>
      <description>论文概况 Link：OpTile: Toward Optimal Tiling in 360-degree Video Streaming&#xA;Level：ACM MM 17&#xA;Keyword：Dynamic tile division, Optimize encoding efficiency, Optimize tile size&#xA;背景知识 编码过程概述 对一帧图像中的每一个 block，编码算法在当前帧的已解码部分或由解码器缓冲的临近的帧中搜索类似的 block。&#xA;当编码器在邻近的帧中找到一个 block 与当前 block 紧密匹配时，它会将这个类似的 block 编码进一个动作向量中。&#xA;编码器计算当前 block 和引用 block 之间像素点的差异，通过应用变换（如离散余弦变换），量化变换系数以及对剩余稀疏矩阵系数集应用无损熵编码（如 Huffman 编码）对计算出的差异进行编码。&#xA;对编码过程的影响 基于 tile 的方式会减少可用于拷贝的 block 数量，增大了可供匹配的 tile 之间的距离。 不同的投影方式会影响编码变换输出的系数稀疏性，而这会降低视频编码效率。 投影过程 因为直接对 360 度图像和视频的编码技术还没有成熟，所以 360 度推流系统目前还需要先将 3D 球面投影到 2D 平面上。&#xA;目前应用最广的投影技术主要是 ERP 和 CMP，分别被 YouTube 和 Meta 采用。&#xA;ERP 投影 基于球面上点的左右偏航角$\theta$与上下俯仰角$\phi$将其映射到宽高分别为$W$和$H$的矩形上。</description>
    </item>
    <item>
      <title>Note for RainbowDQN and Multitype Tiles</title>
      <link>http://localhost:1313/posts/papers/note-for-rainbowDQN&#43;tiles/</link>
      <pubDate>Sat, 11 Dec 2021 16:14:15 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-rainbowDQN&#43;tiles/</guid>
      <description>论文概况 Level：IEEE Transaction on multimedia 21&#xA;Keyword：Rainbow-DQN, Multi-type tiles, Full streaming system&#xA;问题形式化 模型 原始视频用网格划分成$N$块 tile，每个 tile 都被转码成$M$个不同的质量等级$q_i$。&#xA;基于传输控制模块得出的结果，播放器请求$t_i$个 tile 的$q_i$质量的版本并将其存储在缓冲区中，对应的缓冲区大小为$l_i$。&#xA;用户 Viewport 的信息用$V$表示，可以确定 FOV 的中心。&#xA;根据$V$可以将 tile 划分成 3 种类型：FOV、OOS、Base。&#xA;FOV 中的 tile 被分配更高的码率；&#xA;OOS 按照与$V$的距离逐步降低质量等级$q_i$；&#xA;Base 总是使用低质量等级$q_{Base}$但使用完整的分辨率。&#xA;传输的 tile 在同步完成之后交给渲染器渲染。&#xA;播放器根据各项指标计算可以评估播放性能：&#xA;$&amp;lt;V, B, Q, F, E&amp;gt;$：viewport 信息$V$，网络带宽$B$，FOV 质量$Q$，重缓冲频率$F$，传输效率$E$。&#xA;传输控制模块用于确定每个 tile 的质量等级$q_i$和缓冲区大小$l_i$。&#xA;传输控制模块优化的最终目标是获取最大的性能： $$ performance = E_{max},\ QoE \in accept\ range $$&#xA;带宽评估 收集每个 tile 的下载日志来评估带宽。&#xA;使用指数加权移动平均算法 EWMA使评估结果光滑，来应对网络波动。&#xA;第$t$次评估结果使用$B_t$表示，用下式计算： $$ B_t = \beta B_{t-1} + (1-\beta)b_t $$ $b_t$是 B 的第$t$次测量值；$\beta$是 EWMA 的加权系数。</description>
    </item>
    <item>
      <title>Note for 360ProbDASH</title>
      <link>http://localhost:1313/posts/papers/note-for-360ProbDASH/</link>
      <pubDate>Thu, 09 Dec 2021 10:20:15 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-360ProbDASH/</guid>
      <description>论文概况 Link: 360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming&#xA;Level: ACM MM 17&#xA;Keyword:&#xA;Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation&#xA;工作范围与目标 应用层-&amp;gt;基于 tile-&amp;gt;viewport 预测的可能性模型+预期质量的最大化&#xA;针对小 buffer 提出了target-buffer-based rate control算法来避免重缓冲事件（避免卡顿）&#xA;提出 viewport 预测的可能性模型计算 tile 被看到的可能性（避免边缘效应）&#xA;形式化 QoE-driven 优化问题：&#xA;在传输率受限的情况下最小化 viewport 内的质量失真和空间质量变化（获取受限状态下最好的视频质量）&#xA;问题建模 形式化参数&#xA;$M*N$个 tile，M 指 tile 序列的序号，N 指不同的码率等级&#xA;$r_{i, j}$指比特率，$d_{i, j}$指失真，$p_{i}$指被看到的可能性（$\sum_{i=1}^{N}p_{i} = 1$）&#xA;$\Phi(X)$指质量失真，$\Psi(X)$指质量变化&#xA;目标&#xA;找到推流段的集合：$X = {x_{i, j}}$，其中${x_{i, j}} = 1$指被第$&amp;lt;i, j&amp;gt;$个 tile 被选中；$x_{i, j} = 0$则是未选中。 $$ \underset{X}{min}\ \Phi(X) + \eta \cdot \Psi(X) \ s.</description>
    </item>
    <item>
      <title>Note for Dante</title>
      <link>http://localhost:1313/posts/papers/note-for-dante/</link>
      <pubDate>Wed, 08 Dec 2021 22:14:15 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note-for-dante/</guid>
      <description>论文概况 Link: https://dl.acm.org/doi/10.1145/3232565.3234686&#xA;Level: SIGCOMM 18&#xA;Keyword: UDP+FOV-aware+FEC&#xA;工作范围 目标 在给定序列的帧中，为每个 tile设定 FEC 冗余，根据其被看到的可能性的加权最小化平均质量降低。&#xA;问题建模 输入 估计的丢包率$p$、发送速率$f$、有$n$个 tile 的$m$个帧($&amp;lt;i, j&amp;gt;$来表示第$i$个帧的第$j$个 tile&#xA;第$&amp;lt;i, j&amp;gt;$个 tile 的大小$v_{i, j}$、第$&amp;lt;i, j&amp;gt;$个 tile 被看到的可能性$\gamma_{i, j}$、&#xA;如果第$&amp;lt;i, j&amp;gt;$ 个 tile 没有被恢复的质量降低率、最大延迟$T$&#xA;输出&#xA;第$&amp;lt;i, j&amp;gt;$个 tile 的 FEC 冗余率$r_{i, j} = \frac{冗余包数量}{原始包数量}$&#xA;最优化问题的形式化 $$ minimize\ \sum_{0&amp;lt;i\le m}\sum_{0&amp;lt;j\le n} \gamma_{i, j}d_{i, j}(p, r_{i, j}) $$&#xA;$$ subject\ \ to\ \ \frac{1}{f}\sum_{0&amp;lt;i\le m}\sum_{0&amp;lt;j\le n}v_{i, j}(1+r_{i, j}) \le T $$</description>
    </item>
    <item>
      <title>沉浸式流媒体传输的实际度量</title>
      <link>http://localhost:1313/posts/papers/note11/</link>
      <pubDate>Mon, 22 Nov 2021 15:21:59 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note11/</guid>
      <description>度量指标 viewport 预测精度。 使用预测的 viewport 坐标和实际用户的 viewport 坐标的大圈距离来量化。 视频质量。 viewport 内部的 tile 质量（1～5）。 tile 在最高质量层之上花费的时间。 根据用户视线的分布而提出的加权质量度量。 度量参数 分块策略 带宽 延迟 viewport 预测 HTTP 版本 持久化的连接数量 </description>
    </item>
    <item>
      <title>沉浸式推流中应用层的优化</title>
      <link>http://localhost:1313/posts/papers/note10/</link>
      <pubDate>Mon, 15 Nov 2021 10:13:18 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note10/</guid>
      <description>背景 大多数的 HAS 方案使用 HTTP/1.1 协议进行请求-回应的事务来取得需要的资源、缓冲取到的视频段并以线性的顺序播放。传统的 HAS 中，只需要 1 个 GET 请求来取得下一个视频的暂时的部分。只要视频段的持续时间比网络内的时延高，这种方法就可行。&#xA;在基于 VR 的 HAS 方案中，播放 1 条视频片段就需要取得多种资源：1 次 GET 请求需要同时请求基础的 tile 层和每个空间视频 tile。使用 4x4 的 tile 方案时，客户端需要发起不少于 17 次 GET 请求。使用 1 s 数量级的分段持续时间，即使是 20 ms 的微小网络延迟也会显着阻碍客户端和服务器之间的整体吞吐量，因此会导致较低的视频质量。&#xA;解决方案 使用多条持久的 TCP 连接 大多数的现代浏览器都支持同时建立并维持多达 6 条 TCP 连接来减少页面加载时间，并行地获取请求的资源。这允许增加整体吞吐量，并部分消除网络延迟引入的空闲 RTT 周期。&#xA;类似地，基于 VR 的 HAS 客户端可以使用多个 TCP 连接并行下载不同的 tile。&#xA;使用 HTTP/2 协议的服务端 push 特性 HTTP/2 协议引入了请求和相应的多路复用、头部压缩和请求优先级的特性，这可以减少页面加载时间。&#xA;服务端直接 push 短视频片段可以减少视频的启动时间和端到端延迟。&#xA;并且，服务端 push 特性可以应用在基于 tile 的 VR 视频推流中，客户端可以向服务器同时请求一条视频片段的所有 tile。</description>
    </item>
    <item>
      <title>沉浸式流媒体面临的挑战和启示</title>
      <link>http://localhost:1313/posts/papers/note9/</link>
      <pubDate>Sun, 14 Nov 2021 19:06:10 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note9/</guid>
      <description>最终的目标 主要的挑战是用户的临场感，这可以通过避免虚拟的线索来创造出接近真实的世界。&#xA;具体的任务 从 360 度视频的采集到显示的过程中，引入了好几种失真。&#xA;应该重点增加新的拼接、投影和分包方式以减少噪音。&#xA;除了捕获和使用 360 度视频来表示真实世界和实际交互内容之外，环境中还包括 3D 对象。&#xA;3D 对象的合并对于真实的视图而言是一个挑战。&#xA;因为在推流会话中，用户的头部移动高度可变，所以固定的 tiling 方案可能会导致非最优的 viewport 质量。&#xA;推流框架中的 tile 数量应该被动态选择，进而提高推流质量。&#xA;自适应的机制应该足够智能来根据环境因素精确地做出适应。&#xA;应该制定基于深度强化学习的策略，来给 360 度视频帧中不同区域的 tile 分配合适的比特率。&#xA;用户在 360 度视频中的自由导航很容易让其感觉忧虑自己错过了什么重要的东西。&#xA;在 360 度视频中导航的时候，需要支持自然的可见角度方向。&#xA;丰富的环境应配备新颖的定向机制，以支持 360 度视频，同时降低认知负荷，以克服此问题。&#xA;真实的导航依赖 viewport 预测机制。&#xA;现代的预测方式应该使用时空图像特性以及用户的位置信息，采用合适的编解码器卷积 LSTM 结构来减少长期预测误差。&#xA;沉浸式的场景随着用户的交互应该发生变化。&#xA;由于用户与场景的交互而产生的新挑战是通过编码和传输透视图创建的。&#xA;因此预测用户的行为来实现对交互内容的高效编码和推流非常关键。&#xA;对 360 度视频的质量获取方法和度量手段需要进一步研究。&#xA;360 度视频中特殊的音效需要引起注意。</description>
    </item>
    <item>
      <title>360度视频的音频处理</title>
      <link>http://localhost:1313/posts/papers/note8/</link>
      <pubDate>Sun, 14 Nov 2021 16:52:20 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note8/</guid>
      <description>背景 空间音频是一种全球状空间环绕的声音方式，采用多个声音通道来模拟现实世界中听到的声音。&#xA;360 度视频由于空间音频而变得更加可靠，因为声音的通道特性使其能够穿越时间和空间。&#xA;360 度视频显示系统在制作空间音频音轨方面的重要性无论怎样强调都不为过&#xA;空间音频的再现技术 物理重建 物理重建技术用于合成尽可能接近所需信号的整个声场。&#xA;立体声配置在最流行的声音再现方法中使用两个扬声器，以促进更多的空间信息（包括距离、方向感、环境和舞台合奏）。而多信道再现方法在声学环境中使用，并在消费类设备中流行。&#xA;多信道再现技术 同样的声压场也通过其他物理重建技术产生，如环境中存在的环境声学和波场合成（WFS）。&#xA;需要麦克风阵列来捕获更多的空间声场。&#xA;因为不能直接用于声场特性分析，麦克风记录的内容需要后期处理。&#xA;麦克风阵列用于语音增强、声源分离、回声消除和声音再现。&#xA;感知重建 心理声学技术用于感知重建，以产生对空间声音特征的感知。&#xA;感知重建技术复制空间音频的自然听觉感受来表示物理音频。&#xA;双耳录制技术 双耳录制技术是立体声录制的一种扩展形式，提供 3D 的听觉体验。&#xA;双耳录制技术通过使用两个 360 度麦克风尽可能的复制人耳，这与使用定向麦克风捕捉声音的常规立体声录音相同。&#xA;假人头部的 360 度麦克风用作人耳的代理，因为它提供了耳朵的精确几何坐标。&#xA;假人头部还产生与人头轮廓相互作用的声波。借助 360 度麦克风，与任何其他记录方法相比，空间立体图像的捕获更精确。&#xA;头部相关传递函数（HRTF） 用于双耳音频的实时技术中，以再现复杂的线索，帮助我们通过过滤音频信号来定位声音。&#xA;多个因素（如耳朵、头部和听力环境）会影响线索，因为在现实中，我们会重新定位自己以定位声音。&#xA;选择合适的录音/重放技术对于使听到的声音与真实场景中的体验相同至关重要。&#xA;环境声学 概述 环境声学也被称为 3D 音频，被用于记录、混成和播放一个中心点周围的 360 度音频。&#xA;区别 环境音频和传统的环绕声技术不同。&#xA;双声道和传统环绕声技术背后的原理是相同的，都是通过将声音信号送到特定的扬声器来创建音频。&#xA;环境音频不受任何特定扬声器的预先限制，因为它在即使音域旋转的情况下，也能创造出平滑的音频。&#xA;传统环绕声的格式只有在声音场景保持静态的情况下才能提供出色的成像效果。&#xA;环境音频提供一个完整的球体，将声音均匀地传播到整个球体。&#xA;格式 环境音频有 6 种格式，分别为：A、B、C、D、E、G。&#xA;用途 一阶环境音频的用途 第一阶的环境音频或 B 格式的环境音频，其麦克风用于使用四面体阵列表示线性 VR。&#xA;此外，这些在四个通道中进行处理，例如提供非定向压力水平的“W”。同时，“X、Y 和 Z”分别促进了从前到后、从侧到侧以及从上到下的方向信息。&#xA;一阶环境音频仅适用于相对较小的场景，因为其有限的空间保真度会影响声音定位。&#xA;高阶环境音频的用途 高阶环境音频通过增加更多的麦克风来增强一阶环境音频的性能效率。&#xA;总结 </description>
    </item>
    <item>
      <title>自适应策略之viewport依赖型</title>
      <link>http://localhost:1313/posts/papers/note7/</link>
      <pubDate>Sun, 14 Nov 2021 13:24:59 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note7/</guid>
      <description>概述 在 360 度视频的推流过程中，根据用户头部的运动自适应地动态选择推流的区域，调整其比特率，以达到节省带宽的目的。&#xA;通常的实现方式 在服务端提供几个自适应集，来在遇到用户头部的突然运动的情况时，能保证 viewport 的平滑转换。&#xA;提出 QER(Quality-focused Regios)的概念使 viewport 内部的视频分辨率高于 viewport 之外的视频分辨率。&#xA;非对称的方式以不同的空间分辨率推流来节省带宽。&#xA;在播放过程中，客户端根据用户的方向来请求不同分辨率版本的视频。 优点是即使客户端对用户的方面做了错误预测，低质量的内容仍然可以在 viewport 中生成。 缺点是在大多数场景下，这种方案需要巨大的存储开销和处理负载。 自适应推流参数 可用带宽和网络吞吐量 Viewport 预测的位置 客户端播放器的可用缓冲 参数计算公式 第 n 个估计的 Viewport：$V^e(n)$&#xA;$V^e(n) = V_{fb}$&#xA;$V_{fb}$是最新报告的 viewport 位置&#xA;第 n 个估计的吞吐量：$T^e(n)$&#xA;$T^e(n) = T_{fb}$&#xA;$T_{fb}$是最新报告的吞吐量&#xA;比特率：$R_{bits}$&#xA;$R_{bits} = (1-\beta)T^e(n)$&#xA;$\beta$是安全边缘&#xA;第 n 个帧的客观度量质量：$VQ(k)$和最终客观度量质量$VQ$&#xA;$VQ=\frac{1}{L}\sum^L_{k=1}VQ(k)$&#xA;$VQ(k) = \sum_{t=1}^{T^n}w_k(k) * D^n_t(V_t, k)$&#xA;$w_k = \frac{A(t,k)}{A_{vp}}$&#xA;$L=总帧数$&#xA;$w_k$表示在第 k 个帧中与 viewport 所重叠的 tile 程度</description>
    </item>
    <item>
      <title>沉浸式流媒体现有标准</title>
      <link>http://localhost:1313/posts/papers/note6/</link>
      <pubDate>Thu, 11 Nov 2021 20:08:03 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note6/</guid>
      <description>OMAF(Omnidirectional Media Format) OMAF是第 1 个国际化的沉浸式媒体格式，描述了对 360 度视频进行编码、演示、消费的方法。&#xA;OMAF与与现有格式兼容，包括编码（例如HEVC），文件格式（例如ISOBMFF），交付信号（例如DASH，MMT）。&#xA;OMAF中还包括编码、投影、分包和 viewport 方向的元数据。&#xA;OMAF+DASH-&amp;gt;MPD OMAF 与 DASH 相结合，再加上一些额外的描述构成了 MPD 文件格式，用于向客户端通知 360 度媒体的属性。&#xA;OMAF 规定了 9 中媒体配置文件，包括 3 种视频配置文件：基于 HEVC 的 viewport 独立型、基于 HEVC 的 viewport 依赖型、基于 AVC 的 viewport 依赖型。&#xA;OMAF 为视角独立型的推流提供了无视 viewport 位置的连续的视频帧质量。&#xA;常规的 HEVC 编码方式和 DASH 推流格式可以用于 viewport 独立型的推流工作。&#xA;但是使用 HEVC/AVC 编码方式的基于 viewport 的自适应操作是 OMAF 的一项技术开发，允许无限制地使用矩形 RWP 来增强 viewport 区域的质量。&#xA;CMAF(Common Media Application Format) 致力于提供跨多个应用和设备之间的统一的编码格式和媒体配置文件。&#xA;CMAF 使请求低延迟的 segment 成为可能。</description>
    </item>
    <item>
      <title>自适应360度视频推流挑战</title>
      <link>http://localhost:1313/posts/papers/note5/</link>
      <pubDate>Thu, 04 Nov 2021 11:01:18 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note5/</guid>
      <description>背景 用户使用头戴设备比使用传统显示器观看 360 度视频内容时的满意度对于扰乱更加敏感。&#xA;沉浸式的体验受到不完美的视角预测和高度动态化的网络状况的消极影响。&#xA;目前主要面临的挑战有以下 4 个：&#xA;Viewport 预测 背景 HMD 的本质特征是快速响应用户头部的移动。当用户改变 viewport 时 HMD 处理交互并检测相关的 viewport 来精确播放器的信息，这样视野就能以正常的可视角度被提供给用户。Viewport 预测在优化的 360 度视频推流中非常必要。配备有位置传感器的可穿戴 HMD 允许客户端更新其视角方向相应的视角场景。&#xA;分类 内容不可知的方式基于历史信息对 viewport 进行预测。 内容感知的方式需要视频内容信息来预测未来的 viewport。 内容不可知方式 分类 平均线性回归 LR 航位推算 DR 聚类 机器学习 ML 编解码器体系结构 现有成果 Qian&amp;rsquo;s work——LR 使用平均线性回归和加权线性回归模型来做 viewport 预测，之后对与预测区域重叠的 tile 进行整体推流。&#xA;当预测后 0.5s、1s、2s 加权线性回归表现更好 Petrangeli&amp;rsquo;s work——LR 将被划分成 tile 的等矩形的帧分成 3 个区域：viewport 区、相邻区、其他区。&#xA;结合观察者头部的移动，将可变比特率分配给可见和不可见区域。&#xA;作者利用最近（100 毫秒）用户观看历史的线性外推来预测未来的注视点。&#xA;Mavlankar and Girod&amp;rsquo;s work——运动向量 使用运动向量比如观察者的平移、倾斜、缩放等方向上的速度和加速度，来执行视角区域预测。&#xA;La Fuente&amp;rsquo;s work——运动向量 考虑了两种预测变体：角速度和角加速度，从用户以前的方向数据来估计未来的头部方向。按照预测结果分配不同的量化参数到每个 tile 上。</description>
    </item>
    <item>
      <title>沉浸式流媒体网络问题的相关解决方案</title>
      <link>http://localhost:1313/posts/papers/note4/</link>
      <pubDate>Sat, 30 Oct 2021 19:20:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note4/</guid>
      <description>概况 现有的沉浸式流媒体应用都对带宽、QoS 和计算需求有着高要求，这主要得益于 5G 网络。&#xA;传统的中心化云计算和云存储体系结构不适于实时的高码率内容分发。&#xA;边缘缓存和移动边缘计算成为了推动沉浸式流媒体发展的关键技术。&#xA;解决方案 360 度视频的边缘协助推流 背景 主要的视频内容可以被传送到边缘节点乃至下游客户端来满足高分辨率等级和严格的低延迟要求。&#xA;在边缘计算中，处理和存储的任务被从核心网转移到边缘节点例如基站、微型数据中心和机顶盒等。&#xA;Hou&amp;rsquo;s work 提出边缘/云服务器渲染可以使计算更加轻便，可以让无线 VR/AR 体验可行并且便携。&#xA;Zhang&amp;rsquo;s work 为 VR 多人游戏提出了一种混合边缘云基础架构，中心云负责更新全局游戏事件，边缘云负责管理视图更新和大规模的帧渲染任务，以此来支持大量的在线联机人数的低延迟游戏。&#xA;进一步陈述了一种服务器选择算法，它基于 QoS 和玩家移动的影响确保所有 VR 玩家之间的公平性。&#xA;Lo&amp;rsquo;s work 考虑了为 360 度视频渲染提供边缘协助的设备的异质性。&#xA;边缘服务器将 HEVC tile 流转码为 viewport 视频流并传输到多个客户端。 最优化算法根据视频质量、HMD 类型和带宽动态决定边缘节点服务哪个客户端。 边缘缓存策略 背景 传统视频的缓冲方案并不能直接应用到 360 度视频上。&#xA;为了在启用边缘缓存的网络中促进 360 度视频的传输，两个传输节点之间的代理缓存被部署来使用户侧的内容可用。&#xA;边缘缓存能从实质上减少重复的传输并且可以使内容服务器更加可扩展。&#xA;Mahzai&amp;rsquo;s work 基于其他用户的观看行为为 360 度视频的流行内容提出了一种缓存策略。&#xA;与最不常用 (LFU) 和最近最少使用 (LRU) 缓存策略相比，在缓存使用方面的性能分别提高了至少 40% 和 17%。 Papaioannou&amp;rsquo;s work 提出了基于 tile 分辨率和需求统计信息的缓存策略，用最少的错误，提高要求 tile 的和缓存 tile 这两种版本的 viewport 覆盖率。</description>
    </item>
    <item>
      <title>自适应360度视频推流方案</title>
      <link>http://localhost:1313/posts/papers/note3/</link>
      <pubDate>Mon, 25 Oct 2021 09:34:10 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note3/</guid>
      <description>概述 360 度视频的推流手段逐渐从视角独立型方案变成基于 tile 的视角依赖型方案。&#xA;相比于常规视频，360 度视频被编码成全向的场景。&#xA;自适应 360 度视频推流利用 DASH 框架来实现比特率的自适应。&#xA;分类 Viewport-Independent Streaming 服务端的任务 使用如 ERP、CMP 等视角独立型的投影方式，360 度视频被投影到一个球体上。 客户端的任务 投影之后的视频直接被传送到客户端，并不需要来自传感器的方向信息。 客户端需要支持对应的投影格式。 客户端像处理传统视频一样完成比特率自适应。 基于网络特征向将要到来的 segment 请求相同投影格式的表示 DASH 插件需要支持相同质量视频的推流。&#xA;应用 视角独立型推流主要用于体育、教育和旅游视频内容。&#xA;优点 简单 缺点 相比于视角依赖型方案视频编码效率低了 30%。 为不可见的区域要求大量带宽和解码资源。 Viewport-Dependent Streaming 终端设备的任务 只接受特定的视频帧内容，包括等于或大于视角角度的可见信息。 监测相关的视角作为用户头部移动的回应，并且向服务端发送信号来精确播放器信息。 为服务端准备和用户方向相关的几个自适应集。 客户端的任务 根据网络情况和估计的视角位置决定获取哪个自适应集。 难点 可视区域的确定 与用户头部移动的同步 质量调整 提供平滑的播放体验 现有的工作 各种投影方式在实际推流中表现如何？ 相比于金字塔格式，为视角依赖型投影方案提出的多分辨率变体有最好的研究和开发(RD)性能。 偏移 CMP 获得了 5.6%到 16.4%的平均可见质量。 提出的框架可以基于已知的网络资源和未来的视角位置适应视角的尺寸和质量。 相比于理想的下载过程，这种二维自适应策略可以花费 20%的额外网络带宽下载超过 57%的额外视频块。 如何在网络资源受限的情况下提供高质量的推流？ 为视角依赖型推流产生不同质量的 segment。 当流中只有有限的 representation 时，利用 Quality Emphasized Regions 策略来缩放特定区域的分辨率。 在拥塞网络条件下，执行了基于网络回应的视角大小和比特率的联合适应，结果显示，相比于传送全部的 360 度场景，动态的视角覆盖率提供了更好的画面质量。 这种基于网络回应的自适应也确保基于整体拥塞变化做调整时能改善视频质量。 为立体视频的背景和前景视图采用不对称质量。 可以分别为背景块和前景块分别节省 15%和 41%的比特率。 DASH 需要做什么？ manifest 中需要包含视角位置信息和投影元数据。 优化获取 random access point 的周期来优化视角分辨率自适应体验。 考虑低延迟和活跃的视角切换。 Tile-based Streaming 传统视频被分成多个块，360 度视频在块的基础上还被分成多个大小相等或者不等的 tile，以此更加精确地调整画面的细节质量。</description>
    </item>
    <item>
      <title>自适应视频推流方案</title>
      <link>http://localhost:1313/posts/papers/note2/</link>
      <pubDate>Thu, 21 Oct 2021 10:50:54 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note2/</guid>
      <description>概述 自适应方案可以在处理不同目标对象时帮助改善推流体验。&#xA;目标主要包括视频质量、功耗、负载均衡等在移动无线网和有线网接入的情形。&#xA;适应性的视频比特率需要同时匹配网络条件和质量目标的需求。&#xA;分类 服务端适应 大多数服务端适应的方案要求客户端发送系统或网络相关信息。&#xA;质量导向的适应方案（Quality-Oriented Adaptive Scheme/QOAS） 向终端用户提供了高知觉质量的媒体内容。&#xA;QOAS 是 C-S 架构，决策在服务器端产生。&#xA;QOAS 基于客户知觉质量的反馈，提供对推流质量等级的调整。&#xA;智能优先级适应方案（intelligent Prioritized Adaptive Scheme/iPAS） 专用于 802.11 网络。&#xA;iPAS 服务器上的基于固有印象的带宽分配模块被用于组合 QoS 相关的参数和视频内容特征来进行内容的优先级分类和带宽份额分配。&#xA;通过区分多媒体流，iPAS 提供可用无线信道的优先级分配。&#xA;设备导向的适应方案（Device-Oriented Adaptive multimedia Scheme/DOAS） 专用于 LTE 网络，建立在 LTE 下行链路调度机制之上。&#xA;DOAS 专门根据设备特性实现适配，尤其为多屏终端用户提供了卓越的 QoE。 客户端适应 基于吞吐量的自适应方案 这类方案基于估计的网络吞吐量从服务端选择视频的比特率。&#xA;HTTP 客户端通过之前的观察记录来估计网络的吞吐量。 通过测量端获取时间（segment fetch time/SFT）来代表发起和收到回复的瞬时 HTTP GET 请求之间的时间段，以此来确定一个推流会话中吞吐量的变化，进而独立地做出适应决策。 在分布式网络中，同时考虑并发和顺序的 SFT。通过比较实际的和理想的 SFT 来选择未来的 segment 的质量等级。 FESTIVE 算法 适用于多个 HAS 客户端共享一个常见的拥塞带宽链路的情形。&#xA;以效率、稳定性、公平性为度量因素的适应性算法。&#xA;探索了一种为分段调度、吞吐量估计和比特率选择而生的健壮的机制。&#xA;包含一个随机调度器来调度下一个视频块的下载。&#xA;多个客户端共享容量为$W$的满带宽链路，每个客户端$x$在$t$时刻播放的视频比特率为$b_x,_t$ ，需要避免以下 3 种问题：</description>
    </item>
    <item>
      <title>360度流媒体面临的挑战、机遇和解决方案</title>
      <link>http://localhost:1313/posts/papers/note1/</link>
      <pubDate>Wed, 20 Oct 2021 20:08:38 +0800</pubDate>
      <guid>http://localhost:1313/posts/papers/note1/</guid>
      <description>360 度流媒体视频框架 视频采集和拼接 使用不同的 360 度视频采集相机可以将视频内容存储为 3D 的球形内容&#xA;使用不同的投影策略实现降维 策略主要分为 2 种：视角独立型和视角依赖型&#xA;视角独立型 整个 3D 的视频内容被按照统一的质量投影到 2D 平面上&#xA;主要包括等距长方形投影和立方贴图投影&#xA;等距长方形投影(ERP) 使用左右偏向和俯仰值将观察者周围的球体展平到二维表面上&#xA;视角范围：左 180 度～右 180 度、上 90 度～下 90 度&#xA;缺点：&#xA;极点处会使用比赤道处更多的像素进行表示，会消耗有限的带宽 由于图像失真导致压缩效率不足 立方贴图投影(CMP) 六面立方体组合用于将球体的像素映射到立方体上的相关像素&#xA;在游戏中被广泛应用&#xA;优点：&#xA;节省空间，相比于等距长方形投影视频体积能减少 25% 缺点：&#xA;只能渲染有限的用户视野 视角依赖型 视角内的内容比之外的内容有更高保真度的表示&#xA;主要包括金字塔投影、截断方形金字塔投影(TSP)和偏移立方贴图投影&#xA;金字塔投影 球体被投影到一个金字塔上，基础部分有最高的质量，大多数的投影区域属于用户的视角方向&#xA;优点：&#xA;节省空间，降低 80%的视频体积 缺点：&#xA;用户以 120 度旋转视角时，视频的质量会像旋转 180 度一样急速下降 截断方形金字塔投影 大体情况和金字塔投影相同，区别在与使用了被截断的方形金字塔&#xA;优点：&#xA;减少了边缘数据，提高了高码率视频的推流性能 缺点：&#xA;使边缘更加锐利 偏移立方贴图投影 与原始的立方贴图投影类似，球体的像素点被投影到立方体的 6 个面上&#xA;优点：&#xA;视角方向的内容会有更高的质量，提供平滑的视频质量变化 缺点：&#xA;存储开销很大 编码视频内容 目前主要的编码方式有 AVC/H.</description>
    </item>
  </channel>
</rss>

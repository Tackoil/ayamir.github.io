<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multimedia on Ayamir&#39;s blog</title>
    <link>http://localhost:1313/tags/multimedia/</link>
    <description>Recent content in Multimedia on Ayamir&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 25 Apr 2024 19:02:12 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/multimedia/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>H264 Encode</title>
      <link>http://localhost:1313/posts/knowledge/h264-encode/</link>
      <pubDate>Tue, 23 Jan 2024 01:05:20 +0800</pubDate>
      <guid>http://localhost:1313/posts/knowledge/h264-encode/</guid>
      <description>编码框架 编码器包含两个方向的码流分支：&#xA;从左到右的前向码流分支为编码过程；&#xA;从右到左的反向码流分支为重建过程。&#xA;前向编码分支 以 16x16 像素的 MB 为单位进行处理，首先从当前输入的视频图像(Frame or Field)中取一个待编码宏块$F_n$，该宏块以帧内或者帧间的模式进行编码，生成一个预测宏块$P$。&#xA;如果是帧内编码，$P$由当前 Slice 里面已经编码、解码、重构并且还没进行去块滤波的宏块 $μF_n&amp;rsquo;$ 使用帧内预测得到。当前宏块 $μF_n&amp;rsquo;$ 减去预测宏块 $P$，得到残差块$D_n$，对残差块 $D_n$ 进行整数变换（一般是 4x4，或者 8x8）、量化后得到一组系数 $X$ ，再对 $X$ 进行重排序和熵编码，就完成了一个宏块的编码过程。对于 P 帧和 B 帧，如果 ME 时候找不到最佳匹配块那也会使用帧内预测编码。&#xA;经过熵编码的码流加上宏块解码所需的一些信息，如预测模式、量化步长、描述宏块运动预测补偿的运动矢量信息等，就组成了该宏块压缩后的码流，Slice 中所有 MB 的码流加上 Slice 头信息就组成了 Slice 的编码码流，再通过 NAL 层进行传输或存储。图像参数集 PPS 和序列参数集 SPS 则由 NAL 单独进行传输。&#xA;后向重建分支 在后向重建分支中，对量化后的宏块系数 $X$ 进行解码从而得到重建宏块，后续宏块进行编码需要从已重建的宏块中寻找参考块。宏块重建过程如下： 宏块系数 $X$ 经过反量化和反变换之后，得到残差宏块 $D_n$ 的近似值 $D_n&amp;rsquo;$ ，预测块 $P$ 加上 $D_n&amp;rsquo;$ 得到未滤波的重构宏块 $μF_n&amp;rsquo;$ ，再做环路滤波来减少块效应，即得到了最终的重构宏块 $F_n&amp;rsquo;$ ，当图像中所有宏块都重建完成后，就形成了重建图像。&#xA;后向重建分支其实就是包含在编码中的完整解码流程，与真正解码器的唯一区别是： 其预测块 P 直接从前向编码分支中得到，而真正的解码器需要利用码流中解出的预测块信息获得预测块 P。当前图像的已重建宏块会被用做帧内预测的参考，而完整的重建图像会被加入参考帧列表，作为未来编码图像帧预测的参考图像。</description>
    </item>
    <item>
      <title>多媒体基础知识</title>
      <link>http://localhost:1313/posts/knowledge/mm-base/</link>
      <pubDate>Mon, 13 Dec 2021 10:03:17 +0800</pubDate>
      <guid>http://localhost:1313/posts/knowledge/mm-base/</guid>
      <description>媒体处理过程 解协议 将流媒体传输方案中要求的数据解析为标准的相应封装格式数据。&#xA;音视频在网络中传播时需要遵守对应的传输方案所要求的格式，如 DASH、HLS 将媒体内容分解成一系列小片段，每个片段有不同的备用码率版本。&#xA;同时应用层的协议会要求在媒体文件本身之外，传输信令数据（如对播放的控制或网络状态的描述）&#xA;解协议的过程会去除信令数据并保留音视频内容，需要的话还要对视频段进行拼接，最终将其还原成传输之前的媒体格式如 MP4，FLV 等。&#xA;封装格式 封装格式如 AVI、MPEG、Real Video 将音频和视频组合打包成一个完整的文件.&#xA;封装格式不会影响视频的画质，影响画质的是视频的编码格式。&#xA;解封装过程就是将打包好的封装格式分离成某种编码的音频压缩文件和视频压缩文件，有时也包含字幕和脚本。&#xA;比如 FLV 或 TS 格式数据，解封装之后得到 H.264-AVC 编码的视频码流和 AAC 编码的音频码流。&#xA;编码 视频的本质是一帧又一帧的图片。&#xA;所以对于一部每秒 30 帧，90 分钟，分辨率为 1920x1080，24 位的真彩色的视频，在压缩之前的大小$S$满足：&#xA;$$ 一帧大小s = 1920 * 1080 * 24 = 49766400(bit) = 6220800(Byte) \ 总帧数n = 90 * 60 * 30 = 162000 \ 总大小S = s * n = 6220800 * 162000 = 1.0077696*10^{12}(Byte) \approx 939(GB) $$</description>
    </item>
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Content-Based Predict on Ayamir&#39;s blog</title>
    <link>https://ayamir.github.io/tags/content-based-predict/</link>
    <description>Recent content in Content-Based Predict on Ayamir&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 25 Apr 2024 19:02:12 +0800</lastBuildDate>
    <atom:link href="https://ayamir.github.io/tags/content-based-predict/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Note for Content Based Vp for Live Streaming (2)</title>
      <link>https://ayamir.github.io/posts/papers/note-for-content-based-vp-for-live-streaming-2/</link>
      <pubDate>Tue, 25 Jan 2022 11:59:24 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-content-based-vp-for-live-streaming-2/</guid>
      <description>LiveObj LiveDeep方法利用卷积层从视频内容中提取深层特征，不受动态背景的影响。然而在整个推流会话中需要更新一个带有大量权重的巨大的神经网络模型。同时因为没有历史视频和用户的轨迹的数据，模型需要在运行时从随机权重开始训练。而这会导致两个问题：&#xA;模型需要花很长时间从一次预测错误中恢复； 在初始化的阶段预测率成功率很低； 为了解决这两个问题，提出预训练的模型来分析视频内容，对视频的语义进行层次化。&#xA;基于对内容的分析，进一步设计了一个轻量级用户模型，将用户偏好映射到不同的视频内容。&#xA;用户观看行为分析 在直播推流中，不能通过分析其他用户的行为模式来得到特定用户的ROI，因此只能直接从视频内容本身入手。&#xA;通过对视频内容从空间和时间两个维度的分析得出结论：用户的ROI与物体的大小无关，而是很大程度上依赖于物体在视频中的语义，即用户倾向于观看有意义的事物。&#xA;这一结论可以给出推断FoV的直觉：基于检测视频中有意义的物体。&#xA;Methods 首先提出两种直观的通过分析视频内容的视点预测方法，进一步总结这些方法的局限性，并逐步切换到对LiveObj的讨论。&#xA;Basic method Basic方法检测视频中所有的对象并使用其中心作为预测的中心。&#xA;给出每个帧中的 $k$ 个物体， $\vec{O} = [o_1, o_2, o_3, &amp;hellip;, o_k]$ ，其中每个 $o_i(i = 1, &amp;hellip;, k)$ 表示物体的中心坐标： $o_i = &amp;lt;o^{(x)}_i, o^{(y)}_i&amp;gt;$ 。&#xA;最终的预测中心点坐标可以计算出来： $$ C_x = \frac{1}{k} \sum^{k}_{i=1} o^{(x)}_i;\ C_y = \frac{1}{k} \sum^{k}_{i=1} o^{(y)}_i $$&#xA;Over-Cover method 受LiveMotion方法的启发，其创建了不规则的预测FoV来覆盖更多的潜在的区域，Over-Cover的方式预测的FoV会覆盖所有包含物体的区域。&#xA;采用YOLOv3来处理帧并检测物体，接着每个检测到的对象生成与该对象共享相同中心的预测子视图，所有子视图的聚合形成最终的预测视口。&#xA;Summary for intuitive methods Basic方式可能会在多个物体的场景中无法正确选择目标；&#xA;Over-Cover方式覆盖所有可能的目标来满足较高的精度，但会导致更高的带宽使用量；&#xA;Velocity方式能很快的适应用户偏好的变化，但是预测精度在长期预测的情况下会显著下降；&#xA;LiveObj Method Over-Cover方法将所有检测到的目标合并到预测的FoV中而导致冗余问题，而用户一次只能观看其中的几个。&#xA;为了解决这个问题，提出基于用户的反馈选择最吸引人的目标，例如用户当前的FoV来形成预测的FoV。&#xA;基于这种想法而提出LiveObj，一种基于轨迹的 VP 方式，通过从Over-Cover方法的结果中过滤掉用户更小可能性看到的目标来缩小最终的FoV。&#xA;Object Detection：处理视频帧并检测目标； User View Estimation：分析用户反馈并用Velocity的方式估计FoV； Object tracking：追踪用户观看的目标； RL-based modeling：接受估计出的FoV和被追踪的目标，最终更新每个分块的状态（选中或未选中） Object Detection and Tracking Detection：YOLOv3；</description>
    </item>
    <item>
      <title>Content Based VP for Live Streaming (1)</title>
      <link>https://ayamir.github.io/posts/papers/note-for-content-based-vp-for-live-streaming-1/</link>
      <pubDate>Sat, 22 Jan 2022 18:03:09 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-content-based-vp-for-live-streaming-1/</guid>
      <description>LiveMotion Motivation 基于视频中物体的运动模式来做对应的FoV预测。&#xA;将用户的FoV轨迹与视频内容中运动物体的轨迹结合到一起考虑：&#xA;细节可以参见：note-for-content-motion-viewport-prediction.&#xA;LiveDeep 受限于Motion识别算法，前面提出的LiveMotion只能作用于有清晰并且容易分别的前景背景边界的视频，其健壮性并不能满足全景直播推流的场景。&#xA;Method LiveDeep处理问题的场景为：&#xA;视频内容在线生成； 没有历史用户数据； 预测需要满足实时性的要求； LiveDeep的设计原则：&#xA;online：在线训练在线预测； lifelong：模型在整个视频播放会话中更新； real-time：预测带来的处理延迟不能影响推流延迟； CNN的设计：&#xA;在推流会话的运行时收集并标注训练数据； 以交替迭代的方式进行基于当前视频片段的推理和基于之前视频片段的训练； 子采样少部分的代表帧来运行 VP 以满足实时性的要求； Framework Setup 分包器将视频按照 DASH 标准将视频分段，每个段作为训练模型和预测的单元； 考虑到不同的视频可能具有不同的帧速率，在每个单元中统一采样 $k$ 帧而非以固定的采样率采样； 将每帧图像划分成 $x \times y$ 个分块，最终每个单元中要处理的分块数为 $k \times x \times y$ ； 训练集来自于用户的实时反馈，根据实际FoV和预测FoV之间的差距来标注数据； 用户的轨迹数据来自于用户的实时头部轨迹，采样的帧与CNN模块采样的帧同步； Details 在用于训练的图像还没有被标注之前并不能直接预测，所以 CNN 模块只能以随机的权重给出预测结果。用预测结果与实际结果计算出损失值之后以此来更新 CNN 模型； LSTM 模型只能以用户观看到视频之后的实际轨迹作为训练的输入输入数据； 对下一个片段而言，首先使用两个模块独立做出预测。每个模块的预测都基于子采样之后的 $k$ 个帧； 为了产生对整个片段的预测结果，假设相邻的帧之间共享相同的视野中心（时空局部性）； 取两个模块预测输出的共同的部分作为最终的预测结果； CNN Module 使用经典的 CNN：VGG 作为骨干网络，修改最后一层，只输出两类：感兴趣的和不感兴趣的。&#xA;推理和视口生成 直观上的想法是选择被分类为感兴趣的部分，并且这些所选部分在原始帧中的位置将指示其他帧中可能感兴趣的FoV。&#xA;实际上存在的问题是：几乎所有的部分都被分类为感兴趣的一类，最终结果是整个帧被选择作为预测的结果。&#xA;所以不直接使用 CNN 网络的输出，而是在被分类为感兴趣的部分中进一步细分。通过对输出的分数排序并选择前 $M$ 份比例的输出作为最终的结果，这样通过控制 $M$ 的大小可以调整精度和消耗的带宽。</description>
    </item>
    <item>
      <title>Note for Content Assisted Prediction</title>
      <link>https://ayamir.github.io/posts/papers/note-for-content-assisted-prediction/</link>
      <pubDate>Thu, 06 Jan 2022 15:17:33 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-content-assisted-prediction/</guid>
      <description>论文概况 Link：Content Assisted Viewport Prediction for Panoramic Video Streaming&#xA;Level：IEEE CVPR 2019 CV4ARVR&#xA;Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion&#xA;主要工作 基于轨迹预测 输入：历史窗口轨迹&#xA;模型：64 个神经元的单层 LSTM，在输入层后面加上一个额外的减法层进行点归一化，以及一个加法层来恢复输出之前的值；用 ADAM 进行优化，MAE 作为损失函数。&#xA;跨用户热图 除了观看者自己的历史 FOV 轨迹之外，其他观看者对同一视频帧的观看方向也有启发性。&#xA;对视频的每一帧，首先收集用户的观看方向（坐标使用原始的来自三个方向的欧拉角表示，而非经纬度）。&#xA;接着将坐标投影到用经纬度表示的 180x360 像素的平面图上，对于图中的每个像素点，可以数出其被看到的次数；并对周围像素应用二维高斯光滑。&#xA;上面的过程可以为视频生成热图：&#xA;视频帧的显著图 鉴于观看相同的全景视频时跨用户行为的共性，进一步假设是内容促使多个观众观看公共区域，因此提取出每个帧的显著图可能会表明用户的 RoI。&#xA;对特定的视频帧，应用经典的特征密集型方法——Ittykoch，它首先根据强度、边缘、颜色和方向将图像分解为多个特征通道，然后将它们组合成识别显著区域。&#xA;除了在静态视频帧上检测显著性之外，进一步进行背景减法来减少不太可能感兴趣的区域：应用基于高斯混合的背景/前景分割算法，高级思想是在连续帧之间临时过滤变化的像素点。&#xA;结合上面这两个过程可以为视频帧提取时间显著图。&#xA;多模态融合 使用包含 3 个 LSTM 分支的深度学习模型来融合上述的几种预测方式的结果。&#xA;基于轨迹的 LSTM（图中绿色分支）从历史窗口 $hw$ 中接受 $n$ 个坐标的输入，接着预测未来窗口 $pw$ 中的 $m$ 个坐标，用 $trj_y_{i}$ 表示；&#xA;基于热图的 LSTM（图中蓝色分支）将每个预测步骤对应的视频帧的热图作为输入，并在 $pw$ 中输出第 2 组 $m$ 个坐标的预测，用 $ht_y_{i}$ 表示：&#xA;对于每个热图，让其通过 3 个卷积层，每个卷积层后面都有一个最大池化层。然后，在此图像特征提取之后，应用展平步骤和 1 个密集层来回归坐标（经纬度表示）。</description>
    </item>
    <item>
      <title>Note for Content Motion Viewport Prediction</title>
      <link>https://ayamir.github.io/posts/papers/note-for-content-motion-viewport-prediction/</link>
      <pubDate>Mon, 20 Dec 2021 10:47:18 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-content-motion-viewport-prediction/</guid>
      <description>论文概况 Link：Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking&#xA;Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019&#xA;Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model&#xA;Workflow Tracking：VR motion 追踪算法：应用了高斯混合模型来检测物体的运动。 Recovery：基于反馈的错误恢复算法：在运行时考虑实际的用户 viewport 来自动更正潜在的预测错误。 Update：viewport 动态更新算法：动态调整预测的 viewport 大小去覆盖感兴趣的潜在 viewport，同时尽可能保证最低的带宽消耗。 Evaluation：经验用户/视频评估：构建 VR viewport 预测方法原型，使用经验 360°视频和代表性的头部移动数据集评估。 全景直播推流的预备知识 VR 推流直播 相比于传统的 2D 视频推流的特别之处：&#xA;VR 系统是交互式的，viewport 的选择权在客户端； 呈现给用户的最终视图是整个视频的一部分； 用户头部移动的模式 在大量的 360°视频观看过程中，用户主要的头部移动模式有 4 种，使用$i-j\ move$来表示；&#xA;其中$i$表示处于运动中的物体数量；$j$表示所有运动物体的运动方向的平均数。&#xA;$1-1\ move$：单个物体以单一方向移动； $1-n\ move$：单个物体以多个方向移动； $m-n\ move$：多个物体以多个方向移动； $Arbitrary\ move$：用户不跟随任何感兴趣的物体而移动，viewport 切换随机； 现有的直播 VR 推流中的 viewport 预测方法是基于速度的方式，这种方式只对$1-1\ move$这一种模式有效。</description>
    </item>
  </channel>
</rss>
